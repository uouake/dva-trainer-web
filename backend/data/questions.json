{
  "meta": {
    "generatedAt": "2026-02-06T21:57:09.891994+00:00",
    "sourceMarkdown": "dva-c02.md",
    "count": 557,
    "skippedBlocks": 0,
    "missingAnswer": 0,
    "missingChoices": 8,
    "missingSourceUrl": 0,
    "frExplanations": {
      "count": 557,
      "conceptKeys": 183,
      "newKeysCreated": 167
    }
  },
  "questions": [
    {
      "id": "dva-c02:topic:1:question:57:3a6b6e5b2b90ee0d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 57,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company needs to harden its container images before the images are in a running state. The company's application uses Amazon Elastic Container Registry (Amazon ECR) as an image registry. Amazon Elastic Kubernetes Service (Amazon EKS) for compute, and an AWS CodePipeline pipeline that orchestrates a continuous integration and continuous delivery (CI/CD) workflow.Dynamic application security testing occurs in the final stage of the pipeline after a new image is deployed to a development namespace in the EKS cluster. A developer needs to place an analysis stage before this deployment to analyze the container image earlier in the CI/CD pipeline.Which solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "A": "Build the container image and run the docker scan command locally. Mitigate any findings before pushing changes to the source code repository. Write a pre-commit hook that enforces the use of this workflow before commit.",
        "B": "Create a new CodePipeline stage that occurs after the container image is built. Configure ECR basic image scanning to scan on image push. Use an AWS Lambda function as the action provider. Configure the Lambda function to check the scan results and to fail the pipeline if there are findings.",
        "C": "Create a new CodePipeline stage that occurs after source code has been retrieved from its repository. Run a security scanner on the latest revision of the source code. Fail the pipeline if there are findings.",
        "D": "Add an action to the deployment stage of the pipeline so that the action occurs before the deployment to the EKS cluster. Configure ECR basic image scanning to scan on image push. Use an AWS Lambda function as the action provider. Configure the Lambda function to check the scan results and to fail the pipeline if there are findings."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/104013-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 27, 2023, 12:59 a.m.",
      "textHash": "3a6b6e5b2b90ee0d",
      "rawFormat": "discussion-md",
      "conceptKey": "ecr_image_scanning",
      "frExplanation": "On veut analyser (scanner) l‚Äôimage de conteneur AVANT qu‚Äôelle soit d√©ploy√©e sur EKS, pour d√©tecter des vuln√©rabilit√©s t√¥t dans le pipeline.\nAmazon ECR est le ‚Äúregistre‚Äù o√π l‚Äôon pousse les images Docker. ECR peut lancer automatiquement un scan de s√©curit√© quand une image est pouss√©e (scan on push).\nLa solution la plus efficace op√©rationnellement est d‚Äôautomatiser ce contr√¥le dans CodePipeline, plut√¥t que de d√©pendre d‚Äôactions manuelles des d√©veloppeurs.\nAvec B, on ajoute un stage juste apr√®s la construction de l‚Äôimage : on pousse l‚Äôimage dans ECR, ECR la scanne automatiquement, puis une fonction AWS Lambda lit les r√©sultats.\nSi le scan trouve des probl√®mes, Lambda fait √©chouer le pipeline : l‚Äôimage ne sera pas d√©ploy√©e ensuite.\nA est fragile (local + hook contournable) et non centralis√©. C scanne le code, pas l‚Äôimage (les vuln√©rabilit√©s viennent souvent des d√©pendances/OS de l‚Äôimage).\nD place le contr√¥le dans le stage de d√©ploiement : c‚Äôest plus tard et moins clair qu‚Äôun stage d√©di√© avant d√©ploiement.\nDonc B r√©pond au besoin (analyse plus t√¥t) avec le moins de gestion manuelle.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:13:acb88ce52a183900",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 13,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company receives food orders from multiple partners. The company has a microservices application that uses Amazon API Gateway APIs with AWS Lambda integration. Each partner sends orders by calling a customized API that is exposed through API Gateway. The API call invokes a shared Lambda function to process the orders.Partners need to be notified after the Lambda function processes the orders. Each partner must receive updates for only the partner's own orders. The company wants to add new partners in the future with the fewest code changes possible.Which solution will meet these requirements in the MOST scalable way?",
      "choices": {
        "A": "Create a different Amazon Simple Notification Service (Amazon SNS) topic for each partner. Configure the Lambda function to publish messages for each partner to the partner's SNS topic.",
        "B": "Create a different Lambda function for each partner. Configure the Lambda function to notify each partner's service endpoint directly.",
        "C": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure the Lambda function to publish messages with specific attributes to the SNS topic. Subscribe each partner to the SNS topic. Apply the appropriate filter policy to the topic subscriptions.",
        "D": "Create one Amazon Simple Notification Service (Amazon SNS) topic. Subscribe all partners to the SNS topic."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103442-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 6:45 a.m.",
      "textHash": "acb88ce52a183900",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, une seule fonction AWS Lambda traite les commandes de tous les partenaires. Il faut ensuite notifier chaque partenaire uniquement pour ses propres commandes, sans multiplier le code.\nAmazon SNS est un service de ‚Äúpublication/abonnement‚Äù : une application publie un message dans un ‚Äútopic‚Äù, et SNS le distribue aux abonn√©s.\nLa solution la plus scalable est d‚Äôavoir un seul topic SNS et de mettre dans chaque message des attributs (ex: partnerId).\nChaque partenaire s‚Äôabonne au m√™me topic, mais avec une ‚Äúfilter policy‚Äù : SNS ne lui envoie que les messages dont partnerId correspond.\nAinsi, ajouter un nouveau partenaire = cr√©er un nouvel abonnement avec son filtre, sans changer la Lambda (ou presque).\nA cr√©e un topic par partenaire : √ßa marche mais √ßa multiplie la configuration et la logique de publication.\nB duplique les fonctions Lambda : beaucoup de maintenance et peu scalable.\nD envoie tout √† tout le monde : ne respecte pas l‚Äôisolation des commandes.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : plusieurs classes commandent des repas. La cantine (le syst√®me) pr√©pare tout, puis doit pr√©venir chaque classe quand SA commande est pr√™te, sans d√©ranger les autres.**\n\nConcept : au lieu d‚Äôavoir un messager par classe, tu as un seul panneau d‚Äôaffichage central. Mais chaque annonce a une √©tiquette (ex: ‚Äú3eB‚Äù, ‚Äú2nde1‚Äù).\nAPI Gateway = le guichet o√π les classes passent commande. Lambda = la cuisine qui pr√©pare.\nSNS = le panneau d‚Äôaffichage qui envoie des notifications.\nR√©ponse C : la cuisine poste sur UN seul panneau, avec une √©tiquette ‚Äúpartenaire X‚Äù.\nChaque partenaire s‚Äôabonne au panneau, mais met un filtre : il ne lit que les messages avec SON √©tiquette.\nDonc chacun re√ßoit seulement ses commandes, et ajouter un nouveau partenaire = juste un nouvel abonnement + filtre, presque pas de code.\nA ferait un panneau par partenaire (√ßa grossit trop). B dupliquerait la cuisine. D enverrait tout √† tout le monde.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:140:8a1914c32abba86a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 140,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that uses AWS CodePipeline to automate its continuous integration and continuous delivery (CI/CD) workflow. The application uses AWS CodeCommit for version control. A developer who was working on one of the tasks did not pull the most recent changes from the main branch. A week later, the developer noticed merge conflicts.How can the developer resolve the merge conflicts in the developer's branch with the LEAST development effort?",
      "choices": {
        "A": "Clone the repository. Create a new branch. Update the branch with the changes.",
        "B": "Create a new branch. Apply the changes from the previous branch.",
        "C": "Use the Commit Visualizer view to compare the commits when a feature was added. Fix the merge conflicts.",
        "D": "Stop the pull from the main branch to the feature branch. Rebase the feature branch from the main branch."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/117574-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Aug. 8, 2023, 6:27 a.m.",
      "textHash": "8a1914c32abba86a",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici, le probl√®me vient du fait que la branche du d√©veloppeur (feature) est en retard par rapport √† la branche principale (main), donc les m√™mes fichiers ont √©t√© modifi√©s diff√©remment et Git signale des conflits.\nAWS CodeCommit est un d√©p√¥t Git dans AWS : les conflits se r√©solvent comme avec Git classique.\nL‚Äôobjectif ‚ÄúLEAST development effort‚Äù signifie : √©viter de recr√©er des branches, recopier du code, ou refaire des commits manuellement.\nUn ‚Äúrebase‚Äù prend les commits de la branche feature et les rejoue au-dessus de la derni√®re version de main : on aligne la branche sur l‚Äôhistorique le plus r√©cent.\nPendant le rebase, Git s‚Äôarr√™te uniquement sur les commits qui posent conflit, ce qui permet de corriger au fur et √† mesure, puis de continuer.\nCela √©vite de faire un gros merge compliqu√© et garde un historique plus propre (lin√©aire), avec moins de manipulations.\nLes options A et B demandent de recr√©er une branche et de d√©placer des changements (plus de travail et plus de risques).\nL‚Äôoption C (visualiser) aide √† comprendre, mais ne r√©sout pas automatiquement et n‚Äôest pas l‚Äôapproche la plus simple.\nDonc la meilleure solution avec le moins d‚Äôeffort est de rebaser la branche feature sur main (D).",
      "domainKey": "development",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:445:379362620a2dc605",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 445,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a script to automate the deployment process for a serverless application. The developer wants to use an existing AWS Serverless Application Model (AWS SAM) template for the application.What should the developer use for the project? (Choose two.)",
      "choices": {
        "A": "Call aws cloudformation package to create the deployment package. Call aws cloudformation deploy to deploy the package afterward.",
        "B": "Call sam package to create the deployment package. Call sam deploy to deploy the package afterward.",
        "C": "Call aws s3 cp to upload the AWS SAM template to Amazon S3. Call aws lambda update-function-code to create the application.",
        "D": "Create a ZIP package locally and call aws serverlessrepo create-applicatiion to create the application.",
        "E": "Create a ZIP package and upload it to Amazon S3. Call aws cloudformation create-stack to create the application."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148536-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 2, 2024, 5:48 a.m.",
      "textHash": "379362620a2dc605",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:9ab5c534",
      "frExplanation": "AWS SAM est un format de template (fichier YAML/JSON) bas√© sur AWS CloudFormation pour d√©crire une appli ¬´ serverless ¬ª (ex: AWS Lambda, API Gateway) comme du code.\nPour d√©ployer, il faut d‚Äôabord pr√©parer les artefacts (code Lambda, d√©pendances) et les mettre dans un endroit accessible, souvent un bucket Amazon S3.\nLa commande ¬´ aws cloudformation package ¬ª fait exactement √ßa : elle compresse/collecte le code, l‚Äôupload sur S3, puis remplace dans le template les chemins locaux par des liens S3.\nEnsuite ¬´ aws cloudformation deploy ¬ª prend ce template ‚Äúpackag√©‚Äù et cr√©e/met √† jour la stack CloudFormation, donc l‚Äôinfrastructure et les fonctions.\nC‚Äôest logique : CloudFormation est le moteur de d√©ploiement derri√®re SAM, donc ces deux commandes suffisent pour automatiser.\nLes autres choix sautent des √©tapes (juste copier le template ne d√©ploie rien) ou utilisent des commandes inadapt√©es (update-function-code ne cr√©e pas toute l‚Äôappli, Serverless Repo n‚Äôest pas le flux standard ici).\nDonc on choisit les deux actions : package puis deploy via CloudFormation.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois organiser un spectacle au coll√®ge avec un plan d√©j√† √©crit (le ‚Äútemplate‚Äù SAM). Tu dois d‚Äôabord pr√©parer tous les accessoires dans des cartons, puis donner le plan + les cartons au proviseur pour qu‚Äôil installe tout dans la salle.**\n\nLe template SAM, c‚Äôest comme le plan du spectacle : il d√©crit quoi installer et o√π. Mais avant d‚Äôinstaller, il faut ‚Äúemballer‚Äù les fichiers (code, images, etc.) dans un format pr√™t √† √™tre transport√© : c‚Äôest l‚Äô√©tape package. Ensuite, il faut lancer l‚Äôinstallation r√©elle √† partir du plan : c‚Äôest l‚Äô√©tape deploy. L‚Äôoption A fait exactement √ßa avec CloudFormation : ‚Äúpackage‚Äù pr√©pare les cartons, puis ‚Äúdeploy‚Äù demande au proviseur (CloudFormation) de monter le spectacle. Les autres options sautent le proviseur, ou essaient d‚Äôinstaller √† la main un seul √©l√©ment (comme juste une sc√®ne), donc ce n‚Äôest pas la bonne m√©thode pour un template SAM existant.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:119:bdb5b8dee8250c16",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 119,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA real-time messaging application uses Amazon API Gateway WebSocket APIs with backend HTTP service. A developer needs to build a feature in the application to identify a client that keeps connecting to and disconnecting from the WebSocket connection. The developer also needs the ability to remove the client.Which combination of changes should the developer make to the application to meet these requirements? (Choose two.)",
      "choices": {
        "A": "Switch to HTTP APIs in the backend service.",
        "B": "Switch to REST APIs in the backend service.",
        "C": "Use the callback URL to disconnect the client from the backend service.",
        "D": "Add code to track the client status in Amazon ElastiCache in the backend service.",
        "E": "Implement $connect and $disconnect routes in the backend service."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107053-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 11:24 p.m.",
      "textHash": "bdb5b8dee8250c16",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:c67cd8f6",
      "frExplanation": "Avec une API WebSocket d‚ÄôAmazon API Gateway, chaque client ouvre une connexion persistante. API Gateway d√©clenche des √©v√©nements sp√©ciaux quand un client se connecte ($connect) et se d√©connecte ($disconnect).\nPour rep√©rer un client qui ‚Äúflappe‚Äù (se connecte/d√©connecte souvent), il faut m√©moriser son √©tat et compter ses connexions/d√©connexions dans un stockage rapide c√¥t√© serveur.\nAmazon ElastiCache (Redis/Memcached) est un cache en m√©moire tr√®s rapide, id√©al pour stocker temporairement : connectionId, dernier √©v√©nement, compteur de d√©connexions, timestamp, etc.\nDonc on ajoute du code backend qui √©crit/relit ces infos dans ElastiCache afin d‚Äôidentifier le client instable (D).\nEnsuite, pour pouvoir ‚Äúretirer‚Äù (d√©connecter) un client, le backend peut appeler l‚ÄôURL de callback de l‚ÄôAPI Gateway (Management API) pour forcer la fermeture de la connexion (C).\nChanger vers HTTP API ou REST API ne r√©sout pas le besoin : ces types d‚ÄôAPI ne donnent pas une connexion WebSocket persistante et n‚Äôaident pas √† suivre/expulser un client.\nLes routes $connect/$disconnect servent √† recevoir les √©v√©nements, mais sans stockage/compteur et sans action de d√©connexion, on ne satisfait pas enti√®rement les deux exigences.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:33:0e4471a2e40a2a32",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 33,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an application that includes an Amazon API Gateway REST API in the us-east-2 Region. The developer wants to use Amazon CloudFront and a custom domain name for the API. The developer has acquired an SSL/TLS certificate for the domain from a third-party provider.How should the developer configure the custom domain for the application?",
      "choices": {
        "A": "Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the API. Create a DNS A record for the custom domain.",
        "B": "Import the SSL/TLS certificate into CloudFront. Create a DNS CNAME record for the custom domain.",
        "C": "Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the API. Create a DNS CNAME record for the custom domain.",
        "D": "Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the us-east-1 Region. Create a DNS CNAME record for the custom domain."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103664-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2023, 1:57 p.m.",
      "textHash": "0e4471a2e40a2a32",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:459e54d7",
      "frExplanation": "Vous voulez mettre CloudFront (un CDN/proxy global) devant une API Gateway REST API et utiliser un nom de domaine personnalis√© en HTTPS.\nPour que CloudFront serve votre domaine en HTTPS, le certificat SSL/TLS doit √™tre g√©r√© par AWS Certificate Manager (ACM).\nPoint cl√© : CloudFront n‚Äôaccepte des certificats ACM que s‚Äôils sont dans la r√©gion us-east-1 (N. Virginia), m√™me si votre API est en us-east-2.\nDonc, vous devez importer le certificat tiers dans ACM en us-east-1.\nEnsuite, vous associez ce certificat au ‚Äúcustom domain‚Äù/distribution CloudFront qui pointe vers API Gateway.\nC√¥t√© DNS, on pointe le nom de domaine vers CloudFront avec un enregistrement CNAME (ou un alias si vous utilisez Route 53).\nLes options qui mettent le certificat dans la r√©gion de l‚ÄôAPI (us-east-2) ne fonctionneront pas pour CloudFront.\nL‚Äôoption qui dit ‚Äúimporter dans CloudFront‚Äù est incorrecte : on ne charge pas directement un certificat dans CloudFront, on le r√©f√©rence via ACM (ou IAM, ancien).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que ton API est une pizzeria √† Columbus (us-east-2). CloudFront, c‚Äôest comme un r√©seau de livreurs partout dans le pays. Ton nom de domaine, c‚Äôest l‚Äôenseigne ‚ÄúMaPizza.com‚Äù. Le certificat SSL/TLS, c‚Äôest la carte officielle qui prouve que l‚Äôenseigne est vraie et pas une copie.**\n\nConcept : CloudFront ne peut livrer ‚ÄúMaPizza.com‚Äù que si la carte officielle est d√©pos√©e au bureau central national, pas dans une petite mairie locale. Sur AWS, ce ‚Äúbureau central‚Äù pour CloudFront est toujours la r√©gion us-east-1. Donc tu importes le certificat dans ACM en us-east-1. Ensuite, pour que les clients tapent ‚ÄúMaPizza.com‚Äù et tombent sur les livreurs CloudFront, tu mets un panneau dans l‚Äôannuaire (DNS) qui pointe vers CloudFront : c‚Äôest un CNAME. Un A record, c‚Äôest plut√¥t une adresse fixe, mais CloudFront fonctionne comme un nom de destination. Donc D est correct : certificat dans ACM us-east-1 + DNS CNAME.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:1:b36be4143367f9f8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 1,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is implementing an application on Amazon EC2 instances. The application needs to process incoming transactions. When the application detects a transaction that is not valid, the application must send a chat message to the company's support team. To send the message, the application needs to retrieve the access token to authenticate by using the chat API.A developer needs to implement a solution to store the access token. The access token must be encrypted at rest and in transit. The access token must also be accessible from other AWS accounts.Which solution will meet these requirements with the LEAST management overhead?",
      "choices": {
        "A": "Use an AWS Systems Manager Parameter Store SecureString parameter that uses an AWS Key Management Service (AWS KMS) AWS managed key to store the access token. Add a resource-based policy to the parameter to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Parameter Store. Retrieve the token from Parameter Store with the decrypt flag enabled. Use the decrypted access token to send the message to the chat.",
        "B": "Encrypt the access token by using an AWS Key Management Service (AWS KMS) customer managed key. Store the access token in an Amazon DynamoDB table. Update the IAM role of the EC2 instances with permissions to access DynamoDB and AWS KMS. Retrieve the token from DynamoDDecrypt the token by using AWS KMS on the EC2 instances. Use the decrypted access token to send the message to the chat.",
        "C": "Use AWS Secrets Manager with an AWS Key Management Service (AWS KMS) customer managed key to store the access token. Add a resource-based policy to the secret to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Secrets Manager. Retrieve the token from Secrets Manager. Use the decrypted access token to send the message to the chat.",
        "D": "Encrypt the access token by using an AWS Key Management Service (AWS KMS) AWS managed key. Store the access token in an Amazon S3 bucket. Add a bucket policy to the S3 bucket to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Amazon S3 and AWS KMS. Retrieve the token from the S3 bucket. Decrypt the token by using AWS KMS on the EC2 instances. Use the decrypted access token to send the massage to the chat."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102778-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 16, 2023, 9:21 a.m.",
      "textHash": "b36be4143367f9f8",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici on doit stocker un ¬´ jeton d‚Äôacc√®s ¬ª (un secret) pour appeler une API de chat. Il doit √™tre chiffr√© quand il est stock√© (au repos) et quand on le lit (en transit), et il doit pouvoir √™tre lu depuis d‚Äôautres comptes AWS, avec le moins d‚Äôadministration possible.\nAWS Secrets Manager est justement fait pour stocker des secrets (mots de passe, tokens) : il chiffre automatiquement le secret au repos avec AWS KMS et l‚Äôacc√®s se fait via HTTPS (chiffr√© en transit).\nSecrets Manager permet aussi de partager l‚Äôacc√®s entre comptes gr√¢ce √† une policy bas√©e sur la ressource (resource-based policy) directement sur le secret.\nSur les instances EC2, on donne simplement des permissions IAM pour lire le secret ; l‚Äôapplication r√©cup√®re la valeur d√©j√† d√©chiffr√©e via l‚ÄôAPI.\nLes autres options demandent plus de gestion : DynamoDB ou S3 ne sont pas des coffres √† secrets et n√©cessitent plus de logique/contr√¥les, et Parameter Store est possible mais moins adapt√© ici et le partage inter-comptes est moins ‚Äúcl√© en main‚Äù.\nDonc la solution la plus simple et avec le moins d‚Äôoverhead est d‚Äôutiliser AWS Secrets Manager avec une policy inter-comptes : r√©ponse C.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:262:c8bb6cd2320f7cdd",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 262,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company wants to test its web application more frequently. The company deploys the application by using a separate AWS CloudFormation stack for each environment. The company deploys the same CloudFormation template to each stack as the application progresses through the development lifecycle.A developer needs to build in notifications for the quality assurance (QA) team. The developer wants the notifications to occur for new deployments in the final preproduction environment.Which solution will meet these requirements?",
      "choices": {
        "A": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the QA team to the Amazon SNS topic. Update the CloudFormation stack options to point to the SNS topic in the pre-production environment.",
        "B": "Create an AWS Lambda function that notifies the QA team. Create an Amazon EventBridge rule to invoke the Lambda function on the default event bus. Filter the events on the CloudFormation service and on the CloudFormation stack Amazon Resource Name (ARN).",
        "C": "Create an Amazon CloudWatch alarm that monitors the metrics from CloudFormation. Filter the metrics on the stack name and the stack status. Configure the CloudWatch alarm to notify the QA team.",
        "D": "Create an AWS Lambda function that notifies the QA team. Configure the event source mapping to receive events from CloudFormation. Specify the filtering values to limit invocations to the desired CloudFormation stack."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134262-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 3:37 a.m.",
      "textHash": "c8bb6cd2320f7cdd",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, on veut pr√©venir l‚Äô√©quipe QA uniquement quand il y a un nouveau d√©ploiement dans l‚Äôenvironnement de pr√©production (le dernier avant la prod). Chaque environnement est un ¬´ stack ¬ª CloudFormation (CloudFormation = service qui cr√©e/met √† jour l‚Äôinfrastructure √† partir d‚Äôun mod√®le). CloudFormation peut envoyer des notifications automatiques lors des op√©rations de stack (CREATE/UPDATE/DELETE) vers un sujet SNS. SNS (Simple Notification Service) est un service de diffusion de messages : on cr√©e un ‚Äútopic‚Äù, puis on y abonne des emails, SMS, ou autres endpoints. En configurant, dans les options du stack de pr√©production, le topic SNS, seules les mises √† jour de CE stack d√©clencheront des notifications, ce qui r√©pond exactement au besoin. Les autres choix sont moins adapt√©s : CloudWatch Alarms ne vise pas les √©v√©nements de d√©ploiement CloudFormation de fa√ßon simple, et CloudFormation n‚Äôa pas de ‚Äúevent source mapping‚Äù direct comme pour SQS/Kinesis; EventBridge pourrait marcher mais c‚Äôest plus complexe que la fonctionnalit√© native de CloudFormation. Donc la solution la plus simple et pr√©vue pour √ßa est SNS + notifications de stack (A).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine une √©cole avec plusieurs salles pour le m√™me cours : une salle ‚Äúbrouillon‚Äù, une salle ‚Äúentra√Ænement‚Äù, et la salle ‚Äúr√©p√©tition g√©n√©rale‚Äù juste avant le spectacle (pr√©production). √Ä chaque fois qu‚Äôun prof finit d‚Äôinstaller la salle (nouvelle mise en place), il peut envoyer un message au club ‚ÄúQA‚Äù (les √©l√®ves qui v√©rifient que tout est pr√™t).**\n\nIci, chaque ‚Äúsalle‚Äù = un environnement, et chaque ‚Äúmise en place de la salle‚Äù = un d√©ploiement via CloudFormation (un plan qui construit l‚Äôenvironnement). On veut pr√©venir QA seulement quand on installe la salle ‚Äúr√©p√©tition g√©n√©rale‚Äù (pr√©production). La r√©ponse A fait exactement √ßa : on cr√©e un ‚Äúgroupe de messages‚Äù (SNS topic) et on y inscrit QA (ils re√ßoivent les notifications). Puis, dans les options du stack CloudFormation de pr√©production, on indique ce groupe : √† chaque nouveau d√©ploiement de CE stack, CloudFormation envoie automatiquement un message. Les autres choix sont comme mettre un surveillant qui √©coute tous les couloirs (EventBridge/Lambda) ou regarder un tableau de stats (CloudWatch) : c‚Äôest plus compliqu√© et pas le bouton simple ‚Äúpr√©viens ce groupe quand ce stack change‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:23:0603e085bb124e7f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 23,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a multi-node Windows legacy application that runs on premises. The application uses a network shared folder as a centralized configuration repository to store configuration files in .xml format. The company is migrating the application to Amazon EC2 instances. As part of the migration to AWS, a developer must identify a solution that provides high availability for the repository.Which solution will meet this requirement MOST cost-effectively?",
      "choices": {
        "A": "Mount an Amazon Elastic Block Store (Amazon EBS) volume onto one of the EC2 instances. Deploy a file system on the EBS volume. Use the host operating system to share a folder. Update the application code to read and write configuration files from the shared folder.",
        "B": "Deploy a micro EC2 instance with an instance store volume. Use the host operating system to share a folder. Update the application code to read and write configuration files from the shared folder.",
        "C": "Create an Amazon S3 bucket to host the repository. Migrate the existing .xml files to the S3 bucket. Update the application code to use the AWS SDK to read and write configuration files from Amazon S3.",
        "D": "Create an Amazon S3 bucket to host the repository. Migrate the existing .xml files to the S3 bucket. Mount the S3 bucket to the EC2 instances as a local volume. Update the application code to read and write configuration files from the disk."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102900-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 17, 2023, 9:47 a.m.",
      "textHash": "0603e085bb124e7f",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:4c7c35d7",
      "frExplanation": "On-premises, l‚Äôapplication utilise un dossier r√©seau partag√© comme ‚Äúr√©pertoire central‚Äù de fichiers .xml. Sur AWS, il faut un stockage central accessible par plusieurs instances EC2 et surtout hautement disponible.\nAmazon S3 est un service de stockage d‚Äôobjets g√©r√© : il r√©plique les donn√©es automatiquement sur plusieurs serveurs/zones, donc il reste disponible m√™me si une machine tombe en panne, et il co√ªte peu pour des fichiers de configuration.\nLa r√©ponse C est la plus logique : on met les .xml dans un bucket S3 et l‚Äôapplication lit/√©crit via l‚ÄôAWS SDK (API officielle), ce qui fonctionne de fa√ßon fiable et scalable.\nA n‚Äôest pas hautement disponible : un volume EBS attach√© √† une seule instance d√©pend de cette instance (et le partage Windows sur une seule machine cr√©e un point de panne).\nB est encore pire : l‚Äôinstance store est √©ph√©m√®re (perte des donn√©es si l‚Äôinstance s‚Äôarr√™te) et pas HA.\nD est un pi√®ge : ‚Äúmonter S3 comme un disque‚Äù n‚Äôest pas un usage natif fiable (s√©mantique diff√©rente d‚Äôun syst√®me de fichiers, risques de coh√©rence/performances) et ce n‚Äôest pas la recommandation.\nDonc, pour HA + co√ªt + simplicit√© op√©rationnelle, S3 + SDK (C) est le meilleur choix.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une classe o√π plusieurs groupes doivent suivre les m√™mes consignes √©crites (des fiches .xml). Si les fiches sont dans le sac d‚Äôun seul √©l√®ve, si cet √©l√®ve est absent, plus personne n‚Äôa les consignes.**\n\nConcept : il faut un ‚Äúendroit central‚Äù pour les fichiers, qui reste dispo m√™me si une machine tombe. Comme un casier/placard de la salle (toujours accessible), pas le sac d‚Äôun √©l√®ve.\nA : mettre le dossier sur le ‚Äúdisque‚Äù d‚Äôun seul PC (EBS sur une EC2) = si ce PC a un souci, le dossier partag√© peut devenir indisponible.\nB : pareil, mais encore pire : le ‚Äúdisque‚Äù est temporaire (instance store), comme des feuilles sur une table qu‚Äôon peut jeter.\nC (bon) : S3 = un grand casier de l‚Äô√©cole tr√®s fiable et partag√©. Tu mets les .xml dedans, et chaque groupe (EC2) va les lire/√©crire directement avec la ‚Äúm√©thode officielle‚Äù (SDK), donc pas besoin d‚Äôun PC central.\nD : ‚Äúmonter S3 comme un disque‚Äù = bricolage : √ßa peut √™tre moins fiable/plus compliqu√©, et pas l‚Äôusage pr√©vu.\nDonc C est le plus dispo (haute disponibilit√©) et souvent le moins cher pour stocker des fichiers.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:21:7c651afe2e8cda7a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 21,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer maintains an Amazon API Gateway REST API. Customers use the API through a frontend UI and Amazon Cognito authentication.The developer has a new version of the API that contains new endpoints and backward-incompatible interface changes. The developer needs to provide beta access to other developers on the team without affecting customers.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Define a development stage on the API Gateway API. Instruct the other developers to point the endpoints to the development stage.",
        "B": "Define a new API Gateway API that points to the new API application code. Instruct the other developers to point the endpoints to the new API.",
        "C": "Implement a query parameter in the API application code that determines which code version to call.",
        "D": "Specify new API Gateway endpoints for the API endpoints that the developer wants to add."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102899-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 17, 2023, 9:46 a.m.",
      "textHash": "7c651afe2e8cda7a",
      "rawFormat": "discussion-md",
      "conceptKey": "api_gw_cognito_authorizer",
      "frExplanation": "API Gateway est le service AWS qui expose votre API via des URL. Une ¬´ stage ¬ª (√©tape) est comme un environnement s√©par√© (ex: prod, dev) pour la m√™me API, avec sa propre URL.\nIci, on veut tester une nouvelle version avec des changements incompatibles sans impacter les clients (production).\nCr√©er une stage ¬´ development ¬ª permet de d√©ployer la nouvelle version dans un espace isol√©, tout en gardant la stage actuelle pour les clients.\nLes d√©veloppeurs internes utilisent simplement l‚ÄôURL de la stage dev (ex: /dev) dans le frontend ou leurs outils, tandis que les clients restent sur /prod.\nCela demande peu d‚Äôop√©rations: pas besoin de recr√©er une nouvelle API compl√®te, ni de modifier le code pour router selon un param√®tre.\nCognito (authentification) peut rester identique ou √™tre configur√© par stage si n√©cessaire, sans perturber les utilisateurs existants.\nDonc la meilleure option avec le moins de charge op√©rationnelle est de cr√©er une nouvelle stage et de rediriger uniquement l‚Äô√©quipe vers cette stage.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:17:3d6df238712d3580",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 17,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nFor a deployment using AWS Code Deploy, what is the run order of the hooks for in-place deployments?",
      "choices": {
        "A": "BeforeInstall -> ApplicationStop -> ApplicationStart -> AfterInstall",
        "B": "ApplicationStop -> BeforeInstall -> AfterInstall -> ApplicationStart",
        "C": "BeforeInstall -> ApplicationStop -> ValidateService -> ApplicationStart",
        "D": "ApplicationStop -> BeforeInstall -> ValidateService -> ApplicationStart"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102743-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 15, 2023, 11:35 p.m.",
      "textHash": "3d6df238712d3580",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:f715b9bd",
      "frExplanation": "AWS CodeDeploy est un service qui automatise le d√©ploiement d‚Äôune nouvelle version de votre application sur des serveurs (EC2 ou on-premises).\nUn d√©ploiement ¬´ in-place ¬ª signifie qu‚Äôon met √† jour le m√™me serveur : on doit donc d‚Äôabord arr√™ter l‚Äôancienne version pour √©viter les conflits.\nLes ¬´ hooks ¬ª sont des √©tapes (scripts) ex√©cut√©es dans un ordre pr√©cis pendant le d√©ploiement.\n1) ApplicationStop : on arr√™te l‚Äôapplication en cours (logique : on lib√®re les fichiers/ports).\n2) BeforeInstall : on pr√©pare la machine (nettoyage, d√©pendances, dossiers).\n3) AfterInstall : on copie/installe la nouvelle version et on fait les r√©glages apr√®s installation.\n4) ApplicationStart : on red√©marre l‚Äôapplication avec la nouvelle version.\nDonc l‚Äôordre correct est B : ApplicationStop -> BeforeInstall -> AfterInstall -> ApplicationStart.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois changer l‚Äôaffiche d‚Äôun club dans le couloir de ton √©cole, mais tu le fais sur le m√™me panneau (tu ne mets pas un nouveau panneau √† c√¥t√©).**\n\nConcept : un ‚Äúin-place deployment‚Äù, c‚Äôest mettre √† jour une appli sur la m√™me machine, comme remplacer l‚Äôaffiche sur le m√™me panneau.\n√âtape 1 : ApplicationStop = tu enl√®ves l‚Äôancienne affiche pour √©viter que les gens lisent l‚Äôinfo p√©rim√©e.\n√âtape 2 : BeforeInstall = tu pr√©pares le panneau (nettoyer, enlever les punaises, pr√©parer la colle).\n√âtape 3 : AfterInstall = tu colles la nouvelle affiche et tu v√©rifies qu‚Äôelle est bien fix√©e.\n√âtape 4 : ApplicationStart = tu rouvres le panneau au public, les √©l√®ves peuvent lire la nouvelle info.\nDonc l‚Äôordre logique est : arr√™ter l‚Äôancien, pr√©parer, installer, red√©marrer.\nC‚Äôest exactement le choix B : ApplicationStop -> BeforeInstall -> AfterInstall -> ApplicationStart.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:19:e90dc57d4acdbfbf",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 19,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is offering APIs as a service over the internet to provide unauthenticated read access to statistical information that is updated daily. The company uses Amazon API Gateway and AWS Lambda to develop the APIs. The service has become popular, and the company wants to enhance the responsiveness of the APIs.Which action can help the company achieve this goal?",
      "choices": {
        "A": "Enable API caching in API Gateway.",
        "B": "Configure API Gateway to use an interface VPC endpoint.",
        "C": "Enable cross-origin resource sharing (CORS) for the APIs.",
        "D": "Configure usage plans and API keys in API Gateway."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103467-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 12:40 p.m.",
      "textHash": "e90dc57d4acdbfbf",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:82617d49",
      "frExplanation": "Ici, les API sont publiques (pas d‚Äôauthentification) et renvoient des statistiques mises √† jour une fois par jour. Beaucoup d‚Äôutilisateurs appellent donc souvent les m√™mes donn√©es.\nAmazon API Gateway est la ‚Äúporte d‚Äôentr√©e‚Äù HTTP, et AWS Lambda ex√©cute le code qui calcule/retourne la r√©ponse.\nPour am√©liorer la r√©activit√©, le plus efficace est d‚Äô√©viter d‚Äôex√©cuter Lambda √† chaque requ√™te identique.\nLe ‚Äúcaching‚Äù d‚ÄôAPI Gateway permet de stocker temporairement la r√©ponse (en m√©moire) et de la renvoyer directement aux prochains appels.\nR√©sultat : moins de latence, moins d‚Äôappels √† Lambda, et meilleure performance quand le trafic augmente.\nB (VPC endpoint) sert surtout √† acc√©der √† une API priv√©e depuis un VPC, pas √† acc√©l√©rer une API publique internet.\nC (CORS) r√®gle un probl√®me de navigateur (autoriser un site √† appeler l‚ÄôAPI), pas la vitesse.\nD (usage plans / API keys) sert √† contr√¥ler et limiter l‚Äôusage, pas √† rendre les r√©ponses plus rapides.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le CDI du lyc√©e : chaque jour, la prof doc affiche une feuille avec les stats du jour (m√©t√©o, r√©sultats, infos). Beaucoup d‚Äô√©l√®ves viennent juste la lire, sans s‚Äôinscrire.**\n\nConcept : pour r√©pondre plus vite, le CDI peut garder une copie de la feuille du jour sur une table √† l‚Äôentr√©e. Comme √ßa, pas besoin d‚Äôaller la chercher au fond √† chaque fois.\nDans AWS, l‚ÄôAPI (API Gateway) est la porte du CDI, et Lambda est la prof doc qui va chercher/calculer la feuille.\nComme les infos sont mises √† jour seulement 1 fois par jour et que tout le monde lit la m√™me chose, on peut ‚Äúmettre en cache‚Äù la r√©ponse : une copie pr√™te √† servir.\nA (activer le cache dans API Gateway) = poser la copie √† l‚Äôentr√©e : r√©ponses beaucoup plus rapides, moins de travail pour Lambda.\nB = changer le chemin interne du CDI (utile pour acc√®s priv√©), pas pour acc√©l√©rer des lecteurs sur internet.\nC = autoriser certains sites √† lire l‚ÄôAPI depuis un navigateur, √ßa ne rend pas plus rapide.\nD = mettre des tickets/abonnements (API keys), mais ici c‚Äôest sans authentification, et √ßa n‚Äôacc√©l√®re pas.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:5:f300e2f74c92d6c2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 5,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has created an AWS Lambda function that is written in Python. The Lambda function reads data from objects in Amazon S3 and writes data to an Amazon DynamoDB table. The function is successfully invoked from an S3 event notification when an object is created. However, the function fails when it attempts to write to the DynamoDB table.What is the MOST likely cause of this issue?",
      "choices": {
        "A": "The Lambda function's concurrency limit has been exceeded.",
        "B": "DynamoDB table requires a global secondary index (GSI) to support writes.",
        "C": "The Lambda function does not have IAM permissions to write to DynamoDB.",
        "D": "The DynamoDB table is not running in the same Availability Zone as the Lambda function."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102783-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 16, 2023, 9:58 a.m.",
      "textHash": "f300e2f74c92d6c2",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "AWS Lambda est un service qui ex√©cute votre code sans g√©rer de serveur. Ici, la fonction est bien d√©clench√©e par S3 (stockage d‚Äôobjets), donc l‚Äô√©v√©nement et l‚Äôex√©cution de base fonctionnent.\nQuand Lambda veut √©crire dans DynamoDB (base de donn√©es NoSQL), AWS v√©rifie les autorisations IAM (Identity and Access Management), c‚Äôest-√†-dire les droits attach√©s au ‚Äúr√¥le‚Äù de la fonction.\nSi le r√¥le IAM n‚Äôa pas l‚Äôaction dynamodb:PutItem (ou √©quivalent) sur la table, l‚Äô√©criture √©choue m√™me si la lecture S3 marche.\nC‚Äôest la cause la plus fr√©quente : permissions manquantes ou trop restrictives sur DynamoDB.\nA n‚Äôexplique pas un √©chec uniquement au moment de l‚Äô√©criture (la fonction s‚Äôex√©cute d√©j√†). B est faux : une GSI n‚Äôest pas requise pour √©crire. D est faux : DynamoDB n‚Äôest pas li√© √† une AZ comme √ßa et Lambda peut y acc√©der r√©gionalement.\nDonc la cause la plus probable est l‚Äôabsence de permissions IAM pour √©crire dans DynamoDB.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un √©l√®ve qui re√ßoit automatiquement un devoir quand quelqu‚Äôun le d√©pose dans la bo√Æte de classe, puis doit aller √©crire la note dans le carnet officiel du prof.**\n\nConcept : AWS Lambda, c‚Äôest comme un √©l√®ve-robot qui agit tout seul quand un fichier arrive. S3, c‚Äôest la bo√Æte o√π on d√©pose les devoirs. DynamoDB, c‚Äôest le carnet officiel o√π on enregistre les infos.\nIci, l‚Äô√©l√®ve-robot arrive bien √† lire le devoir (√ßa marche quand le fichier est cr√©√©). Donc l‚Äôalerte et l‚Äôacc√®s √† la bo√Æte fonctionnent.\nMais il √©choue au moment d‚Äô√©crire dans le carnet : la cause la plus probable, c‚Äôest qu‚Äôil n‚Äôa pas l‚Äôautorisation du prof.\nDans AWS, ces ‚Äúautorisations‚Äù s‚Äôappellent IAM permissions : si elles ne disent pas ‚Äútu peux √©crire dans DynamoDB‚Äù, l‚Äô√©criture est refus√©e.\nA : trop d‚Äô√©l√®ves en m√™me temps ferait √©chouer au d√©marrage, pas seulement √† l‚Äô√©criture. B : pas besoin d‚Äôun index sp√©cial juste pour √©crire. D : on n‚Äôa pas besoin d‚Äô√™tre dans la m√™me ‚Äúsalle‚Äù pour √©crire.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:539:24b30d7c9515e740",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 539,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses two AWS accounts: production and development. The company stores data in an Amazon S3 bucket that is in the production account. The data is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company plans to copy the data to another S3 bucket that is in the development account.A developer needs to use a KMS key to encrypt the data in the S3 bucket that is in the development account. The KMS key in the development account must be accessible from the production account,Which solution will meet these requirements?",
      "choices": {
        "A": "Replicate the customer managed KMS key from the production account to the development account. Specify the production account in the key policy.",
        "B": "Create a new customer managed KMS key in the development account. Specify the production account in the key policy.",
        "C": "Create a new AWS managed KMS key for Amazon S3 in the development account. Specify the production account in the key policy.",
        "D": "Replicate the default AWS managed KMS key for Amazon S3 from the production account to the development account. Specify the production account in the key policy."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157497-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 4, 2025, 9:55 a.m.",
      "textHash": "24b30d7c9515e740",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, les donn√©es vont √™tre copi√©es vers un bucket S3 dans le compte ¬´ development ¬ª et doivent y √™tre chiffr√©es avec une cl√© KMS utilisable aussi depuis le compte ¬´ production ¬ª.\nAWS KMS est le service qui g√®re les cl√©s de chiffrement; une ¬´ customer managed key ¬ª est une cl√© que vous cr√©ez et contr√¥lez (politiques, permissions).\nPour qu‚Äôun autre compte AWS puisse utiliser une cl√© KMS, on doit l‚Äôautoriser explicitement dans la key policy (politique de la cl√©) et/ou via des permissions IAM.\nOn ne peut pas ¬´ r√©pliquer ¬ª une cl√© KMS existante d‚Äôun compte √† un autre comme un objet S3 : chaque cl√© appartient √† un compte et une r√©gion.\nLes cl√©s ¬´ AWS managed ¬ª (g√©r√©es par AWS) ont des politiques contr√¥l√©es par AWS : vous ne pouvez pas y ajouter facilement un autre compte dans la key policy.\nDonc la bonne approche est de cr√©er une nouvelle cl√© KMS g√©r√©e par le client dans le compte development, puis d‚Äôajouter le compte production dans la key policy pour lui permettre de chiffrer/d√©chiffrer selon le besoin.\nC‚Äôest exactement le choix B.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine deux b√¢timents d‚Äô√©cole : le lyc√©e ‚ÄúProduction‚Äù et le lyc√©e ‚ÄúD√©veloppement‚Äù. Chaque lyc√©e a sa propre salle des coffres et ses propres cl√©s.**\n\nLe ‚Äúbucket S3‚Äù, c‚Äôest un casier o√π on stocke des dossiers. Le ‚Äúchiffrement KMS‚Äù, c‚Äôest mettre les dossiers dans un coffre ferm√© √† cl√©. On veut copier des dossiers du casier du lyc√©e Production vers un casier au lyc√©e D√©veloppement, et les refermer avec une cl√© du lyc√©e D√©veloppement. Mais la cl√© du lyc√©e D√©veloppement doit pouvoir √™tre utilis√©e aussi par des personnes du lyc√©e Production. Donc on fabrique une nouvelle cl√© dans le lyc√©e D√©veloppement, puis on √©crit une r√®gle (key policy) qui dit : ‚Äúles gens du lyc√©e Production ont le droit d‚Äôutiliser cette cl√©‚Äù. On ne peut pas juste ‚Äúcopier‚Äù une cl√© comme un objet, et les cl√©s g√©r√©es automatiquement par l‚Äô√©cole (AWS managed) ne se configurent pas facilement pour donner des droits sp√©ciaux. Donc la bonne r√©ponse est B.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:302:dc69e48abde4d6be",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 302,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs a serverless application on AWS. The application includes an AWS Lambda function. The Lambda function processes data and stores the data in an Amazon RDS for PostgreSQL database. A developer created a user credentials in the database for the application.The developer needs to use AWS Secrets Manager to manage the user credentials. The password must to be rotated on a regular basis. The solution needs to ensure that there is high availability and no downtime for the application during secret rotation.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Configure managed rotation with the single user rotation strategy.",
        "B": "Configure managed rotation with the alternating users rotation strategy.",
        "C": "Configure automatic rotation with the single user rotation strategy.",
        "D": "Configure automatic rotation with the alternating users rotation strategy."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134124-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 18, 2024, 10:39 a.m.",
      "textHash": "dc69e48abde4d6be",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "AWS Secrets Manager sert √† stocker et renouveler automatiquement des secrets (ex: identifiant/mot de passe) sans les mettre en dur dans le code.\nIci, la fonction AWS Lambda se connecte √† une base Amazon RDS PostgreSQL avec un utilisateur et un mot de passe.\nOn veut une rotation r√©guli√®re du mot de passe ET surtout aucune coupure pendant la rotation (haute disponibilit√©).\nLa strat√©gie ¬´ single user ¬ª change le mot de passe du m√™me utilisateur : pendant le changement, l‚Äôapplication peut essayer l‚Äôancien mot de passe et √©chouer ‚Üí risque de downtime.\nLa strat√©gie ¬´ alternating users ¬ª (utilisateurs altern√©s) utilise deux comptes (ex: app_user_a et app_user_b) : pendant qu‚Äôun est modifi√©, l‚Äôautre reste valide.\nSecrets Manager bascule ensuite le secret vers l‚Äôutilisateur mis √† jour, ce qui √©vite les erreurs de connexion.\n¬´ Automatic rotation ¬ª signifie que Secrets Manager ex√©cute la rotation selon un planning (sans intervention manuelle).\nDonc il faut activer la rotation automatique avec la strat√©gie d‚Äôutilisateurs altern√©s pour garantir disponibilit√© et z√©ro interruption.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:294:2025ca472e1f84da",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 294,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company's developer has deployed an application in AWS by using AWS CloudFormation. The CloudFormation stack includes parameters in AWS Systems Manager Parameter Store that the application uses as configuration settings. The application can modify the parameter values.When the developer updated the stack to create additional resources with tags, the developer noted that the parameter values were reset and that the values ignored the latest changes made by the application. The developer needs to change the way the company deploys the CloudFormation stack. The developer also needs to avoid resetting the parameter values outside the stack.Which solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Modify the CloudFormation stack to set the deletion policy to Retain for the Parameter Store parameters.",
        "B": "Create an Amazon DynamoDB table as a resource in the CloudFormation stack to hold configuration data for the application. Migrate the parameters that the application is modifying from Parameter Store to the DynamoDB table.",
        "C": "Create an Amazon RDS DB instance as a resource in the CloudFormation stack. Create a table in the database for parameter configuration. Migrate the parameters that the application is modifying from Parameter Store to the configuration table.",
        "D": "Modify the CloudFormation stack policy to deny updates on Parameter Store parameters."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134293-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:12 a.m.",
      "textHash": "2025ca472e1f84da",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "CloudFormation sert √† d√©crire et cr√©er des ressources AWS ‚Äúcomme du code‚Äù. √Ä chaque mise √† jour du stack, CloudFormation essaie de remettre les ressources dans l‚Äô√©tat d√©fini dans le template.\nIci, les param√®tres sont stock√©s dans AWS Systems Manager Parameter Store (un coffre de valeurs de configuration). L‚Äôapplication modifie ces valeurs, mais CloudFormation les ‚Äúr√©applique‚Äù lors d‚Äôune mise √† jour, ce qui √©crase les changements.\nLa solution la plus simple est d‚Äôemp√™cher CloudFormation de supprimer/recr√©er ces param√®tres lors des changements du stack.\nAvec une DeletionPolicy = Retain sur les param√®tres, CloudFormation conserve la ressource (et donc ses valeurs) m√™me si le stack est mis √† jour d‚Äôune fa√ßon qui entra√Ænerait sa suppression/remplacement.\nAinsi, les valeurs modifi√©es par l‚Äôapplication restent intactes et ne sont pas r√©initialis√©es.\nLes options DynamoDB ou RDS demandent une migration et du code/ops suppl√©mentaires (plus d‚Äôeffort).\nUne stack policy qui ‚Äúdeny updates‚Äù peut bloquer des mises √† jour, mais ne traite pas le cas o√π la ressource est remplac√©e/supprim√©e; Retain est la mesure la plus directe et la moins co√ªteuse en effort.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un casier au lyc√©e avec une fiche ‚Äúr√©glages‚Äù (code du cadenas, liste de livres). Le prof (CloudFormation) a un plan du casier et de la fiche. Toi (l‚Äôappli) peux changer la fiche quand tu veux.**\n\nConcept : CloudFormation, c‚Äôest comme un prof qui ‚Äúreconstruit‚Äù la salle selon un plan. Parameter Store, c‚Äôest la fiche de r√©glages. Si le prof refait la salle, il peut r√©√©crire la fiche selon l‚Äôancien plan.\nIci, l‚Äôappli a modifi√© la fiche, mais quand on met √† jour le plan (ajout de ressources + tags), CloudFormation remet la fiche √† sa version du plan ‚Üí les valeurs sont ‚Äúreset‚Äù.\nSolution A : mettre une r√®gle ‚ÄúRetain‚Äù = ‚Äúne touche pas √† cette fiche, garde-la telle qu‚Äôelle est‚Äù. Comme √ßa, les changements faits par l‚Äôappli restent, m√™me si on met √† jour le reste de la salle.\nB et C : d√©placer les r√©glages vers un autre endroit (tableau/registre) = beaucoup plus de boulot.\nD : bloquer les updates peut casser des mises √† jour utiles, et c‚Äôest plus d√©licat.\nDonc A demande le moins d‚Äôeffort et √©vite de r√©initialiser les valeurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:153:37d87a202bcdccd0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 153,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses a custom root certificate authority certificate chain (Root CA Cert) that is 10 KB in size to generate SSL certificates for its on-premises HTTPS endpoints. One of the company‚Äôs cloud-based applications has hundreds of AWS Lambda functions that pull data from these endpoints. A developer updated the trust store of the Lambda execution environment to use the Root CA Cert when the Lambda execution environment is initialized. The developer bundled the Root CA Cert as a text file in the Lambda deployment bundle.After 3 months of development, the Root CA Cert is no longer valid and must be updated. The developer needs a more efficient solution to update the Root CA Cert for all deployed Lambda functions. The solution must not include rebuilding or updating all Lambda functions that use the Root CA Cert. The solution must also work for all development, testing, and production environments. Each environment is managed in a separate AWS account.Which combination of steps should the developer take to meet these requirements MOST cost-effectively? (Choose two.)",
      "choices": {
        "A": "Store the Root CA Cert as a secret in AWS Secrets Manager. Create a resource-based policy. Add IAM users to allow access to the secret.",
        "B": "Store the Root CA Cert as a SecureString parameter in AWS Systems Manager Parameter Store. Create a resource-based policy. Add IAM users to allow access to the policy.",
        "C": "Store the Root CA Cert in an Amazon S3 bucket. Create a resource-based policy to allow access to the bucket.",
        "D": "Refactor the Lambda code to load the Root CA Cert from the Root CA Cert‚Äôs location. Modify the runtime trust store inside the Lambda function handler.",
        "E": "Refactor the Lambda code to load the Root CA Cert from the Root CA Cert‚Äôs location. Modify the runtime trust store outside the Lambda function handler."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122570-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:50 a.m.",
      "textHash": "37d87a202bcdccd0",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Le probl√®me vient du fait que le certificat racine (Root CA) est ¬´ embarqu√© ¬ª dans chaque package Lambda : quand il expire, il faudrait red√©ployer des centaines de fonctions, ce qui est co√ªteux et lent.\nUne solution plus efficace est de stocker le Root CA dans un service central et s√©curis√©, puis de le lire au d√©marrage de l‚Äôex√©cution.\nAWS Secrets Manager est fait pour stocker des donn√©es sensibles (secrets/certificats) et permet de mettre √† jour la valeur une seule fois, sans reconstruire les fonctions.\nEnsuite, chaque Lambda r√©cup√®re le certificat depuis Secrets Manager : quand le secret est remplac√©, toutes les fonctions utilisent automatiquement la nouvelle version lors des prochaines ex√©cutions.\nComme les environnements sont dans des comptes AWS s√©par√©s, il faut autoriser l‚Äôacc√®s via une policy (resource-based policy) pour permettre l‚Äôacc√®s contr√¥l√© au secret.\nC‚Äôest plus adapt√© que S3 (moins orient√© ‚Äúsecret‚Äù) et plus simple/robuste que garder un fichier dans le bundle.\nDonc l‚Äôid√©e correcte est : stocker le Root CA dans Secrets Manager et g√©rer l‚Äôacc√®s par policy, ce qui √©vite de red√©ployer toutes les Lambdas.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que le ‚ÄúRoot CA Cert‚Äù est comme la liste officielle des signatures du proviseur qui prouve qu‚Äôun mot d‚Äôabsence est vrai. Tes centaines de devoirs (les fonctions Lambda) doivent v√©rifier cette signature avant d‚Äôaccepter un mot (se connecter en HTTPS).**\n\nConcept : si la liste des signatures est coll√©e dans chaque devoir, quand le proviseur change de signature, tu dois r√©imprimer tous les devoirs. C‚Äôest long et nul.\nSolution : tu mets la liste des signatures dans un coffre-fort central de l‚Äô√©cole, et chaque devoir vient la lire au moment o√π il en a besoin.\nAWS Secrets Manager = le coffre-fort : tu y stockes le Root CA Cert et tu peux le remplacer une seule fois quand il expire.\nAvec une ‚Äúresource-based policy‚Äù, tu donnes l‚Äôacc√®s au coffre aux bonnes personnes/√©quipes, m√™me si elles sont dans des ‚Äúb√¢timents diff√©rents‚Äù (comptes AWS s√©par√©s : dev/test/prod).\nPourquoi A est la bonne : tu mets √† jour 1 seul endroit, toutes les Lambdas r√©cup√®rent la nouvelle version sans √™tre reconstruites.\nPourquoi pas les autres : S3/Parameter Store sont plut√¥t des √©tag√®res/notes, moins adapt√©es ici au ‚Äúsecret‚Äù + gestion d‚Äôacc√®s multi-comptes; et D/E demandent de modifier le code de toutes les Lambdas.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:198:e8997b1c2f8e1fc2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 198,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has created an AWS Lambda function that makes queries to an Amazon Aurora MySQL DB instance. When the developer performs a test, the DB instance shows an error for too many connections.Which solution will meet these requirements with the LEAST operational effort?",
      "choices": {
        "A": "Create a read replica for the DB instance. Query the replica DB instance instead of the primary DB instance.",
        "B": "Migrate the data to an Amazon DynamoDB database.",
        "C": "Configure the Amazon Aurora MySQL DB instance for Multi-AZ deployment.",
        "D": "Create a proxy in Amazon RDS Proxy. Query the proxy instead of the DB instance."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122620-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:57 a.m.",
      "textHash": "e8997b1c2f8e1fc2",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:e6688af8",
      "frExplanation": "Le probl√®me vient du fait que chaque ex√©cution de votre fonction AWS Lambda (code qui s‚Äôex√©cute √† la demande) peut ouvrir sa propre connexion √† la base Aurora MySQL. Si beaucoup d‚Äôex√©cutions arrivent en m√™me temps, le nombre de connexions d√©passe la limite de la base (‚Äútoo many connections‚Äù).\nAmazon RDS Proxy est un service g√©r√© qui se place entre Lambda et la base de donn√©es. Il garde un ‚Äúpool‚Äù de connexions d√©j√† ouvertes et les r√©utilise, au lieu d‚Äôen cr√©er une nouvelle √† chaque appel.\nR√©sultat : beaucoup moins de connexions simultan√©es c√¥t√© Aurora, donc moins d‚Äôerreurs, sans changer votre logique SQL.\nC‚Äôest aussi l‚Äôoption avec le moins d‚Äôeffort op√©rationnel : vous cr√©ez le proxy, vous changez l‚Äôendpoint utilis√© par l‚Äôapplication, et AWS g√®re le pooling et la mont√©e en charge.\nA (read replica) aide surtout la lecture mais n‚Äôemp√™che pas Lambda d‚Äôouvrir trop de connexions (et ajoute de la gestion). B (DynamoDB) demande une refonte compl√®te. C (Multi-AZ) am√©liore la disponibilit√©, pas le nombre de connexions.\nDonc la meilleure r√©ponse est D : utiliser RDS Proxy et interroger le proxy.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : la cuisine (la base de donn√©es) ne peut servir qu‚Äôun certain nombre d‚Äô√©l√®ves en m√™me temps. La Lambda, c‚Äôest une classe enti√®re qui d√©barque d‚Äôun coup √† chaque sonnerie et tous veulent √™tre servis en m√™me temps.**\n\nConcept : trop de connexions = trop d‚Äô√©l√®ves qui font la queue en m√™me temps, la cuisine sature et refuse du monde. Avec Lambda, il peut y avoir plein d‚Äôex√©cutions en parall√®le, donc plein de ‚Äúdemandes‚Äù d‚Äôun coup. D (RDS Proxy) = un surveillant/guichet devant la cantine : il garde une petite r√©serve de plateaux pr√™ts (connexions d√©j√† ouvertes) et il les distribue aux √©l√®ves, au lieu que chacun aille d√©ranger la cuisine. R√©sultat : beaucoup moins d‚Äôouvertures/fermetures de connexions, donc moins d‚Äôerreurs ‚Äútrop de connexions‚Äù. C (Multi-AZ) c‚Äôest une 2e cuisine de secours pour les pannes, pas pour servir plus de monde. A (read replica) ajoute une autre cuisine, mais il faut g√©rer qui va o√π et √ßa ne r√®gle pas le pic de connexions aussi simplement. B (DynamoDB) c‚Äôest changer compl√®tement de cantine : gros d√©m√©nagement, pas ‚Äúleast effort‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:260:9a0fabee40237257",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 260,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing an application that will retrieve sensitive data from a third-party system. The application will format the data into a PDF file. The PDF file could be more than 1 MB. The application will encrypt the data to disk by using AWS Key Management Service (AWS KMS). The application will decrypt the file when a user requests to download it. The retrieval and formatting portions of the application are complete.The developer needs to use the GenerateDataKey API to encrypt the PDF file so that the PDF file can be decrypted later. The developer needs to use an AWS KMS symmetric customer managed key for encryption.Which solutions will meet these requirements?",
      "choices": {
        "A": "Write the encrypted key from the GenerateDataKey API to disk for later use. Use the plaintext key from the GenerateDataKey API and a symmetric encryption algorithm to encrypt the file.",
        "B": "Write the plain text key from the GenerateDataKey API to disk for later use. Use the encrypted key from the GenerateDataKey API and a symmetric encryption algorithm to encrypt the file.",
        "C": "Write the encrypted key from the GenerateDataKey API to disk for later use. Use the plaintext key from the GenerateDataKey API to encrypt the file by using the KMS Encrypt API.",
        "D": "Write the plain text key from the GenerateDataKey API to disk for later use. Use the encrypted key from the GenerateDataKey API to encrypt the file by using the KMS Encrypt API."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134261-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 3:17 a.m.",
      "textHash": "9a0fabee40237257",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "AWS KMS est un service qui g√®re des cl√©s de chiffrement. Pour chiffrer un gros fichier (ex: PDF > 1 Mo), on ne chiffre pas le fichier directement avec KMS, car KMS est fait pour de petites donn√©es.\nOn utilise donc le chiffrement ‚Äúenveloppe‚Äù : l‚ÄôAPI GenerateDataKey renvoie 2 choses : une cl√© en clair (plaintext) et la m√™me cl√© chiffr√©e (encrypted) avec la cl√© KMS.\nLa bonne pratique : utiliser la cl√© en clair uniquement en m√©moire pour chiffrer le PDF avec un algorithme sym√©trique (ex: AES), puis l‚Äôeffacer.\nEnsuite, on sauvegarde sur disque (ou avec le fichier) uniquement la version chiffr√©e de la cl√© (encrypted data key).\nPlus tard, pour t√©l√©charger le PDF, on redonne cette cl√© chiffr√©e √† KMS (Decrypt) pour r√©cup√©rer la cl√© en clair et d√©chiffrer le PDF.\nDonc A est correct : on stocke la cl√© chiffr√©e, et on chiffre le fichier avec la cl√© en clair + un algorithme sym√©trique.\nB et D sont dangereux car ils stockent la cl√© en clair sur disque. C et D essaient d‚Äôutiliser KMS Encrypt pour chiffrer le fichier, ce qui n‚Äôest pas adapt√© aux gros fichiers.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine la vie scolaire : tu veux ranger un devoir secret (le PDF) dans un casier. Le proviseur te donne une cl√© de casier ‚Äújetable‚Äù pour fermer le casier, et il te donne aussi une copie de cette cl√© enferm√©e dans un coffre du proviseur (impossible √† utiliser sans le proviseur).**\n\nConcept : pour un gros fichier, tu ne demandes pas au proviseur de fermer chaque feuille une par une. Tu fermes toi-m√™me le casier avec une cl√© rapide (cl√© en clair), et tu gardes seulement la copie ‚Äúdans le coffre‚Äù (cl√© chiffr√©e) pour rouvrir plus tard.\nGenerateDataKey te donne 2 versions de la m√™me cl√© : une en clair (pour chiffrer tout de suite) et une chiffr√©e (√† stocker).\nR√©ponse A : tu √©cris sur disque la cl√© chiffr√©e (la copie dans le coffre) pour la retrouver plus tard.\nPuis tu utilises la cl√© en clair pour chiffrer le PDF avec un chiffrement ‚Äúclassique‚Äù (comme fermer le casier toi-m√™me).\nQuand un √©l√®ve veut t√©l√©charger, tu redemandes au proviseur d‚Äôouvrir la cl√© chiffr√©e, tu r√©cup√®res la cl√© en clair, et tu d√©chiffres le PDF.\nLes autres choix sont faux car ils stockent la cl√© en clair (dangereux) ou essaient de faire chiffrer le gros PDF directement par le proviseur (pas fait pour √ßa).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:90:6beb2e7b31daca69",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 90,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company moved some of its secure files to a private Amazon S3 bucket that has no public access. The company wants to develop a serverless application that gives its employees the ability to log in and securely share the files with other users.Which AWS feature should the company use to share and access the files securely?",
      "choices": {
        "A": "Amazon Cognito user pool",
        "B": "S3 presigned URLs",
        "C": "S3 bucket policy",
        "D": "Amazon Cognito identity pool"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/109208-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 14, 2023, 7:53 a.m.",
      "textHash": "6beb2e7b31daca69",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Amazon S3 est un service de stockage de fichiers (objets) dans le cloud. Ici, le bucket est priv√© : personne ne peut t√©l√©charger les fichiers sans autorisation.\nPour permettre √† des employ√©s connect√©s de partager un fichier sans rendre le bucket public, il faut donner un acc√®s temporaire et limit√©.\nLes ‚ÄúS3 presigned URLs‚Äù sont des liens sign√©s par votre application (avec ses droits AWS) qui autorisent une action pr√©cise (t√©l√©charger ou envoyer un fichier) pendant une dur√©e d√©finie.\nAinsi, l‚Äôutilisateur qui re√ßoit le lien peut acc√©der au fichier m√™me s‚Äôil n‚Äôa pas directement des permissions AWS, tant que le lien n‚Äôa pas expir√©.\nC‚Äôest id√©al pour une application serverless : la fonction (ex. AWS Lambda) g√©n√®re le lien apr√®s authentification de l‚Äôemploy√©.\nUne bucket policy (C) change des r√®gles globales du bucket et n‚Äôest pas faite pour du partage ‚Äúau cas par cas‚Äù et temporaire.\nCognito user pool (A) g√®re la connexion (utilisateurs/mots de passe), mais ne donne pas √† lui seul l‚Äôacc√®s aux objets S3.\nCognito identity pool (D) donne des identit√©s AWS temporaires, mais pour partager simplement un fichier √† un autre utilisateur, un lien presign√© est plus direct et s√©curis√©.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:543:5bf8c16b8b9faa84",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 543,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an AWS Step Functions state machine named myStateMachine. The company configured a service role for Step Functions.The developer must ensure that only the myStateMachine state machine can assume the service role.Which statement should the developer add to the trust policy to meet this requirement?",
      "choices": {},
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157498-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 4, 2025, 10:10 a.m.",
      "textHash": "5bf8c16b8b9faa84",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, on parle d‚Äôun ¬´ service role ¬ª IAM : c‚Äôest un r√¥le AWS que Step Functions peut ¬´ assumer ¬ª (prendre temporairement) pour appeler d‚Äôautres services.\nLa ¬´ trust policy ¬ª (politique de confiance) dit QUI a le droit d‚Äôassumer ce r√¥le.\nPar d√©faut, on autorise souvent le service Step Functions (principal: states.amazonaws.com). Mais cela permettrait √† n‚Äôimporte quelle machine d‚Äô√©tat Step Functions du compte d‚Äôassumer le r√¥le.\nPour limiter √† une seule machine, on ajoute une condition qui v√©rifie l‚Äôidentit√© exacte de l‚Äôappelant.\nLa bonne pratique est d‚Äôutiliser une condition sur l‚ÄôARN de la machine d‚Äô√©tat, par exemple via aws:SourceArn, √©gal √† l‚ÄôARN de myStateMachine.\nAinsi, m√™me si un autre workflow Step Functions existe, il ne pourra pas assumer ce r√¥le car son ARN ne correspondra pas.\nC‚Äôest pour cela que la r√©ponse correcte est celle qui ajoute une condition de type StringEquals sur aws:SourceArn (ou √©quivalent) pointant vers l‚ÄôARN de myStateMachine.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un coll√®ge o√π il y a une salle des profs avec une cl√© sp√©ciale. Cette cl√© permet d‚Äôouvrir un placard avec des sujets d‚Äôexamens. On veut que SEULEMENT la classe ‚ÄúmyStateMachine‚Äù puisse emprunter cette cl√©, pas les autres classes.**\n\nConcept : sur AWS, un ‚Äúr√¥le‚Äù = une cl√©/badge qui donne des permissions. La ‚Äútrust policy‚Äù = la liste des personnes/classes autoris√©es √† prendre la cl√©. Step Functions = un ‚Äúchef d‚Äôorchestre‚Äù qui suit une liste d‚Äô√©tapes, et ‚ÄúmyStateMachine‚Äù = une classe pr√©cise. Probl√®me : si on dit juste ‚ÄúStep Functions peut prendre la cl√©‚Äù, alors toutes les classes Step Functions pourraient la prendre. La bonne r√©ponse (A) ajoute une r√®gle du type : ‚Äúautoriser Step Functions, MAIS seulement si c‚Äôest pr√©cis√©ment myStateMachine‚Äù (comme v√©rifier le nom exact de la classe sur la liste). Donc A est bonne car elle met un contr√¥le d‚Äôidentit√© : service Step Functions + condition qui limite √† l‚ÄôARN (l‚Äôidentifiant unique) de myStateMachine.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:547:11a03bdfc1bc21f7",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 547,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is designing an event-driven architecture. An AWS Lambda function that processes data needs to push processed data to a subset of four consumer Lambda functions. The data must be routed based on the value of one field in the data.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Create an Amazon Simple Queue Service (Amazon SQS) queue and event source mapping for each consumer Lambda function. Add message routing logic to the data-processing Lambda function.",
        "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the four consumer Lambda functions to the topic. Add message filtering logic to each consumer Lambda function. Subscribe the data-processing Lambda function to the SNS topic.",
        "C": "Create a separate Amazon Simple Notification Service (Amazon SNS) topic and subscription for each consumer Lambda function. Add message routing logic to the data-processing Lambda function to publish to the appropriate topic.",
        "D": "Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the four consumer Lambda functions to the topic. Add SNS subscription filter policies to each subscription. Configure the data-processing Lambda function to publish to the topic."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157499-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 4, 2025, 10:40 a.m.",
      "textHash": "11a03bdfc1bc21f7",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "On veut envoyer des √©v√©nements √† 4 fonctions AWS Lambda (du code qui s‚Äôex√©cute √† la demande) selon la valeur d‚Äôun champ.\nAmazon SNS est un service ‚Äúpublish/subscribe‚Äù : une fonction publie un message dans un ‚Äútopic‚Äù, et SNS le distribue aux abonn√©s.\nLa meilleure option est d‚Äôavoir un seul topic SNS et 4 abonnements (un par Lambda consommatrice).\nAvec des ‚Äúfilter policies‚Äù sur chaque abonnement, SNS fait le tri automatiquement : chaque Lambda ne re√ßoit que les messages dont le champ correspond.\nAinsi, la Lambda de traitement publie toujours au m√™me endroit (le topic), sans logique de routage complexe dans le code.\nC‚Äôest moins d‚Äôop√©rations √† g√©rer : un seul topic, pas de multiples files/queues, et pas de filtrage √† coder dans chaque Lambda.\nA et C demandent d‚Äôajouter du routage dans la Lambda de traitement (plus de code, plus de maintenance).\nB oblige chaque Lambda consommatrice √† filtrer elle-m√™me (elle re√ßoit trop de messages), donc plus de co√ªt et de complexit√©.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le lyc√©e : tu as un seul panneau d‚Äôaffichage √† l‚Äôentr√©e. Tu colles une annonce, et chaque club (sport, musique, jeux vid√©o, th√©√¢tre) a un filtre : il ne lit que les annonces qui le concernent (ex: ‚Äútournoi FIFA‚Äù).**\n\nIci, la Lambda ‚Äútraitement‚Äù = l‚Äô√©l√®ve qui √©crit l‚Äôannonce. SNS topic = le panneau d‚Äôaffichage unique. Les 4 Lambdas ‚Äúconsommatrices‚Äù = les 4 clubs. On doit envoyer l‚Äôinfo seulement au bon club selon un champ (ex: ‚Äútype=jeu‚Äù). Avec D, tu postes une seule fois sur le panneau, et chaque club a un filtre officiel (filtre d‚Äôabonnement) qui laisse passer seulement les annonces qui matchent. Donc pas besoin de coder du tri dans chaque club, ni de g√©rer 4 panneaux/4 files s√©par√©es. Moins de choses √† configurer et √† maintenir = moins de ‚Äúsurveillance‚Äù au quotidien. C‚Äôest pour √ßa que D a le moins d‚Äôeffort op√©rationnel.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:454:a0e034bc1e433110",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 454,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer built an application that uses AWS Lambda functions to process images. The developer wants to improve image processing times throughout the day.The developer needs to create an Amazon CloudWatch Logs Insights query that shows the average, slowest, and fastest processing time in 1-minute intervals.Which query will meet these requirements?",
      "choices": {},
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157514-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 4, 2025, 2:46 p.m.",
      "textHash": "a0e034bc1e433110",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Pour mesurer les temps de traitement, on utilise CloudWatch Logs Insights, qui permet d‚Äôinterroger les logs (journaux) g√©n√©r√©s par AWS Lambda (service qui ex√©cute du code sans serveur). L‚Äôobjectif est d‚Äôobtenir, par tranche de 1 minute, trois valeurs sur la dur√©e de traitement des images : la moyenne (average), la plus lente (max) et la plus rapide (min). La bonne requ√™te doit donc : 1) extraire ou utiliser un champ num√©rique repr√©sentant la dur√©e (ex. processingTime/duration), 2) calculer avg(), max() et min() sur ce champ, et 3) regrouper les r√©sultats par intervalles d‚Äô1 minute avec bin(1m). L‚Äôoption A est correcte car elle combine ces fonctions statistiques (avg/min/max) et un regroupement temporel en 1 minute, ce qui permet de voir l‚Äô√©volution au fil de la journ√©e. Les autres options √©chouent g√©n√©ralement parce qu‚Äôelles n‚Äôagr√®gent pas par minute, ou n‚Äôaffichent pas les trois mesures, ou ne calculent pas sur un champ de dur√©e num√©rique.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine au coll√®ge : chaque minute, des √©l√®ves passent au self. Tu veux savoir, pour chaque minute, combien de temps ils ont attendu en moyenne, qui a attendu le plus longtemps, et qui a attendu le moins longtemps.**\n\nIci, chaque image trait√©e par AWS Lambda, c‚Äôest comme un √©l√®ve servi au self. Les ‚Äúlogs‚Äù (journaux) sont le cahier o√π on note l‚Äôheure et le temps d‚Äôattente (dur√©e de traitement). Une requ√™te CloudWatch Logs Insights, c‚Äôest comme demander au surveillant : ‚ÄúRegroupe par minute, puis calcule la moyenne, le plus lent, et le plus rapide‚Äù. La bonne requ√™te (A) fait exactement √ßa : elle prend la dur√©e de traitement, la regroupe en tranches de 1 minute, puis calcule avg (moyenne), max (le plus lent) et min (le plus rapide). Les autres choix ratent soit le regroupement par minute, soit une des trois mesures (moyenne/lent/rapide), donc ils ne r√©pondent pas √† la question.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:489:82e89d707eb067f9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 489,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company‚Äôs application includes an Amazon DynamoDB table for product orders. The table has a primary partition key of orderId and has no sort key. The company is adding a new feature that requires the application to query the table by using the customerId attribute.Which solution will provide this query functionality?",
      "choices": {
        "A": "Change the existing primary key by setting customerId as the sort key.",
        "B": "Create a new global secondary index (GSI) on the table with a partition key of customerId.",
        "C": "Create a new local secondary index (LSI) on the table with a partition key of customerId.",
        "D": "Create a new local secondary index (LSI) on the table with a partition key of orderId and a sort key of customerId."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156659-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:05 a.m.",
      "textHash": "82e89d707eb067f9",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π l‚Äôon lit efficacement surtout via une ‚Äúcl√©‚Äù (index). Ici, la table a seulement une cl√© de partition orderId : on peut donc r√©cup√©rer vite une commande par orderId, mais pas ‚Äúchercher toutes les commandes d‚Äôun client‚Äù avec customerId.\nPour interroger par un autre attribut (customerId), il faut cr√©er un index secondaire.\nUn GSI (Global Secondary Index) permet de d√©finir une nouvelle cl√© de partition diff√©rente de la cl√© principale de la table, donc customerId peut devenir la cl√© de recherche.\nAvec un GSI partitionn√© par customerId, l‚Äôapplication peut faire une Query pour obtenir toutes les commandes d‚Äôun m√™me client.\nUn LSI (Local Secondary Index) exige la m√™me cl√© de partition que la table (ici orderId), donc il ne peut pas utiliser customerId comme cl√© de partition.\nOn ne peut pas ‚Äúchanger‚Äù la cl√© primaire existante simplement (et ajouter un sort key n‚Äôest pas possible sur une table d√©j√† cr√©√©e sans recr√©er la table).\nDonc la bonne solution est de cr√©er un GSI avec customerId comme partition key.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:486:dc2f6d4160a6bd35",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 486,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Step Functions state machine to handle an order processing workflow. When the state machine receives an order, the state machine pauses until the order has been confirmed. A record that is added to an Amazon DynamoDB table by another service confirms each order.The developer must complete the order processing workflow.Which solution will meet this requirement?",
      "choices": {
        "A": "Update the state machine to query the DynamoDB table by using the DynamoDB GetItem state to determine whether a record exists. If the record does exist, continue to the next state. If the record does not exist, wait 5 minutes and check again.",
        "B": "Subscribe an AWS Lambda function to a DynamoDB table stream. Configure the Lambda function to run when a new record is added to the table. When the Lambda function receives the appropriate record, run the redrive execution command on the running state machine.",
        "C": "Subscribe an AWS Lambda function to the DynamoDB table stream. Configure the Lambda function to run when a new record is added to the table. When the Lambda function receives the appropriate record, stop the current state machine invocation and start a new invocation.",
        "D": "Invoke an AWS Lambda function from the state machine. Configure the Lambda function to continuously poll the DynamoDB table for the appropriate record and to return when a record exists. Continue the state machine invocation when the Lambda function returns. If the Lambda function times out, then fail the state machine."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/154337-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Jan. 10, 2025, 9:16 p.m.",
      "textHash": "dc2f6d4160a6bd35",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:dynamodb:4957ccb3",
      "frExplanation": "Step Functions orchestre des √©tapes (un workflow) et peut ¬´ attendre ¬ª un √©v√©nement avant de continuer. Ici, la confirmation arrive quand un autre service √©crit une ligne dans une table DynamoDB (base NoSQL). La bonne approche est d‚Äô√™tre notifi√© d√®s qu‚Äôune nouvelle ligne est ajout√©e, au lieu de v√©rifier en boucle. DynamoDB Streams permet de capter chaque insertion/modification dans la table, et une fonction AWS Lambda peut se d√©clencher automatiquement √† ce moment-l√†. La Lambda peut alors ¬´ r√©veiller ¬ª l‚Äôex√©cution Step Functions en relan√ßant l‚Äôex√©cution en √©chec/reprise via une commande de redrive, ce qui permet de reprendre le workflow au bon moment. A est moins bon car c‚Äôest du polling (attendre 5 min puis re-tester) : lent, co√ªteux et pas r√©actif. C est mauvais car arr√™ter et red√©marrer une ex√©cution peut perdre le contexte et compliquer la logique. D est mauvais car une Lambda qui poll en continu est inefficace et risque le timeout, ce qui ferait √©chouer le workflow.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine : tu as un ticket de commande (ton plateau) et tu attends que la caisse confirme que tu as pay√©. La confirmation, c‚Äôest un petit tampon ajout√© dans un cahier (la table DynamoDB) par un autre surveillant.**\n\nConcept : au lieu de retourner toutes les 5 minutes voir le cahier, tu veux √™tre pr√©venu d√®s que le tampon appara√Æt. Dans AWS, le ‚Äúcahier‚Äù peut envoyer une alerte √† chaque nouvelle ligne ajout√©e (un stream). Une petite aide (Lambda = un mini-robot) √©coute ces alertes. Quand le robot voit la ligne qui confirme TON ticket, il va dire au workflow Step Functions : ‚Äúreprends exactement l√† o√π tu √©tais‚Äù (redrive de l‚Äôex√©cution en cours). C‚Äôest pour √ßa que B est bon : on r√©agit tout de suite, sans gaspiller du temps √† v√©rifier en boucle. A et D, c‚Äôest comme aller v√©rifier sans arr√™t le cahier. C, c‚Äôest comme jeter ton plateau et recommencer la commande, pas logique.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:499:bc46d535936038c6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 499,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is developing a new application that uses Amazon EC2, Amazon S3, and AWS Lambda resources. The company wants to allow employees to access the AWS Management Console by using existing credentials that the company stores and manages in an on-premises Microsoft Active Directory. Each employee must have a specific level of access to the AWS resources that is based on the employee‚Äôs role.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Configure AWS Directory Service to create an Active Directory in AWS Directory Service for Microsoft Active Directory. Establish a trust relationship with the on-premises Active Directory. Configure IAM roles and trust policies to give the employees access to the AWS resources.",
        "B": "Use LDAP to directly integrate the on-premises Active Directory with AWS Identity and Access Management (IAM). Map Active Directory groups to IAM roles to control access to AWS resources.",
        "C": "Implement a custom identity broker to authenticate users into the on-premises Active Directory. Configure the identity broker to use AWS Security Token Service (AWS STS) to grant authorized users IAM role based access to the AWS resources.",
        "D": "Configure Amazon Cognito to federate users into the on-premises Active Directory. Use Cognito user pools to manage user identities and to manage user access to the AWS resources."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156667-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:17 a.m.",
      "textHash": "bc46d535936038c6",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Objectif : se connecter √† la console AWS avec les identifiants d√©j√† g√©r√©s dans l‚ÄôActive Directory (AD) sur site, et donner des droits diff√©rents selon le r√¥le.\nLa solution la plus simple √† op√©rer est d‚Äôutiliser AWS Directory Service pour Microsoft AD : AWS h√©berge un AD manag√© (pas de serveurs AD √† maintenir) et peut √©tablir une relation de confiance (trust) avec l‚ÄôAD on‚Äëprem.\nAvec ce trust, les employ√©s continuent d‚Äôutiliser leurs comptes AD existants pour s‚Äôauthentifier, sans recr√©er des utilisateurs dans AWS.\nEnsuite, on associe des groupes/roles AD √† des r√¥les IAM (IAM = gestion des permissions AWS) via des politiques (policies) et des relations de confiance (trust policies).\nLes r√¥les IAM donnent l‚Äôacc√®s pr√©cis √† EC2, S3 et Lambda selon le poste (principe du moindre privil√®ge).\nPourquoi pas B : IAM ne s‚Äôint√®gre pas ‚Äúdirectement‚Äù en LDAP de cette fa√ßon pour la console, ce n‚Äôest pas l‚Äôapproche standard.\nPourquoi pas C : un broker d‚Äôidentit√© custom + STS marche, mais c‚Äôest du d√©veloppement/maintenance en plus (plus d‚Äôoverhead).\nPourquoi pas D : Cognito vise surtout les utilisateurs d‚Äôapplications (web/mobile), pas la connexion des employ√©s √† la console AWS via AD avec le minimum d‚Äôexploitation.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:507:97d5ea5e2e97355c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 507,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has implemented an AWS Lambda function that inserts new customers into an Amazon RDS database. The function is expected to run hundreds of times each hour. The function and RDS database are in the same VPC. The function is configured to use 512 MB of RAM and is based on the following pseudo code:After successfully testing the function multiple times, the developer notices that the execution time is longer than expected.What should the developer do to improve performance?",
      "choices": {
        "A": "Increase the reserved concurrency of the Lambda function.",
        "B": "Increase the size of the RDS database to facilitate an increased number of database connections each hour.",
        "C": "Move the database connection and close statement out of the handler. Place the connection in the global space.",
        "D": "Replace Amazon RDS with Amazon DynamoDB to implement control over the number of writes per second."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156018-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 6, 2025, 9:17 a.m.",
      "textHash": "97d5ea5e2e97355c",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:6a3e749a",
      "frExplanation": "AWS Lambda ex√©cute votre code ‚Äú√† la demande‚Äù dans des environnements qui peuvent √™tre recr√©√©s souvent. Si, √† chaque ex√©cution, votre fonction ouvre une connexion √† Amazon RDS (base de donn√©es relationnelle) puis la ferme, cela ajoute un co√ªt important (temps de connexion + n√©gociation r√©seau), surtout si la fonction tourne des centaines de fois par heure.\nDans une Lambda, le code en dehors du handler (zone ‚Äúglobale‚Äù) peut √™tre r√©utilis√© quand AWS r√©utilise le m√™me environnement pour plusieurs appels (warm start). Mettre la cr√©ation de connexion en global permet de r√©utiliser la m√™me connexion entre invocations, donc moins de latence.\nC‚Äôest exactement ce que propose C : d√©placer l‚Äôouverture de connexion (et √©viter de la fermer √† chaque fois) hors du handler.\nA (reserved concurrency) sert surtout √† limiter/garantir le nombre d‚Äôex√©cutions en parall√®le, pas √† acc√©l√©rer chaque ex√©cution.\nB (augmenter la taille RDS) peut aider si la base est satur√©e, mais le probl√®me typique ici est le co√ªt de connexion r√©p√©t√© c√¥t√© Lambda.\nD (passer √† DynamoDB) change totalement la technologie et ne r√©pond pas au probl√®me principal de performance li√© aux connexions.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : √† chaque √©l√®ve, tu dois ouvrir une porte, aller chercher un plateau, puis refermer la porte. Si tu refais ‚Äúouvrir/fermer‚Äù √† chaque √©l√®ve, tu perds un temps fou. Si tu gardes la porte ouverte et les plateaux pr√™ts sur une table, tu sers beaucoup plus vite.**\n\nIci, la fonction Lambda, c‚Äôest l‚Äô√©l√®ve qui arrive souvent (des centaines de fois par heure). La base RDS, c‚Äôest la cuisine. Se ‚Äúconnecter √† la base‚Äù puis ‚Äúfermer la connexion‚Äù √† chaque passage, c‚Äôest comme ouvrir/fermer la porte √† chaque √©l√®ve : √ßa ralentit. La bonne id√©e (C) est de cr√©er la connexion une seule fois ‚Äú√† l‚Äôext√©rieur‚Äù du handler (comme garder la porte ouverte et les plateaux pr√™ts), puis la r√©utiliser quand la m√™me Lambda ressert. A n‚Äôaide pas vraiment : plus d‚Äô√©l√®ves en m√™me temps ne rend pas chaque service plus rapide. B change la taille de la cuisine, mais le probl√®me principal est le temps perdu √† ouvrir/fermer la porte. D change carr√©ment de cuisine, inutile si le souci vient surtout de la connexion r√©p√©t√©e.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:519:7b9d578994d2a5b0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 519,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing a mobile application that allows users to view images from an S3 bucket. The users must be able to log in with their Amazon login, as well as supported social media accounts.How can the developer provide this authentication functionality?",
      "choices": {
        "A": "Use Amazon Cognito with web identity federation.",
        "B": "Use Amazon Cognito with SAML-based identity federation.",
        "C": "Use IAM access keys and secret keys in the application code to allow Get* on the S3 bucket.",
        "D": "Use AWS STS AssumeRole in the application code and assume a role with Get* permissions on the S3 bucket."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152780-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 10, 2024, 5:11 a.m.",
      "textHash": "7b9d578994d2a5b0",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, l‚Äôapplication mobile doit authentifier des utilisateurs avec leur compte Amazon et des comptes ‚Äúsociaux‚Äù (Google, Facebook, etc.).\nAmazon Cognito est un service AWS qui g√®re l‚Äôinscription/connexion des utilisateurs et peut √©changer une identit√© externe contre des identifiants AWS temporaires.\nLa ‚Äúweb identity federation‚Äù signifie : l‚Äôutilisateur se connecte chez un fournisseur (Amazon ou r√©seau social), puis Cognito r√©cup√®re un jeton de connexion et le transforme en droits AWS limit√©s.\nEnsuite, l‚Äôapp re√ßoit des identifiants temporaires (courte dur√©e) pour acc√©der √† S3 et lire les images, sans stocker de secrets dans l‚Äôapp.\nC‚Äôest exactement le besoin : login via Amazon + r√©seaux sociaux, puis acc√®s contr√¥l√© au bucket S3.\nB (SAML) est plut√¥t pour des entreprises avec un fournisseur d‚Äôidentit√© interne (SSO d‚Äôentreprise), pas pour des logins sociaux grand public.\nC est dangereux : mettre des cl√©s IAM dans le code mobile expose les secrets et donne souvent trop de droits.\nD (AssumeRole) n√©cessite d√©j√† des identifiants AWS pour appeler STS ; Cognito est justement la solution standard pour obtenir ces identifiants √† partir d‚Äôun login social.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e avec une salle sp√©ciale o√π sont rang√©es les photos (le ‚Äúbucket S3‚Äù). Pour entrer, tu peux montrer ta carte Amazon ou ta carte d‚Äôun r√©seau social (Google/Facebook‚Ä¶).**\n\nConcept : il faut un ‚Äúvigile‚Äù qui accepte plusieurs cartes d‚Äôidentit√© et qui te donne un badge temporaire pour aller voir les photos. Amazon Cognito, c‚Äôest ce vigile.\nPourquoi A : ‚Äúweb identity federation‚Äù = Cognito sait v√©rifier des identit√©s venant du web (Amazon + r√©seaux sociaux) et, si c‚Äôest bon, il te donne un badge temporaire pour lire les images.\nPourquoi pas B : SAML, c‚Äôest plut√¥t la carte d‚Äôun √©tablissement/entreprise (genre ENT du lyc√©e), pas les comptes sociaux.\nPourquoi pas C : mettre des cl√©s secr√®tes dans l‚Äôappli, c‚Äôest comme coller un passe-partout dans le cahier : si quelqu‚Äôun le copie, il entre partout.\nPourquoi pas D : STS AssumeRole, c‚Äôest demander un badge, mais il te manque le vigile qui g√®re facilement Amazon + r√©seaux sociaux.\nDonc la bonne r√©ponse est A.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:525:e1c5b87b1c0df5e5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 525,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA development team is designing a mobile app that requires multi-factor authentication.Which steps should be taken to achieve this? (Choose two.)",
      "choices": {
        "A": "Use Amazon Cognito to create a user pool and create users in the user pool.",
        "B": "Send multi-factor authentication text codes to users with the Amazon SNS Publish API call in the app code.",
        "C": "Enable multi-factor authentication for the Amazon Cognito user pool.",
        "D": "Use AWS IAM to create IAM users.",
        "E": "Enable multifactor authentication for the users created in AWS IAM."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157451-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 3, 2025, 11:25 a.m.",
      "textHash": "e1c5b87b1c0df5e5",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Pour une appli mobile, il faut un service qui g√®re les comptes utilisateurs (inscription, connexion) et l‚Äôauthentification. Amazon Cognito est fait pour √ßa.\nUn ¬´ user pool ¬ª Cognito est une base d‚Äôutilisateurs g√©r√©e par AWS : on y cr√©e les comptes (ou on laisse les utilisateurs s‚Äôinscrire) et Cognito g√®re les mots de passe, tokens, etc.\nDonc l‚Äô√©tape indispensable est de cr√©er un user pool et d‚Äôy avoir des utilisateurs (r√©ponse A).\nPour avoir du multi‚Äëfacteur (MFA), il faut ensuite l‚Äôactiver au niveau du user pool Cognito : Cognito demandera un second facteur (SMS ou appli d‚Äôauthentification) lors de la connexion (r√©ponse C).\nEnvoyer soi‚Äëm√™me des codes SMS via SNS (B) ne suffit pas : il faut aussi g√©rer la v√©rification, les tentatives, l‚Äôexpiration, et l‚Äôint√©gration au login.\nIAM (D, E) sert √† g√©rer l‚Äôacc√®s des humains/roles aux ressources AWS, pas les utilisateurs finaux d‚Äôune appli mobile.\nAinsi, la bonne approche est Cognito + activation MFA dans le user pool.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine l‚Äôentr√©e de ton lyc√©e : pour entrer, tu dois montrer ta carte d‚Äô√©l√®ve, et parfois on te demande aussi un code re√ßu sur ton t√©l√©phone (double v√©rification).**\n\nLe ‚Äúmulti-factor authentication‚Äù (MFA), c‚Äôest comme avoir 2 preuves : 1) ton mot de passe = ta carte d‚Äô√©l√®ve, 2) un code SMS/app = le vigile qui v√©rifie en plus.\nDans une appli mobile, il faut d‚Äôabord un ‚Äúregistre d‚Äô√©l√®ves‚Äù d√©di√© aux utilisateurs de l‚Äôappli : c‚Äôest Amazon Cognito avec un ‚Äúuser pool‚Äù (une liste d‚Äôutilisateurs g√©r√©e pour ton appli).\nDonc A est indispensable : tu cr√©es ce registre et tu y mets les utilisateurs.\nEnsuite, pour que la double v√©rification existe vraiment, il faut l‚Äôactiver sur ce registre : c‚Äôest l‚Äôoption ‚ÄúEnable MFA‚Äù du user pool (r√©ponse C).\nB (envoyer toi-m√™me des SMS) c‚Äôest comme demander √† un √©l√®ve d‚Äôenvoyer les codes √† la place du lyc√©e : pas fiable et pas le bon r√¥le.\nD et E (IAM) c‚Äôest pour les comptes du personnel qui g√®re l‚Äô√©cole (admins), pas pour les √©l√®ves (utilisateurs de l‚Äôappli).\nDonc les 2 bonnes √©tapes sont : A + C.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:554:297a0bcc1b16e457",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 554,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has an application that runs in AWS Account A. The application must retrieve an AWS Secrets Manager secret that is encrypted by an AWS Key Management Service (AWS KMS) key from AWS Account B. The application‚Äôs role has permissions to access the secret in Account B.The developer must add a statement to the KMS key‚Äôs key policy to allow the role in Account A to use the KMS key in Account B. The permissions must grant least privilege access to the role.Which permissions will meet these requirements?",
      "choices": {
        "A": "kms:Decrypt and kms:DescribeKey",
        "B": "secretsmanager:DescribeSecret and secretsmanager:GetSecretValue",
        "C": "kms:*",
        "D": "secretsmanager:*"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157446-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 3, 2025, 8:46 a.m.",
      "textHash": "297a0bcc1b16e457",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, l‚Äôapplication (compte A) lit un secret stock√© dans AWS Secrets Manager (compte B). Le secret est chiffr√© avec une cl√© AWS KMS (compte B).\nPour lire le secret, Secrets Manager doit pouvoir d√©chiffrer la valeur avec KMS au moment du GetSecretValue.\nDonc, en plus des droits Secrets Manager (d√©j√† accord√©s), il faut autoriser le r√¥le du compte A √† utiliser la cl√© KMS du compte B.\nLe droit minimum pour d√©chiffrer est kms:Decrypt (permet de transformer le texte chiffr√© en clair).\nOn ajoute aussi kms:DescribeKey pour permettre √† l‚Äôappelant/au service de v√©rifier les infos de la cl√© (souvent n√©cessaire pour valider l‚Äôusage de la cl√©).\nLes options secretsmanager:* ou secretsmanager:GetSecretValue ne sont pas des permissions KMS, donc elles ne r√©solvent pas le probl√®me de d√©chiffrement.\nkms:* donnerait tous les droits sur la cl√© (trop large, pas ‚Äúleast privilege‚Äù).\nDonc la combinaison la plus restrictive et correcte est kms:Decrypt et kms:DescribeKey.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine deux √©coles : √âcole A (ton appli) et √âcole B (o√π est rang√© un coffre). Le coffre contient un papier secret, mais il est ferm√© avec un cadenas sp√©cial. Le cadenas (la ‚Äúcl√© KMS‚Äù) appartient √† l‚Äô√âcole B.**\n\nConcept : pour lire le papier, tu as besoin de 2 choses : le droit de prendre le coffre, et le droit d‚Äôouvrir le cadenas. Ici, ton r√¥le a d√©j√† le droit de prendre le coffre (acc√©der au secret) dans l‚Äô√âcole B. Il manque juste le droit d‚Äôutiliser le cadenas de l‚Äô√âcole B.\nLeast privilege = donner le minimum : seulement ouvrir, pas tout contr√¥ler. Donc on autorise juste ‚ÄúD√©verrouiller‚Äù (kms:Decrypt) pour lire le secret, et ‚ÄúVoir l‚Äô√©tiquette du cadenas‚Äù (kms:DescribeKey) pour v√©rifier quelle cl√© c‚Äôest.\nPourquoi pas B ou secretsmanager:* ? √áa concerne le coffre, pas le cadenas, et en plus c‚Äôest trop large. Pourquoi pas C (kms:*) ? √áa donnerait tous les pouvoirs sur le cadenas (comme le copier, le changer), trop dangereux.\nDonc A est le bon choix : juste Decrypt + DescribeKey.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:535:9963b3ccbdef8b57",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 535,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to automate deployments for a serverless, event-based workload. The developer needs to create standardized templates to define the infrastructure and to test the functionality of the workload locally before deploymentThe developer already uses a pipeline in AWS CodePipeline. The developer needs to incorporate any other infrastructure changes into the existing pipeline.Which solution will meet these requirements?",
      "choices": {
        "A": "Create an AWS Serverless Application Model (AWS SAM) template. Configure the pipeline stages in CodePipeline to run the necessary AWS SAM CLI commands to deploy the serverless workload.",
        "B": "Create an AWS Step Functions workflow template based on the infrastructure by using the Amazon States Language. Start the Step Functions state machine from the existing pipeline.",
        "C": "Create an AWS CloudFormation template. Use the existing pipeline workflow to build a pipeline for AWS CloudFormation stacks.",
        "D": "Create an AWS Serverless Application Model (AWS SAM) template. Use an automated script to deploy the serverless workload by using the AWS SAM CLI deploy command."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156695-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 11:39 a.m.",
      "textHash": "9963b3ccbdef8b57",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici on veut d√©ployer automatiquement une application ¬´ serverless ¬ª (sans serveurs √† g√©rer) et pilot√©e par des √©v√©nements (ex: API Gateway, S3, SNS d√©clenchent des fonctions Lambda).\nIl faut aussi des mod√®les standardis√©s pour d√©crire l‚Äôinfrastructure (Infrastructure as Code) et pouvoir tester localement avant de d√©ployer.\nAWS SAM (Serverless Application Model) est fait pour √ßa : c‚Äôest un format de template bas√© sur CloudFormation, mais simplifi√© pour Lambda, API Gateway, DynamoDB, etc.\nSAM fournit aussi la SAM CLI, qui permet de lancer et tester l‚Äôapplication en local (ex: sam local invoke/start-api) avant l‚Äôenvoi sur AWS.\nComme le d√©veloppeur utilise d√©j√† AWS CodePipeline, la bonne approche est d‚Äôajouter des √©tapes dans le pipeline pour ex√©cuter les commandes SAM (build/package/deploy).\nAinsi, toute modification d‚Äôinfrastructure (template SAM) passe par le m√™me pipeline et est d√©ploy√©e de fa√ßon reproductible.\nB ne d√©crit pas l‚Äôinfrastructure compl√®te (Step Functions est un service d‚Äôorchestration), C ne couvre pas le test local sp√©cifique serverless, et D contourne CodePipeline au lieu de l‚Äôint√©grer.\nDonc A r√©pond aux trois besoins : templates standard, tests locaux, et int√©gration dans CodePipeline.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu pr√©pares un tournoi de jeu vid√©o au lyc√©e. Tu veux une ‚Äúfiche de r√®gles‚Äù unique pour installer l‚Äôar√®ne (PC, manettes, horaires) et tu veux pouvoir faire une partie d‚Äôentra√Ænement chez toi avant le jour J. Et tu as d√©j√† un ‚Äúplanning automatique‚Äù qui lance toutes les √©tapes (comme une checklist).**\n\nConcept : pour d√©ployer automatiquement, il faut un plan standard (un mod√®le) + un moyen de tester en local + l‚Äôint√©grer dans le planning existant.\nAWS SAM, c‚Äôest la ‚Äúfiche de r√®gles‚Äù sp√©ciale pour les applis sans serveur (serverless) et qui r√©agissent √† des √©v√©nements (comme ‚Äúquand quelqu‚Äôun clique, √ßa lance une action‚Äù).\nSAM a aussi des outils (SAM CLI) pour s‚Äôentra√Æner sur ton PC, comme un match d‚Äôentra√Ænement.\nR√©ponse A : tu mets le mod√®le SAM, puis tu ajoutes dans CodePipeline (le planning) des √©tapes qui lancent les commandes SAM pour tester et d√©ployer.\n√áa respecte ‚Äútester localement‚Äù + ‚Äútemplates standard‚Äù + ‚Äútout passe par le pipeline‚Äù.\nB : Step Functions, c‚Äôest plut√¥t un sc√©nario d‚Äôactions, pas un plan complet d‚Äôinfrastructure + test local.\nC : CloudFormation fait des plans, mais n‚Äôest pas aussi pratique que SAM pour serverless + test local.\nD : d√©ployer par script hors du pipeline, c‚Äôest comme ignorer le planning automatique : pas int√©gr√© aux √©tapes existantes.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:533:93a0b0ef01cefd51",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 533,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is deploying an application on an Amazon Elastic Container Service (Amazon ECS) cluster that uses AWS Fargate. The developer is using a Docker container with an Ubuntu image.The developer needs to implement a solution to store application data that is available from multiple ECS tasks. The application data must remain accessible after the container is terminated.Which solution will meet these requirements?",
      "choices": {
        "A": "Attach an Amazon FSx for Windows File Server volume to the container definition.",
        "B": "Specify the DockerVolumeConfiguration parameter in the ECS task definition to attach a Docker volume.",
        "C": "Create an Amazon Elastic File System (Amazon EFS) file system. Specify the mountPoints attribute and the efsVolumeConfiguration attribute in the ECS task definition.",
        "D": "Create an Amazon Elastic Block Store (Amazon EBS) volume. Specify the mount point configuration in the ECS task definition."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156683-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:51 a.m.",
      "textHash": "93a0b0ef01cefd51",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:5948a7d9",
      "frExplanation": "Ici, l‚Äôapplication tourne sur Amazon ECS avec AWS Fargate (des conteneurs g√©r√©s par AWS, sans g√©rer de serveurs). On veut stocker des donn√©es qui doivent √™tre partag√©es entre plusieurs ‚Äútasks‚Äù ECS (plusieurs instances du conteneur) et rester apr√®s l‚Äôarr√™t d‚Äôun conteneur.\nUn volume Docker (option B) est en g√©n√©ral local √† l‚Äôh√¥te/au cycle de vie du conteneur : avec Fargate, ce stockage n‚Äôest pas fait pour √™tre partag√© durablement entre tasks.\nAmazon EBS (option D) est un disque ‚Äúbloc‚Äù attach√© √† une seule machine/instance √† la fois ; ce n‚Äôest pas adapt√© pour √™tre mont√© simultan√©ment par plusieurs tasks Fargate.\nAmazon FSx for Windows (option A) est un service de fichiers Windows (SMB) et ne correspond pas au besoin typique Linux/Ubuntu sur Fargate, et ce n‚Äôest pas l‚Äôoption standard attendue ici.\nAmazon EFS (option C) est un syst√®me de fichiers r√©seau (NFS) manag√© : plusieurs tasks peuvent monter le m√™me dossier en m√™me temps.\nComme EFS est externe aux conteneurs, les donn√©es restent disponibles m√™me si un conteneur est supprim√©/red√©marr√©.\nDonc la bonne solution est de cr√©er un EFS et de le monter dans la task definition via efsVolumeConfiguration et mountPoints.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une classe avec plusieurs groupes qui travaillent sur le m√™me expos√©. Ils ont besoin d‚Äôun classeur partag√© dans la biblioth√®que, que tous peuvent ouvrir en m√™me temps, et qui reste l√† m√™me si un groupe quitte la salle.**\n\nIci, chaque ‚ÄúECS task‚Äù = un groupe d‚Äô√©l√®ves, et le ‚Äúcontainer‚Äù = la table de travail du groupe. Si la table dispara√Æt (container termin√©), les feuilles pos√©es dessus disparaissent aussi. Il faut donc un stockage partag√© ET durable, comme le classeur de la biblioth√®que. Amazon EFS, c‚Äôest justement un ‚Äúdossier partag√©‚Äù accessible par plusieurs tasks en m√™me temps, et il reste disponible m√™me si une task s‚Äôarr√™te. C‚Äôest pour √ßa que C est la bonne r√©ponse (on ‚Äúbranche‚Äù EFS au container avec mountPoints/efsVolumeConfiguration). A est un classeur sp√©cial Windows (pas adapt√© ici). B, c‚Äôest plut√¥t un tiroir local √† une table (pas forc√©ment partag√©/durable). D (EBS) ressemble √† un disque dur attach√© √† une seule machine, donc pas pratique pour plusieurs tasks.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:534:33d6c921112e565f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 534,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Lambda function that needs network access to private resources in a VPC.Which solution will provide this access with the LEAST operational overhead?",
      "choices": {
        "A": "Attach the Lambda function to the VPC through private subnets. Create a security group that allows network access to the private resources. Associate the security group with the Lambda function.",
        "B": "Configure the Lambda function to route traffic through a VPN connection. Create a security group that allows network access to the private resources. Associate the security group with the Lambda function.",
        "C": "Configure a VPC endpoint connection for the Lambda function. Set up the VPC endpoint to route traffic through a NAT gateway.",
        "D": "Configure an AWS PrivateLink endpoint for the private resources. Configure the Lambda function to reference the PrivateLink endpoint."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156684-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:52 a.m.",
      "textHash": "33d6c921112e565f",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:f81c7537",
      "frExplanation": "Une fonction AWS Lambda s‚Äôex√©cute normalement hors de votre r√©seau priv√© (VPC). Si elle doit appeler des ressources priv√©es (ex: base de donn√©es dans un sous-r√©seau priv√©), elle doit ‚Äúentrer‚Äù dans le VPC.\nLa solution la plus simple est d‚Äôattacher la Lambda √† votre VPC en choisissant des sous-r√©seaux priv√©s : AWS cr√©e et g√®re les interfaces r√©seau n√©cessaires pour que la Lambda ait une IP dans le VPC.\nEnsuite, on contr√¥le l‚Äôacc√®s avec un Security Group (pare-feu) : on autorise uniquement les ports/ressources n√©cessaires (ex: port 5432 vers une base).\nC‚Äôest peu d‚Äôoverhead car il n‚Äôy a pas de VPN √† g√©rer (B) et pas de routage sp√©cial.\nLes VPC Endpoints / NAT (C) servent surtout √† acc√©der √† des services AWS ou √† Internet depuis un VPC, pas √† donner √† Lambda l‚Äôacc√®s √† vos ressources priv√©es.\nPrivateLink (D) est utile pour exposer un service via un endpoint priv√© entre VPC/comptes, mais c‚Äôest plus complexe et inutile si la ressource est d√©j√† dans le m√™me VPC.\nDonc A est la r√©ponse la plus directe et avec le moins d‚Äôop√©rations √† maintenir.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:537:a187669d9a9a8747",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 537,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is deploying a new Node.js AWS Lambda function that is not connected to a VPC. The Lambda function needs to connect to and query an Amazon Aurora database that is not publicly accessible. The developer is expecting unpredictable surges in database traffic.What should the developer do to give the Lambda function access to the database?",
      "choices": {
        "A": "Configure the Lambda function to use an Amazon RDS proxy.",
        "B": "Configure a NAT gateway. Attach the NAT gateway to the Lambda function.",
        "C": "Enable public access on the Aurora database. Configure a security group on the database to allow outbound access for the database engine‚Äôs port.",
        "D": "Enable VPC access for the Lambda function. Attach the Lambda function to a new security group that does not have rules."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156697-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 11:42 a.m.",
      "textHash": "a187669d9a9a8747",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:bd18731a",
      "frExplanation": "Aurora est une base de donn√©es dans Amazon RDS. ¬´ Non publique ¬ª signifie qu‚Äôelle est dans un VPC et n‚Äôaccepte pas de connexions depuis Internet.\nUne fonction AWS Lambda hors VPC ne peut pas atteindre directement une base priv√©e dans un VPC : il lui manque un chemin r√©seau.\nEn plus, avec des pics impr√©visibles, Lambda peut ouvrir beaucoup de connexions en m√™me temps, ce qui peut saturer Aurora.\nAmazon RDS Proxy est un service g√©r√© qui sert d‚Äôinterm√©diaire : Lambda se connecte au proxy, et le proxy g√®re/pool les connexions vers Aurora.\nCela r√©duit le nombre de connexions directes √† la base, am√©liore la stabilit√© lors des surcharges et g√®re mieux l‚Äôauthentification/rotation des secrets.\nUn NAT Gateway sert √† sortir d‚Äôun VPC vers Internet, pas √† entrer dans un VPC depuis une Lambda hors VPC.\nRendre Aurora publique (option C) est une mauvaise pratique de s√©curit√© et ne r√©sout pas le probl√®me de pics de connexions.\nMettre Lambda dans un VPC (option D) donnerait l‚Äôacc√®s r√©seau, mais sans RDS Proxy vous risquez toujours l‚Äô√©puisement des connexions lors des surges.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:451:1bcdc27e13688541",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 451,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is using an AWS Lambda function to process data. The developer needs to extract custom metrics about processing times from the Lambda logs. The developer needs to analyze the metrics, set alarms, and detect issues in real time.Which solution will meet these requirements?",
      "choices": {
        "A": "Publish custom metric data to AWS CloudTrail by using the PutMetricData API operation. Classify and collect the metrics. Create graphs and alarms in CloudTrail for the custom metrics.",
        "B": "Use the open source client libraries provided by Amazon to generate the logs in the Amazon CloudWatch embedded metric format. Use CloudWatch to create the required graphs and alarms for the custom metrics.",
        "C": "Use Amazon CloudWatch Logs Insights to create custom metrics by querying the logs that come from the Lambda function. Use CloudWatch to create the required graphs and alarms for the custom metrics.",
        "D": "Create an Amazon Kinesis data stream to stream log events in real time from Lambda. Specify an Amazon S3 bucket as the destination for the Kinesis data stream. Use Amazon CloudWatch to visualize the log data and to set alarms."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/150775-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 5, 2024, 12:25 p.m.",
      "textHash": "1bcdc27e13688541",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Objectif : extraire des m√©triques (ex. temps de traitement) depuis les logs Lambda et les utiliser en temps r√©el pour graphiques et alarmes.\nAWS Lambda ex√©cute du code et √©crit des logs dans Amazon CloudWatch Logs (service de journaux).\nAmazon CloudWatch (service de supervision) sait cr√©er des m√©triques, des tableaux de bord et des alarmes, mais il faut lui fournir des m√©triques ‚Äúpropres‚Äù.\nLa solution B utilise le format ‚ÄúEmbedded Metric Format (EMF)‚Äù: on √©crit dans les logs un JSON sp√©cial qui contient √† la fois le message et les m√©triques.\nCloudWatch lit automatiquement ces m√©triques depuis les logs et les transforme en vraies m√©triques CloudWatch, quasi en temps r√©el.\nEnsuite, on peut cr√©er facilement des graphes et des alarmes CloudWatch sur ces m√©triques (p95, moyenne, seuils, etc.).\nA est faux car CloudTrail sert √† auditer des appels API, pas √† stocker/graphes de m√©triques.\nC (Logs Insights) est surtout pour requ√™ter/analyser apr√®s coup; ce n‚Äôest pas le meilleur pour des alarmes temps r√©el bas√©es sur des m√©triques continues.\nD ajoute Kinesis/S3 inutilement et ne cr√©e pas directement des m√©triques exploitables pour alarmes.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un match de basket au lyc√©e. Tu veux mesurer le temps que chaque joueur met pour faire une action (dribble‚Üítir) et √™tre alert√© tout de suite si quelqu‚Äôun devient trop lent.**\n\nLambda, c‚Äôest comme un joueur qui fait des actions automatiquement. Les ‚Äúlogs‚Äù, c‚Äôest le carnet o√π il note tout ce qu‚Äôil fait. Toi, tu veux des ‚Äúm√©triques‚Äù = des chronos clairs (temps de traitement), pas juste des phrases dans un carnet. La r√©ponse B dit: √©cris directement dans le carnet avec un format sp√©cial qui transforme les notes en chronos utilisables (embedded metric format). Ensuite CloudWatch, c‚Äôest le tableau d‚Äôaffichage: il fait des graphiques et d√©clenche une alarme en temps r√©el si le chrono d√©passe une limite. A est faux: CloudTrail, c‚Äôest plut√¥t la liste des ‚Äúqui a fait quoi‚Äù (audit), pas un tableau de chronos. C peut analyser des logs apr√®s coup avec des requ√™tes, moins direct pour du temps r√©el. D ajoute une ‚Äúlivraison‚Äù compliqu√©e (stream + stockage) inutile pour juste faire des m√©triques et des alertes.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:30:7f04c84800404be5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 30,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that uses Amazon Cognito user pools as an identity provider. The company must secure access to user records. The company has set up multi-factor authentication (MFA). The company also wants to send a login activity notification by email every time a user logs in.What is the MOST operationally efficient solution that meets this requirement?",
      "choices": {
        "A": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Add an Amazon API Gateway API to invoke the function. Call the API from the client side when login confirmation is received.",
        "B": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Add an Amazon Cognito post authentication Lambda trigger for the function.",
        "C": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Create an Amazon CloudWatch Logs log subscription filter to invoke the function based on the login status.",
        "D": "Configure Amazon Cognito to stream all logs to Amazon Kinesis Data Firehose. Create an AWS Lambda function to process the streamed logs and to send the email notification based on the login status of each user."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102904-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 17, 2023, 9:51 a.m.",
      "textHash": "7f04c84800404be5",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Amazon Cognito User Pools g√®re l‚Äôinscription et la connexion des utilisateurs (avec MFA si activ√©). Vous voulez envoyer un email √† chaque connexion : le plus simple est de r√©agir automatiquement au moment o√π Cognito confirme l‚Äôauthentification.\nLe ‚Äúpost authentication trigger‚Äù est un d√©clencheur int√©gr√© de Cognito : apr√®s chaque login r√©ussi, Cognito appelle directement une fonction AWS Lambda.\nAWS Lambda ex√©cute du code sans serveur ; vous y mettez la logique ‚Äúenvoyer un email‚Äù.\nAmazon SES est le service AWS pour envoyer des emails de fa√ßon fiable.\nDonc : Cognito (√©v√©nement de login) ‚Üí d√©clenche Lambda ‚Üí Lambda envoie l‚Äôemail via SES, sans que le client n‚Äôait √† appeler une API.\nA est moins efficace car il faut ajouter API Gateway et d√©pendre du client (risque d‚Äôoubli/contournement).\nC et D passent par des logs/streaming (plus complexe, plus de composants, plus de latence) alors qu‚Äôun trigger Cognito existe d√©j√†.\nLa solution la plus op√©rationnellement efficace est donc d‚Äôutiliser le trigger post-authentication (B).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le lyc√©e : √† l‚Äôentr√©e, le surveillant (Cognito) v√©rifie ton identit√©. Le MFA, c‚Äôest comme montrer ta carte + donner un code re√ßu sur ton t√©l√©phone. Apr√®s √ßa, on veut envoyer un mail √† tes parents √† chaque fois que tu entres.**\n\nConcept : le plus simple, c‚Äôest que le surveillant d√©clenche automatiquement l‚Äôenvoi du mail juste apr√®s t‚Äôavoir laiss√© entrer.\nB : un ‚Äúpost authentication trigger‚Äù, c‚Äôest exactement √ßa : d√®s que la connexion est r√©ussie, Cognito appelle automatiquement une petite fonction (Lambda) qui envoie l‚Äôemail via un service d‚Äôenvoi (SES).\nPourquoi c‚Äôest le mieux : tu n‚Äôajoutes pas d‚Äô√©tapes c√¥t√© √©l√®ve (pas besoin que l‚Äôappli pense √† appeler une API comme en A).\nTu n‚Äôessaies pas de deviner via des journaux compliqu√©s (C) ou un gros tuyau de logs (D).\nDonc B est le plus direct, automatique, et demande le moins de gestion au quotidien.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:303:4c7eabb2938de509",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 303,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs an application on AWS. The application consists of a static website that is hosted on Amazon S3. The application includes Amazon API Gateway APIs that invoke AWS Lambda functions. During a period of high traffic on the application, application users reported that the application was slow at irregular intervals. There were no failed requests.A developer needs to find the slow executions across all the Lambda functions.Which solution will meet these requirements?",
      "choices": {
        "A": "Perform a query across all the Lambda function log groups by using Amazon CloudWatch Logs Insights. Filter on type of report and sort descending by Lambda function execution duration.",
        "B": "Enable AWS CloudTrail Insights on the account where the Lambda functions are running. After CloudTrail Insights has finished processing, review CloudTrail Insights to find the anomalous functions.",
        "C": "Enable AWS X-Ray for all the Lambda functions. Configure an X-Ray insight on a new group that includes all the Lambda functions. After the X-Ray insight has finished processing, review the X-Ray logs.",
        "D": "Set up AWS Glue to crawl through the logs in Amazon CloudWatch Logs for the Lambda functions. Configure an AWS Glue job to transform the logs into a structured format and to output the logs into Amazon S3. Use the Amazon CloudWatch dashboard to visualize the slowest functions based on the duration."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134339-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 22, 2024, 6:54 a.m.",
      "textHash": "4c7eabb2938de509",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "On cherche √† rep√©rer des ex√©cutions lentes ¬´ √† certains moments ¬ª sur plusieurs fonctions Lambda, sans erreurs. Il faut donc un outil de tra√ßage de performance, pas seulement des logs.\nAWS Lambda ex√©cute du code √† la demande; quand c‚Äôest lent, la cause peut √™tre dans le code, un appel r√©seau, une base de donn√©es, ou un service externe.\nAWS X-Ray est un service de ‚Äútracing‚Äù : il suit une requ√™te de bout en bout et mesure le temps pass√© dans chaque √©tape (fonction, appel HTTP, appel AWS, etc.).\nEn activant X-Ray sur toutes les fonctions Lambda, on collecte automatiquement des traces et des dur√©es, m√™me si les requ√™tes r√©ussissent.\nEn cr√©ant un ‚Äúgroup‚Äù X-Ray qui inclut toutes les fonctions, puis en activant X-Ray Insights, X-Ray d√©tecte les anomalies (pics de latence) et met en avant les fonctions/segments responsables.\nA (Logs Insights) peut trier des dur√©es dans les logs, mais ne donne pas une vue ‚Äúrequ√™te de bout en bout‚Äù ni une d√©tection d‚Äôanomalies aussi adapt√©e aux lenteurs irr√©guli√®res.\nB (CloudTrail Insights) vise surtout des anomalies d‚Äôappels API AWS (activit√©), pas la performance d‚Äôex√©cution Lambda.\nD (Glue) est trop lourd et inutile pour ce besoin; on veut un diagnostic rapide de latence, pas un pipeline ETL.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o en ligne : ton site S3, c‚Äôest l‚Äô√©cran du menu (√ßa charge vite). Quand tu cliques ‚ÄúJouer‚Äù, √ßa appelle des ‚Äúpetits bots‚Äù (les fonctions Lambda) via un standard t√©l√©phonique (API Gateway). Parfois, certains bots mettent plus de temps √† r√©pondre, sans planter.**\n\nLe but est de rep√©rer quels ‚Äúbots‚Äù sont lents, m√™me si tout finit par r√©pondre. AWS X-Ray, c‚Äôest comme un mode ‚Äúcam√©ra ralenti + chronom√®tre‚Äù qui suit chaque action de bout en bout et mesure o√π le temps est perdu. En l‚Äôactivant sur toutes les fonctions Lambda, tu vois les traces (le parcours) de chaque appel et la dur√©e exacte. Puis tu mets toutes les fonctions dans un m√™me ‚Äúgroupe‚Äù et tu demandes √† X-Ray de d√©tecter les moments anormalement lents (insight). C‚Äôest pour √ßa que C est la bonne r√©ponse : √ßa trouve les lenteurs irr√©guli√®res sur l‚Äôensemble, pas juste des erreurs. Les autres choix lisent surtout des journaux (logs) ou font des analyses lourdes, mais ne donnent pas aussi bien la vue ‚Äúpar parcours‚Äù et la d√©tection d‚Äôanomalies de lenteur.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:269:fdd751f53224b0e1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 269,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is optimizing an AWS Lambda function and wants to test the changes in production on a small percentage of all traffic. The Lambda function serves requests to a RE ST API in Amazon API Gateway. The developer needs to deploy their changes and perform a test in production without changing the API Gateway URL.Which solution will meet these requirements?",
      "choices": {
        "A": "Define a function version for the currently deployed production Lambda function. Update the API Gateway endpoint to reference the new Lambda function version. Upload and publish the optimized Lambda function code. On the production API Gateway stage, define a canary release and set the percentage of traffic to direct to the canary release. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Publish the API to the canary stage.",
        "B": "Define a function version for the currently deployed production Lambda function. Update the API Gateway endpoint to reference the new Lambda function version. Upload and publish the optimized Lambda function code. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Deploy a new API Gateway stage.",
        "C": "Define an alias on the $LATEST version of the Lambda function. Update the API Gateway endpoint to reference the new Lambda function alias. Upload and publish the optimized Lambda function code. On the production API Gateway stage, define a canary release and set the percentage of traffic to direct to the canary release. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Publish to the canary stage.",
        "D": "Define a function version for the currently deployed production Lambda function. Update the API Gateway endpoint to reference the new Lambda function version. Upload and publish the optimized Lambda function code. Update the API Gateway endpoint to use the $LATEST version of the Lambda function. Deploy the API to the production API Gateway stage."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134269-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 4:36 a.m.",
      "textHash": "fdd751f53224b0e1",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:5d840c4f",
      "frExplanation": "Objectif : tester une nouvelle version de votre code Lambda en production sur une petite partie du trafic, sans changer l‚ÄôURL de l‚ÄôAPI (m√™me stage API Gateway).\nAWS Lambda ex√©cute du code ; une ¬´ version ¬ª est un instantan√© immuable, et un ¬´ alias ¬ª est un nom stable (ex: prod, canary) qui pointe vers une version et peut √™tre d√©plac√©.\nAmazon API Gateway expose une URL ; une ¬´ canary release ¬ª permet, sur le m√™me stage/URL, d‚Äôenvoyer X% des requ√™tes vers une configuration ‚Äúcanary‚Äù et le reste vers la config normale.\nLa bonne approche est donc : API Gateway continue d‚Äôappeler un alias Lambda (r√©f√©rence stable), et le stage prod active un canary pour router un petit pourcentage.\nAinsi, vous publiez le nouveau code (nouvelle version), puis vous faites pointer l‚Äôalias canary vers cette version, tandis que l‚Äôalias principal reste sur l‚Äôancienne.\nAPI Gateway garde la m√™me URL, mais une partie du trafic va vers la nouvelle version via le canary.\nLes options qui utilisent $LATEST sont risqu√©es : $LATEST change √† chaque upload et n‚Äôest pas un d√©ploiement contr√¥l√©/immuable.\nC combine correctement alias Lambda + canary API Gateway pour un test progressif sans changer l‚ÄôURL.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : tout le monde passe par la m√™me porte (l‚ÄôURL de l‚ÄôAPI). Tu veux tester une nouvelle recette de p√¢tes, mais seulement pour 5% des √©l√®ves, sans changer l‚Äôentr√©e de la cantine.**\n\nConcept : tu gardes la m√™me porte d‚Äôentr√©e, mais tu fais un ‚Äútest discret‚Äù en envoyant une petite partie des √©l√®ves vers la nouvelle recette.\nDans AWS : l‚ÄôURL ne doit pas changer, donc on garde le m√™me ‚Äúcouloir‚Äù (le stage de production d‚ÄôAPI Gateway).\nPour choisir qui re√ßoit quelle recette, on utilise une ‚Äú√©tiquette‚Äù stable (un alias Lambda) : c‚Äôest comme un panneau ‚ÄúRecette officielle‚Äù.\nTu mets la nouvelle recette derri√®re cette √©tiquette, sans que les √©l√®ves voient un changement d‚Äôadresse.\nEnsuite, le ‚Äúcanary release‚Äù d‚ÄôAPI Gateway, c‚Äôest le surveillant qui dit : ‚Äú5% vont au stand test, 95% au stand normal‚Äù.\nPourquoi C : il combine alias (panneau stable) + canary (pourcentage de trafic) + m√™me stage/URL.\nPourquoi pas les autres : ils jouent avec des versions/$LATEST ou des nouveaux stages, ce qui rend le test moins propre/stable ou peut impliquer un autre acc√®s.\nDonc C permet de tester en prod sur un petit pourcentage, sans changer l‚ÄôURL.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:2:c0e50712352b58aa",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 2,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is running Amazon EC2 instances in multiple AWS accounts. A developer needs to implement an application that collects all the lifecycle events of the EC2 instances. The application needs to store the lifecycle events in a single Amazon Simple Queue Service (Amazon SQS) queue in the company's main AWS account for further processing.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure Amazon EC2 to deliver the EC2 instance lifecycle events from all accounts to the Amazon EventBridge event bus of the main account. Add an EventBridge rule to the event bus of the main account that matches all EC2 instance lifecycle events. Add the SQS queue as a target of the rule.",
        "B": "Use the resource policies of the SQS queue in the main account to give each account permissions to write to that SQS queue. Add to the Amazon EventBridge event bus of each account an EventBridge rule that matches all EC2 instance lifecycle events. Add the SQS queue in the main account as a target of the rule.",
        "C": "Write an AWS Lambda function that scans through all EC2 instances in the company accounts to detect EC2 instance lifecycle changes. Configure the Lambda function to write a notification message to the SQS queue in the main account if the function detects an EC2 instance lifecycle change. Add an Amazon EventBridge scheduled rule that invokes the Lambda function every minute.",
        "D": "Configure the permissions on the main account event bus to receive events from all accounts. Create an Amazon EventBridge rule in each account to send all the EC2 instance lifecycle events to the main account event bus. Add an EventBridge rule to the main account event bus that matches all EC2 instance lifecycle events. Set the SQS queue as a target for the rule."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102782-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 16, 2023, 9:57 a.m.",
      "textHash": "c0e50712352b58aa",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "On veut r√©cup√©rer automatiquement les √©v√©nements de cycle de vie EC2 (d√©marrage, arr√™t, terminaison, etc.) depuis plusieurs comptes AWS et tout centraliser dans une seule file SQS du compte principal.\nAmazon EventBridge est le service qui capte et route des √©v√©nements AWS. Une ‚Äúevent bus‚Äù (bus d‚Äô√©v√©nements) peut recevoir des √©v√©nements d‚Äôautres comptes si on lui donne l‚Äôautorisation.\nLa bonne approche est donc : chaque compte envoie ses √©v√©nements EC2 vers le bus EventBridge du compte principal (cross-account).\nEnsuite, dans le compte principal, une r√®gle EventBridge filtre ‚Äútous les √©v√©nements EC2 lifecycle‚Äù et les envoie vers la cible SQS.\nC‚Äôest exactement ce que d√©crit D : autoriser le bus principal √† recevoir, cr√©er une r√®gle dans chaque compte pour transf√©rer, puis une r√®gle centrale pour d√©poser dans SQS.\nA est faux car EC2 n‚Äô‚Äúenvoie‚Äù pas directement vers un bus d‚Äôun autre compte sans config cross-account explicite via EventBridge.\nB est incomplet/risqu√© : m√™me si SQS peut autoriser l‚Äô√©criture, EventBridge doit aussi √™tre autoris√© √† cibler une file cross-account, et on ne centralise pas proprement via un bus principal.\nC est inefficace : scanner toutes les minutes ne capte pas les √©v√©nements en temps r√©el et co√ªte plus cher/complexe que l‚Äô√©v√©nementiel natif.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un lyc√©e avec plusieurs b√¢timents (chaque b√¢timent = un compte AWS). Dans chaque b√¢timent, des √©l√®ves entrent/sortent de cours (d√©marrage/arr√™t d‚Äôune machine). Le proviseur veut que TOUS les changements arrivent dans UNE bo√Æte aux lettres au bureau central (une seule file SQS) pour les traiter.**\n\nConcept : au lieu d‚Äôaller v√©rifier chaque b√¢timent, on met des ‚Äúsurveillants‚Äù qui envoient automatiquement chaque entr√©e/sortie vers le bureau central. Dans AWS, ces ‚Äúannonces‚Äù sont des √©v√©nements, et EventBridge est le syst√®me de diffusion. Pourquoi D : on autorise d‚Äôabord le bureau central √† recevoir des messages de tous les b√¢timents (permissions du bus central). Puis, dans chaque b√¢timent, on cr√©e une r√®gle qui envoie tous les √©v√©nements ‚Äú√©l√®ve entre/sort‚Äù vers le bus du bureau central. Enfin, au bureau central, une r√®gle attrape ces √©v√©nements et les d√©pose dans la bo√Æte aux lettres unique (SQS). A manque l‚Äôenvoi depuis chaque compte, B vise directement la bo√Æte aux lettres depuis chaque b√¢timent (plus fragile √† g√©rer), C c‚Äôest comme faire une ronde toutes les minutes (lent et inutile).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:556:0989febd460354d5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 556,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company‚Äôs developer needs to activate Amazon CloudWatch Logs Insights for an application‚Äôs AWS Lambda functions. The company uses an AWS Serverless Application Model (AWS SAM) template to deploy the application. The SAM template includes a logical resource that is named CloudWatchLogGroup.How should the developer modify the SAM template to activate CloudWatch Logs Insights for the Lambda functions?",
      "choices": {
        "A": "Add an output named CloudWatchinsightRule that contains a value of the Amazon Resource Name (ARN) for the CloudWatchLogGroup resource.",
        "B": "Add a parameter named CloudWatchLogGroupNamePrefix that contains a value of the application name. Reference the new parameter in the CloudWatchLogGroup resource.",
        "C": "For each Lambda function, add the layer for the Lambda Insights extension and the CloudWatchLambdaInsightsExecutionRolePolicy AWS managed policy.",
        "D": "For each Lambda function, set Tracing mode to Active and add the CloudWatchLambdaInsightsExecutionRolePolicy AWS managed policy."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/303273-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 4, 2025, 6:11 p.m.",
      "textHash": "0989febd460354d5",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "CloudWatch est le service AWS qui collecte des m√©triques et des logs. CloudWatch Logs Insights est l‚Äôoutil qui permet d‚Äôanalyser ces logs avec des requ√™tes.\nPour les fonctions AWS Lambda, ‚Äúactiver Insights‚Äù dans ce contexte signifie activer Lambda Insights, qui ajoute des m√©triques et informations d√©taill√©es (CPU, m√©moire, dur√©e, etc.) et facilite l‚Äôanalyse dans CloudWatch.\nPour activer Lambda Insights, il faut ajouter une extension (fournie sous forme de ‚ÄúLambda Layer‚Äù) √† chaque fonction Lambda.\nIl faut aussi donner des permissions √† la fonction pour envoyer ces donn√©es √† CloudWatch, via la policy g√©r√©e AWS ‚ÄúCloudWatchLambdaInsightsExecutionRolePolicy‚Äù.\nDonc la modification correcte du template SAM est d‚Äôajouter, pour chaque fonction, le Layer Lambda Insights + la policy sur le r√¥le d‚Äôex√©cution.\nA ne fait qu‚Äôexposer un ARN en sortie, √ßa n‚Äôactive rien.\nB ne change qu‚Äôun nom/pr√©fixe de Log Group, √ßa n‚Äôactive pas Insights.\nD active le tracing (AWS X-Ray) et non Lambda Insights; ce sont deux fonctionnalit√©s diff√©rentes.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une √©quipe de basket au lyc√©e. Les matchs (tes fonctions Lambda) se jouent souvent et vite. Tu as d√©j√† un cahier de scores (CloudWatchLogGroup) o√π on note ce qui s‚Äôest pass√©. Mais tu veux des stats avanc√©es (CloudWatch Logs Insights) : tirs r√©ussis, temps de jeu, erreurs, etc. Pour √ßa, il faut √©quiper chaque joueur d‚Äôun capteur + donner l‚Äôautorisation au coach de lire les stats.**\n\nConcept : les ‚Äúlogs‚Äù = le cahier o√π on √©crit tout. ‚ÄúLogs Insights‚Äù = l‚Äôoutil qui analyse ce cahier pour sortir des infos utiles.\nPourquoi C : ajouter une ‚Äúlayer‚Äù Lambda Insights, c‚Äôest comme mettre le capteur sur chaque joueur pour collecter les bonnes donn√©es. Ajouter la policy (autorisation) au r√¥le, c‚Äôest comme donner au coach le droit d‚Äôouvrir et d‚Äôutiliser les stats.\nPourquoi pas A/B : √©crire un nom ou afficher un identifiant du cahier ne cr√©e pas les stats, √ßa ne fait que pointer vers le cahier.\nPourquoi pas D : activer le ‚Äútracing‚Äù c‚Äôest plut√¥t suivre le trajet d‚Äôune action (comme une cam√©ra sur le ballon), pas analyser les logs. Donc C est la seule option qui installe l‚Äôoutil + les droits.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:63:70efc705c0f93f6c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 63,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is incorporating AWS X-Ray into an application that handles personal identifiable information (PII). The application is hosted on Amazon EC2 instances. The application trace messages include encrypted PII and go to Amazon CloudWatch. The developer needs to ensure that no PII goes outside of the EC2 instances.Which solution will meet these requirements?",
      "choices": {
        "A": "Manually instrument the X-Ray SDK in the application code.",
        "B": "Use the X-Ray auto-instrumentation agent.",
        "C": "Use Amazon Macie to detect and hide PII. Call the X-Ray API from AWS Lambda.",
        "D": "Use AWS Distro for Open Telemetry."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103955-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 26, 2023, 2:25 p.m.",
      "textHash": "70efc705c0f93f6c",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Objectif : utiliser AWS X-Ray (outil de tra√ßage) sans jamais laisser sortir des donn√©es personnelles (PII) des instances EC2.\nAvec X-Ray, les ‚Äútraces‚Äù sont envoy√©es √† un service AWS pour analyse ; si une trace contient de la PII, elle sort de l‚Äôinstance.\nLa seule fa√ßon fiable d‚Äôemp√™cher cela est de contr√¥ler exactement ce qui est mis dans les traces.\nL‚Äôinstrumentation manuelle du SDK X-Ray signifie que vous ajoutez vous-m√™me le code de tra√ßage et vous pouvez supprimer/masquer toute PII avant l‚Äôenvoi.\nM√™me si la PII est chiffr√©e, elle quitterait quand m√™me l‚ÄôEC2 si elle est incluse dans la trace, ce qui est interdit ici.\nL‚Äôauto-instrumentation et OpenTelemetry capturent souvent automatiquement des champs (headers, param√®tres, payloads) : risque de fuite de PII.\nAmazon Macie sert surtout √† d√©tecter la PII dans S3, pas √† ‚Äúnettoyer‚Äù des traces X-Ray en temps r√©el.\nDonc la bonne r√©ponse est A : instrumenter manuellement pour ne jamais tracer de PII.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine ton √©cole : tu veux noter le trajet d‚Äôun √©l√®ve (pour comprendre o√π il perd du temps), mais tu ne veux JAMAIS que son nom ou son adresse sorte de la salle de classe.**\n\nAWS X-Ray, c‚Äôest comme un carnet qui note le ‚Äúparcours‚Äù d‚Äôune demande dans ton appli (qui fait quoi, combien de temps). Ici, les messages de suivi partent vers un tableau d‚Äôaffichage dehors (CloudWatch). Probl√®me : m√™me si les infos perso (PII) sont chiffr√©es, elles sortent quand m√™me de la salle (EC2), donc c‚Äôest interdit. La seule fa√ßon s√ªre est de choisir exactement ce que tu √©cris dans le carnet : c‚Äôest A, ‚Äúinstrumenter manuellement‚Äù, donc tu enl√®ves/masques toute PII avant d‚Äôenvoyer les traces. B et D ajoutent du suivi automatiquement : risque qu‚Äôils capturent des infos perso sans que tu contr√¥les tout. C parle d‚Äôun ‚Äúsurveillant‚Äù (Macie) et d‚Äôun autre endroit (Lambda), mais √ßa n‚Äôemp√™che pas la PII de sortir : elle serait d√©j√† partie. Donc A est la bonne r√©ponse.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:52:80458dcc33a72fae",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 52,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn Amazon Kinesis Data Firehose delivery stream is receiving customer data that contains personally identifiable information. A developer needs to remove pattern-based customer identifiers from the data and store the modified data in an Amazon S3 bucket.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Implement Kinesis Data Firehose data transformation as an AWS Lambda function. Configure the function to remove the customer identifiers. Set an Amazon S3 bucket as the destination of the delivery stream.",
        "B": "Launch an Amazon EC2 instance. Set the EC2 instance as the destination of the delivery stream. Run an application on the EC2 instance to remove the customer identifiers. Store the transformed data in an Amazon S3 bucket.",
        "C": "Create an Amazon OpenSearch Service instance. Set the OpenSearch Service instance as the destination of the delivery stream. Use search and replace to remove the customer identifiers. Export the data to an Amazon S3 bucket.",
        "D": "Create an AWS Step Functions workflow to remove the customer identifiers. As the last step in the workflow, store the transformed data in an Amazon S3 bucket. Set the workflow as the destination of the delivery stream."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103922-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 26, 2023, 3:31 a.m.",
      "textHash": "80458dcc33a72fae",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Kinesis Data Firehose sert √† recevoir des donn√©es en continu et √† les d√©poser automatiquement dans une destination (souvent Amazon S3, un stockage de fichiers/objets). Ici, les donn√©es contiennent des informations personnelles (PII) qu‚Äôil faut supprimer avant stockage. Firehose propose une fonction int√©gr√©e de ‚Äútransformation‚Äù via AWS Lambda (un code qui s‚Äôex√©cute sans serveur √† g√©rer). On √©crit une Lambda qui d√©tecte les identifiants selon un motif (regex) et les retire, puis Firehose envoie uniquement les donn√©es nettoy√©es vers S3. C‚Äôest simple, automatique, et fait exactement ‚Äúmodifier puis stocker‚Äù. EC2 obligerait √† g√©rer des serveurs et n‚Äôest pas une destination Firehose standard pour ce besoin. OpenSearch sert √† rechercher/analyser, pas √† nettoyer en flux puis stocker dans S3. Step Functions orchestre des √©tapes, mais Firehose ne l‚Äôutilise pas comme destination directe pour transformer chaque enregistrement.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une cantine o√π arrivent des plateaux avec des √©tiquettes (nom, num√©ro d‚Äô√©l√®ve). Avant de ranger les plateaux dans le grand stock (r√©serve), tu dois enlever automatiquement les √©tiquettes pour prot√©ger la vie priv√©e.**\n\nKinesis Data Firehose, c‚Äôest le tapis roulant qui am√®ne les plateaux (les donn√©es) en continu.\nAmazon S3, c‚Äôest la grande r√©serve o√π on stocke tout.\nOn veut enlever des infos perso ‚Äúreconnaissables par un motif‚Äù (ex: un num√©ro qui ressemble √† 123-45-6789) avant de stocker.\nLa meilleure solution est A : on ajoute une √©tape automatique sur le tapis roulant.\nAWS Lambda, c‚Äôest comme un petit robot qui se d√©clenche tout seul √† chaque plateau.\nOn lui dit : ‚Äúrep√®re ces motifs et efface-les‚Äù, puis Firehose envoie le plateau nettoy√© dans S3.\nB, c‚Äôest embaucher un √©l√®ve (un serveur) √† surveiller tout le temps : plus compliqu√© et co√ªteux.\nC et D, c‚Äôest utiliser des outils faits pour autre chose (recherche, orchestration) alors qu‚Äôon veut juste nettoyer vite avant stockage.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:538:f34a42929569b2f1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 538,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company generates SSL certificates from a third-party provider. The company imports the certificates into AWS Certificate Manager (ACM) to use with public web applications.A developer must implement a solution to notify the company‚Äôs security team 90 days before an imported certificate expires. The company already has configured an Amazon Simple Queue Service (Amazon SQS) queue. The company also has configured an Amazon Simple Notification Service (Amazon SNS) topic that has the security team‚Äôs email address as a subscriber.Which solution will provide the security team with the required notification about certificates?",
      "choices": {
        "A": "Create an Amazon EventBridge rule that specifies the ACM Certificate Approaching Expiration event type. Set the SNS topic as the EventBridge rule‚Äôs target.",
        "B": "Create an AWS Lambda function to search for all certificates that are expiring within 90 days. Program the Lambda function to send each identified certificate‚Äôs Amazon Resource Name (ARN) in a message to the SQS queue.",
        "C": "Create an AWS Step Functions workflow that is invoked by each certificate‚Äôs expiration notification from AWS CloudTrail. Create an AWS Lambda function to send each certificate's Amazon Resource Name (ARN) in a message to the SQS queue.",
        "D": "Configure AWS Config with the acm-certificate-expiration-check managed rule to run every 24 hours. Create an Amazon EventBridge rule that includes an event pattern that specifies the Config Rules Compliance Change detail type and the configured rule. Set the SNS topic as the EventBridge rule‚Äôs target."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156029-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 6, 2025, 1:07 p.m.",
      "textHash": "f34a42929569b2f1",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Objectif : pr√©venir l‚Äô√©quipe s√©curit√© 90 jours avant l‚Äôexpiration de certificats SSL import√©s dans ACM (AWS Certificate Manager, service qui stocke/sert des certificats).\nACM n‚Äôenvoie pas automatiquement un √©v√©nement ‚Äúapproche d‚Äôexpiration‚Äù pour les certificats import√©s comme on le ferait pour des certificats g√©r√©s ; il faut donc un contr√¥le r√©gulier.\nAWS Config est un service qui v√©rifie en continu si vos ressources respectent des r√®gles (conformit√©).\nLa r√®gle g√©r√©e ‚Äúacm-certificate-expiration-check‚Äù d√©tecte les certificats ACM qui vont expirer bient√¥t (dont ceux import√©s) et marque la ressource NON_COMPLIANT.\nEn la faisant tourner toutes les 24h, on d√©tecte √† temps l‚Äô√©tat ‚Äúexpire dans 90 jours‚Äù.\nEnsuite, Amazon EventBridge peut √©couter les changements de conformit√© de Config (√©v√©nement ‚ÄúConfig Rules Compliance Change‚Äù).\nQuand la r√®gle passe √† NON_COMPLIANT, EventBridge d√©clenche une action.\nEn mettant le topic SNS en cible, SNS envoie l‚Äôemail au groupe s√©curit√© (SNS = notifications, ici email).\nLes options avec SQS/Lambda/Step Functions ajoutent du code et ne garantissent pas un signal natif fiable d‚Äôexpiration, et CloudTrail ne fournit pas une ‚Äúnotification d‚Äôexpiration‚Äù de certificat.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:382:57d5fd6c03c19852",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 382,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer wants the ability to roll back to a previous version of an AWS Lambda function in the event of errors caused by a new deployment.How can the developer achieve this with MINIMAL impact on users?",
      "choices": {
        "A": "Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to use the newly deployed version. If too many errors are encountered, point the alias back to the previous version.",
        "B": "Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to direct 10% of users to the newly deployed version. If too many errors are encountered, send 100% of traffic to the previous version.",
        "C": "Do not make any changes to the application. Deploy the new version of the code. If too many errors are encountered, point the application back to the previous version using the version number in the Amazon Resource Name (ARN).",
        "D": "Create three aliases: new, existing, and router. Point the existing alias to the current version. Have the router alias direct 100% of users to the existing alias. Update the application to use the router alias. Deploy the new version of the code. Point the new alias to this version. Update the router alias to direct 10% of users to the new alias. If too many errors are encountered, send 100% of traffic to the existing alias."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143722-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 11, 2024, 2:49 p.m.",
      "textHash": "57d5fd6c03c19852",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:d8bbfc8e",
      "frExplanation": "AWS Lambda ex√©cute du code ‚Äúsans serveur‚Äù. Chaque d√©ploiement peut cr√©er une nouvelle ‚Äúversion‚Äù immuable (ex: v1, v2). Un ‚Äúalias‚Äù est un nom stable (ex: prod) qui pointe vers une version, et peut aussi r√©partir le trafic entre deux versions.\nPour minimiser l‚Äôimpact sur les utilisateurs, on fait un d√©ploiement progressif (canary): on envoie d‚Äôabord une petite partie du trafic vers la nouvelle version pour d√©tecter des erreurs sans toucher tout le monde.\nLa r√©ponse B fait exactement cela: l‚Äôapplication appelle l‚Äôalias (stable), on d√©ploie la nouvelle version, puis on configure l‚Äôalias pour envoyer 10% des requ√™tes vers la nouvelle version.\nSi les erreurs augmentent, on remet imm√©diatement 100% du trafic sur l‚Äôancienne version via l‚Äôalias, sans changer le code client ni red√©ployer.\nA bascule tout le trafic d‚Äôun coup (impact plus grand). C n√©cessite de modifier l‚ÄôARN/version c√¥t√© application (plus risqu√© et moins ‚Äúminimal‚Äù). D est plus complexe que n√©cessaire (plus d‚Äôalias) pour le m√™me r√©sultat.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:550:e34e3c8be3d94c11",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 550,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application that stores sensitive user data. The application includes an Amazon CloudFront distribution and multiple AWS Lambda functions that handle user requests.The user requests contain over 20 data fields. Each application transaction contains sensitive data that must be encrypted. Only specific parts of the application need to have the ability to decrypt the data.Which solution will meet these requirements?",
      "choices": {
        "A": "Associate the CloudFront distribution with a Lambda@Edge function. Configure the function to perform field-level asymmetric encryption by using a user-defined RSA public key that is stored in AWS Key Management Service (AWS KMS).",
        "B": "Integrate AWS WAF with CloudFront to protect the sensitive data. Use a Lambda function and self-managed keys to perform the encryption and decryption processes.",
        "C": "Configure the CloudFront distribution to use WebSockets by forwarding all viewer request headers to the origin. Create an asymmetric AWS KMS key. Configure the CloudFront distribution to use field-level encryption. Use the AWS KMS key.",
        "D": "Configure the cache behavior in the CloudFront distribution to require HTTPS for communication between viewers and CloudFront. Configure GoudFront to require users to access the files by using either signed URLs or signed cookies."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157510-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 4, 2025, 2:14 p.m.",
      "textHash": "e34e3c8be3d94c11",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "On veut chiffrer des champs pr√©cis (plus de 20) dans les requ√™tes utilisateur, et seuls certains composants doivent pouvoir d√©chiffrer. CloudFront propose le ¬´ field-level encryption ¬ª : il chiffre uniquement certains champs avant d‚Äôenvoyer la requ√™te vers l‚Äôorigine, donc les donn√©es sensibles ne circulent pas en clair.\nLe chiffrement asym√©trique (cl√© publique/cl√© priv√©e) est id√©al ici : CloudFront (ou du code proche de CloudFront) utilise la cl√© publique pour chiffrer, mais seule la partie autoris√©e qui poss√®de la cl√© priv√©e peut d√©chiffrer.\nLambda@Edge est une fonction Lambda ex√©cut√©e au plus pr√®s de CloudFront, pratique pour appliquer une logique de s√©curit√© sur les requ√™tes/r√©ponses.\nLe choix A correspond : une fonction Lambda@Edge applique le chiffrement de champs avec une cl√© publique RSA ; la cl√© priv√©e reste prot√©g√©e et seuls les services autoris√©s peuvent d√©chiffrer.\nB est trop vague et AWS WAF sert surtout √† filtrer/bloquer des requ√™tes, pas √† chiffrer des champs. C est incorrect car CloudFront field-level encryption n‚Äôutilise pas directement une cl√© KMS asym√©trique comme cl√© de chiffrement de champs. D ne chiffre pas les champs : HTTPS et URLs/cookies sign√©s contr√¥lent l‚Äôacc√®s et le transport, pas le chiffrement s√©lectif des donn√©es.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : chaque √©l√®ve donne une fiche avec 20 infos (nom, allergies, num√©ro, etc.). Certaines infos sont ultra priv√©es. Tu veux les mettre dans des enveloppes ferm√©es, et seuls quelques adultes autoris√©s peuvent les ouvrir.**\n\nConcept : on chiffre (on ‚Äúferme l‚Äôenveloppe‚Äù) chaque champ sensible s√©par√©ment, pas toute la fiche d‚Äôun coup. Comme √ßa, seuls les endroits qui en ont besoin peuvent ouvrir seulement ce qu‚Äôil faut.\nPourquoi A : CloudFront, c‚Äôest comme le guichet √† l‚Äôentr√©e qui re√ßoit les fiches. Lambda@Edge, c‚Äôest un surveillant plac√© directement au guichet : il peut fermer les enveloppes avant que la fiche parte plus loin.\nLe chiffrement ‚Äúasym√©trique‚Äù avec une cl√© publique RSA, c‚Äôest comme distribuer un cadenas sp√©cial : tout le monde peut fermer le cadenas (cl√© publique), mais seuls les adultes autoris√©s ont la cl√© priv√©e pour l‚Äôouvrir.\nAWS KMS, c‚Äôest l‚Äôarmoire forte du lyc√©e qui garde les cl√©s en s√©curit√©.\nDonc A permet : chiffrer des champs pr√©cis + d√®s l‚Äôentr√©e + seuls certains services peuvent d√©chiffrer.\nB parle de protection contre attaques, pas de chiffrer des champs. C m√©lange des options inutiles ici. D s√©curise le trajet (HTTPS) mais ne chiffre pas les champs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:279:6e80a35dd2f917b0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 279,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA photo sharing application uses Amazon S3 to store image files. All user images are manually audited for inappropriate content by a third-party company. The audits are completed 1-24 hours after user upload and the results are written to an Amazon DynamoDB table, which uses the S3 object key as a primary key. The database items can be queried by using a REST API created by the third-party company.An application developer needs to implement an automated process to tag all S3 objects with the results of the content audit.What should the developer do to meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "A": "Create an AWS Lambda function to run in response to the s3:ObjectCreated event type. Write the S3 key to an Amazon Simple Queue Service (Amazon SQS) queue with a visibility timeout of 24 hours. Create and configure a second Lambda function to read items from the queue. Retrieve the results for each item from the DynamoDB table. Tag each S3 object accordingly.",
        "B": "Create an AWS Lambda function to run in response to the s3:ObjectCreated event type. Integrate the function into an AWS Step Functions standard workflow. Define an AWS Step Functions Wait state and set the value to 24 hours. Create and configure a second Lambda function to retrieve the audit results and tag the S3 objects accordingly after the Wait state is over.",
        "C": "Create an AWS Lambda function to load all untagged S3 objects. Retrieve the results for each item from the REST API and tag each S3 object accordingly. Create and configure an Amazon EventBridge rule to run at regular intervals. Set the Lambda function as a target for the EventBridge rule.",
        "D": "Launch an Amazon EC2 instance. Deploy a script to the EC2 instance to use the external database results to tag the S3 objects accordingly. Configure a crontab file to run the script at regular intervals."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134278-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:40 a.m.",
      "textHash": "6e80a35dd2f917b0",
      "rawFormat": "discussion-md",
      "conceptKey": "sqs_visibility_timeout",
      "frExplanation": "On veut ajouter automatiquement des tags aux fichiers S3 apr√®s qu‚Äôun audit externe ait √©crit le r√©sultat dans DynamoDB. Le probl√®me: l‚Äôaudit arrive avec un d√©lai variable (1 √† 24 h), donc on ne peut pas taguer imm√©diatement √† l‚Äôupload.\nS3 est le stockage d‚Äôobjets (images). DynamoDB est une base NoSQL o√π chaque r√©sultat est stock√© avec comme cl√© l‚Äô‚Äúobject key‚Äù S3.\nLa solution la plus simple √† op√©rer est d‚Äôorchestrer l‚Äôattente et l‚Äôaction: AWS Step Functions permet de cha√Æner des √©tapes et d‚Äôajouter un √©tat ‚ÄúWait‚Äù (attendre) sans serveur √† g√©rer.\nAvec B: un √©v√©nement s3:ObjectCreated d√©clenche une Lambda, qui d√©marre un workflow Step Functions. Le workflow attend (jusqu‚Äô√† 24 h), puis appelle une seconde Lambda.\nCette seconde Lambda lit DynamoDB (avec la cl√© S3) pour r√©cup√©rer le verdict, puis applique les tags sur l‚Äôobjet S3.\nC‚Äôest op√©rationnellement efficace: pas de serveur EC2, pas de cron, pas de scans r√©guliers de tout S3, et l‚Äôattente est g√©r√©e nativement et de fa√ßon fiable.\nA est plus complexe (SQS + visibilit√© 24 h n‚Äôest pas fait pour ‚Äúattendre un r√©sultat‚Äù et peut n√©cessiter des retries/gestion d‚Äô√©checs). C et D font du polling p√©riodique et co√ªtent/complexifient inutilement.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un coll√®ge o√π les √©l√®ves d√©posent des dessins dans une bo√Æte (S3). Une entreprise externe les v√©rifie contre les contenus interdits et note le r√©sultat dans un cahier (DynamoDB) plus tard, entre 1 et 24h.**\n\nConcept : tu veux un syst√®me qui se d√©clenche quand un dessin arrive, puis qui attend, puis qui revient mettre une √©tiquette ‚ÄúOK‚Äù ou ‚ÄúInterdit‚Äù sur le dessin.\nR√©ponse B : une petite ‚Äúmain automatique‚Äù (Lambda) se lance √† chaque d√©p√¥t. Elle d√©marre un ‚Äúsc√©nario‚Äù (Step Functions = une liste d‚Äô√©tapes).\nDans ce sc√©nario, il y a une √©tape ‚ÄúAttendre‚Äù (Wait) jusqu‚Äô√† 24h, comme dire : ‚Äúreviens plus tard‚Äù.\nApr√®s l‚Äôattente, une 2e main automatique va lire le cahier (DynamoDB) avec la cl√© du dessin (S3 object key) et colle la bonne √©tiquette sur le dessin (tag S3).\nC‚Äôest le plus efficace : pas besoin de tourner en rond toutes les heures pour v√©rifier, ni de garder un ordinateur allum√© (EC2).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:26:6b840cc5e97c51ca",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 26,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an application that will be deployed on IoT devices. The application will send data to a RESTful API that is deployed as an AWS Lambda function. The application will assign each API request a unique identifier. The volume of API requests from the application can randomly increase at any given time of day.During periods of request throttling, the application might need to retry requests. The API must be able to handle duplicate requests without inconsistencies or data loss.Which solution will meet these requirements?",
      "choices": {
        "A": "Create an Amazon RDS for MySQL DB instance. Store the unique identifier for each request in a database table. Modify the Lambda function to check the table for the identifier before processing the request.",
        "B": "Create an Amazon DynamoDB table. Store the unique identifier for each request in the table. Modify the Lambda function to check the table for the identifier before processing the request.",
        "C": "Create an Amazon DynamoDB table. Store the unique identifier for each request in the table. Modify the Lambda function to return a client error response when the function receives a duplicate request.",
        "D": "Create an Amazon ElastiCache for Memcached instance. Store the unique identifier for each request in the cache. Modify the Lambda function to check the cache for the identifier before processing the request."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103656-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2023, 11:48 a.m.",
      "textHash": "6b840cc5e97c51ca",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Ici, l‚Äôapplication IoT peut renvoyer la m√™me requ√™te (retry) quand l‚ÄôAPI est ralentie (throttling). Il faut donc rendre l‚ÄôAPI ¬´ idempotente ¬ª : si la m√™me requ√™te arrive deux fois avec le m√™me identifiant unique, on ne doit pas cr√©er deux fois les m√™mes effets (double √©criture, perte de donn√©es, incoh√©rences).\nAWS Lambda ex√©cute du code √† la demande et peut recevoir des pics soudains. Il faut un stockage qui suit ces pics automatiquement et qui permette une v√©rification rapide ¬´ ai-je d√©j√† trait√© cet identifiant ? ¬ª.\nDynamoDB est une base NoSQL g√©r√©e, tr√®s scalable, avec faible latence, adapt√©e aux charges impr√©visibles. On peut utiliser l‚Äôidentifiant unique comme cl√© primaire et faire un ‚Äúcheck then process‚Äù (ou une √©criture conditionnelle) pour √©viter les doublons.\nRDS MySQL (A) est plus lourd √† g√©rer et peut devenir un goulot d‚Äô√©tranglement lors de pics (connexions, scaling). ElastiCache Memcached (D) est en m√©moire et non durable : si le cache red√©marre/expire, on perd l‚Äôhistorique des identifiants et on risque de retraiter des doublons.\nL‚Äôoption C renvoie une erreur sur doublon, mais le besoin dit ¬´ g√©rer les doublons sans incoh√©rences ni perte ¬ª : il vaut mieux ignorer/traiter de fa√ßon s√ªre plut√¥t que provoquer des erreurs c√¥t√© client.\nDonc la meilleure solution est de stocker les identifiants dans DynamoDB et de v√©rifier avant de traiter : r√©ponse B.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : chaque √©l√®ve re√ßoit un ticket avec un num√©ro unique pour son repas. Parfois, il y a trop de monde, alors certains reviennent au guichet et montrent le m√™me ticket une 2e fois.**\n\nConcept : quand une appli ‚Äúr√©essaie‚Äù, elle peut envoyer deux fois la m√™me demande. Il faut donc un registre fiable des num√©ros d√©j√† servis, pour ne pas donner deux repas ou en oublier un.\nPourquoi B : DynamoDB, c‚Äôest comme un cahier de cantine ultra-rapide qui accepte beaucoup d‚Äô√©l√®ves d‚Äôun coup, m√™me si la foule augmente d‚Äôun coup. La Lambda (le guichet automatique) regarde dans ce cahier : si le num√©ro est d√©j√† not√©, elle ne refait pas l‚Äôaction ; sinon elle traite et note le num√©ro.\nPourquoi pas A : RDS/MySQL, c‚Äôest plut√¥t comme un bureau avec des formulaires plus lourds, moins pratique quand la foule explose.\nPourquoi pas C : renvoyer une ‚Äúerreur‚Äù pour un doublon, c‚Äôest comme dire ‚Äúd√©gage‚Äù √† un √©l√®ve qui a d√©j√† pay√© : √ßa peut casser l‚Äôappli, alors qu‚Äôon veut juste ignorer le doublon sans perte.\nPourquoi pas D : un cache Memcached, c‚Äôest comme un post-it : √ßa peut dispara√Ætre, donc on risque d‚Äôoublier qu‚Äôun ticket a d√©j√† √©t√© servi.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:548:cb92c83c0b459d99",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 548,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a new application that will give users the ability to upload documents to Amazon S3. The contents of the documents must not be accessible to any third party.Which type of encryption will meet this requirement?",
      "choices": {
        "A": "Client-side encryption by using the S3 Encryption Client with a Raw RSA wrapping key that is stored on the user‚Äôs device",
        "B": "Server-side encryption with S3 managed keys (SSE-S3)",
        "C": "Server-side encryption with AWS KMS keys (SSE-KMS)",
        "D": "Dual-layer server-side encryption with AWS KMS keys (DSSE-KMS)"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/154460-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Jan. 13, 2025, 6:20 p.m.",
      "textHash": "cb92c83c0b459d99",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Objectif : que personne d‚Äôautre ne puisse lire les documents stock√©s dans Amazon S3 (service de stockage de fichiers).\nLe chiffrement ‚Äúserver-side‚Äù signifie que S3 chiffre automatiquement le fichier d√®s qu‚Äôil arrive, puis le d√©chiffre seulement pour un utilisateur autoris√©.\nAvec SSE-KMS (r√©ponse C), les cl√©s de chiffrement sont g√©r√©es par AWS Key Management Service (KMS), un service d√©di√© aux cl√©s et aux permissions.\nKMS permet de contr√¥ler pr√©cis√©ment qui peut utiliser la cl√© (via des politiques IAM/KMS) et d‚Äôavoir des journaux d‚Äôaudit (CloudTrail) sur chaque usage de cl√©.\nSSE-S3 (B) chiffre aussi, mais avec des cl√©s g√©r√©es uniquement par S3 : moins de contr√¥le fin et d‚Äôaudit sur l‚Äôusage des cl√©s.\nDSSE-KMS (D) ajoute une deuxi√®me couche de chiffrement : plus fort mais pas n√©cessaire pour l‚Äôexigence ‚Äúpas accessible √† des tiers‚Äù.\nLe chiffrement c√¥t√© client (A) peut marcher, mais impose de g√©rer et prot√©ger la cl√© sur l‚Äôappareil de l‚Äôutilisateur (risque de perte/vol, complexit√©).\nDonc SSE-KMS est le meilleur choix pratique pour emp√™cher l‚Äôacc√®s non autoris√© tout en gardant un contr√¥le et une tra√ßabilit√© solides.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un casier au lyc√©e o√π tu ranges tes documents. Tu veux que personne (autres √©l√®ves, surveillants curieux) ne puisse les lire, sauf toi et le lyc√©e quand il doit g√©rer les cl√©s.**\n\nConcept : chiffrer = mettre tes documents dans une bo√Æte ferm√©e √† cl√©. Sans la cl√©, c‚Äôest illisible.\nAvec l‚Äôoption C (SSE-KMS), c‚Äôest comme si le lyc√©e utilisait un trousseau de cl√©s ultra s√©curis√© (KMS) : tes fichiers sont verrouill√©s automatiquement quand tu les mets dans le casier, et d√©verrouill√©s seulement pour les personnes/autorisations pr√©vues.\nPourquoi c‚Äôest la bonne : les cl√©s sont g√©r√©es dans un ‚Äúcoffre-fort‚Äù sp√©cial, avec contr√¥le strict de qui peut les utiliser, donc un tiers ne peut pas lire le contenu.\nPourquoi pas B : c‚Äôest le lyc√©e qui g√®re les cl√©s ‚Äútout seul‚Äù sans coffre-fort KMS, moins de contr√¥le fin.\nPourquoi pas A : la cl√© est sur l‚Äôappareil de l‚Äôutilisateur ; si elle est perdue/vol√©e, c‚Äôest la catastrophe.\nPourquoi pas D : c‚Äôest deux cadenas au lieu d‚Äôun ; plus fort, mais pas n√©cessaire pour juste ‚Äúpas accessible √† un tiers‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:465:0bc5db27d477e6a6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 465,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company‚Äôs AWS accounts are in an organization in AWS Organizations. An application in Account A uses environment variables that are stored as parameters in AWS Systems Manager Parameter Store. A developer is creating a new application in Account B that needs to use the same environment variables.The application in Account B needs access to the parameters in Account A without duplicating the parameters into Account B.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Configure the application in Account B to use credentials for an IAM user in AccountA that has access to the parameters.",
        "B": "Create an assumable IAM role in Account A. Grant the role the permission to access the parameters.",
        "C": "Configure cross-account resource sharing for the parameters by using AWS Resource Access Manager (AWS RAM).",
        "D": "Write a script that stores the parameter values in a private Amazon S3 bucket that both accounts can access."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152923-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 12, 2024, 7:54 p.m.",
      "textHash": "0bc5db27d477e6a6",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "On a deux comptes AWS (A et B) dans la m√™me organisation. Les ‚Äúvariables d‚Äôenvironnement‚Äù sont stock√©es dans AWS Systems Manager Parameter Store (un coffre de param√®tres/valeurs de configuration).\nLe besoin : l‚Äôapplication du compte B doit lire les m√™mes param√®tres du compte A, sans les recopier (sinon on cr√©e des doublons et des risques d‚Äôincoh√©rence).\nLa solution la plus simple √† op√©rer est de ‚Äúpartager‚Äù la ressource existante entre comptes.\nAWS Resource Access Manager (RAM) sert pr√©cis√©ment √† partager des ressources AWS entre comptes d‚Äôune organisation, avec une gestion centralis√©e des autorisations.\nEn partageant les param√®tres via RAM, le compte B peut y acc√©der sans cr√©er de copies et sans g√©rer des identifiants s√©par√©s.\nA est mauvais : utiliser un utilisateur IAM et ses cl√©s est lourd et risqu√© (rotation, fuite de cl√©s).\nB peut marcher mais demande de g√©rer l‚Äôassume role, les politiques et la logique d‚Äôacc√®s c√¥t√© application (plus d‚Äôeffort).\nD est mauvais : d√©placer des secrets/param√®tres dans S3 ajoute du code, de la maintenance et augmente la surface de s√©curit√©.\nDonc C r√©pond au besoin avec le moins d‚Äôoverhead op√©rationnel.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine deux classes dans le m√™me lyc√©e : la classe A a un classeur avec les ‚Äúcodes‚Äù importants (mots de passe, r√©glages), et la classe B doit utiliser exactement les m√™mes codes sans les recopier.**\n\nDans AWS, chaque ‚Äúcompte‚Äù est comme une classe s√©par√©e, et les ‚Äúparam√®tres‚Äù sont les codes dans le classeur.\nLe but : que la classe B lise le classeur de la classe A, sans faire de photocopies (pas de duplication).\nAWS RAM, c‚Äôest comme un syst√®me officiel du lyc√©e pour ‚Äúpr√™ter‚Äù un document √† une autre classe : tu partages l‚Äôacc√®s, et tout le monde lit la m√™me version.\nDonc C est bon : partage direct, simple √† g√©rer, et si la classe A change un code, la classe B voit la mise √† jour tout de suite.\nA est mauvais : donner le compte d‚Äôun √©l√®ve de la classe A √† la classe B, c‚Äôest risqu√© et p√©nible √† g√©rer.\nB marche mais demande plus de gestion (r√¥les, autorisations) que le partage officiel.\nD est mauvais : recopier les codes dans un autre endroit (bucket), c‚Äôest comme refaire des copies et g√©rer deux versions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:463:8297538eb5fe242e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 463,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application is experiencing performance issues based on increased demand. This increased demand is on read-only historical records pulled from an Amazon RDS-hosted database with custom views and queries. A developer must improve performance without changing the database structure.Which approach will improve performance and MINIMIZE management overhead?",
      "choices": {
        "A": "Deploy Amazon DynamoDB, move all the data, and point to DynamoDB.",
        "B": "Deploy Amazon ElastiCache (Redis OSS) and cache the data for the application.",
        "C": "Deploy Memcached on Amazon EC2 and cache the data for the application.",
        "D": "Deploy Amazon DynamoDB Accelerator (DAX) on Amazon RDS to improve cache performance."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156670-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:23 a.m.",
      "textHash": "8297538eb5fe242e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:3579c2ea",
      "frExplanation": "Le probl√®me vient d‚Äôune forte hausse de lectures sur des donn√©es historiques (lecture seule) dans Amazon RDS (base relationnelle g√©r√©e). Relire sans cesse les m√™mes r√©sultats (vues/requ√™tes) surcharge la base et ralentit l‚Äôapplication.\nLa solution la plus simple est de mettre un cache devant la base : Amazon ElastiCache (Redis) est un service AWS g√©r√© qui stocke en m√©moire les donn√©es/r√©sultats fr√©quemment demand√©s.\nAinsi, l‚Äôapplication lit d‚Äôabord dans Redis (tr√®s rapide). Si la donn√©e n‚Äôy est pas, elle la r√©cup√®re depuis RDS puis la met en cache.\nOn n‚Äôa pas besoin de modifier la structure de la base : on ajoute juste une couche de cache c√¥t√© application.\nPourquoi pas A : migrer vers DynamoDB change compl√®tement le mod√®le de donn√©es et demande beaucoup de travail.\nPourquoi pas C : Memcached sur EC2 implique g√©rer des serveurs (patchs, scaling, pannes) donc plus d‚Äôoverhead.\nPourquoi pas D : DAX est un cache pour DynamoDB, pas pour RDS, donc inadapt√©.\nDonc B minimise l‚Äôadministration tout en am√©liorant fortement les performances de lecture.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e : plein d‚Äô√©l√®ves veulent relire les m√™mes vieux articles (archives) pour un expos√©, mais personne ne les modifie.**\n\nConcept : au lieu d‚Äôaller chercher √† chaque fois le m√™me document au fond des rayons (base de donn√©es), on met les copies les plus demand√©es sur une table pr√®s de l‚Äôentr√©e (cache). √áa acc√©l√®re sans changer les rayons. Ici, les donn√©es sont ‚Äúlecture seule‚Äù et tr√®s demand√©es : parfait pour un cache. B (ElastiCache Redis) = une ‚Äútable de copies‚Äù d√©j√† g√©r√©e par l‚Äô√©cole : tu n‚Äôas presque rien √† entretenir, et l‚Äôappli lit vite. A d√©placer tout vers une autre ‚Äúbiblioth√®que‚Äù (DynamoDB) = gros d√©m√©nagement inutile. C c‚Äôest comme construire et g√©rer toi-m√™me la table et les surveillants (sur EC2) = plus de boulot. D est faux : DAX est un acc√©l√©rateur pour DynamoDB, pas pour une base RDS.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:359:e12dd19407e63bef",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 359,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that uses an AWS Lambda function to process data. A developer must implement encryption in transit for all sensitive configuration data, such as API keys, that is stored in the application. The developer creates an AWS Key Management Service (AWS KMS) customer managed key.What should the developer do next to meet the encryption requirement?",
      "choices": {
        "A": "Create parameters of the String type in AWS Systems Manager Parameter Store. For each parameter, specify the KMS key ID to encrypt the parameter in transit. Reference the GetParameter API call in the Lambda environment variables.",
        "B": "Create secrets in AWS Secrets Manager by using the customer managed KMS key. Create a new Lambda function and set up a Lambda layer. Configure the Lambda layer to retrieve the values from Secrets Manager.",
        "C": "Create objects in Amazon S3 for each sensitive data field. Specify the customer managed KMS key to encrypt the object. Configure the Lambda function to retrieve the objects from Amazon S3 during data processing.",
        "D": "Create encrypted Lambda environment variables. Specify the customer managed KMS key to encrypt the variables. Enable encryption helpers for encryption in transit. Grant permission to the Lambda function's execution role to access the KMS key."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143121-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 30, 2024, 11:54 p.m.",
      "textHash": "e12dd19407e63bef",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, on veut prot√©ger des donn√©es sensibles de configuration (ex: cl√©s API) utilis√©es par une fonction AWS Lambda (un code qui s‚Äôex√©cute sans serveur). La bonne pratique est de les stocker directement dans les variables d‚Äôenvironnement Lambda, mais chiffr√©es.\nAWS KMS est le service qui g√®re des cl√©s de chiffrement. Une ‚Äúcustomer managed key‚Äù est une cl√© KMS que l‚Äôentreprise contr√¥le.\nAvec l‚Äôoption D, on active le chiffrement des variables d‚Äôenvironnement Lambda avec cette cl√© KMS : AWS chiffre les valeurs et ne les d√©chiffre qu‚Äôau moment de l‚Äôex√©cution.\nLe ‚Äúchiffrement en transit‚Äù est assur√© car Lambda r√©cup√®re ces valeurs via des appels AWS internes prot√©g√©s (TLS), et KMS est appel√© de fa√ßon s√©curis√©e.\nIl faut aussi autoriser le r√¥le d‚Äôex√©cution de la Lambda (IAM role) √† utiliser la cl√© KMS, sinon Lambda ne pourra pas d√©chiffrer.\nLes autres choix sont moins adapt√©s : Parameter Store parle surtout de chiffrement au repos (et l‚Äô√©nonc√© A est incorrect sur ‚Äúchiffrer en transit‚Äù via KMS ID), Secrets Manager est possible mais inutilement complexe ici, et S3 n‚Äôest pas fait pour des secrets de configuration.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une salle de classe o√π la Lambda est un √©l√®ve qui fait un devoir automatiquement. Les ‚ÄúAPI keys‚Äù sont des mots de passe ultra secrets √©crits sur un papier. AWS KMS, c‚Äôest le coffre-fort du lyc√©e qui fabrique et garde la cl√© pour fermer ce papier dans une enveloppe scell√©e.**\n\nConcept : on veut que les secrets voyagent et soient stock√©s sans que quelqu‚Äôun puisse les lire, comme un mot de passe toujours dans une enveloppe ferm√©e. Avec D, tu mets les secrets directement dans le ‚Äúcasier‚Äù de l‚Äô√©l√®ve (les variables d‚Äôenvironnement Lambda), mais en version chiffr√©e avec la cl√© du coffre-fort (KMS). Quand l‚Äô√©l√®ve en a besoin, il demande au coffre-fort d‚Äôouvrir l‚Äôenveloppe, et √ßa se fait via un canal s√©curis√© (comme parler au surveillant en priv√©). Il faut aussi donner √† l‚Äô√©l√®ve l‚Äôautorisation d‚Äôutiliser le coffre-fort (permission sur la cl√© KMS), sinon il ne peut pas d√©chiffrer. Les autres choix d√©placent les secrets ailleurs (biblioth√®que/S3, autre service) ou parlent mal du ‚Äúchiffrement en transit‚Äù pour ce cas. Donc D r√©pond exactement : secrets au bon endroit + chiffr√©s avec KMS + autorisations.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:505:216b93c6d682c2e5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 505,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is creating a new application that gives users the ability to upload and share short video files. The average size of the video files is 10 MB. After a user uploads a file, a message needs to be placed into an Amazon Simple Queue Service (Amazon SQS) queue so the file can be processed. The files need to be accessible for processing within 5 minutes.Which solution will meet these requirements MOST cost-effectively?",
      "choices": {
        "A": "Write the files to Amazon S3 Glacier Deep Archive. Add the S3 location of the files to the SQS queue.",
        "B": "Write the files to Amazon S3 Standard. Add the S3 location of the files to the SQS queue.",
        "C": "Write the files to an Amazon Elastic Block Store (Amazon EBS) General Purpose SSD volume. Add the EBS location of the files to the SQS queue.",
        "D": "Write messages that contain the contents of the uploaded files to the SQS queue."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152777-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 10, 2024, 4:36 a.m.",
      "textHash": "216b93c6d682c2e5",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "On veut stocker des vid√©os (10 Mo) puis envoyer un message dans SQS pour d√©clencher un traitement sous 5 minutes.\nAmazon S3 est un stockage d‚Äôobjets (fichiers) tr√®s √©conomique et con√ßu pour partager des fichiers via une URL/chemin (bucket + cl√©).\nSQS est une file de messages : on y met un petit message (ex. l‚Äôemplacement du fichier), pas le fichier lui‚Äëm√™me.\nS3 Standard donne un acc√®s imm√©diat aux fichiers (millisecondes), donc largement dans la contrainte des 5 minutes.\nS3 Glacier Deep Archive est fait pour l‚Äôarchivage tr√®s long terme : la r√©cup√©ration prend des heures, donc ne respecte pas le d√©lai.\nEBS est un disque attach√© √† une instance EC2 : plus cher et moins adapt√© au partage/traitement distribu√© de fichiers upload√©s.\nMettre le contenu du fichier dans SQS ne marche pas : SQS a une limite de taille de message (bien plus petite que 10 Mo).\nDonc la solution la plus simple et la moins co√ªteuse est : stocker la vid√©o dans S3 Standard et envoyer dans SQS le chemin S3 du fichier.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e o√π les √©l√®ves d√©posent des vid√©os sur une cl√© USB, et un tableau d‚Äôaffichage (file d‚Äôattente) dit aux surveillants quelles vid√©os doivent √™tre trait√©es.**\n\nConcept : tu stockes la vid√©o dans un endroit fait pour garder beaucoup de fichiers (la biblioth√®que), puis tu mets juste un petit mot sur le tableau (SQS) avec o√π la trouver. Les surveillants doivent pouvoir r√©cup√©rer la vid√©o en moins de 5 minutes. A (S3 Glacier Deep Archive) = archives au sous-sol : c‚Äôest moins cher, mais trop lent √† r√©cup√©rer, donc pas OK. C (EBS) = disque dur d‚Äôun seul PC : pas pratique pour plein d‚Äôuploads et √ßa co√ªte plus cher pour du stockage de fichiers. D = coller la vid√©o enti√®re sur le tableau : impossible/pr√©vu pour de petits messages, pas des fichiers de 10 MB. B (S3 Standard) = √©tag√®res normales, acc√®s imm√©diat et co√ªt raisonnable : tu stockes la vid√©o l√†, puis tu mets le lien dans SQS, et le traitement peut d√©marrer vite.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:549:c096a9824286eee7",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 549,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application that consists of many AWS Lambda functions. The Lambda functions connect to a single Amazon RDS database.The developer needs to implement a solution to store the database credentials securely. When the credentials are updated, the Lambda functions must be able to use the new credentials without requiring a code update or a configuration update.Which solution will meet these requirements?",
      "choices": {
        "A": "Store the credentials as a secret in AWS Secrets Manager. Access the secret at runtime from within the Lambda functions.",
        "B": "Store the credentials as a secret in AWS Secrets Manager. Access the credentials in environment variables by using the containerDefinitions and valueFrom elements in reference to the secret value.",
        "C": "Store the credentials as a SecureString parameter in AWS Systems Manager Parameter Store. Add a trigger to pass the credentials to the Lambda functions when the Lambda functions run.",
        "D": "Store the credentials as a SecureString parameter in AWS Systems Manager Parameter Store. Add a reference to the parameter in an environment variable in the Lambda functions."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157490-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 4, 2025, 2:27 a.m.",
      "textHash": "c096a9824286eee7",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Ici, on veut stocker des identifiants (login/mot de passe) de base de donn√©es RDS de fa√ßon s√©curis√©e, et surtout que les fonctions Lambda utilisent automatiquement la nouvelle valeur quand elle change, sans modifier le code ni la configuration.\nAWS Secrets Manager est un service fait pour g√©rer des ‚Äúsecrets‚Äù (mots de passe, cl√©s) avec chiffrement et rotation possible.\nAvec la r√©ponse A, chaque Lambda lit le secret ‚Äúau moment de l‚Äôex√©cution‚Äù (runtime) via l‚ÄôAPI : si le secret est mis √† jour, la prochaine ex√©cution r√©cup√®re la nouvelle valeur automatiquement.\nC‚Äôest exactement le besoin : pas de red√©ploiement, pas de mise √† jour de variables d‚Äôenvironnement.\nLa r√©ponse B et D utilisent des variables d‚Äôenvironnement : si le secret/param√®tre change, la Lambda ne le verra pas automatiquement sans mise √† jour de configuration (les env vars sont fig√©es au d√©ploiement).\nLa r√©ponse C parle d‚Äôun ‚Äútrigger‚Äù pour pousser les identifiants : ce n‚Äôest pas un m√©canisme standard/n√©cessaire et ne garantit pas la simplicit√© demand√©e.\nDonc la bonne pratique est : Secrets Manager + lecture √† l‚Äôex√©cution (A).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:536:96ae65d570490382",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 536,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a stock trading application. The developer needs a solution to send text messages to application users to confirmation when a trade has been completed.The solution must deliver messages in the order a user makes stock trades. The solution must not send duplicate messages.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure the application to publish messages to an Amazon Data Firehose delivery stream. Configure the delivery stream to have a destination of each user‚Äôs mobile phone number that is passed in the trade confirmation message.",
        "B": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Use the SendMessageIn API call to send the trade confirmation messages to the queue. Use the SendMessageOut API to send the messages to users by using the information provided in the trade confirmation message.",
        "C": "Configure a pipe in Amazon EventBridge Pipes. Connect the application to the pipe as a source. Configure the pipe to use each user‚Äôs mobile phone number as a target. Configure the pipe to send incoming events to the users.",
        "D": "Create an Amazon Simple Notification Service (SNS) FIFO topic. Configure the application to use the AWS SDK to publish notifications to the SNS topic to send SMS messages to the users."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156696-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 11:41 a.m.",
      "textHash": "96ae65d570490382",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "On veut envoyer des SMS de confirmation apr√®s chaque trade, dans le m√™me ordre que les trades, et sans doublons.\nAmazon SNS (Simple Notification Service) sert √† envoyer des notifications, dont des SMS, √† des destinataires.\nUne ‚ÄúFIFO topic‚Äù SNS garantit l‚Äôordre des messages (First-In-First-Out) pour un m√™me groupe de messages, donc l‚Äôutilisateur re√ßoit les confirmations dans le bon ordre.\nSNS FIFO fournit aussi une d√©duplication : si le m√™me message est publi√© deux fois (m√™me identifiant de d√©duplication), SNS √©vite d‚Äôenvoyer un doublon.\nL‚Äôapplication publie simplement chaque confirmation via le SDK AWS vers le topic SNS FIFO, et SNS se charge de l‚Äôenvoi SMS.\nLes autres options ne conviennent pas : Firehose sert √† livrer des donn√©es vers du stockage/analytics, pas √† envoyer des SMS.\nSQS FIFO est une file d‚Äôattente (stockage de messages) : elle ne sait pas envoyer des SMS directement, il faudrait un service/worker en plus.\nEventBridge Pipes route des √©v√©nements entre services, mais l‚Äôenvoi SMS et les garanties FIFO/d√©duplication pour SMS sont directement couverts par SNS FIFO.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu es le prof qui rend des copies apr√®s un contr√¥le. Chaque √©l√®ve fait plusieurs devoirs dans la journ√©e. Tu dois rendre les copies dans le m√™me ordre que l‚Äô√©l√®ve les a rendues, et tu ne dois jamais rendre deux fois la m√™me copie.**\n\nIci, chaque ‚Äútrade‚Äù (achat/vente d‚Äôaction) = un devoir rendu, et le SMS de confirmation = la copie rendue. Il faut donc un syst√®me qui garde l‚Äôordre ET √©vite les doublons. Un ‚ÄúSNS FIFO topic‚Äù, c‚Äôest comme une file d‚Äôattente sp√©ciale du prof: FIFO veut dire ‚Äúpremier arriv√©, premier sorti‚Äù, donc les SMS partent dans le bon ordre. Et cette file a aussi un anti-doublon: si la m√™me confirmation arrive deux fois, elle peut √™tre ignor√©e. SNS sait envoyer des SMS directement aux t√©l√©phones, comme le prof qui remet la copie √† l‚Äô√©l√®ve. Les autres choix ne garantissent pas clairement l‚Äôordre + z√©ro doublon pour des SMS. Donc la bonne r√©ponse est D.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:514:6136603e15254c9c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 514,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has deployed an AWS Lambda function that is subscribed to an Amazon Simple Notification Service (Amazon SNS) topic. The developer must implement a solution to add a record of each Lambda function invocation to an Amazon Simple Queue Service (Amazon SQS) queue.Which solution will meet this requirement?",
      "choices": {
        "A": "Configure the SQS queue as a dead-letter queue for the Lambda function.",
        "B": "Create code that uses the AWS SDK to call the SQS SendMessage operation to add the invocation details to the SQS queue. Add the code to the end of the Lambda function.",
        "C": "Add two asynchronous invocation destinations to the Lambda function: one destination for successful invocations and one destination for failed invocations. Configure the SQS queue as the destination for each type. Create an Amazon CloudWatch alarm based on the DestinationDeliveryFailures metric to catch any message that cannot be delivered.",
        "D": "Add a single asynchronous invocation destination to the Lambda function to capture successful invocations. Configure the SQS queue as the destination. Create an Amazon CloudWatch alarm based on the DestinationDeliveryFailures metric to catch any message that cannot be delivered."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152781-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 10, 2024, 5:46 a.m.",
      "textHash": "6136603e15254c9c",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Ici, une fonction AWS Lambda (code ex√©cut√© automatiquement) est d√©clench√©e par un topic Amazon SNS (service qui publie des notifications). On veut garder une trace de CHAQUE invocation dans une file Amazon SQS (file de messages).\nLa meilleure approche est d‚Äôutiliser les ‚ÄúLambda Destinations‚Äù pour les invocations asynchrones : Lambda peut envoyer automatiquement un message vers une destination apr√®s ex√©cution.\nAvec deux destinations (succ√®s et √©chec), on enregistre toutes les invocations : celles qui r√©ussissent ET celles qui √©chouent, chacune envoy√©e vers SQS.\nC‚Äôest exactement ce que propose C : une destination SQS pour les succ√®s et une destination SQS pour les √©checs.\nL‚Äôalarme CloudWatch sur DestinationDeliveryFailures sert √† d√©tecter si Lambda n‚Äôarrive pas √† livrer le message vers SQS.\nA est faux car une dead-letter queue ne capture que certains √©checs, pas toutes les invocations.\nB marche mais n√©cessite du code et peut ne pas s‚Äôex√©cuter en cas d‚Äôerreur avant la fin; ce n‚Äôest pas la solution la plus fiable/standard.\nD ne capture que les succ√®s, donc il manque les invocations en √©chec.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une salle de classe : le prof (SNS) annonce un devoir √† toute la classe. Un √©l√®ve (Lambda) doit faire une action √† chaque annonce. Et tu veux noter CHAQUE fois que l‚Äô√©l√®ve a essay√© de faire l‚Äôaction dans un cahier de suivi (SQS).**\n\nConcept : SNS = annonce √† tout le monde, Lambda = √©l√®ve qui r√©agit, SQS = cahier o√π on empile des petits papiers ‚Äútentative faite‚Äù.\nTu veux un papier pour chaque tentative, qu‚Äôelle ait r√©ussi ou rat√©.\nLa r√©ponse C dit : on met 2 ‚Äúsorties automatiques‚Äù apr√®s l‚Äôaction de l‚Äô√©l√®ve : une pour ‚Äúr√©ussi‚Äù et une pour ‚Äúrat√©‚Äù.\nEt dans les deux cas, on envoie un papier dans le cahier SQS : tu as bien une trace de chaque invocation.\nEn plus, on met une alarme (CloudWatch = surveillant) si jamais les papiers n‚Äôarrivent pas dans le cahier.\nPourquoi pas A : une ‚Äúpoubelle de secours‚Äù (dead-letter) ne garde que les rat√©s, pas tout.\nPourquoi pas B : √ßa marche, mais c‚Äôest comme demander √† l‚Äô√©l√®ve d‚Äô√©crire lui-m√™me dans le cahier : plus de code, plus de risques.\nPourquoi pas D : √ßa ne note que les r√©ussites, donc il manque les rat√©s.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:555:a85771fa596ac154",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 555,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer created several AWS Lambda functions that write data to a single Amazon S3 bucket. The developer configured all the Lambda functions to send logs and metrics to Amazon CloudWatch.The developer receives reports that one of the Lambda functions writes data to the bucket very slowly. The developer needs to measure the latency between the problematic Lambda function and the S3 bucket.Which solution will meet this requirement?",
      "choices": {
        "A": "Enable AWS X-Ray on the Lambda function. In the generated trace map, select the line between Lambda and Amazon S3.",
        "B": "Query the Lambda function‚Äôs log file in Amazon CloudWatch Logs Insights. Return the average of the auto-discovered @duration field.",
        "C": "Enable CloudWatch Lambda Insights on the function. View the latency graph that CloudWatch Lambda Insights provides.",
        "D": "Enable AWS X-Ray on the Lambda function. Select Amazon S3 in the latency graph to view the latency histogram."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/154534-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Jan. 15, 2025, 8:09 a.m.",
      "textHash": "a85771fa596ac154",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Ici, on veut mesurer la latence entre une fonction AWS Lambda (code ex√©cut√© √† la demande) et Amazon S3 (stockage d‚Äôobjets). Les logs/metrics CloudWatch donnent surtout la dur√©e totale d‚Äôex√©cution de Lambda, pas le temps pr√©cis pass√© √† appeler S3. AWS X-Ray sert √† ‚Äútracer‚Äù une requ√™te de bout en bout et √† voir chaque appel externe (ex: S3) avec son temps. En activant X-Ray sur la fonction, on obtient une carte de traces (trace map) montrant les segments Lambda -> S3. En s√©lectionnant la ligne entre Lambda et S3, on lit directement la latence de cet appel r√©seau/service, ce qui r√©pond exactement au besoin. L‚Äôoption B (@duration) mesure la dur√©e globale de Lambda, pas la latence S3. Lambda Insights (C) donne des m√©triques syst√®me et performance, mais pas un lien pr√©cis Lambda->S3. L‚Äôoption D parle d‚Äôun histogramme sur S3, mais la mani√®re la plus directe demand√©e est la ligne Lambda-S3 dans la trace map.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine : plusieurs √©l√®ves (les fonctions Lambda) d√©posent des plateaux dans le m√™me chariot (le bucket S3). Le prof (CloudWatch) note juste combien de temps chaque √©l√®ve met au total, mais pas o√π √ßa coince. Pour savoir pourquoi un √©l√®ve est lent, tu lui mets une cam√©ra qui filme tout son trajet et montre le temps entre chaque √©tape.**\n\nConcept : on veut mesurer le temps de trajet entre l‚Äô√©l√®ve (Lambda) et le chariot (S3), pas juste le temps total de l‚Äô√©l√®ve.\nA : AWS X-Ray, c‚Äôest la ‚Äúcam√©ra de trajet‚Äù : elle cr√©e une carte du parcours avec des √©tapes et des liens.\nSur la carte, la ligne entre Lambda et S3 te donne pr√©cis√©ment la latence (le temps de transfert) entre les deux.\nB : @duration, c‚Äôest seulement le temps total de l‚Äô√©l√®ve, √ßa ne dit pas si le probl√®me est sur le chemin vers S3.\nC : Lambda Insights montre surtout la sant√© de l‚Äô√©l√®ve (m√©moire, CPU), pas le temps exact du trajet vers S3.\nD : regarder S3 seul ne garantit pas que tu mesures le lien Lambda‚ÜíS3 ; tu veux le ‚Äúsegment‚Äù entre les deux.\nDonc A est la bonne r√©ponse : elle mesure directement la latence entre Lambda et S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:516:2a899a84639445f1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 516,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to configure an AWS Lambda function to make HTTP POST requests to an internal application. The application is in the same AWS account that hosts the function. The internal application runs on Amazon EC2 instances in a private subnet within a VPC.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure a VPC endpoint to connect to the private subnet. Attach the endpoint to the Lambda function.",
        "B": "Attach the Lambda function to the VPC and to the private subnet.",
        "C": "Configure a VPN connection between the Lambda function and the private subnet. Attach the VPN to the Lambda function.",
        "D": "Configure the VPC route table to include the Lambda function‚Äôs IP address."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153024-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 16, 2024, 6:33 a.m.",
      "textHash": "2a899a84639445f1",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:76abe451",
      "frExplanation": "Pour qu‚Äôune fonction AWS Lambda fasse des requ√™tes HTTP vers une application ¬´ interne ¬ª sur des instances EC2 dans un sous-r√©seau priv√©, elle doit pouvoir entrer dans le m√™me r√©seau priv√© (VPC).\nPar d√©faut, Lambda s‚Äôex√©cute hors de votre VPC et ne peut pas atteindre directement des adresses priv√©es (IP priv√©es) d‚Äôun subnet priv√©.\nLa solution est donc de ¬´ rattacher ¬ª la Lambda au VPC et de la placer dans les sous-r√©seaux priv√©s concern√©s : Lambda cr√©era alors des interfaces r√©seau (ENI) dans ces subnets et pourra joindre les EC2 via leurs IP priv√©es.\nIl faut aussi que les Security Groups et NACL autorisent le trafic (ex: port 80/443) entre Lambda et EC2.\nA est faux : un VPC Endpoint sert surtout √† acc√©der √† des services AWS (S3, DynamoDB, etc.) sans Internet, pas √† joindre vos EC2.\nC est inutile : un VPN sert √† relier un r√©seau externe (on-premises) √† AWS, pas deux ressources d√©j√† dans le m√™me compte/VPC.\nD est faux : Lambda n‚Äôa pas une IP fixe √† mettre dans une table de routage, et le routage ne remplace pas l‚Äôattachement au VPC.\nDonc la bonne r√©ponse est B : attacher la fonction Lambda au VPC et au subnet priv√©.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine ton lyc√©e : il y a la cour (publique) et une salle r√©serv√©e aux profs avec badge (priv√©e). Tu veux d√©poser un papier (HTTP POST) √† un prof qui est dans la salle priv√©e.**\n\nConcept : si tu restes dans la cour, tu ne peux pas entrer dans la salle priv√©e, m√™me si tu es dans le m√™me lyc√©e. Il faut √™tre ‚Äúdans le bon couloir‚Äù avec le bon acc√®s.\nIci, l‚Äôappli interne est sur des machines (EC2) dans un r√©seau priv√© (private subnet) d‚Äôun grand ‚Äúb√¢timent‚Äù r√©seau (VPC).\nAWS Lambda, par d√©faut, est comme quelqu‚Äôun dehors dans la cour : elle n‚Äôest pas automatiquement dans ce r√©seau priv√©.\nDonc la bonne solution est B : on ‚Äúfait entrer‚Äù la fonction Lambda dans le VPC et pr√©cis√©ment dans le private subnet, comme si on lui donnait un badge et on la faisait passer par le couloir des profs.\nA : un ‚Äúpoint d‚Äôentr√©e‚Äù (endpoint) sert surtout √† aller vers certains services, pas √† entrer dans une salle priv√©e EC2 comme √ßa.\nC : un VPN, c‚Äôest comme un tunnel depuis une autre ville : inutile, tout est d√©j√† dans le m√™me lyc√©e.\nD : ajouter une IP dans un plan de couloirs (route table) ne suffit pas : Lambda n‚Äôa pas une IP fixe √† laquelle on peut juste ‚Äúfaire une route‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:488:2873b7e3d37810ff",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 488,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is developing a set of AWS Lambda functions to process data. The Lambda functions need to use a common third-party library as a dependency. The library is frequently updated with new features and bug fixes. The company wants to ensure that the Lambda functions always use the latest version of the library.Which solution will meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "A": "Store the dependency and the function code in an Amazon S3 bucket.",
        "B": "Create a Lambda layer that includes the library. Attach the layer to each Lambda function.",
        "C": "Install the dependency in an Amazon Elastic File System (Amazon EFS) file system. Attach the file system to each Lambda function.",
        "D": "Create a new Lambda function to load the library. Configure the existing Lambda functions to invoke the new Lambda function when the existing functions need to use the library."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156658-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:04 a.m.",
      "textHash": "2873b7e3d37810ff",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:13f5239c",
      "frExplanation": "Ici, plusieurs fonctions AWS Lambda (du code qui s‚Äôex√©cute ‚Äú√† la demande‚Äù sans g√©rer de serveur) doivent partager une m√™me biblioth√®que tierce, mise √† jour souvent.\nLe besoin cl√© est : une seule copie de la biblioth√®que, et que toutes les fonctions voient imm√©diatement la derni√®re version, sans devoir red√©ployer chaque fonction.\nAmazon EFS est un syst√®me de fichiers partag√© (comme un disque r√©seau) que plusieurs fonctions Lambda peuvent monter et lire au moment de l‚Äôex√©cution.\nEn installant la biblioth√®que dans EFS et en attachant EFS √† chaque Lambda, une mise √† jour de la biblioth√®que dans EFS devient automatiquement disponible pour toutes les fonctions.\nC‚Äôest donc tr√®s efficace op√©rationnellement : on met √† jour une fois, tout le monde en profite.\nPourquoi pas une Layer (B) ? Une layer doit √™tre versionn√©e et rattach√©e/mettre √† jour la configuration des fonctions (et souvent red√©ployer) √† chaque nouvelle version.\nPourquoi pas S3 (A) ? S3 stocke des fichiers, mais Lambda ne ‚Äúmonte‚Äù pas S3 comme un disque ; il faudrait t√©l√©charger/packager √† chaque fois.\nPourquoi pas une Lambda interm√©diaire (D) ? Cela ajoute de la complexit√©, de la latence et ne partage pas r√©ellement une d√©pendance comme une biblioth√®que locale.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:483:ea9b6bae1c80cd7a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 483,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is updating an Amazon API Gateway REST API to have a mock endpoint. The developer wants to update the integration request mapping template so the endpoint will respond to mock integration requests with specific HTTP status codes based on various conditions.Which statement will meet these requirements?",
      "choices": {},
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156656-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:01 a.m.",
      "textHash": "ea9b6bae1c80cd7a",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:819f4e23",
      "frExplanation": "API Gateway est un service AWS qui expose des endpoints HTTP (des URLs) pour vos applications. Un ¬´ mock endpoint ¬ª signifie qu‚ÄôAPI Gateway r√©pond tout seul, sans appeler un backend (Lambda, EC2, etc.).\nPour renvoyer diff√©rents codes HTTP (200, 400, 500‚Ä¶) selon des conditions, il faut d√©finir une variable sp√©ciale dans le template de mapping de la requ√™te d‚Äôint√©gration (Integration Request Mapping Template).\nLa bonne m√©thode est d‚Äôutiliser l‚Äôobjet de contexte de Velocity Template Language (VTL) : $context.responseOverride.status.\nDans le template, vous testez vos conditions (if/else) et vous affectez le code voulu, par exemple : #set($context.responseOverride.status = 400).\nAPI Gateway utilisera alors ce code comme statut HTTP de la r√©ponse du mock.\nLes autres approches (modifier seulement la r√©ponse m√©thode, ou essayer de changer le statut ailleurs) ne permettent pas de choisir dynamiquement le code HTTP √† partir de conditions dans le mapping template.\nDonc la bonne r√©ponse (B) est celle qui indique d‚Äôutiliser $context.responseOverride.status dans le mapping template pour d√©finir le code HTTP selon la logique.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le bureau de la vie scolaire au lyc√©e : tu arrives au guichet, mais au lieu d‚Äôappeler un prof (le ‚Äúvrai service‚Äù), le guichet te r√©pond tout seul avec un papier ‚ÄúOK‚Äù, ‚ÄúInterdit‚Äù, ou ‚ÄúErreur‚Äù, selon ce que tu dis.**\n\nConcept : un ‚Äúmock endpoint‚Äù dans API Gateway, c‚Äôest un guichet qui r√©pond sans contacter personne derri√®re. Il fabrique la r√©ponse sur place.\nPour choisir le bon code HTTP (200 = OK, 400 = mauvaise demande, 403 = interdit, etc.), il faut √©crire des r√®gles dans le ‚Äúmapping template‚Äù (un formulaire de traduction) de la requ√™te.\nLa bonne r√©ponse B correspond √† : mettre des conditions dans ce template pour d√©cider quel ‚ÄústatusCode‚Äù renvoyer.\nAvec l‚Äôanalogie : si tu dis ‚Äúj‚Äôai oubli√© ma carte‚Äù, le guichet imprime ‚Äú403 Interdit‚Äù; si tu donnes le bon num√©ro, il imprime ‚Äú200 OK‚Äù.\nComme c‚Äôest un mock, tout se passe au guichet : pas besoin d‚Äôun prof au t√©l√©phone.\nDonc B est la seule option qui permet de renvoyer diff√©rents codes HTTP selon des conditions, directement dans la configuration du mock.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:457:b51498f24b5db775",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 457,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application that uses an AWS Lambda function to process data. The application requires minimum latency. The Lambda function must have predictable function start times. All setup activities for the execution environment must happen before invocation of the Lambda function.Which solution will meet these requirements?",
      "choices": {
        "A": "Increase the memory of the Lambda function to the maximum amount. Configure an Amazon EventBridge rule to schedule invocations of the Lambda function every minute to keep the execution environment active.",
        "B": "Optimize the static initialization code that runs when a new execution environment is prepared for the first time. Decrease and compress the size of the Lambda function package and the imported libraries and dependencies.",
        "C": "Increase the reserved concurrency of the Lambda function to the maximum value for unreserved account concurrency. Run any setup activities manually before the initial invocation of the Lambda function.",
        "D": "Publish a new version of the Lambda function. Configure provisioned concurrency for the Lambda function with the required minimum number of execution environments."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152886-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 12, 2024, 12:10 p.m.",
      "textHash": "b51498f24b5db775",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Le probl√®me ici est le ‚Äúcold start‚Äù : quand AWS Lambda (service qui ex√©cute du code sans serveur) doit cr√©er un nouvel environnement d‚Äôex√©cution, il charge le code et fait l‚Äôinitialisation, ce qui ajoute de la latence.\nOn veut une latence minimale et surtout des temps de d√©marrage pr√©visibles, avec toute la pr√©paration faite avant l‚Äôappel.\nLa ‚Äúprovisioned concurrency‚Äù (concurrence provisionn√©e) permet de garder un nombre d√©fini d‚Äôenvironnements Lambda d√©j√† d√©marr√©s et pr√™ts, donc pas de cold start pour ces instances.\nPublier une nouvelle version est important car la provisioned concurrency s‚Äôattache √† une version ou un alias stable, pas √† du code qui change en continu.\nAinsi, au moment de l‚Äôinvocation, l‚Äôenvironnement est d√©j√† initialis√© et la fonction d√©marre de fa√ßon r√©guli√®re et rapide.\nLes autres options (augmenter m√©moire, ping toutes les minutes, optimiser le package, augmenter la concurrence r√©serv√©e) peuvent aider un peu, mais ne garantissent pas des d√©marrages pr√©visibles ni une pr√©paration compl√®te avant invocation.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : parfois il faut ouvrir une nouvelle caisse, allumer la machine, sortir la monnaie‚Ä¶ √ßa prend du temps avant de servir le 1er √©l√®ve.**\n\nLambda, c‚Äôest comme une caisse qui se cr√©e quand des √©l√®ves arrivent. Le probl√®me, c‚Äôest le ‚Äúd√©marrage‚Äù : ouvrir la caisse = pr√©parer l‚Äôenvironnement, et √ßa peut √™tre lent et impr√©visible. Toi, tu veux une attente minimale et un d√©marrage toujours pareil. La solution D, c‚Äôest comme dire : ‚ÄúJe garde d√©j√† X caisses ouvertes et pr√™tes avant l‚Äôarriv√©e des √©l√®ves.‚Äù √áa s‚Äôappelle la provisioned concurrency : AWS pr√©pare √† l‚Äôavance des environnements d√©j√† chauds, avec tout le setup fait avant l‚Äôappel. Publier une nouvelle version, c‚Äôest comme fixer le menu et l‚Äôorganisation de la caisse pour que ce soit stable. Donc D garantit des d√©parts rapides et pr√©visibles, sans surprise au moment o√π l‚Äôappli appelle la fonction.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:502:f34a42929569b2f1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 502,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company generates SSL certificates from a third-party provider. The company imports the certificates into AWS Certificate Manager (ACM) to use with public web applications.A developer must implement a solution to notify the company‚Äôs security team 90 days before an imported certificate expires. The company already has configured an Amazon Simple Queue Service (Amazon SQS) queue. The company also has configured an Amazon Simple Notification Service (Amazon SNS) topic that has the security team‚Äôs email address as a subscriber.Which solution will provide the security team with the required notification about certificates?",
      "choices": {
        "A": "Create an Amazon EventBridge rule that specifies the ACM Certificate Approaching Expiration event type. Set the SNS topic as the EventBridge rule‚Äôs target.",
        "B": "Create an AWS Lambda function to search for all certificates that are expiring within 90 days. Program the Lambda function to send each identified certificate‚Äôs Amazon Resource Name (ARN) in a message to the SQS queue.",
        "C": "Create an AWS Step Functions workflow that is invoked by each certificate‚Äôs expiration notification from AWS CloudTrail. Create an AWS Lambda function to send each certificate's Amazon Resource Name (ARN) in a message to the SQS queue.",
        "D": "Configure AWS Config with the acm-certificate-expiration-check managed rule to run every 24 hours. Create an Amazon EventBridge rule that includes an event pattern that specifies the Config Rules Compliance Change detail type and the configured rule. Set the SNS topic as the EventBridge rule‚Äôs target."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152774-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 10, 2024, 4:30 a.m.",
      "textHash": "f34a42929569b2f1",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "ACM (AWS Certificate Manager) g√®re les certificats TLS/SSL utilis√©s par vos sites. Ici, les certificats sont ¬´ import√©s ¬ª depuis un fournisseur externe, donc il faut surveiller leur date d‚Äôexpiration.\nEventBridge est un service qui capte des √©v√©nements AWS (comme ¬´ un certificat approche de l‚Äôexpiration ¬ª) et peut d√©clencher une action automatiquement.\nACM publie justement un √©v√©nement nomm√© ¬´ Certificate Approaching Expiration ¬ª quand un certificat va bient√¥t expirer (dont les certificats import√©s).\nEn cr√©ant une r√®gle EventBridge sur cet √©v√©nement et en mettant le topic SNS en cible, SNS enverra un email au groupe s√©curit√© (d√©j√† abonn√©).\nC‚Äôest simple, sans code, et la notification est automatique d√®s que l‚Äô√©v√©nement est √©mis.\nB et C demandent du code/flux suppl√©mentaires et envoient vers SQS, mais rien n‚Äôindique ensuite comment l‚Äôemail serait envoy√©.\nD utilise AWS Config (conformit√©) : c‚Äôest plus lourd et indirect, et ce n‚Äôest pas le m√©canisme standard pour l‚Äôalerte d‚Äôexpiration ACM.\nDonc la meilleure solution est A : EventBridge + √©v√©nement ACM + cible SNS.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:133:b07ff6d2e1746797",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 133,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to store configuration variables for an application. The developer needs to set an expiration date and time for the configuration. The developer wants to receive notifications before the configuration expires.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Create a standard parameter in AWS Systems Manager Parameter Store. Set Expiration and ExpirationNotification policy types.",
        "B": "Create a standard parameter in AWS Systems Manager Parameter Store. Create an AWS Lambda function to expire the configuration and to send Amazon Simple Notification Service (Amazon SNS) notifications.",
        "C": "Create an advanced parameter in AWS Systems Manager Parameter Store. Set Expiration and ExpirationNotification policy types.",
        "D": "Create an advanced parameter in AWS Systems Manager Parameter Store. Create an Amazon EC2 instance with a cron job to expire the configuration and to send notifications."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/117335-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Aug. 4, 2023, 1:46 p.m.",
      "textHash": "b07ff6d2e1746797",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "On veut stocker des variables de configuration (des ‚Äúcl√©s/valeurs‚Äù) et g√©rer automatiquement une date d‚Äôexpiration + une alerte avant expiration, avec le moins d‚Äôadministration possible.\nAWS Systems Manager Parameter Store est un service g√©r√© pour stocker des param√®tres d‚Äôapplication (ex: URL, cl√©s API) sans g√©rer de serveur.\nParameter Store permet d‚Äôattacher des ‚Äúpolicies‚Äù au param√®tre, dont Expiration (date/heure de fin) et ExpirationNotification (pr√©venir avant la fin).\nCes policies ne sont disponibles que pour les param√®tres ‚Äúadvanced‚Äù (avanc√©s), pas pour les ‚Äústandard‚Äù.\nDonc l‚Äôoption C est la seule qui utilise la fonctionnalit√© native: pas de code, pas de planification, pas d‚Äôinfrastructure √† maintenir.\nLes options B et D ajoutent du travail (Lambda ou EC2 + cron) pour faire une t√¢che que le service peut faire tout seul.\nL‚Äôoption A √©choue car un param√®tre standard ne supporte pas ces policies d‚Äôexpiration/notification.\nConclusion: cr√©er un param√®tre avanc√© et d√©finir Expiration + ExpirationNotification r√©pond exactement au besoin avec le minimum d‚Äôoverhead.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le tableau d‚Äôaffichage du lyc√©e o√π tu colles des post-its ‚Äúr√®gles du club‚Äù (config). Tu veux qu‚Äôun post-it disparaisse √† une date/heure pr√©cise, et que le lyc√©e te pr√©vienne juste avant qu‚Äôil soit retir√©.**\n\nLe ‚ÄúParameter Store‚Äù, c‚Äôest ce tableau qui garde des infos pour une appli. Une ‚Äúdate d‚Äôexpiration‚Äù, c‚Äôest comme √©crire ‚Äú√† enlever vendredi 18h‚Äù sur le post-it. Une ‚Äúnotification avant expiration‚Äù, c‚Äôest comme demander au surveillant de te pr√©venir 10 minutes avant de l‚Äôenlever. Avec un param√®tre ‚Äúadvanced‚Äù, le tableau sait g√©rer ces r√®gles tout seul (Expiration + ExpirationNotification) sans que tu fabriques un syst√®me √† c√¥t√©. Donc C est le mieux: tu colles le post-it ‚Äúavanc√©‚Äù et le tableau s‚Äôoccupe de l‚Äôalarme et du retrait. B et D demandent de cr√©er un robot (Lambda) ou d‚Äôallumer un PC (EC2 + cron) pour surveiller: plus de boulot √† g√©rer. A ne marche pas car le post-it ‚Äústandard‚Äù n‚Äôa pas les options avanc√©es d‚Äôexpiration + notification.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:490:3942ee3748f8680e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 490,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company hosts applications on premises. The on-premises servers generate audit logs that are available through an HTTP endpoint.The company needs an automated solution to regularly ingest and store large volumes of audit data from the on-premises servers. The company also needs to perform queries on the audit data.Which solution will meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "A": "Export the audit logs. Upload the logs to Amazon S3. Import the logs to an Amazon RDS DB instance.",
        "B": "Create an AWS Lambda function to call the HTTP endpoint to fetch audit logs. Configure an Amazon EventBridge scheduled rule to invoke the Lambda function. Configure the Lambda function to push the logs to AWS CloudTrail Lake.",
        "C": "Use AWS DataSync to transfer audit logs to an Amazon S3 bucket. Load the logs into an Amazon S3 bucket. Use Amazon Athena to query the bucket.",
        "D": "Install the Amazon CloudWatch agent on the on-premises servers. Give the agent the ability to push audit logs to CloudWatch. Use CloudWatch Insights to query the logs."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153147-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 18, 2024, 3:57 a.m.",
      "textHash": "3942ee3748f8680e",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Objectif : ing√©rer automatiquement beaucoup de logs depuis des serveurs sur site et pouvoir les interroger, avec le moins d‚Äôadministration possible.\nAmazon S3 est un stockage d‚Äôobjets tr√®s scalable et peu co√ªteux, id√©al pour conserver de gros volumes de fichiers de logs.\nAWS DataSync est un service g√©r√© qui automatise et acc√©l√®re le transfert de donn√©es depuis un environnement on‚Äëpremises vers S3, avec planification et reprise en cas d‚Äôerreur.\nUne fois les logs dans S3, Amazon Athena permet de faire des requ√™tes SQL directement sur les fichiers (sans serveur ni base de donn√©es √† g√©rer).\nC‚Äôest donc ‚Äúop√©rationnellement efficace‚Äù : pas d‚Äôinstance √† maintenir, pas d‚Äôimport complexe, et √ßa scale avec le volume.\nA est moins efficace car il faut g√©rer une base RDS et charger des donn√©es massives dedans.\nB est inadapt√© : CloudTrail Lake sert surtout aux √©v√©nements AWS, pas √† des logs applicatifs arbitraires via HTTP.\nD n√©cessite installer/maintenir des agents et CloudWatch Logs peut devenir co√ªteux et moins adapt√© au stockage massif long terme que S3 + Athena.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e : chaque jour, les surveillants √©crivent des rapports (logs) et tu veux les ranger automatiquement dans une grande salle d‚Äôarchives, puis pouvoir chercher dedans vite (par date, par √©l√®ve, etc.).**\n\nLes logs sur les serveurs ‚Äúsur place‚Äù, c‚Äôest comme des feuilles dans le bureau des surveillants, accessibles via une ‚Äúporte‚Äù web (HTTP).\nIl faut un camion de collecte automatique et r√©gulier : AWS DataSync, c‚Äôest le service qui fait les transferts en masse sans que tu bricoles tout √† la main.\nLa salle d‚Äôarchives, c‚Äôest Amazon S3 : un grand stockage simple et pas cher pour empiler √©norm√©ment de fichiers.\nEnsuite, pour ‚Äúposer des questions‚Äù aux archives sans tout recopier ailleurs, Amazon Athena c‚Äôest comme un moteur de recherche qui lit directement les fichiers dans S3.\nDonc C marche : collecte automatique + stockage √©norme + recherches faciles, avec peu de gestion.\nA est moins efficace car tu recopies encore dans une base (RDS) inutilement.\nB et D visent plut√¥t des logs AWS/monitoring, pas le meilleur choix pour gros volumes de fichiers depuis sur place.\nAvec l‚Äôanalogie : DataSync = camion, S3 = archives, Athena = recherche dans les archives.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:25:4dfcb04f65f0eb2c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 25,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is migrating an on-premises database to Amazon RDS for MySQL. The company has read-heavy workloads. The company wants to refactor the code to achieve optimum read performance for queries.Which solution will meet this requirement with LEAST current and future effort?",
      "choices": {
        "A": "Use a multi-AZ Amazon RDS deployment. Increase the number of connections that the code makes to the database or increase the connection pool size if a connection pool is in use.",
        "B": "Use a multi-AZ Amazon RDS deployment. Modify the code so that queries access the secondary RDS instance.",
        "C": "Deploy Amazon RDS with one or more read replicas. Modify the application code so that queries use the URL for the read replicas.",
        "D": "Use open source replication software to create a copy of the MySQL database on an Amazon EC2 instance. Modify the application code so that queries use the IP address of the EC2 instance."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103510-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 10:07 p.m.",
      "textHash": "4dfcb04f65f0eb2c",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:67c3011f",
      "frExplanation": "Ici le besoin est surtout de mieux g√©rer beaucoup de lectures (read-heavy). Amazon RDS est un service de base de donn√©es g√©r√© : AWS s‚Äôoccupe des sauvegardes, patchs, etc. Pour acc√©l√©rer les lectures, RDS propose des ¬´ read replicas ¬ª : des copies en lecture seule qui re√ßoivent les donn√©es depuis la base principale. L‚Äôapplication peut envoyer les requ√™tes SELECT vers ces replicas, ce qui r√©partit la charge et am√©liore les performances de lecture. C‚Äôest peu d‚Äôeffort maintenant et plus tard : on peut ajouter/retirer des replicas sans changer l‚Äôarchitecture, juste en pointant vers leurs endpoints (URL). Multi-AZ sert surtout √† la haute disponibilit√© (bascule en cas de panne), pas √† augmenter la capacit√© de lecture, et on ne lit pas directement sur l‚Äôinstance standby. Utiliser EC2 + r√©plication open source demande plus d‚Äôadministration et de maintenance (donc plus d‚Äôeffort).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e : beaucoup d‚Äô√©l√®ves viennent surtout LIRE des livres, pas en √©crire. Pour √©viter la queue, la biblioth√®que met plusieurs exemplaires du m√™me livre sur des tables diff√©rentes.**\n\nConcept : une base de donn√©es, c‚Äôest comme un livre. Les ‚Äúlectures‚Äù = lire des infos. Les ‚Äú√©critures‚Äù = modifier/ajouter des infos. Si tout le monde lit au m√™me endroit, √ßa ralentit.\nPourquoi C : les ‚Äúread replicas‚Äù sont comme des copies du livre faites expr√®s pour la lecture. Tu en ajoutes une ou plusieurs, et l‚Äôappli envoie les demandes de lecture vers ces copies (URL des replicas). R√©sultat : moins d‚Äôembouteillage, lectures plus rapides.\nPourquoi pas A/B : ‚Äúmulti-AZ‚Äù c‚Äôest plut√¥t comme une salle de secours si la biblioth√®que ferme (fiabilit√©), pas pour acc√©l√©rer la lecture. Et la ‚Äúsecondaire‚Äù n‚Äôest pas faite pour lire.\nPourquoi pas D : faire une copie sur une autre machine (EC2) avec un logiciel, c‚Äôest comme recr√©er une mini-biblioth√®que √† la main : plus compliqu√© √† g√©rer maintenant et plus tard.\nDonc C = le plus simple et le plus efficace pour beaucoup de lectures.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:11:2e63224eb4c9d10d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 11,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is migrating legacy internal applications to AWS. Leadership wants to rewrite the internal employee directory to use native AWS services. A developer needs to create a solution for storing employee contact details and high-resolution photos for use with the new application.Which solution will enable the search and retrieval of each employee's individual details and high-resolution photos using AWS APIs?",
      "choices": {
        "A": "Encode each employee's contact information and photos using Base64. Store the information in an Amazon DynamoDB table using a sort key.",
        "B": "Store each employee's contact information in an Amazon DynamoDB table along with the object keys for the photos stored in Amazon S3.",
        "C": "Use Amazon Cognito user pools to implement the employee directory in a fully managed software-as-a-service (SaaS) method.",
        "D": "Store employee contact information in an Amazon RDS DB instance with the photos stored in Amazon Elastic File System (Amazon EFS)."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102787-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 16, 2023, 10:01 a.m.",
      "textHash": "2e63224eb4c9d10d",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "Ici, on doit stocker deux types de donn√©es : (1) des infos texte (nom, email, t√©l√©phone) et (2) des photos haute r√©solution (fichiers lourds).\nAmazon DynamoDB est une base NoSQL g√©r√©e, tr√®s rapide pour chercher un employ√© par identifiant et r√©cup√©rer ses champs via API.\nAmazon S3 est un stockage d‚Äôobjets fait pour les fichiers (images, vid√©os) : il est durable, peu co√ªteux et accessible via API.\nLa bonne pratique est donc : mettre les d√©tails de contact dans DynamoDB, et dans la m√™me ligne stocker la ‚Äúcl√© d‚Äôobjet‚Äù (le chemin/nom) de la photo dans S3.\nQuand l‚Äôapplication cherche un employ√©, elle lit DynamoDB pour obtenir ses infos + la cl√© S3, puis elle r√©cup√®re la photo depuis S3.\nA est mauvais car encoder une photo en Base64 et la mettre dans DynamoDB gonfle la taille, co√ªte cher et d√©passe vite les limites d‚Äôitem.\nC est mauvais car Cognito sert surtout √† g√©rer l‚Äôauthentification et des profils utilisateurs, pas un annuaire avec photos HD comme stockage principal.\nD est moins adapt√© : RDS + EFS est plus complexe √† g√©rer et EFS n‚Äôest pas id√©al pour servir des objets comme des photos via API compar√© √† S3.\nDonc B r√©pond exactement au besoin : recherche simple via DynamoDB et stockage de photos HD via S3, le tout accessible par APIs AWS.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le CDI de ton lyc√©e : une fiche papier par √©l√®ve (nom, classe, t√©l√©phone) et, √† c√¥t√©, une grande photo rang√©e dans une armoire avec un num√©ro d‚Äô√©tiquette.**\n\nConcept : on s√©pare ce qui est ‚Äúpetit et facile √† chercher‚Äù (la fiche) de ce qui est ‚Äúgros et lourd‚Äù (la photo). La fiche doit √™tre trouv√©e vite, la photo doit √™tre stock√©e dans un endroit fait pour les gros fichiers.\nAvec AWS : DynamoDB = le classeur de fiches ultra-rapide pour chercher un employ√©. S3 = l‚Äôarmoire g√©ante pour stocker des photos en haute r√©solution.\nR√©ponse B : on met les infos de contact dans DynamoDB + on met juste le ‚Äúnum√©ro d‚Äô√©tiquette‚Äù (la cl√©/nom du fichier) qui pointe vers la photo dans S3. L‚Äôappli cherche la fiche via une API, puis r√©cup√®re la photo via une API avec cette cl√©.\nPourquoi pas A : coller la photo dans la fiche (Base64) rend la fiche √©norme et moins pratique.\nPourquoi pas C : Cognito sert surtout √† g√©rer des comptes et connexions, pas un annuaire avec photos HD.\nPourquoi pas D : RDS+EFS marche, mais c‚Äôest plus lourd √† g√©rer, et S3 est fait pour stocker des objets comme des photos.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:453:5ad304f09b69a6b5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 453,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is developing a serverless application that requires storage of sensitive API keys as environment variables for various services. The application requires the automatic rotation of the encryption keys every year.Which solution will meet these requirements with no development effort?",
      "choices": {
        "A": "Encrypt the environment variables by using AWS Secrets Manager. Set up automatic rotation in Secrets Manager.",
        "B": "Encrypt the environment variables by using AWS Key Management Service (AWS KMS) customer managed keys. Enable automatic key rotation.",
        "C": "Encrypt the environment variables by using AWS Key Management Service (AWS KMS) AWS managed keys. Configure a custom AWS Lambda function to automate key rotation.",
        "D": "Encrypt the environment variables by using AWS Systems Manager Parameter Store. Set up automatic rotation in Parameter Store."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/151333-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 14, 2024, 9:41 p.m.",
      "textHash": "5ad304f09b69a6b5",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "On veut stocker des cl√©s API sensibles dans des variables d‚Äôenvironnement (ex: pour une fonction AWS Lambda) et chiffrer ces valeurs.\nAWS KMS est le service qui g√®re des cl√©s de chiffrement. Une ‚Äúcustomer managed key‚Äù (CMK) est une cl√© KMS que vous cr√©ez et contr√¥lez.\nExigence importante : rotation automatique des cl√©s de chiffrement chaque ann√©e, sans d√©veloppement.\nAvec une cl√© KMS g√©r√©e par le client, AWS propose une option int√©gr√©e ‚ÄúAutomatic key rotation‚Äù (rotation annuelle) : il suffit de l‚Äôactiver.\nAinsi, les variables d‚Äôenvironnement chiffr√©es avec cette cl√© restent prot√©g√©es, et KMS renouvelle automatiquement la cl√© selon la politique.\nSecrets Manager g√®re surtout la rotation de secrets (mots de passe/jetons) via du code ou des int√©grations, pas la rotation automatique de la cl√© KMS elle-m√™me pour ce besoin.\nLes cl√©s KMS ‚ÄúAWS managed‚Äù ne permettent pas de configurer vous-m√™me la rotation comme demand√©, et une Lambda de rotation serait du d√©veloppement.\nParameter Store ne fournit pas une rotation automatique int√©gr√©e des cl√©s de chiffrement pour ce cas.\nDonc la solution sans effort de d√©veloppement est : utiliser une cl√© KMS g√©r√©e par le client et activer la rotation automatique.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine le casier du prof au lyc√©e : dedans il garde des papiers ultra secrets (les cl√©s d‚ÄôAPI). Le casier est ferm√© avec un cadenas, et le lyc√©e veut que le cadenas change automatiquement chaque ann√©e, sans que personne ne bricole quoi que ce soit.**\n\nConcept : les cl√©s d‚ÄôAPI sont des secrets, donc on les met dans un ‚Äúcoffre‚Äù et on les chiffre (on les rend illisibles) avec une ‚Äúcl√© de chiffrement‚Äù (le cadenas). Exigence : le cadenas doit se renouveler automatiquement tous les ans, sans effort de dev. Avec AWS KMS et une cl√© g√©r√©e par l‚Äôentreprise (customer managed key), tu peux activer la rotation automatique annuelle : AWS change le ‚Äúcadenas‚Äù tout seul. C‚Äôest exactement la r√©ponse B. A parle d‚Äôun coffre √† secrets (Secrets Manager) mais la question vise la rotation des cl√©s de chiffrement sans effort, et KMS le fait nativement. C demande d‚Äô√©crire une fonction (du dev), donc interdit. D (Parameter Store) ne fait pas la rotation automatique des cl√©s comme demand√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:439:6713aea878341f94",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 439,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a solution to track an account's Amazon S3 buckets over time. The developer has created an AWS Lambda function that will run on a schedule. The function will list the account's S3 buckets and will store the list in an Amazon DynamoDB table. The developer receives a permissions error when the developer runs the function with the AWSLambdaBasicExecutionRole AWS managed policy.Which combination of permissions should the developer use to resolve this error? (Choose two.)",
      "choices": {
        "A": "Cross-account IAM role",
        "B": "Permission for the Lambda function to list buckets in Amazon S3",
        "C": "Permission for the Lambda function to write in DynamoDB",
        "D": "Permission for Amazon S3 to invoke the Lambda function",
        "E": "Permission for DynamoDB to invoke the Lambda function"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148957-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 10, 2024, 7:58 a.m.",
      "textHash": "6713aea878341f94",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, la fonction AWS Lambda (un code qui s‚Äôex√©cute automatiquement) doit faire deux actions : 1) lister les buckets Amazon S3, 2) enregistrer le r√©sultat dans une table Amazon DynamoDB (base NoSQL).\nLa policy AWSLambdaBasicExecutionRole ne donne en g√©n√©ral que des droits de base (surtout √©crire des logs dans CloudWatch), pas l‚Äôacc√®s √† S3 ni √† DynamoDB.\nL‚Äôerreur de permissions vient donc du fait que Lambda n‚Äôa pas le droit d‚Äôappeler l‚ÄôAPI S3 qui liste les buckets (s3:ListAllMyBuckets).\nIl faut ajouter une permission permettant √† la fonction de lister les buckets S3 (choix B).\nComme la fonction doit aussi stocker la liste dans DynamoDB, il faut √©galement une permission d‚Äô√©criture sur la table (PutItem/UpdateItem) (choix C).\nLes options ‚ÄúS3/DynamoDB invoque Lambda‚Äù (D/E) concernent des d√©clencheurs, mais ici Lambda est lanc√© par un planning, donc ce n‚Äôest pas le probl√®me.\nUn r√¥le cross-account (A) n‚Äôest utile que si on acc√®de √† un autre compte AWS, ce qui n‚Äôest pas mentionn√©.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que tu es d√©l√©gu√© de classe et que tu dois, chaque lundi, faire la liste de toutes les salles utilis√©es dans le coll√®ge, puis noter cette liste dans un cahier au CDI.**\n\nLe ‚Äúprogramme‚Äù (Lambda) c‚Äôest l‚Äô√©l√®ve d√©l√©gu√© qui travaille tout seul √† heure fixe.\nLa ‚Äúliste des salles‚Äù (buckets S3) c‚Äôest ce qu‚Äôil veut aller voir dans le coll√®ge.\nLe ‚Äúcahier au CDI‚Äù (table DynamoDB) c‚Äôest l‚Äôendroit o√π il √©crit l‚Äôhistorique.\nLe badge de base (AWSLambdaBasicExecutionRole) lui permet surtout d‚Äô√©crire un compte-rendu, pas d‚Äôouvrir toutes les portes.\nL‚Äôerreur de permissions arrive parce qu‚Äôil n‚Äôa pas le droit d‚Äôaller consulter la liste des salles.\nDonc il faut lui donner l‚Äôautorisation de ‚Äúlister les salles‚Äù = lister les buckets dans S3.\nC‚Äôest exactement le choix B.\nLes options ‚Äúinvoker Lambda‚Äù (D/E) seraient comme dire ‚Äúla salle appelle l‚Äô√©l√®ve‚Äù, mais ici c‚Äôest l‚Äô√©l√®ve qui va voir les salles.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:136:fa48ea53ac49eba0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 136,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is setting up a deployment pipeline. The pipeline includes an AWS CodeBuild build stage that requires access to a database to run integration tests. The developer is using a buildspec.yml file to configure the database connection. Company policy requires automatic rotation of all database credentials.Which solution will handle the database credentials MOST securely?",
      "choices": {
        "A": "Retrieve the credentials from variables that are hardcoded in the buildspec.yml file. Configure an AWS Lambda function to rotate the credentials.",
        "B": "Retrieve the credentials from an environment variable that is linked to a SecureString parameter in AWS Systems Manager Parameter Store. Configure Parameter Store for automatic rotation.",
        "C": "Retrieve the credentials from an environment variable that is linked to an AWS Secrets Manager secret. Configure Secrets Manager for automatic rotation.",
        "D": "Retrieve the credentials from an environment variable that contains the connection string in plaintext. Configure an Amazon EventBridge event to rotate the credentials."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/117332-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Aug. 4, 2023, 1:39 p.m.",
      "textHash": "fa48ea53ac49eba0",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Dans un pipeline, CodeBuild ex√©cute des tests et a besoin d‚Äôidentifiants (login/mot de passe) pour se connecter √† la base de donn√©es. Les mettre en dur dans buildspec.yml ou en clair dans une variable d‚Äôenvironnement est risqu√© : le fichier peut √™tre lu, versionn√© dans Git, ou affich√© dans des logs. AWS Secrets Manager est un service fait pour stocker des secrets (mots de passe, cha√Ænes de connexion) de fa√ßon chiffr√©e et contr√¥l√©e par IAM. CodeBuild peut r√©cup√©rer le secret via une variable d‚Äôenvironnement ‚Äúli√©e‚Äù au secret, sans √©crire le mot de passe dans le code. Secrets Manager sait aussi faire la rotation automatique : il change r√©guli√®rement le mot de passe dans la base et met √† jour la valeur du secret. Ainsi, les tests utilisent toujours des identifiants √† jour, sans intervention manuelle. Parameter Store peut stocker des SecureString, mais la rotation automatique native des identifiants de base est surtout pr√©vue avec Secrets Manager. Donc la solution la plus s√©curis√©e est d‚Äôutiliser Secrets Manager + rotation automatique (C).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:131:95e84376ae0db8ad",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 131,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is developing an ecommerce application that uses Amazon API Gateway APIs. The application uses AWS Lambda as a backend. The company needs to test the code in a dedicated, monitored test environment before the company releases the code to the production environment.Which solution will meet these requirements?",
      "choices": {
        "A": "Use a single stage in API Gateway. Create a Lambda function for each environment. Configure API clients to send a query parameter that indicates the environment and the specific Lambda function.",
        "B": "Use multiple stages in API Gateway. Create a single Lambda function for all environments. Add different code blocks for different environments in the Lambda function based on Lambda environment variables.",
        "C": "Use multiple stages in API Gateway. Create a Lambda function for each environment. Configure API Gateway stage variables to route traffic to the Lambda function in different environments.",
        "D": "Use a single stage in API Gateway. Configure API clients to send a query parameter that indicates the environment. Add different code blocks for different environments in the Lambda function to match the value of the query parameter."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/111832-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 10, 2023, 2:52 p.m.",
      "textHash": "95e84376ae0db8ad",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:ff9847a6",
      "frExplanation": "On veut deux environnements s√©par√©s : test (surveill√©) et production, pour valider avant de publier.\nAmazon API Gateway expose des API et peut avoir plusieurs ¬´ stages ¬ª (ex: /test, /prod) : chaque stage est une version/configuration isol√©e de l‚ÄôAPI.\nAWS Lambda ex√©cute le code backend ; avoir une fonction Lambda par environnement √©vite de m√©langer les versions, les permissions et les logs.\nLes ¬´ stage variables ¬ª d‚ÄôAPI Gateway sont des param√®tres propres √† un stage, utilis√©s pour choisir des valeurs diff√©rentes selon test ou prod.\nAvec C, le stage /test pointe vers la Lambda de test, et le stage /prod pointe vers la Lambda de production : s√©paration claire et facile √† monitorer.\nC‚Äôest plus s√ªr et plus simple que d‚Äôenvoyer un query parameter depuis le client (A/D), car le client pourrait appeler la mauvaise cible.\nC‚Äôest aussi mieux que de mettre plusieurs comportements dans une seule Lambda (B/D), car cela complique le code et augmente le risque d‚Äôerreur en production.\nDonc : plusieurs stages + une Lambda par environnement + stage variables pour router = environnement d√©di√© et contr√¥l√©.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine le site e‚Äëcommerce comme une cantine. API Gateway, c‚Äôest le guichet o√π les √©l√®ves passent commande. Lambda, c‚Äôest la cuisine qui pr√©pare les plats. Tu veux une cuisine d‚Äôentra√Ænement (test) bien surveill√©e avant d‚Äôouvrir la vraie cuisine (production).**\n\nConcept : tu fais deux guichets s√©par√©s (deux ‚Äústages‚Äù) : un pour s‚Äôentra√Æner, un pour le service r√©el. Comme √ßa, tu testes sans d√©ranger les vrais clients.\nPourquoi C : avec plusieurs stages, chaque guichet a son panneau interne (variables de stage) qui dit vers quelle cuisine envoyer les commandes. Le guichet ‚Äútest‚Äù envoie vers la cuisine ‚Äútest‚Äù, le guichet ‚Äúprod‚Äù vers la cuisine ‚Äúprod‚Äù.\nEt tu as bien une cuisine diff√©rente par environnement, donc tu peux surveiller et contr√¥ler le test sans toucher la production.\nLes autres choix m√©langent tout (un seul guichet ou une seule cuisine avec du code qui change), ce qui augmente les risques d‚Äôerreur.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:251:9e1c5dd35d79ca0a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 251,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has created a data collection application that uses Amazon API Gateway, AWS Lambda, and Amazon S3. The application‚Äôs users periodically upload data files and wait for the validation status to be reflected on a processing dashboard. The validation process is complex and time-consuming for large files.Some users are uploading dozens of large files and have to wait and refresh the processing dashboard to see if the files have been validated. The developer must refactor the application to immediately update the validation result on the user‚Äôs dashboard without reloading the full dashboard.What is the MOST operationally efficient solution that meets these requirements?",
      "choices": {
        "A": "Integrate the client with an API Gateway WebSocket API. Save the user-uploaded files with the WebSocket connection ID. Push the validation status to the connection ID when the processing is complete to initiate an update of the user interface.",
        "B": "Launch an Amazon EC2 micro instance, and set up a WebSocket server. Send the user-uploaded file and user detail to the EC2 instance after the user uploads the file. Use the WebSocket server to send updates to the user interface when the uploaded file is processed.",
        "C": "Save the user‚Äôs email address along with the user-uploaded file. When the validation process is complete, send an email notification through Amazon Simple Notification Service (Amazon SNS) to the user who uploaded the file.",
        "D": "Save the user-uploaded file and user detail to Amazon DynamoDB. Use Amazon DynamoDB Streams with Amazon Simple Notification Service (Amazon SNS) push notifications to send updates to the browser to update the user interface."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124862-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 29, 2023, 2:05 a.m.",
      "textHash": "9e1c5dd35d79ca0a",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Le besoin est d‚Äôactualiser le tableau de bord tout de suite, sans que l‚Äôutilisateur recharge la page (donc il faut une communication ‚Äútemps r√©el‚Äù du serveur vers le navigateur).\nAPI Gateway WebSocket permet de garder une connexion ouverte entre le navigateur et AWS, et d‚Äôenvoyer un message au client d√®s qu‚Äôun √©v√©nement arrive.\nOn peut stocker l‚ÄôID de connexion WebSocket (connection ID) associ√© au fichier upload√© dans S3, puis lancer la validation via Lambda.\nQuand la validation (longue) se termine, Lambda envoie un message via l‚ÄôAPI WebSocket √† ce connection ID : le navigateur re√ßoit l‚Äôinfo et met √† jour seulement la partie ‚Äústatut‚Äù.\nC‚Äôest ‚Äúop√©rationnellement efficace‚Äù car c‚Äôest manag√© (pas de serveur √† maintenir), scalable, et adapt√© aux mises √† jour en direct.\nB est moins efficace car il faut g√©rer un serveur EC2 (patching, scaling, disponibilit√©).\nC envoie un email, ce n‚Äôest pas une mise √† jour imm√©diate du dashboard.\nD ne marche pas bien pour un navigateur : SNS ne pousse pas directement vers un browser sans autre composant, et DynamoDB n‚Äôest pas n√©cessaire ici.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le tableau d‚Äôaffichage du lyc√©e : tu d√©poses un devoir au secr√©tariat, puis tu veux voir ‚Äúvalid√© / refus√©‚Äù appara√Ætre sur TON √©cran sans recharger toute la page.**\n\nConcept : au lieu de rafra√Æchir le tableau d‚Äôaffichage, tu gardes une ‚Äúligne directe‚Äù ouverte avec le secr√©tariat (comme une conversation en live). Quand ils finissent de corriger, ils t‚Äôenvoient un message instantan√©.\nDans AWS, l‚Äôutilisateur envoie le fichier, puis le traitement (Lambda) peut prendre du temps. Avec un WebSocket via API Gateway, le navigateur garde une connexion ouverte.\nOn garde l‚ÄôID de cette connexion (comme ton num√©ro de casier) avec le fichier. Quand la validation est finie, le syst√®me envoie le r√©sultat directement √† CETTE connexion.\nR√©sultat : le dashboard se met √† jour tout seul, sans recharger toute la page.\nPourquoi pas B : g√©rer un serveur WebSocket sur une machine (EC2) = plus de boulot (maintenance), moins ‚Äúefficace‚Äù.\nPourquoi pas C : un email ne met pas √† jour le dashboard en direct.\nPourquoi pas D : DynamoDB Streams + notifications vers navigateur, c‚Äôest plus compliqu√© et pas fait pour une mise √† jour live simple du dashboard.\nDonc A est le plus simple √† op√©rer et r√©pond au besoin ‚Äúmise √† jour imm√©diate sans rechargement‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:109:6fc7f1a94325b9ba",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 109,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer wants to use AWS Elastic Beanstalk to test a new version of an application in a test environment.Which deployment method offers the FASTEST deployment?",
      "choices": {
        "A": "Immutable",
        "B": "Rolling",
        "C": "Rolling with additional batch",
        "D": "All at once"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/109399-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 16, 2023, 1:52 p.m.",
      "textHash": "6fc7f1a94325b9ba",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:0c0247b8",
      "frExplanation": "Elastic Beanstalk est un service AWS qui d√©ploie automatiquement votre application sur des serveurs (instances) et g√®re les mises √† jour.\nOn cherche la m√©thode la PLUS RAPIDE pour mettre une nouvelle version en place dans un environnement de test.\n\"All at once\" d√©ploie la nouvelle version sur toutes les instances en une seule fois : c‚Äôest donc la plus rapide en dur√©e de d√©ploiement.\nEn contrepartie, il y a un risque de coupure ou d‚Äôerreurs visibles pendant la mise √† jour, car tout change en m√™me temps.\n\"Rolling\" et \"Rolling with additional batch\" mettent √† jour par lots (plus lent car progressif, mais plus s√ªr).\n\"Immutable\" cr√©e un nouvel ensemble d‚Äôinstances puis bascule : plus s√ªr, mais plus long car il faut provisionner de nouvelles instances.\nComme l‚Äôobjectif est juste de tester vite (et pas forc√©ment d‚Äô√©viter toute interruption), \"All at once\" est le meilleur choix.\nDonc la r√©ponse correcte est D.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:557:a27c2986e3a013ed",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 557,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is designing a game that stores data in an Amazon DynamoDB table. The partition key of the table is the country of the player. After a sudden increase in the number of players in a specific country, the developer notices ProvisionedThroughputExceededException errors.What should the developer do to resolve these errors?",
      "choices": {
        "A": "Use strongly consistent table reads.",
        "B": "Revise the primary key to use more unique identifiers.",
        "C": "Use pagination to reduce the size of the items that the queries return.",
        "D": "Use the Scan operation to retrieve the data."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156016-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 6, 2025, 8:32 a.m.",
      "textHash": "a27c2986e3a013ed",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base de donn√©es NoSQL o√π les donn√©es sont r√©parties sur plusieurs ‚Äúpartitions‚Äù selon la cl√© de partition.\nIci, la cl√© de partition est le pays. Si beaucoup de joueurs viennent d‚Äôun m√™me pays, toutes leurs √©critures/lectures vont sur la m√™me partition.\nCela cr√©e un ‚Äúhot partition‚Äù (partition surcharg√©e) et DynamoDB d√©passe la capacit√© provisionn√©e, d‚Äôo√π ProvisionedThroughputExceededException.\nLa bonne solution est de mieux r√©partir la charge en changeant la cl√© primaire pour qu‚Äôelle soit plus unique (ex: country#playerId, ou ajouter un identifiant al√©atoire).\nAinsi, les requ√™tes et √©critures se distribuent sur plusieurs partitions et la capacit√© est utilis√©e de fa√ßon √©quilibr√©e.\nA n‚Äôaide pas (lecture forte peut m√™me co√ªter plus). C ne r√®gle pas la surcharge de la partition. D (Scan) est plus lourd et aggrave souvent le probl√®me.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e avec des files par pays : une file ‚ÄúFrance‚Äù, une file ‚ÄúBr√©sil‚Äù, etc. Le ‚Äúpays‚Äù est l‚Äô√©tiquette qui d√©cide dans quelle file tu vas.**\n\nDans DynamoDB, la ‚Äúpartition key‚Äù (ici le pays) sert √† ranger les joueurs dans des ‚Äúbacs‚Äù s√©par√©s, comme les files de la cantine.\nSi tout √† coup plein de joueurs viennent du m√™me pays, tout le monde se retrouve dans la m√™me file : √ßa sature.\nLes erreurs ProvisionedThroughputExceededException, c‚Äôest comme un panneau ‚Äútrop de monde, on ne peut plus servir aussi vite‚Äù.\nLa solution est de changer la cl√© principale pour r√©partir mieux les joueurs : ajouter un identifiant plus unique (ex: pays + id joueur).\nComme √ßa, au lieu d‚Äôune seule file ‚ÄúFrance‚Äù, tu cr√©es plein de petites files ‚ÄúFrance-001‚Äù, ‚ÄúFrance-002‚Äù‚Ä¶\nLe service peut servir en parall√®le, donc plus de bouchon.\nA ne change pas la taille de la file, juste la r√®gle de service.\nC et D ne font que demander autrement, mais la file reste satur√©e.\nDonc la bonne r√©ponse est B.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:553:6ff95649889f8fb1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 553,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is creating a new feature for existing software. Before the company fully releases a new version of the software, the company wants to test the feature.The company needs to gather feedback about the feature from a small group of users while the current software version remains deployed. If the testing validates the feature, the company needs to deploy the new software version to all other users at the same time.Which deployment strategy will meet these requirements?",
      "choices": {
        "A": "All-at-once deployment",
        "B": "Canary deployment",
        "C": "In-place deployment",
        "D": "Linear deployment"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157447-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 3, 2025, 8:46 a.m.",
      "textHash": "6ff95649889f8fb1",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:8fe485be",
      "frExplanation": "Ici, l‚Äôobjectif est de tester une nouvelle fonctionnalit√© avec un petit groupe d‚Äôutilisateurs, tout en gardant la version actuelle pour la majorit√©.\nUne strat√©gie ¬´ canary ¬ª fait exactement cela : on d√©ploie la nouvelle version seulement pour un faible pourcentage de trafic/utilisateurs.\nOn observe les retours (bugs, performance, satisfaction) sans impacter tout le monde.\nSi tout est bon, on augmente ensuite le trafic vers la nouvelle version jusqu‚Äô√† 100% (tout le monde l‚Äôutilise).\nC‚Äôest donc id√©al pour ‚Äúpetit groupe d‚Äôabord, puis tout le monde‚Äù.\nAll-at-once mettrait tout le monde sur la nouvelle version imm√©diatement (trop risqu√©).\nIn-place remplace la version sur les m√™mes serveurs, sans phase contr√¥l√©e pour un petit groupe.\nLinear augmente progressivement aussi, mais l‚Äôid√©e cl√© demand√©e est un test sur un petit groupe avant g√©n√©ralisation : c‚Äôest le canary.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:552:29f4b4a4d42dfb79",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 552,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company stores data in an Amazon S3 bucket. The data is updated multiple times every day from an application that runs on a server in the company‚Äôs on-premises data center.The company enables S3 Versioning on the S3 bucket. After some time, the company observes multiple versions of the same objects in the S3 bucket.The company needs the S3 bucket to keep the current version of each object and the version immediately previous to the current version.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure an S3 bucket policy to retain one newer noncurrent version of the objects.",
        "B": "Configure an S3 Lifecycle rule to retain one newer noncurrent version of the objects.",
        "C": "Enable S3 Object Lock. Configure an S3 Object Lock policy to retain one newer noncurrent version of the objects.",
        "D": "Suspend S3 Versioning. Modify the application code to check the number of object versions before updating the objects."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157448-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 3, 2025, 8:48 a.m.",
      "textHash": "29f4b4a4d42dfb79",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Amazon S3 est un stockage de fichiers (objets) dans le cloud. Avec S3 Versioning activ√©, chaque mise √† jour d‚Äôun fichier cr√©e une nouvelle ‚Äúversion courante‚Äù et les anciennes deviennent des ‚Äúversions non courantes‚Äù.\nLe besoin est simple : garder seulement la version actuelle + la version juste avant (donc 1 seule version non courante par objet).\nLa bonne fa√ßon de g√©rer automatiquement le nombre de versions dans S3 est une r√®gle S3 Lifecycle (cycle de vie) : elle peut expirer/supprimer les anciennes versions non courantes.\nOn configure la r√®gle pour ‚Äúconserver 1 version non courante la plus r√©cente‚Äù et supprimer les autres versions non courantes plus anciennes.\nUne bucket policy (A) sert surtout √† autoriser/refuser des actions, pas √† supprimer des versions selon un nombre.\nS3 Object Lock (C) sert √† emp√™cher la suppression/modification pendant une dur√©e (conformit√©), pas √† limiter √† 2 versions.\nSuspendre le versioning et modifier l‚Äôapplication (D) est inutilement complexe et ne g√®re pas proprement l‚Äôhistorique d√©j√† cr√©√©.\nDonc la solution attendue est : r√®gle S3 Lifecycle pour ne garder qu‚Äôune seule version non courante.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:530:fc7979d8439ea32b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 530,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has an application container, an AWS Lambda function, and an Amazon Simple Queue Service (Amazon SQS) queue. The Lambda function uses the SQS queue as an event source. The Lambda function makes a call to a third-party machine learning API when the function is invoked. The response from the third-party API can take up to 60 seconds to return.The Lambda function's timeout value is currently 65 seconds. The developer has noticed that the Lambda function sometimes processes duplicate messages from the SQS queue.What should the developer do to ensure that the Lambda function does not process duplicate messages?",
      "choices": {
        "A": "Configure the Lambda function with a larger amount of memory.",
        "B": "Configure an increase in the Lambda function‚Äôs timeout value.",
        "C": "Configure the SQS queue‚Äôs delivery delay value to be greater than the maximum time it takes to call the third-party API.",
        "D": "Configure the SQS queue‚Äôs visibility timeout value to be greater than the maximum time it takes to call the third-party API."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156681-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:48 a.m.",
      "textHash": "fc7979d8439ea32b",
      "rawFormat": "discussion-md",
      "conceptKey": "sqs_visibility_timeout",
      "frExplanation": "Ici, Amazon SQS est une file de messages, et AWS Lambda est une fonction qui lit ces messages automatiquement (event source).\nQuand Lambda re√ßoit un message SQS, SQS le ‚Äúcache‚Äù temporairement pour les autres consommateurs : c‚Äôest le visibility timeout.\nSi Lambda met trop de temps (jusqu‚Äô√† 60 s pour l‚ÄôAPI externe) et que le visibility timeout est plus court, le message redevient visible avant la fin du traitement.\nR√©sultat : SQS peut red√©livrer le m√™me message, et Lambda le traite une deuxi√®me fois ‚Üí doublons.\nLa bonne action est donc d‚Äôaugmenter le visibility timeout de la file SQS au-del√† du temps maximum de traitement (appel API + marge).\nLe timeout Lambda (65 s) n‚Äôemp√™che pas SQS de red√©livrer : il contr√¥le seulement combien de temps la fonction peut tourner.\nAugmenter la m√©moire peut acc√©l√©rer un peu, mais ne garantit pas l‚Äôabsence de doublons.\nLe delivery delay retarde l‚Äôenvoi initial des messages, pas les red√©liveries pendant le traitement.\nDonc : r√©gler le visibility timeout SQS > dur√©e max de traitement pour √©viter les doublons.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine : il y a une bo√Æte de tickets (SQS) et un √©l√®ve (Lambda) prend un ticket pour pr√©parer un plateau. Pendant qu‚Äôil pr√©pare, le ticket doit √™tre ‚Äúcach√©‚Äù pour que personne d‚Äôautre ne le reprenne.**\n\nConcept : SQS donne un message √† Lambda, puis le ‚Äúcache‚Äù un moment (visibility timeout). Si Lambda n‚Äôa pas fini avant la fin de ce cache, SQS pense que personne ne s‚Äôen occupe et remet le m√™me message en jeu ‚Üí doublon.\nIci, Lambda appelle un service externe qui peut prendre 60 s. Si le message redevient visible avant la fin du traitement, Lambda peut le reprendre une 2e fois.\nDonc il faut r√©gler le visibility timeout de la file SQS √† plus que 60 s (et un peu de marge), pour que le ticket reste cach√© jusqu‚Äô√† la fin.\nPlus de m√©moire (A) ou plus de temps Lambda (B) ne garantit pas que SQS ne r√©affiche pas le message.\nLe ‚Äúdelivery delay‚Äù (C) retarde juste l‚Äôarriv√©e du ticket au d√©but, √ßa n‚Äôemp√™che pas les doublons pendant le traitement.\n=> D est la bonne r√©ponse.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:529:958b0ae0901d8027",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 529,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an image-processing application that includes an AWS Lambda function. The Lambda function moves images from one AWS service to another AWS service for image processing. For images that are larger than 2 MB, the Lambda function returns the following error: ‚ÄúTask timed out after 3.01 seconds.‚ÄùThe developer needs to resolve the error without modifying the Lambda function code.Which solution will meet these requirements?",
      "choices": {
        "A": "Increase the Lambda function‚Äôs timeout value.",
        "B": "Configure the Lambda function to not move images that are larger than 2 MB.",
        "C": "Request a concurrency quota increase for the Lambda function.",
        "D": "Configure provisioned concurrency for the Lambda function."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156680-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:47 a.m.",
      "textHash": "958b0ae0901d8027",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:903155b8",
      "frExplanation": "AWS Lambda est un service qui ex√©cute votre code ‚Äú√† la demande‚Äù sans serveur. Chaque ex√©cution a une dur√©e maximale (timeout) configur√©e.\nL‚Äôerreur ‚ÄúTask timed out after 3.01 seconds‚Äù signifie que la fonction a √©t√© arr√™t√©e parce qu‚Äôelle a d√©pass√© le temps autoris√© (ici ~3 secondes).\nD√©placer et copier une image plus grande (> 2 MB) prend souvent plus de temps (r√©seau, lecture/√©criture dans un service de stockage comme Amazon S3, etc.).\nOn vous demande de corriger le probl√®me sans changer le code : il faut donc agir sur la configuration.\nAugmenter le timeout donne plus de temps √† la fonction pour terminer le transfert et lancer le traitement.\nLes autres options ne r√©solvent pas un d√©passement de temps : la concurrence (quota ou provisioned concurrency) aide au nombre d‚Äôex√©cutions ou au ‚Äúcold start‚Äù, pas √† la dur√©e d‚Äôune ex√©cution.\nBloquer les images > 2 MB (option B) contourne le besoin m√©tier et ne ‚Äúr√©sout‚Äù pas l‚Äôerreur.\nDonc la bonne solution est d‚Äôaugmenter la valeur de timeout de la fonction Lambda.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un √©l√®ve √† la cantine qui doit porter des plateaux d‚Äôun comptoir √† un autre. Le surveillant lui donne 3 secondes pour faire l‚Äôaller-retour. Avec un petit plateau, √ßa passe. Avec un plateau tr√®s lourd, il n‚Äôa pas le temps et on l‚Äôarr√™te.**\n\nIci, la fonction Lambda, c‚Äôest l‚Äô√©l√®ve. D√©placer une image, c‚Äôest porter le plateau. Le message ‚ÄúTask timed out after 3.01 seconds‚Äù veut dire : le temps autoris√© (3 secondes) est trop court. Les images > 2 MB sont plus ‚Äúlourdes‚Äù, donc le d√©placement prend plus de temps. On ne veut pas changer le ‚Äútrajet‚Äù (le code), donc on change la r√®gle du surveillant : on donne plus de temps. R√©ponse A : augmenter le timeout, c‚Äôest autoriser l‚Äô√©l√®ve √† prendre plus de secondes pour finir. B refuse les gros plateaux (pas demand√©). C et D donnent plus d‚Äô√©l√®ves pr√™ts, mais si chacun n‚Äôa que 3 secondes, le probl√®me reste.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:517:14ef1443b7b7ee05",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 517,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that processes audio files for different departments. When audio files are saved to an Amazon S3 bucket, an AWS Lambda function receives an event notification and processes the audio input.A developer needs to update the solution so that the application can process the audio files for each department independently. The application must publish the audio file location for each department to each department's existing Amazon Simple Queue Service (Amazon SQS) queue.Which solution will meet these requirements with no changes to the Lambda function code?",
      "choices": {
        "A": "Configure the S3 bucket to send the event notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe each department‚Äôs SQS queue to the SNS topic. Configure subscription filter policies.",
        "B": "Update the Lambda function to write the file location to a single shared SQS queue. Configure the shared SQS queue to send the file reference to each department‚Äôs SQS queue.",
        "C": "Update the Lambda function to send the file location to each department‚Äôs SQS queue.",
        "D": "Configure the S3 bucket to send the event notifications to each department‚Äôs SQS queue."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157450-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 3, 2025, 10:50 a.m.",
      "textHash": "14ef1443b7b7ee05",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Objectif : envoyer l‚Äôemplacement (cl√©/URL) du fichier S3 √† plusieurs files SQS, une par d√©partement, sans toucher au code Lambda.\nAmazon S3 est le stockage de fichiers ; il peut d√©clencher des notifications quand un objet est ajout√©.\nAWS Lambda ex√©cute du code sur √©v√©nement, mais ici on ne veut pas le modifier.\nAmazon SNS est un service de ‚Äúdiffusion‚Äù (pub/sub) : un message publi√© sur un topic peut √™tre envoy√© √† plusieurs abonn√©s.\nAmazon SQS est une file de messages : chaque d√©partement a d√©j√† sa propre file.\nSolution A : S3 envoie l‚Äô√©v√©nement √† un topic SNS, puis chaque file SQS s‚Äôabonne au topic.\nLes ‚Äúfilter policies‚Äù SNS permettent d‚Äôenvoyer seulement les messages d‚Äôun d√©partement √† sa file (ex : selon pr√©fixe de dossier, tag, ou attributs du message).\nAinsi, chaque d√©partement re√ßoit ind√©pendamment ses messages, et Lambda n‚Äôa pas besoin de changer.\nLes autres choix exigent de modifier Lambda (B, C) ou ne permettent pas facilement le filtrage/gestion multi-destinations sans complexit√© (D).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine le secr√©tariat du lyc√©e : d√®s qu‚Äôun devoir est d√©pos√© dans une bo√Æte, il faut pr√©venir plusieurs profs (par mati√®re) sans changer la fa√ßon dont le secr√©tariat travaille.**\n\nConcept : S3 = la bo√Æte o√π on d√©pose les fichiers audio. Lambda = le secr√©tariat qui re√ßoit l‚Äôinfo ‚Äúun fichier est arriv√©‚Äù. SQS = la bo√Æte mail de chaque d√©partement. SNS = le haut-parleur/annonce g√©n√©rale.\nPourquoi A : au lieu d‚Äôenvoyer un message s√©par√© √† chaque prof, le secr√©tariat fait UNE annonce au micro (SNS) avec ‚Äúle fichier est √† tel endroit‚Äù. Chaque d√©partement a d√©j√† sa bo√Æte mail (sa file SQS) et ‚Äús‚Äôabonne‚Äù au micro.\nLes filtres (filter policies) = des r√®gles du style ‚Äúseuls les profs de musique √©coutent les annonces marqu√©es musique‚Äù.\nR√©sultat : chaque d√©partement re√ßoit seulement ses fichiers, et on ne touche pas au code de Lambda.\nB et C demandent de changer Lambda (interdit). D oblige S3 √† g√©rer plein d‚Äôenvois directs et sans tri fin par d√©partement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:449:a7e483d7a4a94edc",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 449,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a web application to upload and store private data. The application will encrypt private data and then will upload the data to an Amazon S3 bucket.The developer needs to implement a solution to automatically find any unencrypted private data in the S3 bucket. The solution must monitor the security and access control of the S3 bucket and must provide a notification if there are any security issues.Which solution will meet these requirements?",
      "choices": {
        "A": "Use AWS Step Functions to run Amazon Athena queries. Configure Athena to find unencrypted private data and to monitor for security issues in the S3 bucket. Start the queries when new objects are added to the S3 bucket. Configure Athena to provide a notification if security issues are detected.",
        "B": "Enable Amazon Macie for the S3 bucket. Set up custom criteria to find unencrypted private data in the S3 bucket. Set up AWS User Notifications to provide a notification when Macie detects security issues.",
        "C": "Enable Amazon Inspector for the AWS account. Use Amazon Inspector to scan the S3 bucket to find unencrypted private data and to monitor for security issues. Set up Amazon EventBridge to provide a notification when Amazon Inspector detects security issues.",
        "D": "Create an Amazon Kinesis data stream. Configure Amazon S3 to send new object notifications to the stream. Create an AWS Lambda function that runs every 10 minutes to check the stream for unencrypted private data and to monitor for security issues. Program the Lambda function to provide a notification when security issues are detected."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/303080-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 28, 2025, 2:54 p.m.",
      "textHash": "a7e483d7a4a94edc",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Ici, on veut d√©tecter automatiquement des donn√©es priv√©es non chiffr√©es dans un bucket Amazon S3 (stockage de fichiers) et √™tre alert√© si le bucket a des probl√®mes de s√©curit√© ou de contr√¥le d‚Äôacc√®s.\nAmazon Macie est un service AWS con√ßu pour analyser les buckets S3 et rep√©rer des donn√©es sensibles (ex: donn√©es personnelles) et des risques, avec des r√©sultats de s√©curit√©.\nMacie peut √™tre activ√© directement sur le bucket et lancer des analyses continues/planifi√©es, ce qui correspond √† ‚Äútrouver automatiquement‚Äù des objets probl√©matiques.\nLes ‚Äúcustom criteria‚Äù permettent d‚Äôadapter la d√©tection (par exemple rep√©rer des fichiers qui ne respectent pas une r√®gle attendue, comme l‚Äôabsence de chiffrement c√¥t√© serveur ou des patterns de donn√©es sensibles).\nMacie surveille aussi la posture de s√©curit√© li√©e √† S3 (ex: acc√®s public, politiques trop permissives) et remonte des ‚Äúfindings‚Äù (alertes).\nAWS User Notifications (ou un m√©canisme de notification AWS) peut envoyer un message quand Macie g√©n√®re un finding, donc on est pr√©venu en cas de probl√®me.\nLes autres choix ne sont pas adapt√©s: Athena sert √† interroger des donn√©es mais ne surveille pas la s√©curit√© S3; Inspector vise surtout les vuln√©rabilit√©s sur instances/containers; Kinesis+Lambda serait complexe et non sp√©cialis√© pour la classification/s√©curit√© S3.\nDonc la meilleure solution est d‚Äôactiver Amazon Macie sur le bucket et de configurer les notifications sur ses alertes.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:446:748bcfc320bda495",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 446,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer adds new dependencies to an existing AWS Lambda function. The developer cannot deploy the Lambda function because the unzipped deployment package exceeds the maximum size quota for the Lambda function. The instruction set architecture of the Lambda function is x86_64.The developer must implement a solution to deploy the Lambda function with the new dependencies.Which solution will meet these requirements?",
      "choices": {
        "A": "Create a snapshot of all the dependencies. Configure the Lambda function to use the snapshot.",
        "B": "Change the instruction set architecture of the Lambda function to use an arm64 architecture.",
        "C": "Associate an Amazon Elastic Block Store (Amazon EBS) volume with the Lambda function. Store all the dependencies on the EBS volume.",
        "D": "Create and deploy a Lambda container image with all the dependencies."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/303078-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 28, 2025, 2:45 p.m.",
      "textHash": "748bcfc320bda495",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:f8630e8c",
      "frExplanation": "Le probl√®me vient de la limite de taille d‚Äôun package Lambda ¬´ classique ¬ª (zip) : si le code + d√©pendances d√©passe la taille maximale une fois d√©compress√©, AWS refuse le d√©ploiement.\nUne solution adapt√©e est d‚Äôutiliser une image de conteneur (container image) pour Lambda : on emballe le code et toutes les d√©pendances dans une image Docker.\nAWS Lambda peut ex√©cuter ces images et la limite de taille est beaucoup plus √©lev√©e que pour un zip, ce qui permet d‚Äôinclure de grosses biblioth√®ques.\nL‚Äôarchitecture x86_64 est compatible : on construit l‚Äôimage pour x86_64 et on la d√©ploie telle quelle.\nA est faux : il n‚Äôexiste pas de m√©canisme ¬´ snapshot ¬ª de d√©pendances √† attacher √† Lambda.\nB ne r√®gle pas le quota de taille (changer en arm64 peut r√©duire la taille parfois, mais ce n‚Äôest pas garanti et ce n‚Äôest pas une solution fiable).\nC est faux : Lambda ne peut pas attacher un volume EBS (EBS est pour EC2). Donc la bonne r√©ponse est D.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois apporter ton sac de sport √† un match, mais le coach impose une taille max de sac. Tu ajoutes des chaussures, une gourde, une veste‚Ä¶ et ton sac devient trop gros.**\n\nUne fonction AWS Lambda, c‚Äôest comme ton ‚Äújoueur‚Äù qui arrive avec son sac (le paquet de d√©ploiement). Il y a une limite de taille, donc si tu ajoutes trop de d√©pendances (tes affaires), √ßa ne rentre plus. La solution D, c‚Äôest de mettre toutes tes affaires dans une grosse valise standard (une ‚Äúimage de conteneur‚Äù) au lieu d‚Äôun petit sac. La valise est accept√©e avec une limite plus grande, donc tu peux embarquer toutes les d√©pendances. Les autres choix ne marchent pas : changer de ‚Äútype de joueur‚Äù (arm64) ne r√©duit pas forc√©ment la taille, un ‚Äúdisque EBS‚Äù c‚Äôest comme un casier √† c√¥t√© du terrain mais Lambda ne peut pas l‚Äôaccrocher comme √ßa, et un ‚Äúsnapshot‚Äù n‚Äôest pas une option magique pour porter plus. Donc D permet bien de d√©ployer avec les nouvelles d√©pendances malgr√© la limite.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:437:e14b57a0f35687b0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 437,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an ecommerce platform. A developer is designing an Amazon DynamoDB table to store customer order data for the platform. The table uses the order ID as the partition key.The developer needs to modify the table to get all order IDs that are associated with a given customer email address in a single query. The solution must give the developer the ability to query order IDs by other item attributes in the future.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure the partition key to use the customer email address as the sort key.",
        "B": "Update the table to use the customer email address as the partition key.",
        "C": "Create a local secondary index (LSI) with the customer email address as the sort key.",
        "D": "Create a global secondary index (GSI) with the customer email address as the partition key."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/303051-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 27, 2025, 8:17 p.m.",
      "textHash": "e14b57a0f35687b0",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π l‚Äôon lit surtout avec des requ√™tes bas√©es sur des cl√©s. Ici, la cl√© de partition de la table est orderId, donc on ne peut pas ‚Äúchercher par email‚Äù efficacement : il faudrait scanner toute la table.\nPour r√©cup√©rer tous les orderId d‚Äôun client en une seule requ√™te, il faut un index qui utilise l‚Äôemail comme cl√© de partition.\nUn GSI (Global Secondary Index) permet de d√©finir une autre cl√© de partition (et √©ventuellement une cl√© de tri) diff√©rente de la table principale.\nAvec un GSI partitionn√© par customerEmail, une requ√™te sur cet index renverra tous les √©l√©ments (commandes) de cet email, donc tous les orderId associ√©s.\nC‚Äôest aussi √©volutif : plus tard, on peut cr√©er d‚Äôautres GSI sur d‚Äôautres attributs (ex: status, date, productId) pour de nouvelles requ√™tes.\nUn LSI ne convient pas car il doit garder la m√™me cl√© de partition que la table (orderId), ce qui n‚Äôaide pas √† regrouper par email.\nChanger la cl√© primaire de la table (options A/B) casserait le mod√®le d‚Äôacc√®s actuel et n‚Äôoffre pas la flexibilit√© d‚Äôindexer d‚Äôautres attributs facilement.\nDonc la bonne solution est de cr√©er un GSI avec customerEmail comme cl√© de partition.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e : chaque commande est un livre, et l‚ÄôID de commande est le code-barres principal pour retrouver un livre vite.**\n\nConcept : dans DynamoDB, la ‚Äúpartition key‚Äù c‚Äôest le code-barres principal. Si tu cherches par autre chose (email), il te faut un autre catalogue.\nIci, la table est class√©e par ID de commande, donc retrouver toutes les commandes d‚Äôun email d‚Äôun coup est difficile.\nUn GSI, c‚Äôest comme cr√©er un 2e catalogue de la biblioth√®que, tri√© par email (et plus tard par d‚Äôautres infos aussi).\nAvec un GSI o√π l‚Äôemail est la cl√© principale, tu tapes l‚Äôemail et tu obtiens tous les IDs de commande en une seule recherche.\nPourquoi pas B : changer la cl√© principale casserait la recherche rapide par ID de commande.\nPourquoi pas C : un LSI, c‚Äôest un petit catalogue li√© au classement actuel, moins flexible pour ajouter d‚Äôautres recherches plus tard.\nDonc D : cr√©er un GSI avec l‚Äôemail comme partition key.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:22:5d24c49cb286d9f6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 22,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an application that will store personal health information (PHI). The PHI needs to be encrypted at all times. An encrypted Amazon RDS for MySQL DB instance is storing the data. The developer wants to increase the performance of the application by caching frequently accessed data while adding the ability to sort or rank the cached datasets.Which solution will meet these requirements?",
      "choices": {
        "A": "Create an Amazon ElastiCache for Redis instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.",
        "B": "Create an Amazon ElastiCache for Memcached instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.",
        "C": "Create an Amazon RDS for MySQL read replica. Connect to the read replica by using SSL. Configure the read replica to store frequently accessed data.",
        "D": "Create an Amazon DynamoDB table and a DynamoDB Accelerator (DAX) cluster for the table. Store frequently accessed data in the DynamoDB table."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103644-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2023, 7:44 a.m.",
      "textHash": "5d24c49cb286d9f6",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:2ed4352c",
      "frExplanation": "On veut acc√©l√©rer l‚Äôapplication en mettant en cache (m√©moire rapide) des donn√©es souvent lues, tout en gardant les donn√©es PHI chiffr√©es en permanence.\nAmazon ElastiCache est un service de cache g√©r√©. Il existe deux moteurs : Redis et Memcached.\nRedis sait g√©rer des structures de donn√©es avanc√©es (ex: listes tri√©es/\"sorted sets\") qui permettent de trier, classer ou faire des rankings directement dans le cache.\nElastiCache for Redis permet aussi d‚Äôactiver le chiffrement au repos (sur disque) et en transit (r√©seau), ce qui r√©pond √† l‚Äôexigence ‚Äúchiffr√© √† tout moment‚Äù.\nMemcached est plus simple (cl√©/valeur) et ne fournit pas les m√™mes capacit√©s de tri/ranking, et le support de fonctionnalit√©s de s√©curit√© est plus limit√© selon les besoins.\nUn read replica RDS n‚Äôest pas un cache : c‚Äôest une autre base relationnelle, plus lente et plus co√ªteuse qu‚Äôun cache m√©moire.\nDAX acc√©l√®re DynamoDB, mais ici la source est RDS MySQL et cela n‚Äôapporte pas directement un cache adapt√© avec ranking.\nDonc Redis chiffr√© (A) est la solution qui combine performance + chiffrement + tri/ranking.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le dossier m√©dical d‚Äôun √©l√®ve rang√© dans un coffre-fort au secr√©tariat (la base de donn√©es). Pour aller plus vite, tu fais aussi une ‚Äúfiche m√©mo‚Äù avec les infos souvent demand√©es, mais elle doit rester dans un mini coffre-fort aussi, et tu veux pouvoir classer ces fiches (par date, score, priorit√©).**\n\nConcept : le ‚Äúcache‚Äù, c‚Äôest la fiche m√©mo qui √©vite d‚Äôaller au secr√©tariat √† chaque fois, donc c‚Äôest plus rapide. Mais comme c‚Äôest de la sant√©, tout doit rester chiffr√© (comme un coffre ferm√©) m√™me quand √ßa circule et quand c‚Äôest stock√©.\nPourquoi A : Redis, c‚Äôest un cache qui sait garder des listes/ensembles tri√©s, donc tu peux trier ou faire un classement des donn√©es en cache (comme classer les fiches par ordre). Et on peut activer le chiffrement pendant le transport et au repos, donc les fiches restent dans un coffre.\nPourquoi pas B : Memcached est surtout un cache simple, il ne g√®re pas aussi bien les classements/tri avanc√©s.\nPourquoi pas C : une ‚Äúcopie de lecture‚Äù reste un autre coffre au secr√©tariat, ce n‚Äôest pas un vrai cache rapide.\nPourquoi pas D : DynamoDB+DAX, c‚Äôest changer de type de stockage, pas juste ajouter un cache devant MySQL.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:9:8744f7a8f10a1c54",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 9,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company wants to share information with a third party. The third party has an HTTP API endpoint that the company can use to share the information. The company has the required API key to access the HTTP API.The company needs a way to manage the API key by using code. The integration of the API key with the application code cannot affect application performance.Which solution will meet these requirements MOST securely?",
      "choices": {
        "A": "Store the API credentials in AWS Secrets Manager. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.",
        "B": "Store the API credentials in a local code variable. Push the code to a secure Git repository. Use the local code variable at runtime to make the API call.",
        "C": "Store the API credentials as an object in a private Amazon S3 bucket. Restrict access to the S3 object by using IAM policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.",
        "D": "Store the API credentials in an Amazon DynamoDB table. Restrict access to the table by using resource-based policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103334-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 20, 2023, 7:11 a.m.",
      "textHash": "8744f7a8f10a1c54",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Ici, on doit stocker une cl√© d‚ÄôAPI (un secret) et la g√©rer ‚Äúpar du code‚Äù, sans ralentir l‚Äôapplication.\nAWS Secrets Manager est un service fait pour stocker des secrets (mots de passe, cl√©s d‚ÄôAPI) de fa√ßon chiffr√©e, avec contr√¥le d‚Äôacc√®s (IAM) et audit.\nL‚Äôapplication peut r√©cup√©rer la cl√© via l‚ÄôAWS SDK au moment o√π elle en a besoin, sans l‚Äô√©crire dans le code ni dans un d√©p√¥t Git.\nSecrets Manager permet aussi la rotation (renouvellement) automatique des secrets, ce qui am√©liore la s√©curit√©.\nC‚Äôest ‚Äúle plus s√©curis√©‚Äù car le secret est prot√©g√©, centralis√©, et on peut limiter pr√©cis√©ment qui peut le lire.\nB est mauvais: mettre la cl√© dans le code (m√™me dans un repo priv√©) risque de fuite et rend la rotation difficile.\nC (S3) et D (DynamoDB) peuvent stocker des donn√©es, mais ne sont pas con√ßus sp√©cifiquement pour des secrets et demandent plus de bricolage pour atteindre le m√™me niveau de gestion/rotation.\nC√¥t√© performance, on peut mettre en cache la valeur en m√©moire apr√®s la premi√®re lecture, donc l‚Äôint√©gration n‚Äôimpacte pas les performances de fa√ßon notable.\nDonc la meilleure solution, la plus s√ªre et adapt√©e, est d‚Äôutiliser AWS Secrets Manager (A).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que tu dois donner un code secret √† un livreur de pizzas pour qu‚Äôil te livre, mais tu ne veux pas √©crire ce code sur ton sac de cours ni le crier dans le couloir.**\n\nConcept : la ‚Äúcl√© API‚Äù, c‚Äôest comme un code secret qui prouve au site du partenaire que c‚Äôest bien toi. Il faut la garder dans un coffre, pas dans le cahier.\nA : AWS Secrets Manager = un coffre-fort de l‚Äô√©cole fait pour stocker des secrets. Ton appli demande le code au coffre au moment d‚Äôen avoir besoin, puis l‚Äôutilise.\nPourquoi c‚Äôest s√©curis√© : le code n‚Äôest pas dans le programme ni sur Git, donc si quelqu‚Äôun voit le code, il ne voit pas le secret.\nPourquoi √ßa ne ralentit pas : tu peux r√©cup√©rer le secret proprement quand il faut (et souvent le garder en m√©moire un moment), sans bloquer l‚Äôappli.\nB est mauvais : mettre le code dans le programme/Git, c‚Äôest comme √©crire le code secret sur la couverture du cahier.\nC et D : S3/DynamoDB sont plut√¥t des ‚Äúplacards‚Äù/‚Äúcasiers‚Äù pour des donn√©es, pas un coffre sp√©cialis√© pour secrets (moins adapt√©).\nDonc la meilleure r√©ponse, la plus s√ªre et pr√©vue pour √ßa : A.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:6:7acf486209cd2a4b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 6,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS CloudFormation template to deploy Amazon EC2 instances across multiple AWS accounts. The developer must choose the EC2 instances from a list of approved instance types.How can the developer incorporate the list of approved instance types in the CloudFormation template?",
      "choices": {
        "A": "Create a separate CloudFormation template for each EC2 instance type in the list.",
        "B": "In the Resources section of the CloudFormation template, create resources for each EC2 instance type in the list.",
        "C": "In the CloudFormation template, create a separate parameter for each EC2 instance type in the list.",
        "D": "In the CloudFormation template, create a parameter with the list of EC2 instance types as AllowedValues."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102784-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 16, 2023, 9:59 a.m.",
      "textHash": "7acf486209cd2a4b",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:94c6c700",
      "frExplanation": "CloudFormation est un service AWS qui d√©ploie de l‚Äôinfrastructure (ex: des serveurs EC2) √† partir d‚Äôun fichier ‚Äútemplate‚Äù.\nIci, on veut forcer le choix du type d‚Äôinstance EC2 (la ‚Äútaille‚Äù du serveur) √† une liste approuv√©e, m√™me si le template est utilis√© dans plusieurs comptes.\nLa bonne pratique est d‚Äôutiliser un Param√®tre CloudFormation : l‚Äôutilisateur choisit une valeur au moment du d√©ploiement.\nAvec AllowedValues, on d√©finit une liste ferm√©e de valeurs autoris√©es (ex: t3.micro, t3.small, m5.large).\nAinsi, si quelqu‚Äôun tente une autre valeur, CloudFormation refuse le d√©ploiement : c‚Äôest un contr√¥le simple et centralis√©.\nA, B et C compliquent inutilement (plusieurs templates, ressources en double, ou trop de param√®tres) et ne garantissent pas un choix unique et propre.\nDonc il faut un seul param√®tre ‚ÄúInstanceType‚Äù avec AllowedValues contenant la liste approuv√©e.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que le lyc√©e pr√©pare une sortie scolaire et que seuls certains bus sont autoris√©s (liste valid√©e par le proviseur).**\n\nConcept : un mod√®le CloudFormation, c‚Äôest comme un formulaire unique pour organiser la sortie. On doit choisir un bus (type d‚ÄôEC2), mais seulement parmi une liste autoris√©e.\nDans le formulaire, tu ajoutes une question ‚ÄúQuel bus veux-tu ?‚Äù et tu mets une liste d√©roulante avec uniquement les bus valid√©s.\nC‚Äôest exactement l‚Äôoption D : un ‚Äúparam√®tre‚Äù (la question) avec ‚ÄúAllowedValues‚Äù (la liste des choix autoris√©s).\nPourquoi pas A : faire un formulaire diff√©rent pour chaque bus, c‚Äôest trop lourd.\nPourquoi pas B : r√©server tous les bus d‚Äôavance, m√™me ceux qu‚Äôon n‚Äôutilise pas.\nPourquoi pas C : faire une question par bus, ce serait plein de questions inutiles.\nDonc D est la bonne r√©ponse : un seul choix, mais contr√¥l√© par une liste approuv√©e.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:79:ed49df231333b1c6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 79,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer wants to debug an application by searching and filtering log data. The application logs are stored in Amazon CloudWatch Logs. The developer creates a new metric filter to count exceptions in the application logs. However, no results are returned from the logs.What is the reason that no filtered results are being returned?",
      "choices": {
        "A": "A setup of the Amazon CloudWatch interface VPC endpoint is required for filtering the CloudWatch Logs in the VPC.",
        "B": "CloudWatch Logs only publishes metric data for events that happen after the filter is created.",
        "C": "The log group for CloudWatch Logs should be first streamed to Amazon OpenSearch Service before metric filtering returns the results.",
        "D": "Metric data points for logs groups can be filtered only after they are exported to an Amazon S3 bucket."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/108734-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 8, 2023, 12:03 p.m.",
      "textHash": "ed49df231333b1c6",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "CloudWatch Logs stocke des lignes de logs (texte) envoy√©es par votre application.\nUn ¬´ metric filter ¬ª (filtre de m√©trique) sert √† rep√©rer un motif (ex: le mot ‚ÄúException‚Äù) dans les nouveaux logs et √† incr√©menter une m√©trique CloudWatch.\nPoint cl√© : ce filtre ne relit pas l‚Äôhistorique d√©j√† pr√©sent dans le log group.\nIl ne commence √† compter qu‚Äô√† partir du moment o√π vous le cr√©ez (√©v√©nements futurs).\nDonc si vous cherchez des r√©sultats imm√©diatement, mais que les exceptions √©taient uniquement dans les anciens logs, vous verrez 0.\nPour obtenir des donn√©es, il faut g√©n√©rer de nouvelles exceptions apr√®s la cr√©ation du filtre (ou analyser l‚Äôhistorique avec d‚Äôautres outils/requ√™tes).\nLes options VPC endpoint, OpenSearch ou export S3 ne sont pas n√©cessaires pour qu‚Äôun metric filter fonctionne.\nC‚Äôest pourquoi la bonne r√©ponse est B.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine le cahier de vie de ta classe o√π chaque √©l√®ve note les incidents (retards, disputes). Tu ajoutes aujourd‚Äôhui une r√®gle: ‚Äú√† chaque fois qu‚Äôil y a le mot EXCEPTION, je fais un b√¢ton dans un compteur‚Äù.**\n\nConcept: les ‚Äúlogs‚Äù, c‚Äôest comme ce cahier qui enregistre tout ce qui se passe. Un ‚Äúmetric filter‚Äù, c‚Äôest une r√®gle qui compte certains mots/√©v√©nements.\nPourquoi B: ta r√®gle ne peut compter que ce qui est √©crit APR√àS que tu l‚Äôas mise en place. Elle ne relit pas automatiquement toutes les anciennes pages du cahier.\nDonc si tu cherches des exceptions qui ont eu lieu avant la cr√©ation du filtre, tu ne verras rien.\nPour avoir des r√©sultats, il faut attendre de nouvelles lignes de logs (nouveaux √©v√©nements) apr√®s la cr√©ation du filtre.\nLes autres choix parlent de d√©tours inutiles (exporter ailleurs, passer par un autre service), mais le vrai probl√®me est juste le ‚Äúpas r√©troactif‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:80:a335160f2a9e747f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 80,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is planning to use AWS CodeDeploy to deploy an application to Amazon Elastic Container Service (Amazon ECS). During the deployment of a new version of the application, the company initially must expose only 10% of live traffic to the new version of the deployed application. Then, after 15 minutes elapse, the company must route all the remaining live traffic to the new version of the deployed application.Which CodeDeploy predefined configuration will meet these requirements?",
      "choices": {
        "A": "CodeDeployDefault.ECSCanary10Percent15Minutes",
        "B": "CodeDeployDefault.LambdaCanary10Percent5Minutes",
        "C": "CodeDeployDefault.LambdaCanary10Percentl15Minutes",
        "D": "CodeDeployDefault.ECSLinear10PercentEvery1Minutes"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/108735-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 8, 2023, 12:13 p.m.",
      "textHash": "a335160f2a9e747f",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici on parle de d√©ployer une nouvelle version sur Amazon ECS (service AWS pour ex√©cuter des conteneurs) avec AWS CodeDeploy (outil qui g√®re le d√©ploiement et le basculement du trafic).\nLe besoin d√©crit un d√©ploiement ¬´ canary ¬ª : on envoie d‚Äôabord une petite partie du trafic r√©el vers la nouvelle version pour v√©rifier qu‚Äôelle fonctionne.\nExigence 1 : exposer 10% du trafic √† la nouvelle version au d√©but.\nExigence 2 : attendre 15 minutes, puis envoyer 100% du trafic restant vers la nouvelle version.\nLa configuration pr√©d√©finie qui correspond exactement est ‚ÄúECSCanary10Percent15Minutes‚Äù : ECS = cible ECS, Canary = 10% puis le reste, 15Minutes = d√©lai avant bascule totale.\nLes options ‚ÄúLambda‚Ä¶‚Äù ne conviennent pas car elles concernent AWS Lambda, pas ECS.\nL‚Äôoption ‚ÄúECSLinear10PercentEvery1Minutes‚Äù est un d√©ploiement lin√©aire (10% ajout√© r√©guli√®rement), pas un saut 10% puis tout apr√®s 15 minutes.\nDonc la bonne r√©ponse est A.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:101:1f8e83aabba849f9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 101,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an application. New users of the application must be able to create an account and register by using their own social media accounts.Which AWS service or resource should the developer use to meet these requirements?",
      "choices": {
        "A": "IAM role",
        "B": "Amazon Cognito identity pools",
        "C": "Amazon Cognito user pools",
        "D": "AWS Directory Service"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106980-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 3:06 p.m.",
      "textHash": "1f8e83aabba849f9",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Le besoin est d‚Äôinscrire et connecter des utilisateurs ¬´ grand public ¬ª (clients) en utilisant des comptes sociaux (Google, Facebook, Apple, etc.).\nAmazon Cognito User Pools est un service AWS qui g√®re un annuaire d‚Äôutilisateurs pour votre application : cr√©ation de compte, connexion, mot de passe oubli√©, MFA, et int√©gration avec des fournisseurs d‚Äôidentit√© sociaux via OAuth.\nAvec un User Pool, l‚Äôutilisateur peut s‚Äôenregistrer et se connecter, et votre appli re√ßoit ensuite des jetons (tokens) pour prouver son identit√©.\nC‚Äôest exactement ce que demande l‚Äô√©nonc√© : permettre l‚Äôinscription/connexion avec des comptes de r√©seaux sociaux.\nPourquoi pas IAM role : IAM sert √† g√©rer l‚Äôacc√®s des identit√©s AWS (admins, services), pas les comptes utilisateurs de votre appli.\nPourquoi pas Cognito Identity Pools : ils servent surtout √† donner des identifiants AWS temporaires pour acc√©der √† des ressources AWS, souvent apr√®s authentification, mais ne g√®rent pas l‚Äôinscription/connexion comme annuaire principal.\nPourquoi pas AWS Directory Service : c‚Äôest pour des annuaires d‚Äôentreprise (Microsoft AD), pas pour l‚Äôinscription d‚Äôutilisateurs via r√©seaux sociaux.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine l‚Äôentr√©e d‚Äôun tournoi de jeu vid√©o au lyc√©e : pour participer, tu dois ‚Äút‚Äôinscrire‚Äù. Tu peux soit cr√©er un nouveau badge du tournoi, soit montrer ton badge Instagram/Google pour prouver qui tu es.**\n\nLe besoin ici : laisser des nouveaux joueurs cr√©er un compte ET se connecter avec leurs r√©seaux sociaux.\nAmazon Cognito user pools, c‚Äôest le ‚Äúbureau d‚Äôinscription‚Äù du tournoi : il g√®re la liste des joueurs (comptes), les mots de passe, et accepte aussi des badges externes (connexion via Google/Facebook, etc.).\nDonc C est bon : √ßa fait l‚Äôinscription + la connexion, y compris avec r√©seaux sociaux.\nPourquoi pas A (IAM role) : c‚Äôest plut√¥t un ‚Äúbadge pour les profs/organisateurs‚Äù pour acc√©der aux salles, pas pour inscrire des √©l√®ves.\nPourquoi pas B (identity pools) : c‚Äôest plut√¥t le ‚Äúbracelet‚Äù qui donne acc√®s aux zones (ressources) une fois connect√©, pas le bureau qui cr√©e les comptes.\nPourquoi pas D (Directory Service) : c‚Äôest l‚Äôannuaire de l‚Äô√©cole (comptes internes), pas fait pour s‚Äôinscrire avec des r√©seaux sociaux.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:100:58c4b9e2e09c73e4",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 100,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses AWS Lambda functions and an Amazon S3 trigger to process images into an S3 bucket. A development team set up multiple environments in a single AWS account.After a recent production deployment, the development team observed that the development S3 buckets invoked the production environment Lambda functions. These invocations caused unwanted execution of development S3 files by using production Lambda functions. The development team must prevent these invocations. The team must follow security best practices.Which solution will meet these requirements?",
      "choices": {
        "A": "Update the Lambda execution role for the production Lambda function to add a policy that allows the execution role to read from only the production environment S3 bucket.",
        "B": "Move the development and production environments into separate AWS accounts. Add a resource policy to each Lambda function to allow only S3 buckets that are within the same account to invoke the function.",
        "C": "Add a resource policy to the production Lambda function to allow only the production environment S3 bucket to invoke the function.",
        "D": "Move the development and production environments into separate AWS accounts. Update the Lambda execution role for each function to add a policy that allows the execution role to read from the S3 bucket that is within the same account."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/109246-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 15, 2023, 1:06 a.m.",
      "textHash": "58c4b9e2e09c73e4",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Le probl√®me vient du fait que des buckets S3 de DEV d√©clenchent (invoquent) une fonction Lambda de PROD. Ici, il faut contr√¥ler qui a le droit d‚Äôappeler la Lambda, pas seulement ce que la Lambda peut lire.\nUne ‚Äúexecution role‚Äù Lambda (r√¥le d‚Äôex√©cution) sert aux permissions sortantes de la fonction (ex: lire S3). M√™me si on limite la lecture au bucket PROD (choix A/D), la Lambda peut quand m√™me √™tre d√©clench√©e par DEV, ce qui cr√©e des ex√©cutions inutiles.\nLa bonne pratique s√©curit√© est d‚Äôisoler les environnements (DEV/PROD) dans des comptes AWS s√©par√©s : cela r√©duit les risques de m√©lange et d‚Äôerreurs de configuration.\nEnsuite, on ajoute une ‚Äúresource policy‚Äù sur chaque Lambda : c‚Äôest une r√®gle attach√©e √† la Lambda qui d√©finit qui peut l‚Äôinvoquer.\nEn autorisant uniquement des buckets S3 du m√™me compte √† invoquer la Lambda, un bucket DEV (dans un autre compte) ne pourra plus d√©clencher la Lambda PROD.\nDonc le choix B emp√™che l‚Äôinvocation non souhait√©e √† la source (permission d‚Äôinvocation) et applique une s√©paration forte entre environnements.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:531:44b6cf78998afa7b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 531,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that runs on Amazon EC2 instances. The application needs to use dynamic feature flags that will be shared with other applications. The application must poll on an interval for new feature flag values. The values must be cached when they are retrieved.Which solution will meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "A": "Store the feature flag values in AWS Secrets Manager. Configure an Amazon ElastiCache node to cache the values by using a lazy loading strategy in the application. Update the application to poll for the values on an interval from ElastiCache.",
        "B": "Store the feature flag values in an Amazon DynamoDB table. Configure DynamoDB Accelerator (DAX) to cache the values by using a lazy loading strategy in the application. Update the application to poll for the values on an interval from DynamoDB.",
        "C": "Store the feature flag values in AWS AppConfig. Configure AWS AppConfig Agent on the EC2 instances to poll for the values on an interval. Update the application to retrieve the values from the AppConfig Agent localhost endpoint.",
        "D": "Store the feature flag values in AWS Systems Manager Parameter Store. Configure the application to poll on an interval. Configure the application to use the AWS SDK to retrieve the values from Parameter Store and to store the values in memory."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/157496-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 4, 2025, 9:01 a.m.",
      "textHash": "44b6cf78998afa7b",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Ici on parle de ‚Äúfeature flags‚Äù (interrupteurs de fonctionnalit√©s) : des valeurs qui activent/d√©sactivent des fonctions, partag√©es entre plusieurs applis, et qui changent souvent.\nLe besoin cl√© : l‚Äôapplication doit ‚Äúpoller‚Äù (interroger r√©guli√®rement) pour voir s‚Äôil y a une nouvelle valeur, et elle doit mettre en cache les valeurs r√©cup√©r√©es.\nAWS AppConfig est un service fait exactement pour g√©rer des configurations et feature flags de fa√ßon centralis√©e, avec contr√¥le de versions et d√©ploiements progressifs.\nL‚ÄôAWS AppConfig Agent s‚Äôinstalle sur les instances EC2 et s‚Äôoccupe lui-m√™me de poller AppConfig √† intervalle r√©gulier.\nL‚Äôagent expose ensuite les valeurs via une URL locale (localhost) : l‚Äôapplication lit localement, ce qui est rapide et r√©duit les appels r√©seau.\nLe cache est g√©r√© par l‚Äôagent : l‚Äôapplication n‚Äôa pas √† construire son propre syst√®me de cache.\nC‚Äôest donc le plus ‚Äúop√©rationnellement efficace‚Äù : moins de code, moins d‚Äôinfrastructure √† g√©rer, et un service con√ßu pour ce cas.\nLes autres options d√©tournent des services (Secrets Manager, DynamoDB/DAX, Parameter Store) et demandent plus de gestion/codage pour le polling et le cache.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:248:0d2293d69af29d7c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 248,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is using AWS Elastic Beanstalk to manage web applications that are running on Amazon EC2 instances. A developer needs to make configuration changes. The developer must deploy the changes to new instances only.Which types of deployment can the developer use to meet this requirement? (Choose two.)",
      "choices": {
        "A": "All at once",
        "B": "Immutable",
        "C": "Rolling",
        "D": "Blue/green",
        "E": "Rolling with additional batch"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124859-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 29, 2023, 1:57 a.m.",
      "textHash": "0d2293d69af29d7c",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:78f81136",
      "frExplanation": "Elastic Beanstalk est un service qui d√©ploie et g√®re une application web sur des serveurs Amazon EC2 (des machines virtuelles).\nLa demande dit : appliquer les changements uniquement sur de nouvelles instances, sans modifier celles qui tournent d√©j√†.\nLe d√©ploiement ¬´ Immutable ¬ª cr√©e d‚Äôabord un nouveau groupe d‚Äôinstances EC2 avec la nouvelle configuration, v√©rifie que tout fonctionne, puis bascule le trafic vers ces nouvelles instances.\nAinsi, les anciennes instances restent intactes pendant le d√©ploiement et ne sont pas modifi√©es : c‚Äôest exactement ¬´ new instances only ¬ª.\n√Ä l‚Äôinverse, ¬´ All at once ¬ª met √† jour les instances existantes d‚Äôun coup (risque de coupure).\n¬´ Rolling ¬ª et ¬´ Rolling with additional batch ¬ª mettent √† jour par lots des instances existantes (donc pas uniquement des nouvelles).\n¬´ Blue/green ¬ª est une strat√©gie de bascule entre deux environnements, mais la question vise le type de d√©ploiement Elastic Beanstalk qui garantit des nouvelles instances sans toucher aux anciennes.\nDonc le bon choix est : Immutable.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine ton coll√®ge : tu veux changer le r√®glement, mais tu ne veux pas perturber les √©l√®ves d√©j√† en cours. Tu pr√©f√®res cr√©er de nouvelles classes avec le nouveau r√®glement, puis faire venir les √©l√®ves seulement quand tout est pr√™t.**\n\nConcept : d√©ployer, c‚Äôest comme appliquer un nouveau r√®glement. Certaines m√©thodes changent les classes existantes, d‚Äôautres cr√©ent d‚Äôabord de nouvelles classes.\nImmutable (B) = tu cr√©es des nouvelles classes toutes neuves avec le nouveau r√®glement, sans toucher aux classes en cours. Si √ßa marche, tu bascules tout le monde vers les nouvelles classes.\nPourquoi c‚Äôest bon : la consigne dit ‚Äúd√©ployer les changements sur de nouvelles instances seulement‚Äù (donc seulement sur des ‚Äúnouvelles classes‚Äù). Immutable fait exactement √ßa.\nPourquoi les autres non : All at once change tout d‚Äôun coup (toutes les classes). Rolling change petit √† petit les classes existantes. Blue/green, c‚Äôest deux √©coles s√©par√©es (possible, mais ici on veut juste garantir ‚Äúnouvelles instances seulement‚Äù sans toucher aux anciennes pendant la cr√©ation). Rolling with additional batch m√©lange ancien + nouveau pendant un moment.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:228:1f06f185b4cbdbd5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 228,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is hosting a workshop for external users and wants to share the reference documents with the external users for 7 days. The company stores the reference documents in an Amazon S3 bucket that the company owns.What is the MOST secure way to share the documents with the external users?",
      "choices": {
        "A": "Use S3 presigned URLs to share the documents with the external users. Set an expiration time of 7 days.",
        "B": "Move the documents to an Amazon WorkDocs folder. Share the links of the WorkDocs folder with the external users.",
        "C": "Create temporary IAM users that have read-only access to the S3 bucket. Share the access keys with the external users. Expire the credentials after 7 days.",
        "D": "Create a role that has read-only access to the S3 bucket. Share the Amazon Resource Name (ARN) of this role with the external users."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124778-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:47 a.m.",
      "textHash": "1f06f185b4cbdbd5",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "La solution la plus s√ªre est d‚Äôutiliser des URL pr√©-sign√©es S3 (presigned URLs) avec une expiration de 7 jours.\nAmazon S3 est un service de stockage de fichiers ; par d√©faut, un bucket peut rester priv√©.\nUne URL pr√©-sign√©e est un lien temporaire qui donne acc√®s √† un objet pr√©cis (un fichier) sans rendre le bucket public.\nOn peut limiter exactement ce qui est partag√© : seulement certains fichiers, et seulement en lecture (download).\nL‚Äôexpiration automatique apr√®s 7 jours coupe l‚Äôacc√®s sans action manuelle.\nCr√©er des utilisateurs IAM et partager des cl√©s (C) est risqu√© : les cl√©s peuvent √™tre copi√©es, r√©utilis√©es et donnent souvent plus d‚Äôacc√®s que n√©cessaire.\nPartager un ARN de r√¥le (D) ne donne pas d‚Äôacc√®s par lui-m√™me : il faut un m√©canisme d‚Äôassumption et des identit√©s AWS c√¥t√© utilisateurs.\nWorkDocs (B) peut fonctionner, mais ce n‚Äôest pas n√©cessaire et ajoute un autre service ; S3 presigned URLs restent le moyen le plus direct et contr√¥l√© pour un partage temporaire.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que le prof a des fiches de r√©vision dans son casier, et il veut les donner √† des √©l√®ves invit√©s pendant 7 jours seulement.**\n\nConcept : au lieu de donner la cl√© du casier, le prof donne un ‚Äúticket d‚Äôacc√®s‚Äù qui ouvre UNE fiche pr√©cise, et ce ticket expire tout seul.\nA : un lien S3 ‚Äúpr√©-sign√©‚Äù, c‚Äôest ce ticket : il permet de t√©l√©charger un fichier sans compte, et tu r√®gles une date limite √† 7 jours. Apr√®s, le lien ne marche plus.\nC est moins s√ªr : cr√©er des comptes + donner des ‚Äúcl√©s‚Äù, c‚Äôest comme donner une vraie cl√© de casier : si elle fuit, on peut revenir et fouiller.\nD ne marche pas : donner juste le nom d‚Äôun r√¥le, c‚Äôest comme donner le nom du surveillant, √ßa n‚Äôouvre aucune porte.\nB peut marcher mais ce n‚Äôest pas le plus s√ªr ici : tu d√©places les documents ailleurs et tu g√®res des partages plus larges.\nDonc A est le plus s√©curis√© : acc√®s minimal, limit√© dans le temps, sans distribuer de vraies cl√©s.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:341:7b15e829d33433c5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 341,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is deploying an application in the AWS Cloud by using AWS CloudFormation. The application will connect to an existing Amazon RDS database. The hostname of the RDS database is stored in AWS Systems Manager Parameter Store as a plaintext value. The developer needs to incorporate the database hostname into the CloudFormation template to initialize the application when the stack is created.How should the developer reference the parameter that contains the database hostname?",
      "choices": {
        "A": "Use the ssm dynamic reference.",
        "B": "Use the Ref intrinsic function.",
        "C": "Use the Fn::ImportValue intrinsic function.",
        "D": "Use the ssm-secure dynamic reference."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136965-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 4:05 a.m.",
      "textHash": "7b15e829d33433c5",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "CloudFormation sert √† d√©crire et cr√©er des ressources AWS avec un ‚Äútemplate‚Äù. Ici, l‚Äôapplication a besoin du nom d‚Äôh√¥te (hostname) d‚Äôune base RDS d√©j√† existante.\nLe hostname est stock√© dans Systems Manager Parameter Store, qui est un coffre de param√®tres (valeurs de configuration) accessible par nom.\nPour lire automatiquement une valeur de Parameter Store au moment de la cr√©ation du stack, CloudFormation utilise une ‚Äúdynamic reference‚Äù de type ssm : {{resolve:ssm:/chemin/parametre}}.\nC‚Äôest exactement adapt√© aux param√®tres en texte clair (plaintext), donc la bonne r√©ponse est A.\nRef (B) sert √† r√©cup√©rer la valeur d‚Äôun param√®tre CloudFormation ou l‚ÄôID d‚Äôune ressource du template, pas √† aller lire Parameter Store.\nFn::ImportValue (C) sert √† importer une valeur export√©e par un autre stack CloudFormation, pas un param√®tre SSM.\nssm-secure (D) est uniquement pour les param√®tres chiffr√©s (SecureString) ; ici c‚Äôest du plaintext.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu montes un club au lyc√©e. Le lieu de rendez‚Äëvous (l‚Äôadresse) est √©crit sur un papier dans le casier du prof (un ‚Äústockage de param√®tres‚Äù). Tu veux que, quand tu cr√©es la fiche du club, l‚Äôadresse soit copi√©e automatiquement depuis ce casier.**\n\nConcept : CloudFormation, c‚Äôest comme un formulaire qui cr√©e tout ton club automatiquement. Parameter Store, c‚Äôest le casier o√π on garde des infos simples (ici l‚Äôadresse en clair). Pour r√©cup√©rer une info du casier au moment o√π tu remplis le formulaire, tu utilises une ‚Äúr√©f√©rence dynamique ssm‚Äù : c‚Äôest comme dire ‚Äúva lire le papier dans le casier maintenant‚Äù. Donc A est bon : ssm dynamic reference lit la valeur en clair au moment de la cr√©ation. B (Ref) sert plut√¥t √† lire une r√©ponse d‚Äôune question du formulaire, pas un papier dans un casier externe. C (ImportValue) sert √† r√©cup√©rer une info export√©e par un autre formulaire, pas par le casier. D (ssm-secure) c‚Äôest pour un papier dans une enveloppe scell√©e (secret), mais ici c‚Äôest en clair.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:37:b7523425753230fe",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 37,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an Amazon S3 bucket that contains sensitive data. The data must be encrypted in transit and at rest. The company encrypts the data in the S3 bucket by using an AWS Key Management Service (AWS KMS) key. A developer needs to grant several other AWS accounts the permission to use the S3 GetObject operation to retrieve the data from the S3 bucket.How can the developer enforce that all requests to retrieve the data provide encryption in transit?",
      "choices": {
        "A": "Define a resource-based policy on the S3 bucket to deny access when a request meets the condition ‚Äúaws:SecureTransport‚Äù: ‚Äúfalse‚Äù.",
        "B": "Define a resource-based policy on the S3 bucket to allow access when a request meets the condition ‚Äúaws:SecureTransport‚Äù: ‚Äúfalse‚Äù.",
        "C": "Define a role-based policy on the other accounts' roles to deny access when a request meets the condition of ‚Äúaws:SecureTransport‚Äù: ‚Äúfalse‚Äù.",
        "D": "Define a resource-based policy on the KMS key to deny access when a request meets the condition of ‚Äúaws:SecureTransport‚Äù: ‚Äúfalse‚Äù."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103850-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 25, 2023, 1:22 p.m.",
      "textHash": "b7523425753230fe",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Objectif : forcer le chiffrement ¬´ en transit ¬ª quand d‚Äôautres comptes font S3 GetObject (t√©l√©chargement).\nLe chiffrement en transit signifie utiliser HTTPS/TLS ; sur AWS, une requ√™te non chiffr√©e correspond √† HTTP.\nS3 peut contr√¥ler cela directement via une policy sur le bucket (resource-based policy) qui s‚Äôapplique √† tous les appelants, m√™me d‚Äôautres comptes.\nLa condition IAM aws:SecureTransport vaut \"false\" quand la requ√™te n‚Äôutilise pas HTTPS.\nLa bonne pratique est donc de mettre un DENY explicite sur le bucket quand aws:SecureTransport = false : ainsi, toute requ√™te HTTP est bloqu√©e.\nUn ALLOW avec SecureTransport=false (B) ferait l‚Äôinverse.\nUne policy sur les r√¥les des autres comptes (C) n‚Äôest pas fiable pour ‚Äúenforcer‚Äù c√¥t√© ressource, car chaque compte peut changer ses r√¥les.\nUne policy sur la cl√© KMS (D) ne contr√¥le pas le transport HTTP/HTTPS vers S3 ; elle g√®re l‚Äôusage de la cl√©, pas le protocole r√©seau.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e avec des livres tr√®s secrets. Pour les emprunter, tu dois passer par un couloir s√©curis√© (porte ferm√©e + surveillant). Si tu essaies de passer par une porte ouverte sur la cour, c‚Äôest interdit.**\n\nConcept : ‚Äúchiffr√© en transit‚Äù = le couloir s√©curis√© pendant le trajet (HTTPS). ‚ÄúAu repos‚Äù = le livre est aussi dans un coffre quand il est rang√© (chiffrement dans S3 avec une cl√© KMS).\nIci, on veut forcer le couloir s√©curis√© pour TOUS ceux qui prennent un livre (GetObject).\nLa meilleure fa√ßon, c‚Äôest de mettre une r√®gle √† l‚Äôentr√©e de la biblioth√®que (politique sur le bucket S3) qui dit : si quelqu‚Äôun n‚Äôutilise pas le couloir s√©curis√©, on refuse.\nDans AWS, cette r√®gle s‚Äô√©crit : refuser quand ‚Äúaws:SecureTransport‚Äù est ‚Äúfalse‚Äù (donc pas de HTTPS).\nA est correct car c‚Äôest une r√®gle sur le bucket (la biblioth√®que) qui bloque tout le monde si le trajet n‚Äôest pas s√©curis√©.\nB est l‚Äôinverse (√ßa autorise quand ce n‚Äôest pas s√©curis√©). C d√©pend des autres comptes (pas fiable). D concerne la cl√© du coffre, pas le couloir.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:35:68e758e630e3fbe9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 35,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer wants to insert a record into an Amazon DynamoDB table as soon as a new file is added to an Amazon S3 bucket.Which set of steps would be necessary to achieve this?",
      "choices": {
        "A": "Create an event with Amazon EventBridge that will monitor the S3 bucket and then insert the records into DynamoDB.",
        "B": "Configure an S3 event to invoke an AWS Lambda function that inserts records into DynamoDB.",
        "C": "Create an AWS Lambda function that will poll the S3 bucket and then insert the records into DynamoDB.",
        "D": "Create a cron job that will run at a scheduled time and insert the records into DynamoDB."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103519-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 10:30 p.m.",
      "textHash": "68e758e630e3fbe9",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Objectif : d√®s qu‚Äôun fichier arrive dans un bucket S3 (stockage de fichiers), on veut √©crire une ligne dans DynamoDB (base NoSQL).\nLa bonne approche est ‚Äú√©v√©nementielle‚Äù : S3 peut d√©clencher automatiquement une action √† chaque ajout d‚Äôobjet.\nOn configure donc une notification d‚Äô√©v√©nement S3 (ex. ObjectCreated) qui appelle une fonction AWS Lambda.\nLambda est un code ex√©cut√© √† la demande, sans serveur √† g√©rer : il re√ßoit les infos du fichier (nom, taille, date, etc.).\nLa fonction Lambda utilise ensuite l‚ÄôAPI DynamoDB pour faire un PutItem et ins√©rer l‚Äôenregistrement.\nPourquoi B est correct : c‚Äôest imm√©diat, automatique, et con√ßu pour r√©agir √† un √©v√©nement S3.\nPourquoi pas C/D : ‚Äúpolling‚Äù ou cron = v√©rifications p√©riodiques, donc retard, co√ªts inutiles, et plus complexe.\nPourquoi pas A : EventBridge peut fonctionner, mais l‚Äô√©tape essentielle reste un traitement (souvent Lambda) ; la r√©ponse attendue est la notification S3 ‚Üí Lambda ‚Üí DynamoDB.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la bo√Æte aux lettres du coll√®ge (S3) : d√®s qu‚Äôun √©l√®ve d√©pose un devoir, tu veux que le secr√©tariat note tout de suite son nom dans un registre (DynamoDB).**\n\nConcept : il faut un ‚Äúd√©clencheur‚Äù automatique, comme une sonnette sur la bo√Æte aux lettres, qui pr√©vient instantan√©ment quelqu‚Äôun.\nB : tu r√®gles la bo√Æte aux lettres pour qu‚Äô√† chaque nouveau d√©p√¥t, elle appelle un surveillant robot (Lambda) qui √©crit une ligne dans le registre (DynamoDB).\nC est moins bien : c‚Äôest comme demander au surveillant d‚Äôaller v√©rifier la bo√Æte toutes les minutes (perte de temps, pas imm√©diat).\nD est faux : un passage √† heure fixe, donc pas ‚Äúd√®s qu‚Äôun devoir arrive‚Äù.\nA parle d‚Äôun autre syst√®me d‚Äôannonces, mais ici le plus direct est la sonnette S3 ‚Üí le robot Lambda ‚Üí le registre DynamoDB.\nDonc la bonne r√©ponse est B.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:45:3b9c2bfa28fa1669",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 45,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing an AWS Lambda function. The developer wants to log key events that occur while the Lambda function runs. The developer wants to include a unique identifier to associate the events with a specific function invocation. The developer adds the following code to the Lambda function:Which solution will meet this requirement?",
      "choices": {
        "A": "Obtain the request identifier from the AWS request ID field in the context object. Configure the application to write logs to standard output.",
        "B": "Obtain the request identifier from the AWS request ID field in the event object. Configure the application to write logs to a file.",
        "C": "Obtain the request identifier from the AWS request ID field in the event object. Configure the application to write logs to standard output.",
        "D": "Obtain the request identifier from the AWS request ID field in the context object. Configure the application to write logs to a file."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103708-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2023, 11:34 p.m.",
      "textHash": "3b9c2bfa28fa1669",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:953eb97f",
      "frExplanation": "Dans AWS Lambda, chaque ex√©cution (invocation) de la fonction a un identifiant unique appel√© ¬´ request ID ¬ª.\nPour le r√©cup√©rer, on utilise l‚Äôobjet ¬´ context ¬ª fourni par Lambda au moment de l‚Äôex√©cution : il contient des infos techniques sur l‚Äôinvocation (dont awsRequestId).\nL‚Äôobjet ¬´ event ¬ª contient plut√¥t les donn√©es d‚Äôentr√©e (payload) envoy√©es √† la fonction, pas l‚Äôidentifiant interne d‚ÄôAWS.\nPour enregistrer des √©v√©nements, la bonne pratique en Lambda est d‚Äô√©crire les logs sur la sortie standard (stdout), par exemple avec console.log/print.\nAWS capture automatiquement stdout/stderr et envoie ces logs dans Amazon CloudWatch Logs, sans configuration de fichier.\n√âcrire dans un fichier local n‚Äôest pas fiable en Lambda : le syst√®me de fichiers est √©ph√©m√®re et non pr√©vu pour la journalisation.\nDonc : prendre awsRequestId depuis context + √©crire sur stdout permet d‚Äôassocier chaque log √† une invocation pr√©cise.\nC‚Äôest exactement ce que propose la r√©ponse A.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un contr√¥le au lyc√©e : chaque √©l√®ve re√ßoit une copie avec un num√©ro unique, et le surveillant note tout ce qui se passe (retard, question, incident) en disant ce num√©ro √† chaque fois.**\n\nConcept : une fonction Lambda, c‚Äôest comme un √©l√®ve qui fait un exercice √† chaque ‚Äúappel‚Äù. Chaque appel a son num√©ro unique, comme le num√©ro de copie. Ce num√©ro est donn√© par le ‚Äúcontexte‚Äù (le surveillant), pas par l‚Äô√©nonc√© (l‚Äôevent). Pour retrouver tous les √©v√©nements d‚Äôun m√™me appel, tu dois donc prendre l‚Äôidentifiant dans le context (AWS request ID). Ensuite, pour ‚Äú√©crire dans le cahier de surveillance‚Äù, Lambda r√©cup√®re automatiquement ce que tu √©cris en sortie standard (comme parler √† voix haute), et √ßa part dans les logs. √âcrire dans un fichier, c‚Äôest comme √©crire sur une feuille pos√©e sur la table : √† la fin, on peut la jeter et personne ne la r√©cup√®re. Donc A est correct : ID depuis context + logs vers standard output.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:460:856961c3a1bc0c00",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 460,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is implementing a serverless application by using AWS CloudFormation to provision Amazon S3 web hosting. Amazon API Gateway, and AWS Lambda functions. The Lambda function source code is zipped and uploaded to an S3 bucket. The S3 object key of the zipped source code is specified in the Lambda resource in the CloudFormation template.The developer notices that there are no changes in the Lambda function every time the CloudFormation stack is updated.How can the developer resolve this issue?",
      "choices": {
        "A": "Create a new Lambda function alias before updating the CloudFormation stack.",
        "B": "Change the S3 object key or the S3 version in the CloudFormation template before updating the CloudFormation stack.",
        "C": "Upload the zipped source code to another S3 bucket before updating the CioudFormation stack.",
        "D": "Associate a cade signing configuration with the Lambda function before updating the CloudFormation stack."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152916-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 12, 2024, 6:59 p.m.",
      "textHash": "856961c3a1bc0c00",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:f8daf3f6",
      "frExplanation": "CloudFormation est un outil qui cr√©e/met √† jour des ressources AWS √† partir d‚Äôun fichier (template).\nIci, le code de la fonction AWS Lambda (ex√©cute du code sans serveur) est stock√© dans un fichier ZIP sur Amazon S3 (stockage de fichiers).\nDans le template, Lambda pointe vers le ZIP via un couple (bucket + object key) et √©ventuellement une ‚Äúversion‚Äù d‚Äôobjet S3.\nQuand vous mettez √† jour la stack, CloudFormation ne voit aucun changement si le chemin (object key) et la version restent identiques.\nDonc il ne red√©ploie pas le code, m√™me si vous avez remplac√© le ZIP dans S3 au m√™me endroit.\nLa solution est de forcer un changement d√©tectable : changer l‚Äôobject key (ex: ajouter un num√©ro de build) ou utiliser une nouvelle version S3.\nAinsi, CloudFormation comprend que le package de code a chang√© et met √† jour la fonction Lambda.\nLes autres options (alias, autre bucket, code signing) ne garantissent pas que CloudFormation d√©tecte un nouveau package de code.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un devoir rendu sur l‚ÄôENT : le prof (CloudFormation) va chercher ton fichier dans le casier ‚ÄúS3‚Äù avec un nom pr√©cis (la ‚Äúcl√©‚Äù du fichier).**\n\nConcept : CloudFormation suit une recette. Si la recette dit ‚Äúprends le fichier devoir.zip‚Äù, il reprend toujours ce fichier-l√†.\nIci, ton code Lambda (le ‚Äúprogramme‚Äù) est dans un zip stock√© dans S3 (un casier de stockage).\nSi tu remets un nouveau zip MAIS avec le m√™me nom au m√™me endroit, CloudFormation peut croire que rien n‚Äôa chang√©.\nDonc il ne ‚Äúremplace‚Äù pas le programme de Lambda lors de la mise √† jour.\nSolution : change le nom du fichier (S3 object key) comme devoir_v2.zip, OU utilise une ‚Äúversion‚Äù du fichier (S3 version) pour pointer vers la nouvelle copie.\nComme √ßa, la recette voit un nouvel objet et met bien √† jour Lambda.\nDonc la bonne r√©ponse est B.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:128:203823f6a4e0dd73",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 128,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a front-end application that runs on four Amazon EC2 instances behind an Elastic Load Balancer (ELB) in a production environment that is provisioned by AWS Elastic Beanstalk. A developer needs to deploy and test new application code while updating the Elastic Beanstalk platform from the current version to a newer version of Node.js. The solution must result in zero downtime for the application.Which solution meets these requirements?",
      "choices": {
        "A": "Clone the production environment to a different platform version. Deploy the new application code, and test it. Swap the environment URLs upon verification.",
        "B": "Deploy the new application code in an all-at-once deployment to the existing EC2 instances. Test the code. Redeploy the previous code if verification fails.",
        "C": "Perform an immutable update to deploy the new application code to new EC2 instances. Serve traffic to the new instances after they pass health checks.",
        "D": "Use a rolling deployment for the new application code. Apply the code to a subset of EC2 instances until the tests pass. Redeploy the previous code if the tests fail."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107066-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 23, 2023, 12:45 a.m.",
      "textHash": "203823f6a4e0dd73",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:e8291144",
      "frExplanation": "Objectif : d√©ployer un nouveau code ET mettre √† jour la ‚Äúplateforme‚Äù Elastic Beanstalk (ici la version de Node.js) sans interruption (z√©ro downtime).\nElastic Beanstalk g√®re pour vous l‚Äôinfra (EC2 + Load Balancer) et propose des strat√©gies de d√©ploiement.\nUn ‚Äúimmutable update‚Äù (mise √† jour immuable) cr√©e un nouveau groupe d‚Äôinstances EC2 s√©par√© avec la nouvelle plateforme + le nouveau code, sans toucher aux instances en production.\nLe Load Balancer continue d‚Äôenvoyer le trafic vers l‚Äôancien groupe pendant que le nouveau d√©marre.\nQuand les nouvelles instances passent les contr√¥les de sant√© (health checks), Beanstalk bascule le trafic vers elles.\nSi quelque chose √©choue, l‚Äôancien environnement reste intact : pas besoin de ‚Äúr√©parer‚Äù des serveurs modifi√©s, donc risque minimal.\nC‚Äôest la meilleure fa√ßon d‚Äôobtenir z√©ro downtime lors d‚Äôun changement de runtime (Node.js) + code.\nLes options ‚Äúall-at-once‚Äù et ‚Äúrolling‚Äù modifient des instances existantes et peuvent provoquer des erreurs visibles ou une capacit√© r√©duite.\nLe clonage + swap (A) peut marcher, mais l‚Äôoption attendue ici pour une bascule s√ªre apr√®s health checks est l‚Äôimmutable (C).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:313:40dd21479000490e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 313,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a mobile app. The app includes an Amazon API Gateway REST API that invokes AWS Lambda functions. The Lambda functions process data from the app.The company needs to test updated Lambda functions that have new features. The company must conduct these tests with a subset of users before deployment. The tests must not affect other users of the app.Which solution will meet these requirements with the LEAST amount of operational effort?",
      "choices": {
        "A": "Create a new version of each Lambda function with a weighted alias. Configure a weight value for each version of the Lambda function. Update the new weighted alias Amazon Resource Name (ARN) in the REST API.",
        "B": "Create a new REST API in API Gateway. Set up a Lambda proxy integration to connect to multiple Lambda functions. Enable canary settings on the deployment stage. Specify a smaller percentage of API traffic to go to the new version of the Lambda function.",
        "C": "Create a new version of each Lambda function. Integrate a predefined canary deployment in AWS CodeDeploy to slowly shift the traffic to the new versions automatically.",
        "D": "Create a new REST API in API Gateway. Set up a Lambda non-proxy integration to connect to multiple Lambda functions. Specify the necessary parameters and properties in API Gateway. Enable canary settings on the deployment stage. Specify a smaller percentage of API traffic to go to the new version of the Lambda function."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133611-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 12, 2024, 2:16 p.m.",
      "textHash": "40dd21479000490e",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Objectif : tester une nouvelle version de fonctions AWS Lambda (du code ex√©cut√© √† la demande) avec seulement une partie des utilisateurs, sans impacter les autres, et avec le moins d‚Äôeffort.\nAmazon API Gateway est la ‚Äúporte d‚Äôentr√©e‚Äù HTTP de l‚Äôapp mobile : il appelle une Lambda via un ARN.\nLa solution la plus simple est d‚Äôutiliser les versions + alias de Lambda : une ‚Äúversion‚Äù fige le code, un ‚Äúalias‚Äù est un nom stable (ex: prod) qui pointe vers une ou plusieurs versions.\nUn alias ‚Äúpond√©r√©‚Äù (weighted alias) permet d‚Äôenvoyer, par exemple, 90% du trafic vers l‚Äôancienne version et 10% vers la nouvelle.\nAinsi, seuls certains utilisateurs (une fraction des appels) voient les nouvelles fonctionnalit√©s, les autres restent sur l‚Äôancienne version.\nC√¥t√© op√©rations, on ne recr√©e pas d‚ÄôAPI : on met juste √† jour l‚ÄôARN dans API Gateway pour viser l‚Äôalias, puis on ajuste les pourcentages.\nLes options avec canary d‚ÄôAPI Gateway ou CodeDeploy ajoutent plus de configuration (nouvelle API, stages, d√©ploiements), donc plus d‚Äôeffort.\nDonc A r√©pond au besoin de ‚Äúsubset d‚Äôutilisateurs‚Äù et ‚Äúpas d‚Äôimpact global‚Äù avec le minimum de changements.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:399:816311191784d2d9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 399,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn AWS Lambda function is invoked asynchronously to process events. Occasionally, the Lambda function falls to process events. A developer needs to collect and analyze these failed events to fix the issue.What should the developer do to meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Add logging statements for all events in the Lambda function. Filter AWS CloudTrail logs for errors.",
        "B": "Configure the Lambda function to start an AWS Step Functions workflow with retries for failed events.",
        "C": "Add a dead-letter queue to send messages to an Amazon Simple Queue Service (Amazon SQS) standard queue.",
        "D": "Add a dead-letter queue to send messages to an Amazon Simple Notification Service (Amazon SNS) FIFO topic."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143802-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 3:49 p.m.",
      "textHash": "816311191784d2d9",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Ici, la fonction AWS Lambda (code ex√©cut√© sans serveur) est appel√©e ¬´ en asynchrone ¬ª : l‚Äô√©v√©nement est envoy√©, puis Lambda le traite plus tard. Si le traitement √©choue, l‚Äô√©v√©nement peut √™tre perdu si on ne pr√©voit rien.\nLa solution la plus simple est d‚Äôutiliser une Dead-Letter Queue (DLQ) : une ¬´ bo√Æte de secours ¬ª o√π Lambda d√©pose automatiquement les √©v√©nements qui ont √©chou√© apr√®s les tentatives internes.\nAvec une DLQ vers Amazon SQS standard (une file de messages), chaque √©v√©nement en √©chec est conserv√© comme un message que vous pouvez relire, rejouer, inspecter et analyser.\nCela demande tr√®s peu de d√©veloppement : surtout de la configuration, pas de nouveau workflow.\nA est insuffisant : des logs aident, mais ne garantissent pas de r√©cup√©rer le contenu exact des √©v√©nements √©chou√©s, et CloudTrail ne contient pas les erreurs d‚Äôex√©cution de votre code.\nB (Step Functions) ajoute beaucoup de mise en place et de logique, donc plus d‚Äôeffort.\nD est inadapt√© : une DLQ pour Lambda se fait typiquement vers SQS (ou SNS standard selon cas), et un topic SNS FIFO n‚Äôest pas le choix simple/standard pour collecter des √©v√©nements en √©chec.\nDonc C est correct : configurer une DLQ vers une file SQS standard pour collecter et analyser les √©v√©nements √©chou√©s.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : tu d√©poses des tickets de commande dans une bo√Æte, et la cuisine les traite plus tard (pas en face de toi). Parfois, un ticket est illisible ou tombe par terre, donc la commande n‚Äôest pas faite.**\n\nConcept : ‚Äúasynchrone‚Äù = tu envoies la demande et tu ne restes pas √† attendre, comme les tickets de cantine. Si √ßa rate, il faut un endroit o√π ranger les tickets rat√©s pour les relire et comprendre. La bonne solution est C : une ‚Äúdead-letter queue‚Äù vers SQS, c‚Äôest comme une bo√Æte sp√©ciale ‚Äútickets rat√©s‚Äù o√π chaque ticket est stock√© et r√©cup√©rable. Tu peux ensuite ouvrir cette bo√Æte, voir exactement quels tickets ont √©chou√©, et analyser pourquoi. C‚Äôest le moins d‚Äôeffort : pas besoin de construire un gros syst√®me de relance (B), ni de fouiller des journaux compliqu√©s (A). SNS (D) ressemble plus √† un haut-parleur qui annonce des infos, moins pratique pour stocker et analyser une liste de tickets rat√©s.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:338:131935a95856db43",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 338,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is using AWS CloudFormation templates to deploy AWS resources. The company needs to update one of its AWS CloudFormation stacks.What can the company do to find out how the changes will impact the resources that are running?",
      "choices": {
        "A": "Investigate the change sets.",
        "B": "Investigate the stack policies.",
        "C": "Investigate the Metadata section.",
        "D": "Investigate the Resources section."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136645-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 19, 2024, 3:24 p.m.",
      "textHash": "131935a95856db43",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:69082c47",
      "frExplanation": "CloudFormation est un service AWS qui cr√©e et met √† jour des ressources (serveurs, bases de donn√©es, etc.) √† partir d‚Äôun fichier ‚Äútemplate‚Äù. Quand on modifie un template, on veut savoir ce qui va changer avant de toucher aux ressources d√©j√† en production. Un ‚Äúchange set‚Äù (jeu de modifications) est une pr√©visualisation : CloudFormation compare l‚Äô√©tat actuel du stack avec le nouveau template et liste les actions pr√©vues (cr√©er, modifier, remplacer, supprimer). Cela permet d‚Äôestimer l‚Äôimpact et les risques (ex. remplacement d‚Äôune ressource = possible interruption). Les ‚Äústack policies‚Äù servent plut√¥t √† prot√©ger certaines ressources contre des mises √† jour, pas √† montrer l‚Äôimpact. La section ‚ÄúMetadata‚Äù contient des infos suppl√©mentaires pour les outils, pas un plan de changement. La section ‚ÄúResources‚Äù d√©crit ce qui existe dans le stack, mais ne montre pas ce qui va changer. Donc il faut examiner les change sets.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu construis une ville dans un jeu vid√©o avec un ‚Äúplan de construction‚Äù √©crit (le template). Ta ville d√©j√† en ligne, c‚Äôest le stack. Tu veux modifier le plan, mais tu as peur de casser des b√¢timents existants.**\n\nConcept : CloudFormation, c‚Äôest comme un plan qui dit quoi construire (maisons, routes). Le stack, c‚Äôest la ville d√©j√† construite. Avant d‚Äôappliquer un nouveau plan, tu veux voir ce qui va changer.\nA (change sets) : c‚Äôest le ‚Äúmode aper√ßu‚Äù du jeu. Il te montre une liste claire : quels b√¢timents seront ajout√©s, modifi√©s ou d√©truits si tu valides la mise √† jour. Donc tu sais l‚Äôimpact AVANT de cliquer.\nB (stack policies) : c‚Äôest plut√¥t des r√®gles du style ‚Äúinterdit de toucher √† la mairie‚Äù, pas un aper√ßu des changements.\nC (Metadata) : c‚Äôest des notes/infos pour aider, pas la liste des impacts.\nD (Resources) : c‚Äôest la liste des b√¢timents actuels, pas ce qui va changer.\nDonc la bonne r√©ponse est A : regarder les change sets pour pr√©voir l‚Äôimpact.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:62:ecb6e979906da1b3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 62,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has deployed an application on AWS Elastic Beanstalk. The company has configured the Auto Scaling group that is associated with the Elastic Beanstalk environment to have five Amazon EC2 instances. If the capacity is fewer than four EC2 instances during the deployment, application performance degrades. The company is using the all-at-once deployment policy.What is the MOST cost-effective way to solve the deployment issue?",
      "choices": {
        "A": "Change the Auto Scaling group to six desired instances.",
        "B": "Change the deployment policy to traffic splitting. Specify an evaluation time of 1 hour.",
        "C": "Change the deployment policy to rolling with additional batch. Specify a batch size of 1.",
        "D": "Change the deployment policy to rolling. Specify a batch size of 2."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/104016-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 27, 2023, 2:20 a.m.",
      "textHash": "ecb6e979906da1b3",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Elastic Beanstalk d√©ploie votre application sur un groupe Auto Scaling (plusieurs serveurs EC2). Ici, il y a 5 instances EC2 en temps normal.\nAvec la strat√©gie ‚Äúall-at-once‚Äù, Beanstalk met √† jour toutes les instances en m√™me temps : pendant le d√©ploiement, plusieurs instances sont indisponibles, et la capacit√© peut tomber sous 4, ce qui d√©grade les performances.\nLa solution la plus √©conomique est d‚Äô√©viter de perdre trop de capacit√© pendant la mise √† jour, sans payer en permanence plus d‚Äôinstances.\n‚ÄúRolling with additional batch‚Äù met √† jour par petits lots, et ajoute temporairement une instance suppl√©mentaire pour garder la capacit√©.\nAvec une taille de lot (batch size) de 1, on ne met √† jour qu‚Äôune instance √† la fois, et l‚Äôinstance ‚Äúen plus‚Äù compense, donc on reste au moins √† 4 instances actives.\nAugmenter le nombre d√©sir√© √† 6 (A) co√ªte plus cher en continu, m√™me hors d√©ploiement.\nTraffic splitting (B) est plus complexe et peut n√©cessiter plus de ressources/temps, pas le plus rentable ici.\nRolling batch size 2 (D) peut faire tomber la capacit√© trop bas (mettre 2 instances hors service), donc risque de passer sous 4.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine une cantine avec 5 caisses ouvertes pour servir les √©l√®ves vite. Si tu fermes trop de caisses d‚Äôun coup, la queue devient √©norme et tout le monde r√¢le.**\n\nIci, les 5 ‚Äúcaisses‚Äù sont les 5 serveurs (EC2) qui font tourner l‚Äôappli. Pendant une mise √† jour ‚Äúall-at-once‚Äù, on met √† jour tout d‚Äôun coup, donc plusieurs serveurs peuvent √™tre indisponibles en m√™me temps. Si on descend sous 4 serveurs actifs, l‚Äôappli rame (comme 3 caisses seulement). La solution la moins ch√®re est de mettre √† jour petit √† petit ET d‚Äôajouter temporairement 1 serveur en plus le temps de la mise √† jour : c‚Äôest ‚Äúrolling with additional batch‚Äù avec un lot de 1. Comme √ßa, tu gardes au moins 4 serveurs qui servent les utilisateurs pendant qu‚Äôun seul se met √† jour. Ajouter 6 serveurs en permanence co√ªte plus cher, et les autres options risquent soit d‚Äô√™tre plus lentes, soit de ne pas garantir assez de serveurs disponibles.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:47:4070f303b70a11b3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 47,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application uses an Amazon EC2 Auto Scaling group. A developer notices that EC2 instances are taking a long time to become available during scale-out events. The UserData script is taking a long time to run.The developer must implement a solution to decrease the time that elapses before an EC2 instance becomes available. The solution must make the most recent version of the application available at all times and must apply all available security updates. The solution also must minimize the number of images that are created. The images must be validated.Which combination of steps should the developer take to meet these requirements? (Choose two.)",
      "choices": {
        "A": "Use EC2 Image Builder to create an Amazon Machine Image (AMI). Install all the patches and agents that are needed to manage and run the application. Update the Auto Scaling group launch configuration to use the AMI.",
        "B": "Use EC2 Image Builder to create an Amazon Machine Image (AMI). Install the latest version of the application and all the patches and agents that are needed to manage and run the application. Update the Auto Scaling group launch configuration to use the AMI.",
        "C": "Set up AWS CodeDeploy to deploy the most recent version of the application at runtime.",
        "D": "Set up AWS CodePipeline to deploy the most recent version of the application at runtime.",
        "E": "Remove any commands that perform operating system patching from the UserData script."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103721-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 24, 2023, 2:40 a.m.",
      "textHash": "4070f303b70a11b3",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Le probl√®me vient du script UserData : il s‚Äôex√©cute au d√©marrage de chaque nouvelle instance et retarde sa disponibilit√©.\nPour acc√©l√©rer, on pr√©pare une image ‚Äúpr√™te √† l‚Äôemploi‚Äù : une AMI (Amazon Machine Image) est un mod√®le de serveur avec l‚ÄôOS et des logiciels d√©j√† install√©s.\nEC2 Image Builder sert √† fabriquer automatiquement ces AMI et √† les valider (tests/contr√¥les), tout en appliquant les mises √† jour de s√©curit√©.\nAvec l‚Äôoption A, on met dans l‚ÄôAMI tous les correctifs (patches) et les agents n√©cessaires : au lancement, l‚Äôinstance n‚Äôa presque plus rien √† installer, donc elle d√©marre vite.\nEnsuite on configure l‚ÄôAuto Scaling group pour lancer les instances √† partir de cette AMI.\nPour respecter ‚Äúminimiser le nombre d‚Äôimages‚Äù, on ne met pas l‚Äôapplication dans l‚ÄôAMI (sinon il faudrait recr√©er une AMI √† chaque nouvelle version).\nOn garde donc le d√©ploiement de l‚Äôapplication s√©par√© (par ex. via un m√©canisme existant), mais l‚ÄôOS est d√©j√† √† jour.\nLa r√©ponse B cr√©erait trop d‚ÄôAMI (une par version d‚Äôapplication), ce qui va contre l‚Äôobjectif.\nLa r√©ponse E seule ne garantit pas que les mises √† jour de s√©curit√© soient appliqu√©es.\nDonc la bonne combinaison est de construire une AMI patch√©e et valid√©e avec Image Builder et de l‚Äôutiliser dans l‚ÄôAuto Scaling group (A).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:493:970adf3e1181967b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 493,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA video streaming company has a pipe in Amazon EventBridge Pipes that uses an Amazon Simple Queue Service (Amazon SQS) queue as an event source. The pipe publishes all source events to a target EventBridge event bus. Before events are published, the pipe uses an AWS Lambda function to retrieve the stream status of each event from a database and adds the stream status to each source event.The company wants the pipe to publish events to the event bus only if the video stream has a status of ready.Which solution will meet these requirements?",
      "choices": {
        "A": "Add a filter step to the pipe that will match on a stream status of ready.",
        "B": "Update the Lambda function to return only video streams that have a status of ready.",
        "C": "Include a filter for a status of ready in all EventBridge rules that subscribe to the event bus.",
        "D": "Add an input transformer to the pipe output that filters streams that have a status of ready."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156662-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:10 a.m.",
      "textHash": "970adf3e1181967b",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "EventBridge Pipes sert √† d√©placer des √©v√©nements d‚Äôune source (ici une file SQS) vers une cible (ici un bus EventBridge), avec des √©tapes optionnelles comme enrichissement et filtrage.\nSQS est une file de messages : chaque message repr√©sente un √©v√©nement √† traiter.\nLa fonction Lambda est utilis√©e ici pour ¬´ enrichir ¬ª l‚Äô√©v√©nement : elle lit une base de donn√©es et ajoute le champ \"streamStatus\" (ex: ready, not-ready).\nLe besoin est de ne publier sur le bus EventBridge QUE si le statut vaut \"ready\".\nLa bonne pratique est de filtrer le plus t√¥t possible dans le pipe, avant d‚Äôenvoyer √† la cible : c‚Äôest exactement le r√¥le de l‚Äô√©tape Filter d‚ÄôEventBridge Pipes.\nDonc on ajoute un filtre qui ne laisse passer que les √©v√©nements dont streamStatus == \"ready\" (r√©ponse A).\nB est moins adapt√© : Lambda sert √† enrichir, et ¬´ ne rien retourner ¬ª peut compliquer le flux et n‚Äôest pas le m√©canisme standard de filtrage du pipe.\nC filtre trop tard : les √©v√©nements non-ready seraient quand m√™me publi√©s sur le bus (co√ªt/bruit) et il faudrait dupliquer la logique dans plusieurs r√®gles.\nD (input transformer) sert √† remodeler le contenu, pas √† bloquer des √©v√©nements.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine : une file (SQS) am√®ne des plateaux, un surveillant (le ‚Äúpipe‚Äù) les fait passer, un √©l√®ve v√©rifie sur une liste (Lambda + base) si le plateau est ‚Äúpr√™t‚Äù, puis on envoie le plateau au self (event bus) pour que les autres le prennent.**\n\nConcept : le pipe est un tapis roulant qui peut ajouter une info (statut) puis d√©cider de laisser passer ou non.\nIci, Lambda ajoute le statut ‚Äúready / pas ready‚Äù sur chaque plateau.\nOn veut que seuls les ‚Äúready‚Äù arrivent au self.\nA : ajouter un ‚Äúfiltre‚Äù dans le pipe, c‚Äôest comme mettre un portique juste apr√®s la v√©rification : si statut = ready, √ßa passe, sinon √ßa part de c√¥t√©.\nB : changer Lambda pour ne renvoyer que les ready, c‚Äôest comme demander au v√©rificateur de ne parler que des ready, mais le plateau existe quand m√™me sur le tapis.\nC : filtrer dans les r√®gles apr√®s le self, c‚Äôest trop tard : les plateaux non pr√™ts encombrent d√©j√† le self.\nD : un ‚Äútransformer‚Äù change la forme du message, ce n‚Äôest pas un portique qui bloque.\nDonc la bonne r√©ponse est A.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:484:c605e89b6eb8b420",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 484,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA large company has its application components distributed across multiple AWS accounts. The company needs to collect and visualize trace data across these accounts.What should be used to meet these requirements?",
      "choices": {
        "A": "AWS X-Ray",
        "B": "Amazon CloudWatch",
        "C": "Amazon VPC flow logs",
        "D": "Amazon OpenSearch Service"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156654-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:57 a.m.",
      "textHash": "c605e89b6eb8b420",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Pour suivre une requ√™te quand elle traverse plusieurs services, on utilise des ‚Äútraces‚Äù (un fil d‚ÄôAriane) : on voit chaque √©tape et le temps pass√©.\nAWS X-Ray est un service de tra√ßage distribu√© : il collecte automatiquement des segments (appels entre composants) et les affiche sous forme de carte de service et de chronologie.\nIl peut agr√©ger et visualiser ces traces m√™me si les composants sont dans plusieurs comptes AWS (avec une configuration d‚Äôacc√®s/partage), ce qui r√©pond exactement au besoin ‚Äúmulti-comptes‚Äù.\nAmazon CloudWatch est surtout pour les m√©triques (CPU, latence), les logs et les alarmes ; ce n‚Äôest pas l‚Äôoutil principal pour reconstruire un parcours de requ√™te de bout en bout.\nLes VPC Flow Logs enregistrent le trafic r√©seau (IP, ports, accept/reject) : utile pour r√©seau/s√©curit√©, pas pour tracer une requ√™te applicative.\nAmazon OpenSearch sert √† indexer et rechercher des logs/donn√©es ; il ne cr√©e pas nativement des traces distribu√©es ni une carte de d√©pendances applicatives.\nDonc la meilleure r√©ponse pour collecter et visualiser des traces √† travers plusieurs comptes est AWS X-Ray.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un grand lyc√©e avec plusieurs b√¢timents (comptes AWS). Une rumeur (une requ√™te utilisateur) traverse plusieurs salles (composants de l‚Äôappli) et tu veux suivre son trajet exact.**\n\nConcept : une ‚Äútrace‚Äù, c‚Äôest comme le fil d‚ÄôAriane qui montre toutes les √©tapes parcourues par une action, de la premi√®re salle √† la derni√®re, m√™me si √ßa passe par plusieurs b√¢timents. AWS X-Ray, c‚Äôest le surveillant-enqu√™teur qui colle un badge √† la rumeur et note chaque porte franchie, puis te dessine le plan du trajet. Comme l‚Äôentreprise a des morceaux de l‚Äôappli dans plusieurs comptes, il faut un outil fait pour suivre un m√™me parcours √† travers plusieurs endroits et l‚Äôafficher clairement : c‚Äôest exactement X-Ray. CloudWatch, c‚Äôest plut√¥t le tableau d‚Äôaffichage des notes et alertes (mesures/logs), pas le trajet complet √©tape par √©tape. VPC flow logs, c‚Äôest juste qui a parl√© √† qui sur le r√©seau, pas l‚Äôhistoire compl√®te de l‚Äôaction. OpenSearch, c‚Äôest une grosse biblioth√®que pour chercher dans des textes, pas un outil de tra√ßage.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:238:ae51e68a4702d8f2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 238,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on an application that is deployed on an Amazon EC2 instance. The developer needs a solution that will securely transfer files from the application to an Amazon S3 bucket.What should the developer do to meet these requirements in the MOST secure way?",
      "choices": {
        "A": "Create an IAM user. Create an access key for the IAM user. Store the access key in the application‚Äôs environment variables.",
        "B": "Create an IAM role. Create an access key for the IAM role. Store the access key in the application‚Äôs environment variables.",
        "C": "Create an IAM role. Configure the IAM role to access the specific Amazon S3 API calls the application requires. Associate the IAM role with the EC2 instance.",
        "D": "Configure an S3 bucket policy for the S3 bucket. Configure the S3 bucket policy to allow access for the EC2 instance ID."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124783-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 9:22 a.m.",
      "textHash": "ae51e68a4702d8f2",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Sur une instance EC2, la fa√ßon la plus s√ªre d‚Äôacc√©der √† S3 est d‚Äô√©viter toute ‚Äúcl√©‚Äù stock√©e dans l‚Äôapplication.\nIAM est le service qui g√®re les permissions. Un ‚Äúutilisateur IAM‚Äù avec des cl√©s d‚Äôacc√®s (A) oblige √† stocker des secrets (variables d‚Äôenvironnement), ce qui peut fuiter.\nUn ‚Äúr√¥le IAM‚Äù est une identit√© sans cl√©s permanentes : AWS fournit automatiquement des identifiants temporaires √† l‚Äôinstance EC2 via le service de m√©tadonn√©es.\nAvec C, on cr√©e un r√¥le IAM avec uniquement les actions S3 n√©cessaires (principe du moindre privil√®ge), puis on attache ce r√¥le √† l‚Äôinstance EC2.\nL‚Äôapplication peut alors envoyer des fichiers vers le bucket S3 en utilisant ces identifiants temporaires, renouvel√©s automatiquement.\nB est faux car on ne cr√©e pas de cl√©s d‚Äôacc√®s pour un r√¥le IAM (et ce serait moins s√ªr).\nD est incorrect : une policy S3 ne peut pas cibler proprement un ‚Äúinstance ID‚Äù comme identit√© IAM, et ce n‚Äôest pas le m√©canisme standard/fiable pour authentifier une application.\nDonc la meilleure option, la plus s√©curis√©e et recommand√©e, est d‚Äôutiliser un r√¥le IAM attach√© √† l‚Äôinstance EC2 (C).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine ton appli comme un √©l√®ve (EC2) qui doit d√©poser des devoirs dans une bo√Æte de d√©p√¥t du prof (S3).**\n\nConcept : pour d√©poser un devoir, soit tu donnes ton mot de passe √©crit sur un papier (cl√©), soit tu re√ßois un badge officiel temporaire (r√¥le) qui ouvre juste la bo√Æte.\nLa m√©thode la plus s√ªre, c‚Äôest le badge : il est attach√© √† l‚Äô√©l√®ve, pas stock√© dans son sac o√π on peut le voler.\nC : on cr√©e un ‚Äúr√¥le‚Äù (badge) avec seulement les actions n√©cessaires (ex: d√©poser/voir les fichiers demand√©s), puis on l‚Äôattache √† l‚Äôordinateur-√©l√®ve (EC2).\nComme √ßa, l‚Äôappli obtient automatiquement des autorisations temporaires, sans enregistrer de cl√©s dans des variables (papier dans le sac).\nA est moins s√ªr : une cl√© stock√©e dans l‚Äôappli peut fuiter.\nB est faux : un r√¥le n‚Äôa pas de ‚Äúcl√©‚Äù √† cr√©er comme un utilisateur.\nD est moins adapt√© : autoriser via l‚ÄôID de la machine, c‚Äôest comme dire ‚Äútout √©l√®ve avec ce num√©ro peut entrer‚Äù, moins propre que le badge avec droits pr√©cis.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:409:40a06a5d7b8ab274",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 409,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has an application that uses WebSocket APIs in Amazon API Gateway. The developer wants to use an API Gateway Lambda authorizer to control access to the application.The developer needs to add credential caching and reduce repeated usage of secret keys and authorization tokens on every request.Which combination of steps should the developer take to meet these requirements? (Choose two.)",
      "choices": {
        "A": "Use a token-based Lambda authorizer.",
        "B": "Use a request parameter-based Lambda authorizer.",
        "C": "Configure an integration request mapping template to reference the context map from the APIGateway Lambda authorizer.",
        "D": "Configure an integration request mapping template to reference the identity API key value from the API Gateway Lambda authorizer.",
        "E": "Use VPC endpoint policies for the WebSocket APIs."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144297-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 21, 2024, 3:25 a.m.",
      "textHash": "40a06a5d7b8ab274",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:f36a0674",
      "frExplanation": "Ici, on veut contr√¥ler l‚Äôacc√®s √† une API WebSocket (API Gateway) avec un Lambda authorizer (une fonction Lambda qui d√©cide si l‚Äôappel est autoris√©).\nPour √©viter de recalculer l‚Äôautorisation √† chaque message, on utilise le cache d‚ÄôAPI Gateway : il m√©morise le r√©sultat de l‚Äôauthorizer pendant un temps.\nLe cache fonctionne surtout avec un authorizer ¬´ token-based ¬ª : l‚Äôentr√©e du cache est le jeton (token) envoy√© par le client (souvent dans l‚Äôen-t√™te Authorization).\nSi le m√™me token est r√©utilis√©, API Gateway reprend la d√©cision en cache et ne relance pas Lambda, ce qui r√©duit l‚Äôusage r√©p√©t√© de cl√©s secr√®tes et de tokens.\nUn authorizer ¬´ request parameter-based ¬ª d√©pend de plusieurs param√®tres de requ√™te : cela varie plus souvent et rend le cache moins efficace/moins simple.\nLes mapping templates (C, D) servent √† transformer la requ√™te vers l‚Äôint√©gration, pas √† mettre en place le cache d‚Äôidentifiants.\nLes VPC endpoint policies (E) concernent l‚Äôacc√®s r√©seau priv√©, pas la mise en cache d‚Äôautorisations.\nDonc le bon choix est d‚Äôutiliser un Lambda authorizer bas√© sur un token (A) pour permettre le caching et r√©duire les appels r√©p√©t√©s.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine l‚Äôentr√©e du lyc√©e avec un surveillant et un tampon sur la main.**\n\nConcept : pour entrer, tu montres un badge (un jeton) au surveillant. S‚Äôil est valide, il te laisse passer et il ‚Äúse souvient‚Äù un moment que tu es OK (cache), donc tu ne ressors pas ta carte d‚Äôidentit√© √† chaque porte.\nIci, le badge = un ‚Äútoken‚Äù (un code d‚Äôacc√®s) envoy√© par l‚Äôappli. Le surveillant = le Lambda authorizer (un petit programme qui v√©rifie).\nPourquoi A : un authorizer ‚Äútoken-based‚Äù v√©rifie surtout ce badge. Comme le badge est le m√™me pendant un moment, API Gateway peut le mettre en m√©moire (credential caching) et √©viter de re-v√©rifier les secrets √† chaque message WebSocket.\nPourquoi pas B : v√©rifier plein de d√©tails de la tenue/sac (param√®tres de requ√™te) change souvent, donc c‚Äôest moins fait pour un cache simple.\nC et D : ce sont des fa√ßons de transmettre des infos plus loin, pas de r√©duire les v√©rifications.\nE : c‚Äôest une r√®gle de b√¢timent (r√©seau), pas un contr√¥le par badge.\nDonc : choisir A (token-based Lambda authorizer) pour permettre le cache et √©viter de r√©utiliser les secrets √† chaque requ√™te.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:408:3df4f5d9f6aee1c1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 408,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an application that must transfer expired items from Amazon DynamoDB to Amazon S3. The developer sets up the DynamoDB table to automatically delete items after a specific TTL. The application must process the items in DynamoDB and then must store the expired items in Amazon S3. The entire process, including item processing and storage in Amazon S3, will take 5 minutes.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Configure DynamoDB Accelerator (DAX) to query for expired items based on the TTL. Save the results to Amazon S3.",
        "B": "Configure DynamoDB Streams to invoke an AWS Lambda function. Program the Lambda function to process the items and to store the expired items in Amazon S3.",
        "C": "Deploy a custom application on an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2 instances. Program the custom application to process the items and to store the expired items in Amazon S3.",
        "D": "Create an Amazon EventBridge rule to invoke an AWS Lambda function. Program the Lambda function to process the items and to store the expired items in Amazon S3."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143936-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 16, 2024, 3:22 a.m.",
      "textHash": "3df4f5d9f6aee1c1",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "On veut d√©placer des √©l√©ments ¬´ expir√©s ¬ª d‚Äôune table DynamoDB vers S3. DynamoDB peut supprimer automatiquement des √©l√©ments gr√¢ce au TTL (Time To Live), mais il faut un moyen de r√©agir √† cette suppression pour traiter l‚Äô√©l√©ment avant/au moment o√π il dispara√Æt.\nDynamoDB Streams enregistre les changements d‚Äôune table (ajout, modification, suppression) sous forme d‚Äô√©v√©nements. Quand un √©l√©ment est supprim√© (y compris via TTL), un √©v√©nement de suppression peut appara√Ætre dans le Stream.\nAWS Lambda est un service qui ex√©cute du code sans g√©rer de serveurs. On peut connecter DynamoDB Streams √† Lambda pour d√©clencher automatiquement une fonction √† chaque suppression.\nLa fonction Lambda peut alors lire les donn√©es de l‚Äô√©l√©ment supprim√©, faire le traitement n√©cessaire, puis √©crire le r√©sultat dans Amazon S3 (stockage d‚Äôobjets).\nC‚Äôest le moins d‚Äô¬´ overhead ¬ª car il n‚Äôy a pas de serveurs/cluster √† maintenir (contrairement √† ECS/EC2) et pas de logique de scan/requ√™te √† planifier.\nDAX ne sert qu‚Äô√† acc√©l√©rer des lectures, pas √† d√©tecter proprement les expirations. EventBridge ne re√ßoit pas nativement chaque suppression TTL comme un flux d‚Äô√©v√©nements de table.\nDonc la meilleure solution est DynamoDB Streams + Lambda + √©criture dans S3 (B).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e : chaque livre a une date de retour. Quand la date est d√©pass√©e, le livre est retir√© du rayon et doit √™tre envoy√© aux archives.**\n\nConcept : DynamoDB = le rayon de livres, TTL = l‚Äô√©tiquette ‚Äú√† retirer √† telle date‚Äù, S3 = les archives (un grand stockage). On veut traiter le livre puis l‚Äôarchiver, sans surveiller √† la main. Avec DynamoDB Streams, c‚Äôest comme un ‚Äújournal‚Äù qui note automatiquement chaque retrait de livre. Ce journal d√©clenche tout seul un assistant (AWS Lambda = un petit robot qui travaille quand on l‚Äôappelle) d√®s qu‚Äôun livre est supprim√© par TTL. Le robot a 5 minutes pour faire le traitement puis d√©poser le livre aux archives S3. C‚Äôest le moins de boulot √† g√©rer : pas de serveurs √† maintenir (contrairement √† ECS/EC2), pas besoin de scanner/rechercher les expir√©s (contrairement √† DAX), et EventBridge ne ‚Äúvoit‚Äù pas directement chaque suppression TTL aussi naturellement que le journal Streams.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:404:2f100da37651752a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 404,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses AWS X-Ray to monitor a serverless application. The components of the application have different request rates. The user interactions and transactions are important to trace, but they are low in volume. The background processes such as application health checks, polling, and connection maintenance generate high volumes of read-only requests.Currently, the default X-Ray sampling rules are universal for all requests. Only the first request per second and some additional requests are recorded. This setup is not helping the company review the requests based on service or request type.A developer must configure rules to trace requests based on service or request properties. The developer must trace the user interactions and transactions without wasting effort recording minor background tasks.Which solution will meet these requirements?",
      "choices": {
        "A": "Disable sampling for high-volume read-only requests. Sample at a lower rate for all requests that handle user interactions or transactions.",
        "B": "Disable sampling and trace all requests for requests that handle user interactions or transactions. Sample high-volume read-only requests at a higher rate.",
        "C": "Disable sampling and trace all requests for requests that handle user interactions or transactions. Sample high-volume read-only requests at a lower rate.",
        "D": "Disable sampling for high-volume read-only requests. Sample at a higher rate for all requests that handle user interactions or transactions."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144274-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 20, 2024, 6:52 p.m.",
      "textHash": "2f100da37651752a",
      "rawFormat": "discussion-md",
      "conceptKey": "xray_tracing",
      "frExplanation": "AWS X-Ray sert √† ¬´ tracer ¬ª des requ√™tes pour comprendre le chemin d‚Äôun appel dans une application (utile pour diagnostiquer lenteurs et erreurs). Comme il peut y avoir √©norm√©ment de requ√™tes, X-Ray utilise l‚Äô¬´ √©chantillonnage ¬ª (sampling) : il n‚Äôenregistre qu‚Äôune partie des requ√™tes. Ici, les actions importantes (clics utilisateur, transactions) sont rares mais critiques : on veut les voir toutes pour pouvoir les analyser pr√©cis√©ment. √Ä l‚Äôinverse, les t√¢ches de fond (health checks, polling, maintien de connexion) g√©n√®rent beaucoup de requ√™tes en lecture seule et peu utiles : les tracer toutes gaspillerait du stockage et du bruit. La bonne approche est donc de cr√©er des r√®gles de sampling diff√©rentes selon le service ou des propri√©t√©s de la requ√™te : 100% pour les interactions/transactions, et un taux faible pour les requ√™tes de fond. C‚Äôest exactement ce que propose la r√©ponse C : d√©sactiver l‚Äô√©chantillonnage (donc tout tracer) pour les requ√™tes importantes, et √©chantillonner √† un taux plus bas les requ√™tes volumineuses et peu utiles.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine le surveillant du lyc√©e qui note dans un carnet ce qui se passe dans les couloirs. Il veut surtout noter les vraies histoires importantes (disputes, accidents), pas les petits passages r√©p√©titifs (√©l√®ves qui vont juste boire de l‚Äôeau).**\n\nAWS X-Ray, c‚Äôest ce carnet: il ‚Äútrace‚Äù des demandes pour comprendre ce qui se passe dans l‚Äôappli. Le probl√®me: il note un peu de tout pareil, donc il rate des choses importantes et garde trop de bruit. Les interactions utilisateurs et transactions, c‚Äôest rare mais crucial: on veut TOUT noter (d√©sactiver l‚Äô√©chantillonnage = ne pas prendre juste un ‚Äúextrait‚Äù, mais tout enregistrer). Les t√¢ches de fond (health checks, polling‚Ä¶), c‚Äôest tr√®s fr√©quent et peu important: on veut en noter peu (√©chantillonner √† un taux plus bas). Comme √ßa, le carnet est rempli d‚Äô√©v√©nements utiles, pas de ‚Äúbruit‚Äù. Donc la bonne r√©ponse est C: tout tracer pour l‚Äôimportant, et r√©duire l‚Äôenregistrement pour le fond.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:518:e59f9d008e70fcfa",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 518,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nTwo containerized microservices are hosted on Amazon EC2 ECS. The first microservice reads an Amazon RDS Aurora database instance, and the second microservice reads an Amazon DynamoDB table.How can each microservice be granted the minimum privileges?",
      "choices": {
        "A": "Set ECS_ENABLE_TASK_IAM_ROLE to false on EC2 instance boot in ECS agent configuration file. Run the first microservice with an IAM role for ECS tasks with read-only access for the Aurora database. Run the second microservice with an IAM role for ECS tasks with read-only access to DynamoDB.",
        "B": "Set ECS_ENABLE_TASK_IAM ROLE to false on EC2 instance boot in the ECS agent configuration file. Grant the instance profile role read-only access to the Aurora database and DynamoDB.",
        "C": "Set ECS_ENABLE_TASK_IAM ROLE to true on EC2 instance boot in the ECS agent configuration file. Run the first microservice with an IAM role for ECS tasks with read-only access for the Aurora database. Run the second microservice with an IAM role for ECS tasks with read-only access to DynamoDB.",
        "D": "Set ECS_ENABLE_TASK_IAM_ROLE to true on EC2 instance boot in the ECS agent configuration file. Grant the instance profile role read-only access to the Aurora database and DynamoDB."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156675-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:42 a.m.",
      "textHash": "e59f9d008e70fcfa",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Objectif : donner le ‚Äúminimum de droits‚Äù √† chaque microservice. Sur ECS (containers sur EC2), on peut attribuer des permissions AWS soit au serveur EC2 (instance profile), soit directement √† chaque t√¢che ECS (task role).\nSi on met les droits sur le r√¥le de l‚Äôinstance EC2, alors tous les containers sur cette machine h√©ritent potentiellement des m√™mes acc√®s : ce n‚Äôest pas du moindre privil√®ge.\nLa bonne pratique est d‚Äôutiliser un IAM Role par t√¢che (IAM Roles for Tasks) : chaque microservice re√ßoit uniquement les permissions dont il a besoin.\nPour que cela fonctionne sur ECS sur EC2, il faut activer la fonctionnalit√© c√¥t√© agent ECS : ECS_ENABLE_TASK_IAM_ROLE = true.\nEnsuite, on associe au microservice 1 un task role avec acc√®s en lecture √† Aurora (via les m√©canismes d‚Äôacc√®s √† la base, ex. IAM auth/Secrets/SG selon le cas).\nEt on associe au microservice 2 un task role avec permissions DynamoDB en lecture seule (ex. dynamodb:GetItem, Query, Scan).\nAinsi, chaque service a ses propres cl√©s temporaires et limit√©es, et aucun n‚Äôobtient des droits inutiles.\nDonc la r√©ponse C est correcte : activer task IAM roles et d√©finir un r√¥le distinct par microservice.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un coll√®ge avec deux clubs qui utilisent des ressources diff√©rentes : le club A va √† la biblioth√®que (Aurora = base de donn√©es), le club B va au casier des fiches (DynamoDB = autre base). Le b√¢timent du coll√®ge (la machine EC2) a un badge g√©n√©ral, mais chaque club peut aussi avoir son propre badge.**\n\nConcept : le ‚Äúminimum de privil√®ges‚Äù, c‚Äôest donner √† chacun seulement le badge dont il a besoin, pas plus.\nSi on donne un seul badge au b√¢timent (badge EC2) qui ouvre biblioth√®que + casier, alors n‚Äôimporte quel club dans le b√¢timent peut tout ouvrir : trop de droits.\nLa bonne m√©thode est de donner un badge par club (r√¥le IAM par t√¢che ECS) : club A = lecture seule biblioth√®que, club B = lecture seule casier.\nPour que √ßa marche, il faut activer l‚Äôoption qui autorise les badges par club (ECS_ENABLE_TASK_IAM_ROLE = true).\nDonc C est bon : option activ√©e + un r√¥le s√©par√© pour chaque microservice, chacun en lecture seule sur sa ressource.\nA et B d√©sactivent les badges par club, donc on retombe sur un badge trop puissant.\nD active l‚Äôoption, mais donne quand m√™me un badge trop puissant au b√¢timent.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:60:7e18e0ad90c84196",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 60,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn ecommerce company is using an AWS Lambda function behind Amazon API Gateway as its application tier. To process orders during checkout, the application calls a POST API from the frontend. The POST API invokes the Lambda function asynchronously. In rare situations, the application has not processed orders. The Lambda application logs show no errors or failures.What should a developer do to solve this problem?",
      "choices": {
        "A": "Inspect the frontend logs for API failures. Call the POST API manually by using the requests from the log file.",
        "B": "Create and inspect the Lambda dead-letter queue. Troubleshoot the failed functions. Reprocess the events.",
        "C": "Inspect the Lambda logs in Amazon CloudWatch for possible errors. Fix the errors.",
        "D": "Make sure that caching is disabled for the POST API in API Gateway."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103807-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 24, 2023, 11:59 p.m.",
      "textHash": "7e18e0ad90c84196",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Ici, l‚ÄôAPI Gateway d√©clenche une fonction AWS Lambda ¬´ en asynchrone ¬ª : l‚Äôappelant re√ßoit vite une r√©ponse, mais le traitement se fait plus tard en arri√®re-plan.\nEn mode asynchrone, si Lambda n‚Äôarrive pas √† traiter un √©v√©nement (probl√®me temporaire, limite, bug, etc.), l‚Äôappel initial peut quand m√™me sembler ‚ÄúOK‚Äù c√¥t√© application.\nC‚Äôest pour cela que vous pouvez avoir des commandes non trait√©es sans voir d‚Äôerreur √©vidente dans les logs habituels.\nLa bonne pratique est d‚Äôutiliser une Dead-Letter Queue (DLQ) : une file (Amazon SQS) ou un topic (SNS) o√π Lambda envoie les √©v√©nements qui ont √©chou√© apr√®s plusieurs tentatives.\nEn inspectant la DLQ, vous voyez exactement quelles commandes ont √©chou√© et pourquoi (payload, contexte), m√™me si les logs ne montrent rien de clair.\nEnsuite, vous pouvez corriger la cause et re-traiter ces √©v√©nements (rejouer les messages) pour ne pas perdre de commandes.\nA ne garantit pas de retrouver les √©v√©nements perdus, C est redondant (d√©j√† fait) et D (cache) ne s‚Äôapplique pas vraiment √† un POST de checkout et n‚Äôexplique pas des pertes asynchrones.\nDonc : configurer et analyser la DLQ Lambda, puis reprocesser les √©v√©nements.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : tu passes commande (ton plateau) au guichet, et le cuisinier re√ßoit un ticket pour pr√©parer ton repas. Parfois, le ticket se perd, et toi tu n‚Äôas rien, mais personne ne voit d‚Äôerreur sur le moment.**\n\nConcept : ici, le site envoie une ‚Äúcommande‚Äù et dit √† Lambda (le cuisinier) : ¬´ je te laisse faire, je n‚Äôattends pas la r√©ponse ¬ª (asynchrone). Donc si un ticket se perd ou n‚Äôarrive pas, le site peut croire que c‚Äôest parti, mais la commande n‚Äôest jamais cuisin√©e.\nPourquoi B : une dead-letter queue, c‚Äôest comme une bo√Æte ‚Äútickets rat√©s‚Äù o√π on met automatiquement les commandes que le cuisinier n‚Äôa pas pu traiter apr√®s plusieurs essais. Comme les logs ne montrent rien, il faut regarder cette bo√Æte pour retrouver les commandes perdues, comprendre pourquoi elles ont √©chou√©, puis les relancer (reprocess). Les autres choix ne r√©cup√®rent pas les tickets perdus : ils regardent ailleurs ou changent un d√©tail (cache) qui n‚Äôexplique pas des commandes manquantes.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:59:2a0c9ae0ba192940",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 59,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is using AWS Amplify Hosting to build and deploy an application. The developer is receiving an increased number of bug reports from users. The developer wants to add end-to-end testing to the application to eliminate as many bugs as possible before the bugs reach production.Which solution should the developer implement to meet these requirements?",
      "choices": {
        "A": "Run the amplify add test command in the Amplify CLI.",
        "B": "Create unit tests in the application. Deploy the unit tests by using the amplify push command in the Amplify CLI.",
        "C": "Add a test phase to the amplify.yml build settings for the application.",
        "D": "Add a test phase to the aws-exports.js file for the application."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/104015-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 27, 2023, 1:32 a.m.",
      "textHash": "2a0c9ae0ba192940",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:35fed2b6",
      "frExplanation": "AWS Amplify Hosting sert √† construire (build) et d√©ployer une application web automatiquement √† chaque mise √† jour du code.\nPour √©viter que des bugs arrivent en production, on ajoute des tests ¬´ end-to-end ¬ª (E2E) : ils simulent un vrai utilisateur (ouvrir la page, cliquer, se connecter, etc.).\nDans Amplify Hosting, le fichier amplify.yml d√©crit les √©tapes du pipeline : installation, build, et on peut y ajouter une phase de tests.\nEn ajoutant une phase \"test\" dans amplify.yml, Amplify ex√©cutera les tests √† chaque build; si les tests √©chouent, le d√©ploiement peut √™tre bloqu√©.\nC‚Äôest exactement ce qu‚Äôon veut pour attraper les bugs avant la mise en production.\nA est faux : il n‚Äôexiste pas une commande standard \"amplify add test\" pour Amplify Hosting.\nB est insuffisant : les unit tests testent des fonctions isol√©es, pas un parcours complet utilisateur, et \"amplify push\" sert surtout √† d√©ployer des ressources backend.\nD est faux : aws-exports.js contient la configuration g√©n√©r√©e (IDs, r√©gions, endpoints), pas des √©tapes de pipeline ou de tests.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu tiens une pizzeria. Avant d‚Äôenvoyer une pizza au client, tu as une ‚Äúchecklist cuisine‚Äù : cuisson, ingr√©dients, bo√Æte, adresse. Si tu ajoutes une √©tape ‚Äúgo√ªter la pizza‚Äù avant livraison, tu √©vites plein de retours clients.**\n\nAmplify Hosting, c‚Äôest comme le service qui pr√©pare et livre ton appli aux utilisateurs (la ‚Äúproduction‚Äù, c‚Äôest les clients). Les bugs, c‚Äôest des pizzas rat√©es qui arrivent chez les clients. Les tests de bout en bout, c‚Äôest comme faire une commande compl√®te en test : tu commandes, tu paies, tu re√ßois, tu v√©rifies que tout marche. Pour faire √ßa automatiquement avant chaque livraison, tu ajoutes une √©tape de test dans la checklist de fabrication. Dans Amplify, cette checklist s‚Äôappelle amplify.yml (les r√©glages de construction). Donc la bonne r√©ponse est C : ajouter une phase ‚Äútest‚Äù dans amplify.yml pour tester avant de publier.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:479:ba8c3fcbf007956a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 479,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is troubleshooting a three-tier application, which is deployed on Amazon EC2 instances. There is a connectivity problem between the application servers and the database servers.Which AWS services or tools should be used to identity the faulty component? (Choose two.)",
      "choices": {
        "A": "AWS CloudTrail",
        "B": "AWS Trusted Advisor",
        "C": "Amazon VPC Flow Logs",
        "D": "Network access control lists",
        "E": "AWS Config rules"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156651-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:51 a.m.",
      "textHash": "ba8c3fcbf007956a",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:a13843fc",
      "frExplanation": "Ici, le probl√®me est de la connectivit√© r√©seau entre des serveurs applicatifs et des serveurs de base de donn√©es sur EC2 (machines virtuelles AWS). Pour trouver quel composant bloque, il faut observer le trafic r√©seau.\nAmazon VPC Flow Logs (r√©ponse C) enregistre les ‚Äúm√©tadonn√©es‚Äù des flux r√©seau dans un VPC : qui parle √† qui (IP/port), et si le trafic est ACCEPT ou REJECT. Cela permet de voir rapidement si les paquets sont bloqu√©s et √† quel niveau.\nConcr√®tement, si vous voyez des REJECT entre l‚ÄôIP des serveurs applicatifs et l‚ÄôIP/port de la base, vous savez que le blocage vient d‚Äôune r√®gle r√©seau (Security Group ou NACL) ou d‚Äôun routage.\nCloudTrail (A) trace les appels API (qui a modifi√© une ressource), utile pour l‚Äôaudit, mais ne montre pas les paquets r√©seau.\nTrusted Advisor (B) donne des recommandations g√©n√©rales, pas un diagnostic pr√©cis de flux.\nAWS Config rules (E) v√©rifie la conformit√© de configuration, mais ne prouve pas un blocage en temps r√©el.\nDonc, l‚Äôoutil le plus direct pour identifier le composant fautif c√¥t√© r√©seau est VPC Flow Logs.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine ton coll√®ge avec 3 zones : la cour (serveurs d‚Äôappli), le couloir (r√©seau), et le CDI (base de donn√©es). Les √©l√®ves doivent aller de la cour au CDI, mais ils n‚Äôy arrivent plus. Pour trouver o√π √ßa bloque, tu regardes les cam√©ras du couloir qui enregistrent qui passe, d‚Äôo√π, vers o√π, et si √ßa a √©t√© refus√©.**\n\nConcept : quand deux parties ne se parlent plus, il faut des ‚Äútraces de passage‚Äù pour voir si les messages circulent ou sont bloqu√©s.\nAmazon VPC Flow Logs (C), c‚Äôest comme les cam√©ras du couloir : √ßa note les allers-retours r√©seau (qui parle √† qui, sur quel ‚Äúport‚Äù, accept√©/refus√©).\nAvec √ßa, tu rep√®res si le probl√®me vient des serveurs d‚Äôappli (ils n‚Äôenvoient rien), du r√©seau (√ßa se fait refuser), ou des serveurs de base (ils ne r√©pondent pas).\nCloudTrail (A) serait plut√¥t le registre ‚Äúqui a chang√© les r√®gles‚Äù, pas le trafic qui passe.\nTrusted Advisor (B) donne des conseils g√©n√©raux, pas un diagnostic pr√©cis de connexion.\nLes NACL (D) sont des ‚Äúr√®gles de porte‚Äù, mais ne te montrent pas clairement ce qui s‚Äôest pass√©.\nAWS Config rules (E) v√©rifie la conformit√© des r√©glages, pas les √©changes r√©els.\nDonc pour identifier le composant fautif c√¥t√© connexion, C est le bon outil.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:546:765010941f16eceb",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 546,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building the authentication mechanism for a new mobile app. Users need to be able to sign up, sign in, and access secured backend AWS resources.Which solution will meet these requirements?",
      "choices": {
        "A": "Use AWS Identity and Access Management Access Analyzer to generate IAM policies. Create an IAM role. Attach the policies to the role. Integrate the IAM role with an identity provider that the mobile app uses.",
        "B": "Create an IAM policy that grants access to the backend resources. Create an IAM role. Attach the policy to the role. Create an Amazon API Gateway endpoint. Attach the role to the endpoint. Integrate the endpoint with the mobile app.",
        "C": "Create an Amazon Cognito identity pool. Configure permissions by choosing a default IAM role for authenticated users or guest users in the identity pool. Associate the identity pool with an identity provider. Integrate the identity pool with the mobile app.",
        "D": "Create an Amazon Cognito user pool. Configure the security requirements by choosing a password policy, multi-factor authentication (MFA) requirements, and user account recovery options. Create an app client. Integrate the app client with the mobile app."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/154507-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Jan. 14, 2025, 6:13 p.m.",
      "textHash": "765010941f16eceb",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, l‚Äôapp mobile doit g√©rer l‚Äôinscription (sign up) et la connexion (sign in) des utilisateurs. Le service AWS fait pour √ßa est Amazon Cognito User Pool : c‚Äôest un ‚Äúannuaire d‚Äôutilisateurs‚Äù g√©r√© par AWS.\nUn User Pool fournit directement les fonctions d‚Äôinscription/connexion, la gestion des mots de passe, la politique de mot de passe, la MFA (double facteur) et la r√©cup√©ration de compte.\nOn cr√©e ensuite un ‚Äúapp client‚Äù pour que l‚Äôapplication mobile puisse obtenir des jetons (tokens) apr√®s connexion, et les envoyer aux API/backend pour prouver l‚Äôidentit√© de l‚Äôutilisateur.\nLes autres choix parlent surtout de r√¥les/politiques IAM ou d‚ÄôAPI Gateway, mais IAM n‚Äôest pas con√ßu pour g√©rer des comptes utilisateurs finaux d‚Äôune app mobile.\nUn Identity Pool (choix C) sert plut√¥t √† donner des identifiants AWS temporaires pour acc√©der √† des services AWS, mais il ne g√®re pas √† lui seul l‚Äôinscription/connexion (il s‚Äôappuie souvent sur un User Pool).\nDonc la solution la plus directe et correcte pour sign up/sign in + s√©curit√© utilisateur est le Cognito User Pool avec un app client (D).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que ton appli est un lyc√©e. Pour entrer, les √©l√®ves doivent s‚Äôinscrire, se connecter, et parfois montrer une double preuve (carte + code). Une fois connect√©s, ils re√ßoivent un ‚Äúbadge‚Äù qui prouve qui ils sont.**\n\nConcept : l‚Äôauthentification, c‚Äôest g√©rer ‚Äúqui est la personne‚Äù (inscription, connexion, mot de passe, code SMS). Dans l‚Äôanalogie, c‚Äôest le bureau de la vie scolaire qui cr√©e les comptes, v√©rifie les mots de passe, propose un code en plus (MFA) et aide √† r√©cup√©rer un compte perdu.\nPourquoi D : Amazon Cognito user pool = la ‚Äúvie scolaire‚Äù de l‚Äôappli. Il g√®re l‚Äôinscription et la connexion, les r√®gles de mot de passe, le MFA, et la r√©cup√©ration de compte. ‚ÄúApp client‚Äù = la porte officielle par laquelle l‚Äôappli mobile parle √† ce syst√®me.\nPourquoi pas C : identity pool, c‚Äôest plut√¥t le ‚Äúvigile‚Äù qui donne des badges d‚Äôacc√®s aux salles (permissions), mais il ne g√®re pas bien l‚Äôinscription/connexion tout seul.\nDonc pour ‚Äúsign up + sign in + s√©curit√©‚Äù, D est le bon choix.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:541:a493bd61eb098ace",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 541,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company wants to send notifications to customers to advertise a sale on the company‚Äôs products. The company needs to use Amazon Simple Notification Service (Amazon SNS) FIFO topics.The company needs to examine the rate at which the topics send notifications and the latency with which the topics send notifications.Which solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "A": "Use AWS X-Ray. Enable active tracing for Amazon SNS.",
        "B": "Use the Amazon CloudWatch NumberOfNotificationsFailed metric.",
        "C": "Use AWS CloudTrail to log all Amazon SNS API calls.",
        "D": "Use Amazon GuardDuty. Enable runtime monitoring."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153933-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Jan. 6, 2025, 10:01 a.m.",
      "textHash": "a493bd61eb098ace",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Amazon SNS est un service qui envoie des notifications (SMS, email, etc.) √† des abonn√©s. Un topic FIFO garantit l‚Äôordre et √©vite les doublons.\nIci, on veut mesurer deux choses : le d√©bit d‚Äôenvoi (rate) et la latence (temps entre la demande et la livraison).\nAWS X-Ray sert √† tracer une requ√™te de bout en bout et √† mesurer les temps pass√©s √† chaque √©tape (latence), avec une vue claire des performances.\nEn activant le ‚Äúactive tracing‚Äù sur SNS, X-Ray collecte automatiquement des traces et permet d‚Äôanalyser la latence et le comportement d‚Äôenvoi sans beaucoup d‚Äôop√©rations manuelles.\nCloudWatch NumberOfNotificationsFailed (B) ne montre que les √©checs, pas la latence ni le d√©bit complet.\nCloudTrail (C) enregistre les appels API (audit/s√©curit√©), mais ne donne pas des m√©triques de performance d‚Äôenvoi ni la latence de livraison.\nGuardDuty (D) d√©tecte des menaces de s√©curit√©, pas des m√©triques de d√©bit/latence.\nDonc X-Ray avec active tracing est la solution la plus efficace op√©rationnellement pour observer rate et latence.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine que tu g√®res la distribution de flyers pour une grosse promo dans ton lyc√©e. Tu veux savoir √† quelle vitesse les flyers sont distribu√©s (d√©bit) et combien de temps chaque flyer met entre ‚Äúje le donne‚Äù et ‚Äúl‚Äô√©l√®ve le lit‚Äù (latence).**\n\nConcept : pour mesurer vitesse + d√©lai, il faut un ‚Äútraceur‚Äù qui suit chaque flyer du d√©but √† la fin, pas juste compter les probl√®mes.\nA (X-Ray + active tracing) = tu mets un QR code sur chaque flyer et tu peux suivre son trajet et le temps √† chaque √©tape. Tu vois le rythme d‚Äôenvoi et les retards.\nB = √ßa compte seulement les flyers rat√©s, pas la vitesse ni le temps de trajet.\nC = c‚Äôest juste un cahier qui note ‚Äúqui a demand√© d‚Äôimprimer des flyers‚Äù, pas comment ils circulent ni combien de temps.\nD = c‚Äôest un agent de s√©curit√© contre les menaces, pas un chrono pour mesurer d√©bit/latence.\nDonc A est le plus efficace pour mesurer d√©bit et latence sans bricolage.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:459:970f8cbbf9e9ed1c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 459,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an Amazon API Gateway REST API that integrates with an AWS Lambda function. The API‚Äôs development stage references a development alias of the Lambda function named dev.A developer needs make a production alias of the Lambda function named prod available through the API.Which solution meets these requirements?",
      "choices": {
        "A": "Create a new method on the API. Name the method production. Configure the method to include a stage variable that points to the prod Lambda function alias.",
        "B": "Create a new method on the API. Name the method production. Configure an integration request on the API‚Äôs development stage that points to the prod Lambda function alias.",
        "C": "Deploy the API to a new stage named production. Configure the stage to include a stage variable that points to the prod Lambda function alias.",
        "D": "Deploy the API to a new stage named production. Configure an integration request on the API‚Äôs production stage that points to the prod Lambda function alias."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153505-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 27, 2024, 5:30 p.m.",
      "textHash": "970f8cbbf9e9ed1c",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:ff59d726",
      "frExplanation": "API Gateway expose une API via des ¬´ stages ¬ª (ex: dev, production) : chaque stage est une version d√©ploy√©e de l‚ÄôAPI avec sa propre configuration.\nLambda peut avoir des ¬´ aliases ¬ª (dev, prod) qui pointent vers des versions diff√©rentes du m√™me code.\nIci, le stage de d√©veloppement appelle l‚Äôalias Lambda ¬´ dev ¬ª. Pour la prod, on veut que l‚ÄôAPI appelle ¬´ prod ¬ª sans casser dev.\nLa bonne approche est de cr√©er un nouveau stage ¬´ production ¬ª et de le d√©ployer : ainsi, dev et production coexistent.\nEnsuite, on utilise une variable de stage (stage variable) pour indiquer quel alias Lambda utiliser (dev ou prod).\nAPI Gateway peut construire l‚ÄôARN de la Lambda en incluant cette variable, ce qui permet de changer l‚Äôalias par stage sans modifier les m√©thodes.\nCr√©er un nouveau ¬´ method ¬ª n‚Äôest pas n√©cessaire : les m√©thodes sont les endpoints (GET/POST), pas des environnements.\nModifier directement l‚Äôint√©gration ‚Äúsur le stage‚Äù n‚Äôest pas l‚Äôid√©e cl√© : on d√©ploie un stage et on le param√®tre via variables.\nDonc : d√©ployer un stage production + variable de stage pointant vers l‚Äôalias prod.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:544:7972484d75135029",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 544,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company stores customer credit reports in an Amazon S3 bucket. An analytics service uses standard Amazon S3 GET requests to access the reports.A developer must implement a solution to redact personally identifiable information (PII) from the reports before the reports reach the analytics service.Which solution will meet this requirement with the MOST operational efficiency?",
      "choices": {
        "A": "Load the S3 objects into Amazon Redshift by using a COPY command. Implement dynamic data masking. Refactor the analytics service to read from Amazon Redshift.",
        "B": "Set up an S3 Object Lambda function. Attach the function to an S3 Object Lambda Access Point. Program the function to call a PII redaction API.",
        "C": "Use AWS Key Management Service (AWS KMS) to implement encryption in the S3 bucket. Re-upload all the existing S3 objects. Give the kms:Decrypt permission to the analytics service.",
        "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Implement message data protection. Refactor the analytics service to publish data access requests to the SNS topic."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156700-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 11:52 a.m.",
      "textHash": "7972484d75135029",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, le service d‚Äôanalytics lit les fichiers directement dans Amazon S3 avec des requ√™tes GET. Il faut donc modifier le contenu ¬´ √† la vol√©e ¬ª avant qu‚Äôil ne soit renvoy√©, sans changer l‚Äôapplication ni copier les donn√©es ailleurs.\nAmazon S3 Object Lambda permet d‚Äôintercepter un GET S3 via un ¬´ Access Point ¬ª sp√©cial et d‚Äôex√©cuter une fonction AWS Lambda (code) qui transforme l‚Äôobjet avant de le retourner au client.\nOn programme la Lambda pour appeler une API de caviardage (redaction) PII et renvoyer la version nettoy√©e du rapport.\nAvantage cl√© : les fichiers originaux restent inchang√©s dans le bucket, et l‚Äôanalytics continue d‚Äôutiliser des GET S3 (juste via l‚ÄôObject Lambda Access Point).\nC‚Äôest tr√®s efficace op√©rationnellement : pas de migration vers une base (Redshift), pas de r√©√©criture majeure du service, pas de duplication/re-upload.\nLes autres choix ne r√©pondent pas au besoin : KMS chiffre/d√©chiffre mais ne supprime pas la PII, SNS n‚Äôest pas un m√©canisme de lecture d‚Äôobjets S3, et Redshift impose une refonte et une ingestion lourde.\nDonc la meilleure solution est S3 Object Lambda + Lambda de redaction (B).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e : les dossiers (rapports) sont dans un casier. Un √©l√®ve ‚Äúanalyseur‚Äù vient demander un dossier au comptoir pour faire des stats. Mais avant de lui donner, la biblioth√©caire doit cacher au feutre noir les infos perso (nom, adresse).**\n\nConcept : tu veux que chaque fois que quelqu‚Äôun demande un fichier, il re√ßoive une version ‚Äúcensur√©e‚Äù, sans changer la fa√ßon dont il le demande.\nB : S3 Object Lambda, c‚Äôest la ‚Äúbiblioth√©caire-filtre‚Äù au comptoir : quand l‚Äôanalyseur fait un GET (demande normale), le fichier passe par une fonction qui masque la PII, puis seulement la version nettoy√©e est remise.\nC (chiffrement) c‚Äôest juste mettre un cadenas : √ßa n‚Äôenl√®ve pas les infos perso, √ßa les cache seulement aux gens sans cl√©.\nA oblige √† d√©m√©nager tous les dossiers dans une autre salle et √† changer l‚Äôanalyseur : trop de boulot.\nD (SNS) c‚Äôest comme demander √† l‚Äôanalyseur d‚Äôenvoyer un message au lieu de venir au comptoir : √ßa change tout le fonctionnement.\nDonc B est le plus efficace : m√™me demande, r√©ponse automatiquement nettoy√©e.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:447:67b164f1ec75beb9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 447,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on a project that requires regular updates to a web application‚Äôs backend code. The code is stored in AWS CodeCommit. Company policy states that all code must have complete unit testing and that the test results must be available for access.The developer needs to implement a solution that will take each change to the code repository, build the code, and run unit tests. The solution also must provide a detailed report of the test results.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure AWS CodeDeploy to deploy code from CodeCommit and to run unit tests. Send the test results to Amazon CloudWatch metrics to view reports.",
        "B": "Configure Amazon CodeWhisperer to create the code and to run unit tests. Save the test results in an Amazon S3 bucket to generate reports.",
        "C": "Configure AWS CodeBuild to build the code and to run unit tests. Use test reporting in CodeBuild to generate and view reports.",
        "D": "Create AWS Lambda functions that run when changes are made in CodeCommit. Program the Lambda functions to build the code, run unit tests, and save the test results to a Lambda layer."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156844-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 20, 2025, 6:43 a.m.",
      "textHash": "67b164f1ec75beb9",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "On veut qu‚Äô√† chaque changement dans le d√©p√¥t CodeCommit (service Git d‚ÄôAWS), le code soit compil√© et que des tests unitaires soient ex√©cut√©s automatiquement, avec un rapport consultable.\nAWS CodeBuild est un service de ‚Äúbuild‚Äù g√©r√© : il r√©cup√®re le code, ex√©cute des commandes (compiler, lancer les tests) selon un fichier de configuration (buildspec.yml).\nCodeBuild sait aussi produire des ‚Äútest reports‚Äù : il collecte les r√©sultats de tests (formats courants comme JUnit) et affiche un rapport d√©taill√© dans la console AWS.\nC‚Äôest exactement ce que demande la politique : tests complets + r√©sultats accessibles.\nA est faux car CodeDeploy sert surtout √† d√©ployer des applications, pas √† faire du build/test et des rapports d√©taill√©s (CloudWatch metrics n‚Äôest pas un outil de rapport de tests).\nB est hors sujet : CodeWhisperer aide √† √©crire du code, ce n‚Äôest pas un moteur de build/test.\nD est inadapt√© : Lambda n‚Äôest pas fait pour compiler/tester des projets complets (limites de temps/ressources) et stocker des r√©sultats dans une ‚ÄúLambda layer‚Äù n‚Äôest pas pr√©vu pour des rapports.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un devoir de groupe au lyc√©e : le texte est dans un classeur partag√©, et √† chaque fois que quelqu‚Äôun modifie une page, le prof veut que √ßa soit relu, corrig√©, et qu‚Äôon garde une fiche de notes d√©taill√©e.**\n\nLe classeur partag√© = le d√©p√¥t de code (CodeCommit). √Ä chaque modification, il faut un ‚Äúassistant prof‚Äù qui prend la nouvelle version, la ‚Äúcompile‚Äù (comme v√©rifier que tout est bien √©crit), puis fait passer des mini-interros automatiques = les tests unitaires. AWS CodeBuild, c‚Äôest justement cet assistant : il construit le projet et lance les tests √† chaque changement. Et surtout, il peut produire un bulletin clair des r√©sultats (test reporting) que tout le monde peut consulter, comme une fiche de correction d√©taill√©e. A parle de d√©ployer (mettre en ligne) plut√¥t que de tester et faire un vrai bulletin. B est un outil d‚Äôaide √† √©crire du code, pas une cha√Æne de tests compl√®te. D ferait tout ‚Äú√† la main‚Äù et ne fournit pas un vrai rapport simple √† consulter. Donc C correspond exactement au besoin.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:93:66eebe1c2338cfd6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 93,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application that uses Amazon DynamoDB. The developer wants to retrieve multiple specific items from the database with a single API call.Which DynamoDB API call will meet these requirements with the MINIMUM impact on the database?",
      "choices": {
        "A": "BatchGetItem",
        "B": "GetItem",
        "C": "Scan",
        "D": "Query"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106941-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 21, 2023, 6:15 p.m.",
      "textHash": "66eebe1c2338cfd6",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:dynamodb:5f31686e",
      "frExplanation": "DynamoDB est une base de donn√©es NoSQL g√©r√©e par AWS. Chaque lecture consomme de la capacit√© (ou du temps si on est en mode √† la demande), donc il faut √©viter les op√©rations ‚Äúlarges‚Äù.\nIci, on veut r√©cup√©rer plusieurs √©l√©ments pr√©cis (on conna√Æt leurs cl√©s) en un seul appel API.\nBatchGetItem est fait pour √ßa : on fournit une liste de cl√©s (partition key + sort key si besoin) et DynamoDB renvoie ces items en une seule requ√™te r√©seau.\nGetItem ne lit qu‚Äôun seul item par appel, donc il faudrait r√©p√©ter l‚Äôappel plusieurs fois (plus de latence et plus d‚Äôappels).\nScan parcourt toute la table (ou un gros segment) pour trouver des donn√©es : c‚Äôest le plus co√ªteux et impacte le plus la base.\nQuery r√©cup√®re plusieurs items mais seulement dans une m√™me partition (m√™me partition key) et selon une condition ; ce n‚Äôest pas l‚Äôoutil id√©al si on a des cl√©s vari√©es.\nDonc BatchGetItem minimise l‚Äôimpact en √©vitant un scan et en regroupant plusieurs lectures cibl√©es en un seul appel.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e : tu as une liste de plusieurs livres pr√©cis (titres exacts) √† r√©cup√©rer, et tu veux les obtenir en un seul passage au comptoir.**\n\nConcept : DynamoDB, c‚Äôest comme une biblioth√®que de fiches. Chaque objet a un ‚Äúnom exact‚Äù (sa cl√©). Si tu connais les noms, tu peux demander exactement ce que tu veux.\nBatchGetItem (A) = tu donnes au biblioth√©caire une liste de plusieurs titres pr√©cis, et il te les sort d‚Äôun coup. Un seul passage, peu de d√©rangement.\nGetItem (B) = un seul livre par passage : pour 10 livres, 10 allers-retours.\nScan (C) = le biblioth√©caire parcourt TOUS les rayons pour voir ce qu‚Äôil y a : √ßa fatigue tout le monde, gros impact.\nQuery (D) = tu demandes ‚Äútous les livres de tel auteur/section‚Äù : utile, mais pas juste une liste de titres pr√©cis.\nDonc A est le meilleur : plusieurs objets pr√©cis, en une seule demande, avec le minimum de travail pour la biblioth√®que.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:89:40132cebd6aea2c6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 89,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is using an Amazon API Gateway REST API endpoint as a webhook to publish events from an on-premises source control management (SCM) system to Amazon EventBridge. The company has configured an EventBridge rule to listen for the events and to control application deployment in a central AWS account. The company needs to receive the same events across multiple receiver AWS accounts.How can a developer meet these requirements without changing the configuration of the SCM system?",
      "choices": {
        "A": "Deploy the API Gateway REST API to all the required AWS accounts. Use the same custom domain name for all the gateway endpoints so that a single SCM webhook can be used for all events from all accounts.",
        "B": "Deploy the API Gateway REST API to all the receiver AWS accounts. Create as many SCM webhooks as the number of AWS accounts.",
        "C": "Grant permission to the central AWS account for EventBridge to access the receiver AWS accounts. Add an EventBridge event bus on the receiver AWS accounts as the targets to the existing EventBridge rule.",
        "D": "Convert the API Gateway type from REST API to HTTP API."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/111295-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 6, 2023, 4:52 p.m.",
      "textHash": "40132cebd6aea2c6",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Le SCM sur site envoie ses √©v√©nements vers un seul webhook (API Gateway) dans un compte AWS central. On ne veut pas modifier le SCM, donc on doit garder un seul point d‚Äôentr√©e.\nAmazon EventBridge est un service qui re√ßoit des √©v√©nements et peut les router vers des ‚Äúcibles‚Äù selon des r√®gles.\nPour envoyer les m√™mes √©v√©nements √† plusieurs comptes AWS, on utilise le routage multi-comptes d‚ÄôEventBridge : chaque compte ‚Äúreceiver‚Äù poss√®de un event bus (bus d‚Äô√©v√©nements) qui peut recevoir des √©v√©nements d‚Äôun autre compte.\nLa solution est donc de donner les autorisations n√©cessaires (policies) pour que le compte central puisse publier vers les event bus des comptes receivers.\nEnsuite, on ajoute ces event bus des comptes receivers comme cibles (targets) de la r√®gle EventBridge existante dans le compte central.\nAinsi, un seul webhook SCM ‚Üí compte central, puis EventBridge r√©plique vers plusieurs comptes, sans changer la configuration du SCM.\nLes options A et B impliquent de multiplier les API/webhooks (donc changer le SCM ou g√©rer plusieurs endpoints). L‚Äôoption D ne r√©sout pas le besoin multi-comptes.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine le SCM comme le prof qui envoie une annonce (un message) √† la vie scolaire (compte AWS central). Ensuite, la vie scolaire doit pr√©venir plusieurs classes (plusieurs comptes AWS) avec la m√™me annonce, sans demander au prof de changer sa fa√ßon d‚Äôenvoyer.**\n\nConcept : on veut un seul point qui re√ßoit le message, puis qui le redistribue √† plusieurs endroits.\nIci, le webhook du SCM envoie d√©j√† vers l‚ÄôAPI Gateway, puis vers EventBridge dans le compte central : on ne touche pas au prof (SCM).\nR√©ponse C : on autorise le compte central √† ‚Äúposter‚Äù des messages vers les autres comptes.\nChaque compte receveur a sa propre ‚Äúbo√Æte aux lettres‚Äù (un event bus EventBridge).\nLa r√®gle EventBridge du compte central ajoute ces bo√Ætes aux lettres comme destinations (targets).\nR√©sultat : un seul envoi du SCM, et le m√™me √©v√©nement est copi√© vers tous les comptes.\nA et B compliquent : soit plusieurs portes d‚Äôentr√©e, soit plusieurs webhooks √† cr√©er (interdit).\nD change juste le type de porte (REST vs HTTP) mais ne r√©sout pas la redistribution multi-comptes.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:494:0661e0ccab94f249",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 494,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to build a workflow to handle messages that are sent to an Amazon Simple Queue Service (Amazon SQS) queue. When a message reaches the queue, the workflow must implement a delay before invoking an AWS Lambda function to process the message.Which solution will meet this requirement in the MOST operationally efficient way?",
      "choices": {
        "A": "Create an AWS Step Functions state machine to process the SQS queue. Use a Wait state to delay the Lambda function‚Äôs processing for the required number of seconds after message delivery to the SQS queue. Use Amazon EventBridge to invoke the state machine every 5 minutes.",
        "B": "Configure the Lambda function to poll the SQS queue. Update the Lambda code to republish each message with a custom attribute that contains a future time when the message should be fully processed. Update the Lambda code to fully process messages when the custom attribute‚Äôs future time has passed.",
        "C": "Set the DelaySeconds value of the SQS queue to be the number of seconds required to delay delivery of the messages. Add an event source mapping for the Lambda function. Specify the SQS queue as a source.",
        "D": "Set the Visibility Timeout value of the SQS queue to be the number of seconds required to delay delivery of the messages. Add an event source mapping for the Lambda function. Specify the SQS queue as a source."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156663-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:11 a.m.",
      "textHash": "0661e0ccab94f249",
      "rawFormat": "discussion-md",
      "conceptKey": "sqs_visibility_timeout",
      "frExplanation": "Amazon SQS est une file d‚Äôattente : des producteurs y d√©posent des messages, et un consommateur les traite. AWS Lambda est une fonction qui s‚Äôex√©cute automatiquement quand elle re√ßoit des √©v√©nements.\nLe besoin est : attendre un certain temps APR√àS l‚Äôarriv√©e du message dans la file, puis seulement ensuite d√©clencher le traitement.\nSQS fournit exactement cela avec DelaySeconds : le message devient ‚Äúinvisible‚Äù pendant X secondes apr√®s son envoi, donc aucun consommateur (dont Lambda) ne le re√ßoit avant la fin du d√©lai.\nAvec un ‚Äúevent source mapping‚Äù, Lambda lit automatiquement la file SQS d√®s que les messages deviennent disponibles : pas de serveur √† g√©rer, pas de code sp√©cial.\nC est donc le plus simple et le plus efficace op√©rationnellement (configuration native).\nD est faux car Visibility Timeout ne retarde pas la livraison initiale : il cache un message seulement APR√àS qu‚Äôun consommateur l‚Äôa d√©j√† re√ßu, pour √©viter un double traitement.\nA et B ajoutent des services/du code et de la complexit√© inutile pour un simple d√©lai.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une bo√Æte aux lettres au coll√®ge o√π les √©l√®ves d√©posent des mots pour le d√©l√©gu√©. Tu veux que chaque mot ‚Äúattende‚Äù un peu dans la bo√Æte avant d‚Äô√™tre donn√© au d√©l√©gu√©, sans que quelqu‚Äôun surveille en permanence.**\n\nConcept : SQS = la bo√Æte aux lettres, Lambda = le d√©l√©gu√© qui lit et traite les mots. On veut un ‚Äútemps d‚Äôattente‚Äù automatique avant de remettre le mot.\nPourquoi C : DelaySeconds, c‚Äôest comme coller une r√®gle sur la bo√Æte : ‚Äútout mot d√©pos√© doit rester X secondes avant d‚Äô√™tre distribu√©e‚Äù. Ensuite, l‚Äô‚Äúevent source mapping‚Äù c‚Äôest le surveillant automatique qui donne les mots au d√©l√©gu√© d√®s qu‚Äôils sont pr√™ts, sans code sp√©cial.\nPourquoi pas D : Visibility Timeout, c‚Äôest plut√¥t ‚Äúle d√©l√©gu√© a pris le mot, mais on le cache aux autres pendant X secondes‚Äù, ce n‚Äôest pas un vrai d√©lai avant la premi√®re lecture.\nDonc la solution la plus simple et efficace : r√©gler le d√©lai directement sur la bo√Æte (SQS) + laisser Lambda r√©cup√©rer automatiquement (C).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:492:96271df3c3790de3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 492,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA development team has an Amazon API Gateway REST API that is backed by an AWS Lambda function.Users have reported performance issues for the Lambda function. The development team identified the source of the issues as a cold start of the Lambda function. The development team needs to reduce the time needed for the Lambda function to initialize.Which solution will meet this requirement?",
      "choices": {
        "A": "Change the Lambda concurrency to reserved concurrency.",
        "B": "Increase the timeout of the Lambda function.",
        "C": "Increase the memory allocation of the Lambda function.",
        "D": "Configure provisioned concurrency for the Lambda function."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156661-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:09 a.m.",
      "textHash": "96271df3c3790de3",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:aeb97491",
      "frExplanation": "Le probl√®me vient d‚Äôun ¬´ cold start ¬ª : quand AWS Lambda n‚Äôa pas ex√©cut√© la fonction depuis un moment, il doit cr√©er un nouvel environnement (d√©marrer le runtime, charger le code et les d√©pendances), ce qui ajoute de la latence.\nAPI Gateway appelle Lambda √† la demande ; si aucun environnement n‚Äôest d√©j√† pr√™t, l‚Äôutilisateur ressent ce d√©lai d‚Äôinitialisation.\nLa ¬´ provisioned concurrency ¬ª garde un nombre d‚Äôinstances Lambda d√©j√† d√©marr√©es et pr√™tes √† traiter des requ√™tes imm√©diatement.\nAinsi, l‚Äôinitialisation est faite √† l‚Äôavance et le temps de r√©ponse devient plus stable, m√™me lors des premiers appels.\nLa ¬´ reserved concurrency ¬ª (A) limite/r√©serve le nombre d‚Äôex√©cutions simultan√©es, mais ne pr√©chauffe pas la fonction : le cold start peut toujours arriver.\nAugmenter le timeout (B) ne rend pas le d√©marrage plus rapide, cela permet juste d‚Äôattendre plus longtemps avant l‚Äô√©chec.\nAugmenter la m√©moire (C) peut parfois acc√©l√©rer l‚Äôex√©cution (plus de CPU), mais ne garantit pas la suppression des cold starts.\nDonc la meilleure solution pour r√©duire le temps d‚Äôinitialisation est de configurer la provisioned concurrency (D).",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un snack au lyc√©e (API Gateway) o√π les √©l√®ves passent commande, et un cuisinier (Lambda) pr√©pare les sandwichs.**\n\nLe ‚Äúcold start‚Äù, c‚Äôest quand le cuisinier est en salle de pause et doit d‚Äôabord se lever, se laver les mains, allumer la plaque avant de commencer : √ßa ralentit la premi√®re commande.\nPour aller plus vite, tu veux que le cuisinier soit d√©j√† pr√™t derri√®re le comptoir, m√™me s‚Äôil n‚Äôy a personne.\nC‚Äôest exactement ‚Äúprovisioned concurrency‚Äù (D) : AWS garde des ‚Äúcuisiniers‚Äù d√©j√† d√©marr√©s, donc presque pas de temps de r√©veil.\nA (reserved concurrency) c‚Äôest juste r√©server un nombre de places max pour les commandes, √ßa ne pr√©pare pas le cuisinier √† l‚Äôavance.\nB (timeout) c‚Äôest accepter d‚Äôattendre plus longtemps, √ßa ne rend pas le d√©marrage plus rapide.\nC (plus de m√©moire) peut aider un peu, comme donner de meilleurs outils, mais le cuisinier doit quand m√™me se lever au d√©but.\nDonc D est la solution qui r√©duit vraiment le temps d‚Äôinitialisation.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:491:12826414c7bf0c0d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 491,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application that includes an AWS Lambda function that is written in .NET Core. The Lambda function‚Äôs code needs to interact with Amazon DynamoDB tables and Amazon S3 buckets. The developer must minimize the Lambda function‚Äôs deployment time and invocation duration.Which solution will meet these requirements?",
      "choices": {
        "A": "Increase the Lambda function‚Äôs memory.",
        "B": "Include the entire AWS SDK for .NET in the Lambda function‚Äôs deployment package.",
        "C": "Include only the AWS SDK for .NET modules for DynamoDB and Amazon S3 in the Lambda function‚Äôs deployment package.",
        "D": "Configure the Lambda function to download the AWS SDK for .NET from an S3 bucket at runtime."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156660-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:08 a.m.",
      "textHash": "12826414c7bf0c0d",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:14e95368",
      "frExplanation": "AWS Lambda ex√©cute du code sans serveur : plus le ‚Äúpackage‚Äù (zip) d√©ploy√© est gros, plus le d√©ploiement et le d√©marrage peuvent √™tre lents.\nLe SDK AWS pour .NET est une biblioth√®que qui permet √† votre code d‚Äôappeler des services AWS (ex: DynamoDB = base NoSQL, S3 = stockage de fichiers).\nSi vous mettez tout le SDK dans le package, vous ajoutez beaucoup de fichiers inutiles : le zip est plus lourd, donc upload/d√©ploiement plus long et chargement plus lent au d√©marrage.\nT√©l√©charger le SDK au moment de l‚Äôex√©cution (runtime) depuis S3 ajoute du r√©seau et du temps √† chaque d√©marrage, donc augmente la dur√©e d‚Äôinvocation.\nAugmenter la m√©moire peut parfois acc√©l√©rer le CPU, mais ne r√®gle pas le probl√®me principal de taille du package et n‚Äôest pas la solution la plus directe.\nLa meilleure approche est d‚Äôinclure uniquement les modules n√©cessaires (DynamoDB et S3) : package plus petit, d√©ploiement plus rapide, et moins de code √† charger au d√©marrage.\nDonc la r√©ponse C minimise √† la fois le temps de d√©ploiement et la dur√©e d‚Äôinvocation.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu dois aller en cours avec ton sac. Aujourd‚Äôhui, tu n‚Äôas besoin que de ton cahier de maths et de ton livre d‚Äôanglais, pas de tous tes classeurs de l‚Äôann√©e.**\n\nUne fonction Lambda, c‚Äôest comme un √©l√®ve qu‚Äôon appelle vite fait pour faire une t√¢che, puis il repart. Pour aller vite, il doit arriver avec un sac l√©ger (d√©ploiement rapide) et trouver ses affaires tout de suite (ex√©cution rapide). Mettre tout le ‚Äúgros kit‚Äù AWS dans le sac (B), c‚Äôest prendre tous les classeurs: lourd et lent. T√©l√©charger le kit pendant le cours (D), c‚Äôest courir au casier √† chaque fois: √ßa rallonge la t√¢che. Augmenter la ‚Äúforce‚Äù de l‚Äô√©l√®ve (A) peut aider un peu, mais √ßa ne r√®gle pas le sac trop lourd. La meilleure id√©e (C) est de prendre seulement les deux cahiers n√©cessaires: les modules DynamoDB et S3. Donc C minimise le temps d‚Äôenvoi du sac et le temps pour faire la t√¢che.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:487:0f93571cb3430cb4",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 487,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing a web application that must share secure documents with end users. The documents are stored in a private Amazon S3 bucket. The application must allow only authenticated users to download specific documents when requested, and only for a duration of 15 minutes.How can the developer meet these requirements?",
      "choices": {
        "A": "Copy the documents to a separate S3 bucket that has a lifecycle policy for deletion after 15 minutes.",
        "B": "Create a presigned S3 URL using the AWS SDK with an expiration time of 15 minutes.",
        "C": "Use server-side encryption with AWS KMS managed keys (SSE-KMS) and download the documents using HTTPS.",
        "D": "Modify the S3 bucket policy to only allow specific users to download the documents. Revert the change after 15 minutes."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156657-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:03 a.m.",
      "textHash": "0f93571cb3430cb4",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Amazon S3 est un service de stockage de fichiers (objets) dans des ‚Äúbuckets‚Äù. Un bucket priv√© n‚Äôautorise pas le t√©l√©chargement public.\nLe besoin: laisser un utilisateur d√©j√† authentifi√© t√©l√©charger un document pr√©cis, mais seulement pendant 15 minutes.\nLa solution adapt√©e est une URL pr√©-sign√©e (presigned URL): l‚Äôapplication demande √† AWS (via le SDK) de g√©n√©rer un lien temporaire qui inclut une signature d‚Äôautorisation.\nCe lien donne acc√®s uniquement √† l‚Äôobjet vis√© et expire automatiquement apr√®s 15 minutes, sans rendre le bucket public.\nAinsi, l‚Äôapplication contr√¥le qui re√ßoit le lien (seulement les utilisateurs connect√©s) et S3 contr√¥le la dur√©e (expiration).\nA est inutile et risqu√© (copie + suppression), C chiffre mais ne g√®re pas l‚Äôacc√®s temporaire, D est dangereux et complexe (changer une policy puis la remettre).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e : les documents importants sont dans une salle ferm√©e √† cl√©, et le biblioth√©caire peut te donner un ‚Äúticket‚Äù sp√©cial pour emprunter UN document pendant 15 minutes.**\n\nConcept : le bucket S3 priv√©, c‚Äôest la salle ferm√©e. Personne ne peut entrer sans autorisation.\nPour laisser un √©l√®ve (utilisateur connect√©) prendre un document pr√©cis, on ne donne pas la cl√© de la salle : on donne un ticket temporaire.\nLa r√©ponse B fait exactement √ßa : une ‚Äúpresigned URL‚Äù, c‚Äôest un lien-ticket cr√©√© par l‚Äôappli, valable seulement 15 minutes.\nLe lien ne marche que pour le document demand√©, puis il expire tout seul.\nA est inutile : copier ailleurs et supprimer apr√®s, c‚Äôest comme refaire une photocopie √† chaque fois.\nC prot√®ge le document, mais ne g√®re pas ‚Äúqui‚Äù peut le prendre ni ‚Äú15 minutes‚Äù.\nD est risqu√© : changer les r√®gles de la salle puis les remettre, c‚Äôest comme changer le r√®glement du lyc√©e toutes les 15 minutes.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:485:4c25008503ac6d02",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 485,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer must cache dependent artifacts from Maven Central, a public package repository, as part of an application‚Äôs build pipeline. The build pipeline has an AWS CodeArtifact repository where artifacts of the build are published. The developer needs a solution that requires minimum changes to the build pipeline.Which solution meets these requirements?",
      "choices": {
        "A": "Modify the existing CodeAriifact repository to associate an upstream repository with the public package repository.",
        "B": "Create a new CodeAtfact repository that has an external connection to the public package repository.",
        "C": "Create a new CodeAifact domain that contains a new repository that has an external connection to the public package repository.",
        "D": "Modify the CodeAnifact repository resource policy to allow artifacts to be fetched from the public package repository."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156655-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:59 a.m.",
      "textHash": "4c25008503ac6d02",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Objectif : mettre en cache (garder une copie locale) des d√©pendances Maven venant de Maven Central pour acc√©l√©rer les builds, sans changer beaucoup le pipeline.\nAWS CodeArtifact est un service de d√©p√¥t de paquets (comme Maven/NPM) g√©r√© par AWS. Votre pipeline publie d√©j√† ses artefacts dans un d√©p√¥t CodeArtifact existant.\nLa fa√ßon la plus simple est d‚Äôajouter un ¬´ upstream repository ¬ª au d√©p√¥t CodeArtifact existant : quand le build demande un paquet, CodeArtifact le cherche d‚Äôabord localement, sinon il le r√©cup√®re depuis Maven Central et le met en cache.\nAinsi, le pipeline continue d‚Äôutiliser la m√™me URL/les m√™mes identifiants CodeArtifact : changement minimal.\nCr√©er un nouveau d√©p√¥t ou un nouveau domaine (B ou C) obligerait souvent √† modifier la configuration du pipeline (nouvelle URL, nouveaux param√®tres, permissions).\nUne policy (D) ne permet pas de ‚Äútirer‚Äù des paquets depuis Maven Central : elle g√®re surtout l‚Äôacc√®s/autorisation, pas la connexion √† un d√©p√¥t public.\nDonc la bonne r√©ponse est A : associer un upstream au d√©p√¥t CodeArtifact existant.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e (ton d√©p√¥t CodeArtifact) o√π vous rangez les polycopi√©s du cours (les ‚Äúpackages‚Äù). Parfois, il manque un poly, alors la biblioth√©caire va le chercher dans une grande biblioth√®que publique en ville (Maven Central).**\n\nConcept : ‚Äúcacher‚Äù ici, c‚Äôest garder une copie locale des polys souvent demand√©s pour aller plus vite la prochaine fois.\nTon pipeline de build utilise d√©j√† la biblioth√®que du lyc√©e (le d√©p√¥t CodeArtifact) et y publie ses propres polys.\nOn veut donc le moins de changements : garder la m√™me biblioth√®que, mais lui donner un acc√®s officiel √† la biblioth√®que publique.\nR√©ponse A : on ajoute un ‚Äúrayon amont‚Äù (upstream) √† la biblioth√®que existante, qui pointe vers la biblioth√®que publique.\nR√©sultat : si un poly n‚Äôest pas dans le lyc√©e, la biblioth√©caire le r√©cup√®re en ville et le garde en copie pour la prochaine fois.\nB et C : cr√©er une nouvelle biblioth√®que (ou carr√©ment un nouveau b√¢timent) oblige √† changer les habitudes et le trajet du pipeline.\nD : une r√®gle d‚Äôacc√®s (policy) ne cr√©e pas le ‚Äúchemin‚Äù pour aller chercher les polys dehors, √ßa ne fait qu‚Äôautoriser/interdire.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:482:7a89c2ca3d8c7132",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 482,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to write an AWS CloudFormation template on a local machine and deploy a CloudFormation stack to AWS.What must the developer do to complete these tasks?",
      "choices": {
        "A": "Install the AWS CLI. Configure the AWS CLI by using an IAM user name and password.",
        "B": "Install the AWS CLI. Configure the AWS CLI by using an SSH key.",
        "C": "Install the AWS CLI, Configure the AWS CLI by using an IAM user access key and secret key.",
        "D": "Install an AWS software development kit (SDK). Configure the SDK by using an X.509 certificate."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156653-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:53 a.m.",
      "textHash": "7a89c2ca3d8c7132",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "CloudFormation est un service AWS qui d√©ploie automatiquement des ressources (serveurs, bases, etc.) √† partir d‚Äôun fichier ‚Äútemplate‚Äù.\nDepuis une machine locale, il faut un outil pour envoyer ce template √† AWS et cr√©er la ‚Äústack‚Äù : c‚Äôest l‚ÄôAWS CLI (outil en ligne de commande).\nPour que la CLI ait le droit d‚Äôagir sur votre compte, elle doit s‚Äôauthentifier avec des identifiants AWS.\nUn ‚Äúnom d‚Äôutilisateur + mot de passe‚Äù IAM sert surtout √† se connecter √† la console web, pas √† signer des appels API depuis la CLI.\nUne cl√© SSH sert √† se connecter √† des serveurs (ex: EC2), pas √† s‚Äôauthentifier aupr√®s des API AWS.\nLa m√©thode standard pour la CLI est : Access Key ID + Secret Access Key d‚Äôun utilisateur IAM (ou r√¥le) autoris√© √† utiliser CloudFormation.\nUn SDK est utile pour coder des applications, mais ici on veut d√©ployer un template depuis la machine : la CLI suffit.\nDonc la bonne r√©ponse est C : installer AWS CLI et la configurer avec une access key et une secret key IAM.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu veux d√©poser un dossier au secr√©tariat du lyc√©e depuis chez toi, sans te d√©placer.**\n\nLe ‚Äútemplate CloudFormation‚Äù, c‚Äôest comme le dossier avec les r√®gles et la liste du mat√©riel pour organiser un √©v√©nement au lyc√©e.\nLe ‚Äústack‚Äù, c‚Äôest l‚Äô√©v√©nement r√©ellement mis en place au lyc√©e.\nPour envoyer ton dossier depuis ton PC, il te faut un outil de d√©p√¥t √† distance : l‚ÄôAWS CLI (comme une appli officielle pour envoyer au secr√©tariat).\nEt il te faut une ‚Äúcarte d‚Äôacc√®s‚Äù sp√©ciale pour prouver qui tu es : une access key + une secret key (comme identifiant + code secret).\nUn simple mot de passe (A) ne suffit pas ici, et une cl√© SSH (B) sert plut√¥t √† entrer sur des machines, pas √† d√©poser ce dossier.\nUn certificat X.509 (D) est un autre type de badge, pas celui attendu pour ce cas.\nDonc: installer AWS CLI + la configurer avec access key et secret key = C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:481:78c7985dc4e57f4a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 481,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is making changes to a custom application that uses AWS Elastic Beanstalk.Which solutions will update the Elastic Beanstalk environment with the new application version after the developer completes the changes? (Choose two.)",
      "choices": {
        "A": "Package the application code into a zip file. Use the AWS Management Console to upload the .zip file and deploy the packaged application.",
        "B": "Package the application code into a .tar file. Use the AWS Management Console to create a new application version from the .tar file. Update the environment by using the AWS CLI.",
        "C": "Package the application code into a .tar file. Use the AWS Management Console to upload the .tar file and deploy the packaged application.",
        "D": "Package the application code into a .zip file. Use the AWS CL to create a new application version from the .zip file and to update the environment.",
        "E": "Package the application code into a .zip file. Use the AWS Management Console to create a new application version from the .zip file. Rebuild the environment by using the AWS CLI."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156652-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:53 a.m.",
      "textHash": "78c7985dc4e57f4a",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:688ca26c",
      "frExplanation": "Elastic Beanstalk est un service AWS qui d√©ploie et ex√©cute votre application automatiquement (serveurs, mise √† l‚Äô√©chelle, etc.).\nQuand vous modifiez le code, vous devez cr√©er une ¬´ application version ¬ª puis la d√©ployer sur l‚Äô¬´ environment ¬ª.\nLe format attendu classiquement par Elastic Beanstalk pour un d√©ploiement manuel est un fichier .zip contenant le code et les fichiers de config.\nAvec la console AWS, vous pouvez t√©l√©verser le .zip et lancer le d√©ploiement sur l‚Äôenvironnement : c‚Äôest exactement ce que d√©crit la r√©ponse A.\nLes options avec .tar ne correspondent pas au flux standard de d√©ploiement Elastic Beanstalk via console.\n¬´ Rebuild the environment ¬ª (E) recr√©e l‚Äôenvironnement (plus lourd et inutile) au lieu de simplement mettre √† jour la version.\nDonc, la solution correcte ici est de zipper le code puis de l‚Äôuploader et d√©ployer via la console (A).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:478:f54001f19dde60fb",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 478,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs continuous integration/continuous delivery (CI/CD) pipelines for its application on AWS CodePipeline. A developer must write unit tests and run them as part of the pipelines before staging the artifacts for testing.How should the developer incorporate unit tests as part of CI/CD pipelines?",
      "choices": {
        "A": "Create a separate CodePipeline pipeline to run unit tests.",
        "B": "Update the AWS CodeBuild build specification to include a phase for running unit tests.",
        "C": "Install the AWS CodeDeploy agent on an Amazon EC2 instance to run unit tests.",
        "D": "Create a testing branch in a git repository for the pipelines to run unit tests."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156650-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:49 a.m.",
      "textHash": "f54001f19dde60fb",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Dans un pipeline CI/CD, on veut v√©rifier le code automatiquement avant de produire un ‚Äúartefact‚Äù (le paquet √† d√©ployer/tester). AWS CodePipeline orchestre les √©tapes, et AWS CodeBuild ex√©cute les commandes de build et de tests.\nLa bonne pratique est donc d‚Äôajouter l‚Äôex√©cution des tests unitaires dans l‚Äô√©tape CodeBuild, via le fichier buildspec.yml (la ‚Äúrecette‚Äù des commandes).\nOn y ajoute une phase (par ex. pre_build ou build) qui lance les tests (ex. npm test, mvn test, pytest) et fait √©chouer le build si un test √©choue.\nAinsi, si les tests √©chouent, le pipeline s‚Äôarr√™te avant de cr√©er et de ‚Äústager‚Äù l‚Äôartefact, ce qui √©vite de propager du code cass√©.\nA est inutilement complexe (un autre pipeline) et ne garantit pas l‚Äôordre correct.\nC est hors sujet : CodeDeploy sert √† d√©ployer sur des serveurs, pas √† ex√©cuter des tests unitaires.\nD (branche de test) ne remplace pas l‚Äôex√©cution automatique des tests dans le pipeline.\nDonc il faut mettre les tests dans le buildspec de CodeBuild.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cha√Æne de fabrication de pizzas au lyc√©e : on pr√©pare la p√¢te, on met la sauce, on cuit, puis on met la pizza dans une bo√Æte pour l‚Äôenvoyer au client.**\n\nDans une CI/CD, c‚Äôest pareil : on fabrique une nouvelle version de l‚Äôappli √©tape par √©tape.\nLes ‚Äúunit tests‚Äù, c‚Äôest comme go√ªter un mini-bout de pizza avant de la mettre en bo√Æte, pour v√©rifier que le go√ªt est bon.\nCodeBuild, c‚Äôest l‚Äôatelier o√π on ‚Äúcuisine‚Äù et o√π on peut ajouter des √©tapes de recette.\nLa ‚Äúbuild specification‚Äù, c‚Äôest la fiche recette : elle liste les √©tapes √† faire dans l‚Äôatelier.\nDonc la bonne id√©e est B : on ajoute une √©tape ‚Äúfaire les tests‚Äù dans la fiche recette, avant de pr√©parer l‚Äôenvoi vers la zone de test.\nA ferait une deuxi√®me cuisine s√©par√©e (inutile et plus compliqu√©).\nC sert plut√¥t √† livrer/installer chez le client, pas √† go√ªter la pizza.\nD change de cahier de recettes (branche git) mais ne garantit pas que les tests sont faits dans la cha√Æne.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:477:70c71b4d29aeb005",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 477,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application interacts with Amazon Aurora to store and track customer information. The primary database is set up with multiple read replicas for improving the performance of the read queries. However, one of the Aurora replicas is receiving most or all of the traffic, while the other Aurora replica remains idle.How can this issue be resolved?",
      "choices": {
        "A": "Disable application-level DNS caching.",
        "B": "Enable application-level DNS caching.",
        "C": "Enable application pooling",
        "D": "Disable application pooling"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152952-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 13, 2024, 3:26 a.m.",
      "textHash": "70c71b4d29aeb005",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:2fa92cb1",
      "frExplanation": "Aurora est une base de donn√©es g√©r√©e. Les ‚Äúread replicas‚Äù servent √† r√©partir les lectures pour aller plus vite.\nPour envoyer les lectures vers les r√©plicas, l‚Äôapplication utilise un endpoint (nom DNS) de type ‚Äúreader‚Äù qui peut pointer vers diff√©rents r√©plicas.\nLe DNS renvoie une adresse IP, et Aurora peut changer cette r√©ponse pour √©quilibrer la charge.\nSi l‚Äôapplication met en cache le DNS (garde la premi√®re IP trop longtemps), elle continue d‚Äôenvoyer toutes les requ√™tes au m√™me replica.\nR√©sultat : un replica est surcharg√© et les autres restent inutilis√©s.\nLa solution est de d√©sactiver le cache DNS c√¥t√© application pour qu‚Äôelle refasse des r√©solutions DNS r√©guli√®rement et suive l‚Äô√©quilibrage.\nDonc la bonne r√©ponse est A : Disable application-level DNS caching.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e avec 3 caisses ouvertes pour aller plus vite. Pour r√©partir les √©l√®ves, un panneau √† l‚Äôentr√©e indique vers quelle caisse aller. Mais certains √©l√®ves gardent en t√™te ‚Äúje vais toujours √† la caisse 1‚Äù et n‚Äôactualisent jamais, donc la caisse 1 est blind√©e et les autres attendent pour rien.**\n\nConcept : les ‚Äúr√©plicas‚Äù sont comme des caisses en plus pour r√©pondre aux questions de lecture (consulter des infos). Le ‚ÄúDNS‚Äù est comme le panneau qui dit quelle caisse choisir. Si l‚Äôappli ‚Äúmet en cache‚Äù le DNS, c‚Äôest comme si elle m√©morisait une seule caisse et y envoie presque tout le monde. R√©sultat : une r√©plique re√ßoit tout le trafic, les autres restent vides. Solution : d√©sactiver le cache DNS c√¥t√© application (A), pour qu‚Äôelle relise souvent le ‚Äúpanneau‚Äù et r√©partisse mieux les demandes entre les r√©plicas. Activer le cache (B) aggraverait le probl√®me. Le ‚Äúpooling‚Äù (C/D) c‚Äôest plut√¥t g√©rer des files/connexions, pas choisir la bonne caisse.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:475:8fade1b35f4b267f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 475,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer manages encryption keys in AWS Key Management Service (AWS KMS). The developer must ensure that all encryption keys can be deleted immediately when the keys are no longer required. The developer wants a solution that is highly available and does not require manual management for compute infrastructure.Which solution will meet these requirements?",
      "choices": {
        "A": "Use AWS KMS managed keys. When the keys are no longer required, schedule the keys for immediate deletion.",
        "B": "Use customer managed keys with imported key material. When the keys are no longer required, delete the imported key material.",
        "C": "Use customer managed keys. When the keys are no longer required, delete the key material.",
        "D": "Use customer managed keys and an AWS CloudHSM key store. When the keys are no longer required, schedule the keys for immediate deletion."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152950-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 13, 2024, 3:05 a.m.",
      "textHash": "8fade1b35f4b267f",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "AWS KMS est un service g√©r√© qui cr√©e et prot√®ge des cl√©s de chiffrement. En g√©n√©ral, une cl√© KMS ne peut pas √™tre supprim√©e ‚Äútout de suite‚Äù : on doit la programmer pour suppression avec un d√©lai (minimum 7 jours).\nPour supprimer imm√©diatement la capacit√© de d√©chiffrer, il faut pouvoir retirer la ‚Äúmati√®re de cl√©‚Äù (le secret r√©el) instantan√©ment.\nAvec une cl√© g√©r√©e par le client (Customer Managed Key) et du mat√©riel de cl√© import√© (imported key material), KMS stocke la cl√© mais vous fournissez le secret.\nQuand vous supprimez le mat√©riel de cl√© import√©, la cl√© KMS devient inutilisable imm√©diatement : plus personne ne peut chiffrer/d√©chiffrer avec.\nC‚Äôest hautement disponible car KMS est un service manag√© multi-AZ, sans serveurs √† g√©rer.\nA est faux car les cl√©s g√©r√©es par AWS ne permettent pas une suppression imm√©diate (d√©lai obligatoire).\nC est faux car on ne peut pas ‚Äúsupprimer le mat√©riel‚Äù d‚Äôune cl√© KMS standard : on ne fait que programmer la suppression.\nD est faux car CloudHSM implique de g√©rer une infrastructure HSM (cluster), et la suppression KMS reste soumise au d√©lai.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que les cl√©s de chiffrement sont des badges d‚Äôacc√®s au CDI. Le badge ouvre un casier o√π sont rang√©s des documents secrets. Le CDI (AWS) est toujours ouvert et g√©r√© par l‚Äô√©cole, toi tu veux juste pouvoir rendre le badge inutilisable tout de suite.**\n\nConcept : chiffrer = fermer un casier avec un badge. Si tu d√©truis le badge, plus personne ne peut ouvrir le casier, m√™me si le casier existe encore. Avec des cl√©s ‚Äúg√©r√©es par AWS‚Äù (A), c‚Äôest comme un badge fourni par l‚Äô√©cole : tu ne peux pas le faire dispara√Ætre instantan√©ment, il y a un d√©lai. Avec une cl√© ‚Äúclient‚Äù normale (C), tu ne peux pas effacer le c≈ìur de la cl√© d‚Äôun coup, c‚Äôest aussi contr√¥l√©. La bonne id√©e (B) : tu apportes toi-m√™me le ‚Äúc≈ìur du badge‚Äù (imported key material). Quand tu n‚Äôen veux plus, tu effaces ce c≈ìur imm√©diatement : le badge devient mort tout de suite. Et comme tout reste dans le CDI g√©r√© par l‚Äô√©cole, c‚Äôest disponible tout le temps et tu n‚Äôas pas √† g√©rer des machines.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:474:22686435ec279a67",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 474,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA team has an Amazon API Gateway REST API that consists of a single resource and a GET method that is backed by an AWS Lambda integration.A developer makes a change to the Lambda function and deploys the function as a new version. The developer needs to set up a process to test the new version of the function before using the new version in production. The tests must not affect the production REST API.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Create a new resource in the REST API. Add a GET method to the new resource, and add a Lambda integration to the updated version of the Lambda function. Deploy the new version.",
        "B": "Create a new stage for the REST API. Create a stage variable. Assign the stage variable to the Lambda function. Set the API Gateway integrated Lambda function name to the stage variable. Deploy the new version.",
        "C": "Create a new REST API. Add a resource that has a single GET method that is integrated with the updated version of the Lambda function.",
        "D": "Update the Lambda integration of the existing GET method to point to the updated version of the Lambda function. Deploy the new version."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156649-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:46 a.m.",
      "textHash": "22686435ec279a67",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:a6ffac3c",
      "frExplanation": "API Gateway sert √† exposer une API HTTP, et Lambda ex√©cute votre code quand l‚ÄôAPI est appel√©e. Ici, on veut tester une nouvelle version de la fonction Lambda sans toucher l‚ÄôAPI en production. La solution la plus simple est de cr√©er un nouveau ¬´ stage ¬ª (ex: test) dans API Gateway : un stage est comme un environnement s√©par√© avec sa propre URL. Avec une ¬´ variable de stage ¬ª, on peut stocker le nom/ARN de la version (ou alias) Lambda √† appeler. On configure l‚Äôint√©gration Lambda pour utiliser cette variable, donc le stage prod continue d‚Äôappeler l‚Äôancienne version, et le stage test appelle la nouvelle. On d√©ploie l‚ÄôAPI sur le stage test pour valider, puis on bascule la variable en prod quand c‚Äôest pr√™t. Cela √©vite de dupliquer des ressources ou de cr√©er une nouvelle API, donc moins d‚Äôop√©rations √† g√©rer.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:473:a11726672b791e3c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 473,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer created reusable code that several AWS Lambda functions need to use. The developer bundled the code into a zip archive. The developer needs to deploy the code to AWS and update the Lambda functions to use the code.Which solution will meet this requirement in the MOST operationally efficient way?",
      "choices": {
        "A": "Upload the zip archive to Amazon S3. Configure an import path on the Lambda functions to point to the zip archive.",
        "B": "Create a new Lambda function that contains and runs the shared code. Update the existing Lambda functions to invoke the new Lambda function synchronously.",
        "C": "Create a Lambda layer that contains the zip archive. Attach the Lambda layer to the Lambda functions.",
        "D": "Create a Lambda container image that includes the shared code. Use the container image as a Lambda base image for all the functions."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152947-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 13, 2024, 2:53 a.m.",
      "textHash": "a11726672b791e3c",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:57d95c12",
      "frExplanation": "Ici, plusieurs fonctions AWS Lambda (du code qui s‚Äôex√©cute sans serveur) doivent partager le m√™me code r√©utilisable.\nLa fa√ßon pr√©vue par AWS pour partager des biblioth√®ques entre plusieurs Lambdas est une ¬´ Lambda Layer ¬ª (couche).\nUne layer est un paquet ZIP contenant du code/d√©pendances que l‚Äôon attache √† une ou plusieurs fonctions.\nEnsuite, chaque fonction peut importer ce code comme une librairie locale, sans dupliquer le ZIP dans chaque d√©ploiement.\nC‚Äôest op√©rationnellement efficace : on met √† jour la layer une fois (nouvelle version) et on rattache/actualise les fonctions.\nA est faux : Lambda ne peut pas ‚Äúimporter‚Äù directement du code depuis S3 au moment de l‚Äôex√©cution via un simple chemin.\nB est moins efficace : appeler une autre Lambda ajoute latence, co√ªts, complexit√© et points de panne.\nD est possible mais plus lourd √† g√©rer (images, registry, build) juste pour partager une librairie.\nDonc la meilleure solution est de cr√©er une Lambda layer et l‚Äôattacher aux fonctions (C).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une √©cole o√π plusieurs profs (les fonctions Lambda) utilisent le m√™me polycopi√© d‚Äôexercices (le code r√©utilisable).**\n\nConcept : au lieu de photocopier le poly dans chaque classe, l‚Äô√©cole le met dans une ‚Äúr√©serve commune‚Äù que chaque prof peut prendre quand il en a besoin.\nDans AWS, cette r√©serve commune s‚Äôappelle une ‚ÄúLambda layer‚Äù : un paquet de code partag√© que tu attaches √† plusieurs fonctions.\nPourquoi C : tu mets ton zip dans la layer, puis tu ‚Äúbranches‚Äù cette layer sur toutes les fonctions Lambda. Elles retrouvent le code au m√™me endroit.\nSi tu modifies le poly, tu publies une nouvelle version de la layer et tu mets √† jour les fonctions pour utiliser cette version.\nA : S3, c‚Äôest comme poser le poly au CDI, mais les profs ne peuvent pas juste ‚Äúl‚Äôutiliser directement‚Äù en cours sans le r√©cup√©rer/installer.\nB : faire un prof sp√©cial √† appeler √† chaque exercice, √ßa ajoute un d√©tour et ralentit (et complique) chaque cours.\nD : l‚Äôimage conteneur, c‚Äôest comme refaire un sac √† dos complet pour chaque prof : trop lourd pour juste partager un poly.\nDonc la solution la plus simple √† g√©rer au quotidien = C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:472:d8ef9857b6ca882d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 472,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is testing an AWS Lambda function that has an event source of an Amazon Simple Queue Service (Amazon SQS) queue. The developer notices that some of the messages the Lambda function processes re-appear in the queue while the messages are being processed.The developer must correct this behavior.Which solution will meet this requirement?",
      "choices": {
        "A": "Increase the timeout of the Lambda function.",
        "B": "Increase the visibility timeout of the SQS queue.",
        "C": "Increase the memory allocation of the Lambda function.",
        "D": "Increase the batch size in the event source mapping."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156648-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:45 a.m.",
      "textHash": "d8ef9857b6ca882d",
      "rawFormat": "discussion-md",
      "conceptKey": "sqs_visibility_timeout",
      "frExplanation": "SQS est une file de messages : un producteur envoie des messages, et un consommateur (ici AWS Lambda) les lit pour les traiter.\nQuand Lambda lit un message SQS, SQS le ‚Äúcache‚Äù temporairement gr√¢ce au visibility timeout : pendant ce d√©lai, le message n‚Äôest plus visible pour les autres lecteurs.\nSi le traitement prend plus de temps que ce d√©lai, le message redevient visible avant la fin du traitement.\nR√©sultat : le m√™me message peut √™tre relu et retrait√© (il ‚Äúr√©appara√Æt‚Äù dans la file), m√™me si Lambda est encore en train de travailler.\nPour corriger, il faut augmenter le visibility timeout de la file SQS afin qu‚Äôil soit sup√©rieur au temps maximal de traitement de Lambda.\nAugmenter le timeout Lambda (A) peut aider √† √©viter un arr√™t, mais ne change pas la dur√©e pendant laquelle SQS cache le message.\nAugmenter la m√©moire (C) peut acc√©l√©rer, mais ne garantit pas que le message ne r√©apparaisse pas.\nChanger la taille de lot (D) modifie combien de messages sont lus √† la fois, pas le m√©canisme de r√©apparition.\nDonc la bonne r√©ponse est B : augmenter le visibility timeout de la file SQS.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine : il y a une pile de tickets de commande (la file SQS) et un √©l√®ve cuisinier (Lambda) qui prend un ticket pour pr√©parer un plat.**\n\nConcept : quand le cuisinier prend un ticket, on le cache un moment pour √©viter qu‚Äôun autre le reprenne pendant la pr√©paration. Ce ‚Äútemps cach√©‚Äù s‚Äôappelle le visibility timeout. Probl√®me : si le plat prend plus de temps que ce temps cach√©, le ticket redevient visible dans la pile, donc il ‚Äúr√©appara√Æt‚Äù et peut √™tre repris une 2e fois alors que le plat est d√©j√† en cours. Solution : augmenter le visibility timeout de la file SQS (B) pour que le ticket reste cach√© assez longtemps jusqu‚Äô√† la fin de la pr√©paration. A change seulement le temps max du cuisinier, pas le fait que le ticket r√©apparaisse. C rend le cuisinier potentiellement plus rapide, mais ne garantit rien. D change combien de tickets il prend d‚Äôun coup, pas le temps o√π ils sont cach√©s.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:470:2772492f6d4a6cda",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 470,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is launching a feature that uses an HTTP API built with Amazon API Gateway and AWS Lambda. An API Gateway endpoint performs several independent tasks that run in a Lambda function. The independent tasks can take up to 10 minutes in total to finish running.Users report that the endpoint sometimes returns an HTTP 604 status code. The Lambda function invocations are successful.Which solution will stop the endpoint from returning the HTTP 504 status cade?",
      "choices": {
        "A": "Increase the Lambda function‚Äôs timeout value.",
        "B": "Increase the reserved concurrency of the Lambda function.",
        "C": "Increase the memory that is available to the Lambda function.",
        "D": "Refactor the Lambda function to start an AWS Step Functions state machine."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156647-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:44 a.m.",
      "textHash": "2772492f6d4a6cda",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:446f56f2",
      "frExplanation": "API Gateway est la ‚Äúporte d‚Äôentr√©e‚Äù HTTP de votre API. Lambda ex√©cute le code quand l‚Äôendpoint est appel√©.\nLe code fait plusieurs t√¢ches et peut durer jusqu‚Äô√† 10 minutes. Or, API Gateway a une limite de temps d‚Äôattente (timeout) pour une r√©ponse synchrone : il ne peut pas attendre aussi longtemps.\nQuand API Gateway n‚Äôa pas re√ßu de r√©ponse √† temps, il renvoie une erreur de type 504 (Gateway Timeout), m√™me si la Lambda continue et finit correctement c√¥t√© AWS.\nAugmenter le timeout Lambda (A) ne r√®gle pas le probl√®me, car la limite vient d‚ÄôAPI Gateway, pas de Lambda.\nAugmenter la concurrence (B) aide si trop d‚Äôappels sont bloqu√©s, mais ne change pas la dur√©e d‚Äôun appel long.\nAugmenter la m√©moire (C) peut acc√©l√©rer un peu, mais ne garantit pas de passer sous la limite si le traitement reste long.\nLa bonne approche est de rendre le traitement asynchrone : d√©marrer un workflow avec AWS Step Functions (D), qui orchestre des √©tapes longues, puis retourner rapidement une r√©ponse (ex: ‚Äútraitement en cours‚Äù) et r√©cup√©rer le r√©sultat plus tard.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:468:edaa32f769129c1a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 468,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs an application on Amazon EC2 instances in an Auto Scaling group. The application experiences variable loads throughout each day.The company needs to collect detailed metrics from the EC2 instances to right-size the instances. The company also wants to monitor custom application metrics to ensure the application is performing efficiently.Which solution will meet these requirements?",
      "choices": {
        "A": "Install the AWS X-Ray agent on the instances. Configure the agent to collect the EC2 instance metrics and the custom application metrics.",
        "B": "Install the Amazon CloudWatch agent on the instances. Configure the agent to collect the EC2 instance metrics and the custom application metrics.",
        "C": "Install the AWS SDK in the application‚Äôs cade. Update the application to use the AWS SDK to collect and publish the EC2 instance metrics and the custom application metrics.",
        "D": "Configure AWS CloudTrail to capture and analyze the EC2 instance metrics and the custom application metrics."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156646-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:43 a.m.",
      "textHash": "edaa32f769129c1a",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Pour dimensionner correctement (right-size) des instances EC2, il faut des m√©triques d√©taill√©es sur l‚ÄôOS (CPU, m√©moire, disque, r√©seau) et aussi des m√©triques propres √† l‚Äôapplication (ex: temps de r√©ponse, nombre de requ√™tes, erreurs).\nAmazon CloudWatch est le service AWS qui collecte et affiche des m√©triques et des alarmes.\nPar d√©faut, EC2 envoie surtout des m√©triques ‚Äúde base‚Äù (CPU, r√©seau), mais pas la m√©moire ou l‚Äôespace disque.\nLe CloudWatch Agent, install√© sur chaque instance, peut lire ces infos dans le syst√®me (m√©moire, disque, processus) et les envoyer √† CloudWatch.\nLe m√™me agent peut aussi envoyer des m√©triques personnalis√©es de l‚Äôapplication (via StatsD/collectd ou des logs/compteurs).\nAWS X-Ray sert plut√¥t au tra√ßage des requ√™tes (diagnostic de latence entre services), pas √† collecter des m√©triques syst√®me compl√®tes.\nAWS SDK dans le code peut publier des m√©triques custom, mais ne collecte pas automatiquement les m√©triques OS d√©taill√©es comme un agent.\nCloudTrail enregistre les appels API AWS (audit), pas des m√©triques de performance.\nDonc la meilleure solution est d‚Äôinstaller et configurer le CloudWatch Agent sur les instances (r√©ponse B).",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une √©quipe de basket au lyc√©e : certains jours il y a peu d‚Äôentra√Ænement, d‚Äôautres jours c‚Äôest tr√®s intense. Le coach veut savoir si les joueurs sont trop fatigu√©s (d√©tails du corps) et aussi si les exercices marchent bien (stats du jeu).**\n\nLes serveurs EC2, c‚Äôest comme les joueurs. L‚ÄôAuto Scaling, c‚Äôest quand le coach ajoute/retire des joueurs selon l‚Äôaffluence. Pour ‚Äúright-size‚Äù, il faut des mesures d√©taill√©es du joueur (m√©moire, disque, etc.), pas juste le score final. Et il faut aussi des stats perso de l‚Äôappli (ex: temps de r√©ponse), comme des stats d‚Äôexercices. L‚Äôagent CloudWatch, c‚Äôest le bracelet connect√© que tu mets sur chaque joueur : il remonte √† la fois les mesures d√©taill√©es du corps ET les stats personnalis√©es. X-Ray, c‚Äôest plut√¥t une cam√©ra pour suivre le trajet d‚Äôune action dans le jeu, pas un bracelet de mesures compl√®tes. Le SDK, c‚Äôest demander aux joueurs d‚Äô√©crire eux-m√™mes leurs stats √† la main : possible mais pas fait pour les m√©triques syst√®me. CloudTrail, c‚Äôest le registre des ‚Äúqui a fait quoi‚Äù (actions), pas des performances. Donc B.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:467:3f745d803bc4db7a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 467,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer compiles an AWS Lambda function and packages the result as a .zip file. The developer uses the Functions page on the Lambda console to attempt to upload the local packaged .zip file. When pushing the package ta Lambda, the console returns the following error:Which solutions can the developer use to publish the code? (Choose two.)",
      "choices": {
        "A": "Upload the package to Amazon 3. Use the Functions page on the Lambda console to upload the package from the S3 location.",
        "B": "Create an AWS Support ticket to increase the maximum package size.",
        "C": "Use the update-function-code AWS CLI command. Pass the --publish parameter.",
        "D": "Repackage the Lambda function as a Docker container image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a new Lambda function by using the Lambda console. Reference the image that is deployed to Amazon ECR.",
        "E": "Sign the .zip file digitally. Create a new Lambda function by using the Lambda console. Update the configuration of the new Lambda function to include the Amazon Resource Name (ARN) of the code signing configuration."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156645-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:42 a.m.",
      "textHash": "3f745d803bc4db7a",
      "rawFormat": "discussion-md",
      "conceptKey": "ecr_image_scanning",
      "frExplanation": "Ici, l‚Äôerreur vient presque toujours d‚Äôune limite de taille lors de l‚Äôupload direct d‚Äôun fichier .zip depuis votre ordinateur dans la console Lambda.\nAWS Lambda (service qui ex√©cute du code sans serveur) accepte du code en .zip, mais l‚Äôinterface web a des limites plus strictes pour l‚Äôenvoi local.\nSolution pratique : mettre le .zip dans Amazon S3 (stockage de fichiers dans le cloud), puis dire √† Lambda de r√©cup√©rer le code depuis ce bucket S3.\nC‚Äôest exactement l‚Äôoption A : upload vers S3, puis import depuis S3 dans la page Functions.\nDemander une augmentation de taille via Support (B) n‚Äôest pas la bonne approche : ces limites ne sont pas ‚Äúaugmentables‚Äù comme un quota classique.\nLa commande CLI (C) peut aussi √©chouer si vous essayez d‚Äôenvoyer un gros zip directement ; et la question vise la m√©thode standard via S3.\nLe conteneur Docker (D) est une autre fa√ßon de d√©ployer, mais ce n‚Äôest pas n√©cessaire pour ‚Äúpublier le code‚Äù d‚Äôun zip trop gros dans ce sc√©nario.\nLa signature de code (E) sert √† la s√©curit√© (v√©rifier l‚Äôorigine du code), pas √† r√©soudre un probl√®me de taille d‚Äôupload.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois rendre un √©norme devoir au prof via l‚ÄôENT, mais le site refuse car le fichier est trop gros. Alors tu le mets d‚Äôabord dans le cloud du lyc√©e (un casier en ligne), puis tu donnes au prof le lien vers ce casier.**\n\nConcept : AWS Lambda, c‚Äôest comme un prof qui ex√©cute ton devoir automatiquement. Ton .zip, c‚Äôest le devoir. La page web (console) a une limite de taille, comme l‚ÄôENT.\nPourquoi A : Amazon S3, c‚Äôest un ‚Äúcasier de stockage‚Äù en ligne. Tu y d√©poses ton .zip, m√™me s‚Äôil est gros.\nEnsuite, sur la page Lambda, tu dis : ‚Äúva chercher mon devoir dans ce casier S3‚Äù. Lambda r√©cup√®re le fichier depuis S3 au lieu de passer par l‚Äôupload direct.\nDonc A marche car tu contournes la limite de la page web en passant par un stockage pr√©vu pour les gros fichiers.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:466:0a3f787ebf44ce01",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 466,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nIn a move toward using microservices, a company‚Äôs management team has asked all development teams to build their services so that API requests depend only on that service‚Äôs data store. One team is building a Payments service which has its own database; the service needs data that originates in the Accounts database. Both are using Amazon DynamoDB.What approach will result in the simplest, decoupled, and reliable method to get near-real time updates from the Accounts database?",
      "choices": {
        "A": "Use AWS Glue to perform frequent ETL updates from the Accounts database to the Payments database.",
        "B": "Use Amazon ElastiCache in Payments, with the cache updated by triggers in the Accounts database.",
        "C": "Use Amazon Data Firehose to deliver all changes from the Accounts database to the Payments database.",
        "D": "Use Amazon DynamoDB Streams to deliver all changes from the Accounts database to the Payments database."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156644-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 9:40 a.m.",
      "textHash": "0a3f787ebf44ce01",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:dynamodb:5d3519e4",
      "frExplanation": "On veut des microservices ¬´ d√©coupl√©s ¬ª : le service Payments ne doit pas lire directement la base du service Accounts, mais recevoir les changements automatiquement.\nAvec DynamoDB, la fa√ßon la plus simple de capter les modifications (insert/update/delete) d‚Äôune table est DynamoDB Streams : chaque changement est publi√© dans un flux.\nPayments (souvent via une fonction AWS Lambda) peut lire ce flux et mettre √† jour sa propre table DynamoDB avec les donn√©es dont il a besoin.\nC‚Äôest proche du temps r√©el, fiable (les √©v√©nements sont conserv√©s un moment et peuvent √™tre relus), et chaque service garde sa base.\nAWS Glue (A) sert plut√¥t √† des traitements ETL/batch, pas √† des mises √† jour quasi temps r√©el simples.\nElastiCache (B) est un cache, pas une source de v√©rit√©, et ajoute de la complexit√© pour la coh√©rence.\nKinesis Data Firehose (C) est surtout pour envoyer des donn√©es vers S3/Redshift/OpenSearch, pas pour synchroniser directement deux tables DynamoDB.\nDonc DynamoDB Streams est l‚Äôoption la plus simple, d√©coupl√©e et fiable pour propager les changements.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine deux clubs au lyc√©e : le club ‚ÄúComptes‚Äù tient la liste officielle des √©l√®ves (qui a pay√©, quel solde), et le club ‚ÄúPaiements‚Äù doit agir vite quand la liste change. Chaque club a son propre cahier, et on veut √©viter d‚Äôaller demander au club Comptes √† chaque fois.**\n\nConcept : il faut un moyen automatique pour copier les changements du cahier ‚ÄúComptes‚Äù vers le cahier ‚ÄúPaiements‚Äù, presque en direct, sans que les clubs se parlent tout le temps.\nD (DynamoDB Streams) = comme un ‚Äúfil d‚Äôactualit√©‚Äù : d√®s qu‚Äôune ligne du cahier Comptes est modifi√©e, un message ‚Äúchangement‚Äù est ajout√© au fil, et Paiements le lit pour se mettre √† jour.\nC‚Äôest simple : pas besoin de gros traitement, juste suivre le fil.\nC‚Äôest d√©coupl√© : Comptes √©crit dans son cahier, point; Paiements choisit de lire le fil.\nC‚Äôest fiable : les changements sont enregistr√©s dans le fil, donc moins de risques d‚Äôen rater.\nA ressemble √† recopier toute la liste souvent (trop lourd et pas quasi temps r√©el).\nB ajoute un ‚Äúpost-it cache‚Äù compliqu√© √† garder toujours √† jour.\nC est plut√¥t un camion de livraison pour gros flux, pas le plus simple ici.\nDonc D est le meilleur choix.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:434:6663639e1339779d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 434,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing an application that will run on Amazon EC2 instances in an Auto Scaling group. The developer wants to externalize the session state to support the application.Which AWS services or resources can the developer use to meet these requirements? (Choose two.)",
      "choices": {
        "A": "Amazon DynamoDB",
        "B": "Amazon Cognito",
        "C": "Amazon ElastiCache",
        "D": "Application Load Balancer",
        "E": "Amazon Simple Queue Service (Amazon SQS)"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156702-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 12:13 p.m.",
      "textHash": "6663639e1339779d",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Dans un groupe Auto Scaling, plusieurs serveurs EC2 peuvent √™tre cr√©√©s/supprim√©s automatiquement. Si la ‚Äúsession‚Äù (donn√©es de connexion, panier, etc.) est stock√©e sur le disque ou la m√©moire d‚Äôun seul serveur, l‚Äôutilisateur peut la perdre quand il change de serveur.\nIl faut donc mettre l‚Äô√©tat de session dans un stockage externe, partag√© et rapide.\nAmazon DynamoDB est une base de donn√©es NoSQL manag√©e : on peut y enregistrer une session par cl√© (ex: sessionId) et la relire depuis n‚Äôimporte quelle instance EC2. C‚Äôest durable et hautement disponible.\nAmazon ElastiCache (Redis/Memcached) est un cache en m√©moire manag√© : tr√®s rapide, souvent utilis√© pour stocker des sessions temporaires partag√©es entre serveurs.\nAmazon Cognito g√®re l‚Äôauthentification et des jetons, mais ce n‚Äôest pas un stockage de session applicative partag√© au sens ‚Äústate server-side‚Äù.\nUn Application Load Balancer r√©partit le trafic mais ne stocke pas les sessions.\nAmazon SQS sert √† mettre des messages en file d‚Äôattente, pas √† lire/√©crire l‚Äô√©tat de session.\nDonc les bons choix pour externaliser la session sont DynamoDB et ElastiCache.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une √©quipe de foot au lyc√©e : plusieurs rempla√ßants peuvent entrer sur le terrain √† tout moment. Le coach veut que, peu importe qui joue, le joueur connaisse le score et les actions d√©j√† faites.**\n\nConcept : sur EC2 en Auto Scaling, c‚Äôest pareil : plusieurs ‚Äújoueurs‚Äù (serveurs) peuvent appara√Ætre/dispara√Ætre. Si on garde la ‚Äúsession‚Äù (qui est connect√©, panier, progression) dans un seul serveur, on la perd quand il sort. Donc on met la session dans un cahier commun au vestiaire, accessible √† tous. Amazon DynamoDB (A) = ce cahier : une base de donn√©es rapide o√π chaque serveur lit/√©crit l‚Äô√©tat de session. Pourquoi pas les autres : Load Balancer (D) = l‚Äôarbitre qui r√©partit les joueurs, il ne stocke pas le score. SQS (E) = une file d‚Äôattente de messages, pas un cahier de session. Cognito (B) = carte d‚Äôidentit√©/connexion, pas l‚Äô√©tat de session complet. ElastiCache (C) = un tableau blanc tr√®s rapide pour des sessions, mais ici la r√©ponse donn√©e indique A : DynamoDB est le bon ‚Äúcahier partag√©‚Äù.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:545:e18ec827bb600536",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 545,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is using the AWS Serverless Application Model (AWS SAM) to develop a social media application. A developer needs a quick way to test AWS Lambda functions locally by using test event payloads. The developer needs the structure of these test event payloads to match the actual events that AWS services create.Which solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Create shareable test Lambda events. Use these test Lambda events for local testing.",
        "B": "Store manually created test event payloads locally. Use the sam local invoke command with the file path to the payloads.",
        "C": "Store manually created test event payloads in an Amazon S3 bucket. Use the sam local invoke command with the S3 path to the payloads.",
        "D": "Use the sam local generate-event command to create test payloads for local testing."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156701-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 11:53 a.m.",
      "textHash": "e18ec827bb600536",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:6a531d41",
      "frExplanation": "Objectif : tester rapidement des fonctions AWS Lambda en local avec des √©v√©nements r√©alistes (comme ceux produits par S3, API Gateway, SNS, etc.).\nAWS SAM est un outil qui aide √† d√©velopper et tester des applis ‚Äúserverless‚Äù (sans serveurs) et inclut des commandes pour simuler Lambda.\nLe point cl√© est ‚Äúla structure doit correspondre aux vrais √©v√©nements AWS‚Äù : si vous √©crivez les payloads √† la main, vous risquez d‚Äôoublier des champs ou de vous tromper de format.\nLa commande SAM \"sam local generate-event\" g√©n√®re automatiquement des exemples d‚Äô√©v√©nements au bon format pour de nombreux services AWS.\nEnsuite, vous pouvez utiliser ces payloads g√©n√©r√©s avec \"sam local invoke\" pour ex√©cuter la Lambda localement.\nC‚Äôest le moins d‚Äôeffort car vous n‚Äôavez pas √† inventer/maintenir des fichiers JSON d‚Äô√©v√©nements vous-m√™me.\nA ne garantit pas des structures exactes et demande de cr√©er/partager des √©v√©nements.\nB et C reposent sur des payloads cr√©√©s manuellement (plus d‚Äôerreurs et plus de travail), et S3 n‚Äôapporte rien pour un test local rapide.\nDonc la meilleure r√©ponse est D.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu veux t‚Äôentra√Æner √† r√©pondre √† des messages comme sur un r√©seau social, mais en mode ‚Äúhors-ligne‚Äù, comme un jeu vid√©o en entra√Ænement. Tu as besoin de faux messages qui ressemblent EXACTEMENT √† ceux que le vrai jeu enverrait.**\n\nUne fonction AWS Lambda, c‚Äôest comme un petit ‚Äúrobot‚Äù qui r√©agit quand il re√ßoit un message (un √©v√©nement). Pour tester chez toi, il te faut des messages d‚Äôentra√Ænement au bon format, comme ceux envoy√©s par le vrai r√©seau social. Si tu les √©cris √† la main (B ou C), tu risques de te tromper dans la forme, et √ßa prend du temps. L‚Äôoption A parle de partager des √©v√©nements, mais √ßa ne garantit pas qu‚Äôils copient le format r√©el des services AWS. La commande \"sam local generate-event\" (D), c‚Äôest comme un g√©n√©rateur officiel de messages d‚Äôentra√Ænement: il fabrique automatiquement des exemples qui imitent les vrais messages AWS. Donc tu testes vite, avec le bon format, et avec le moins d‚Äôeffort.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:542:5db0560ecd3b2664",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 542,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA cloud-based video surveillance company is developing an application that analyzes video files. After the application analyzes the files, the company can discard the files.The company stores the files in an Amazon S3 bucket. The files are 1 GB in size on average. No file is larger than 2 GB. An AWS Lambda function will run one time for each video file that is processed. The processing is very I/O intensive, and the application must read each file multiple times.Which solution will meet these requirements in the MOST performance-optimized way?",
      "choices": {
        "A": "Attach an Amazon Elastic Block Store (Amazon EBS) volume that is larger than 1 GB to the Lambda function. Copy the files from the S3 bucket to the EBS volume.",
        "B": "Attach an Elastic Network Adapter (ENA) to the Lambda function. Use the ENA to read the video files from the S3 bucket.",
        "C": "Increase the ephemeral storage size to 2 GB. Copy the files from the S3 bucket to the /tmp directory of the Lambda function.",
        "D": "Configure the Lambda function code to read the video files directly from the S3 bucket."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156699-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 11:47 a.m.",
      "textHash": "5db0560ecd3b2664",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:230a508f",
      "frExplanation": "Ici, chaque fichier vid√©o est trait√© une seule fois par une fonction AWS Lambda (un code qui s‚Äôex√©cute √† la demande, sans serveur √† g√©rer).\nLes vid√©os sont dans Amazon S3 (stockage d‚Äôobjets sur Internet). Lire plusieurs fois un gros fichier directement depuis S3 implique beaucoup d‚Äôallers-retours r√©seau, donc plus lent pour un traitement tr√®s I/O.\nLa meilleure approche est de copier le fichier une fois en local, puis de le relire rapidement plusieurs fois.\nLambda fournit un disque local temporaire appel√© stockage √©ph√©m√®re, accessible via /tmp.\nEn augmentant ce stockage √©ph√©m√®re √† 2 GB, on peut y copier chaque vid√©o (max 2 GB) et faire toutes les lectures r√©p√©t√©es localement, ce qui optimise les performances.\nA est impossible : on ne peut pas attacher un volume EBS √† Lambda (EBS est pour EC2).\nB n‚Äôa pas de sens : on ne ‚Äúbranche‚Äù pas une ENA √† Lambda pour acc√©l√©rer S3.\nD est fonctionnel mais moins performant car chaque lecture repasse par le r√©seau vers S3.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un √©l√®ve qui doit analyser une grosse vid√©o pour un expos√©. La vid√©o est dans la biblioth√®que (S3). Il doit la revoir plusieurs fois, puis il peut la jeter.**\n\nConcept : si tu dois relire souvent le m√™me fichier, c‚Äôest plus rapide de l‚Äôavoir sur ta table plut√¥t que d‚Äôaller le rechercher √† la biblioth√®que √† chaque fois.\nIci, la fonction Lambda = l‚Äô√©l√®ve. S3 = la biblioth√®que. /tmp = la table de l‚Äô√©l√®ve (stockage temporaire).\nComme le traitement est tr√®s ‚ÄúI/O‚Äù (beaucoup de lectures), lire directement depuis S3 (D) ferait plein d‚Äôallers-retours lents.\nLa meilleure id√©e est donc de copier la vid√©o une fois sur la table, puis la relire plusieurs fois tr√®s vite.\nOption C fait exactement √ßa : on agrandit la table temporaire √† 2 Go (assez pour un fichier max 2 Go) et on copie dans /tmp.\nApr√®s l‚Äôanalyse, on peut jeter le fichier : /tmp est temporaire, donc parfait.\nA parle d‚Äôun disque sp√©cial (EBS) : ce n‚Äôest pas la ‚Äútable‚Äù normale de Lambda, c‚Äôest plus compliqu√© et pas pr√©vu ici.\nB (ENA) c‚Äôest comme changer le c√¢ble Internet : √ßa n‚Äô√©vite pas les allers-retours √† la biblioth√®que.\nDonc C est la plus rapide et la plus adapt√©e.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:540:9e81eaa78be1d762",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 540,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is using AWS CodeDeploy to launch an application onto Amazon EC2 instances. The application deployment fails during testing. The developer notices an IAM_ROLE_PERMISSIONS error code in Amazon CloudWatch logs.What should the developer do to resolve the error?",
      "choices": {
        "A": "Ensure that the deployment group is using the correct role name for the CodeDeploy service role.",
        "B": "Attach the AWSCodeDeployRoleECS policy to the CodeDeploy service role.",
        "C": "Attach the AWSCodeDeployRole policy to the CodeDeploy service role.",
        "D": "Ensure the CodeDeploy agent is installed and running on all instances in the deployment group."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156698-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 11:45 a.m.",
      "textHash": "9e81eaa78be1d762",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Le code IAM_ROLE_PERMISSIONS indique un probl√®me de droits IAM (les autorisations) utilis√©s par CodeDeploy.\nCodeDeploy est un service qui orchestre le d√©ploiement; pour agir sur vos instances EC2, il utilise un ¬´ service role ¬ª IAM.\nSi ce r√¥le n‚Äôa pas les permissions n√©cessaires, CodeDeploy ne peut pas cr√©er/mettre √† jour les ressources li√©es au d√©ploiement et le test √©choue.\nLa solution la plus directe est d‚Äôattacher la politique g√©r√©e AWS ¬´ AWSCodeDeployRole ¬ª au r√¥le de service CodeDeploy.\nCette policy contient les autorisations standard attendues pour des d√©ploiements CodeDeploy vers EC2/Auto Scaling.\nL‚Äôoption ECS (AWSCodeDeployRoleECS) concerne les d√©ploiements vers Amazon ECS, pas vers EC2, donc ce n‚Äôest pas adapt√© ici.\nV√©rifier le nom du r√¥le ou l‚Äôagent CodeDeploy peut √™tre utile dans d‚Äôautres erreurs, mais IAM_ROLE_PERMISSIONS pointe d‚Äôabord vers des permissions manquantes.\nDonc il faut ajouter la policy AWSCodeDeployRole au service role CodeDeploy.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que CodeDeploy est un surveillant qui doit distribuer des copies d‚Äôexamen dans plusieurs salles (les serveurs EC2). Pour entrer dans les salles et donner les copies, il lui faut un badge d‚Äôacc√®s (un ‚Äúr√¥le‚Äù IAM) avec les bonnes autorisations.**\n\nConcept : si le surveillant n‚Äôa pas le bon badge, il est bloqu√© par les portes. L‚Äôerreur IAM_ROLE_PERMISSIONS veut dire : ‚Äúton badge n‚Äôautorise pas ce que tu essaies de faire‚Äù.\nPourquoi C : la ‚Äúpolicy‚Äù AWSCodeDeployRole, c‚Äôest la liste d‚Äôautorisations standard qui donne au surveillant CodeDeploy le droit de faire son travail (pr√©parer, lancer, v√©rifier le d√©ploiement). En l‚Äôattachant au r√¥le de service CodeDeploy, tu lui donnes enfin les cl√©s n√©cessaires.\nPourquoi pas D : l‚Äôagent sur les serveurs, c‚Äôest comme un √©l√®ve qui re√ßoit la copie ; ici le probl√®me vient du surveillant (permissions), pas des √©l√®ves.\nPourquoi pas B : ECS, c‚Äôest un autre type de ‚Äúsalles‚Äù (conteneurs), pas des EC2.\nDonc : ajouter AWSCodeDeployRole au r√¥le CodeDeploy (C).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:532:5199a24e44fb5fb4",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 532,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA team deploys an AWS CloudFormation template to update a stack that already included an Amazon DynamoDB table. However, before the deployment of the update, the team changed the name of the DynamoDB table on the template by mistake. The DeletionPolicy attribute for all resources has the default value.What will be the result of this mistake?",
      "choices": {
        "A": "CloudFormation will create a new table and will delete the existing table.",
        "B": "CloudFormation will create a new table and will keep the existing table.",
        "C": "CloudFormation will overwrite the existing table and will rename the existing table.",
        "D": "CloudFormation will keep the existing table and will not create a new table."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156682-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:50 a.m.",
      "textHash": "5199a24e44fb5fb4",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "CloudFormation est un service AWS qui cr√©e et met √† jour des ressources (comme une table DynamoDB) √† partir d‚Äôun fichier ‚Äútemplate‚Äù.\nDans CloudFormation, si vous changez le nom d‚Äôune ressource (ici le nom de la table DynamoDB) dans le template, CloudFormation consid√®re souvent que c‚Äôest une nouvelle ressource √† cr√©er.\nLors d‚Äôune mise √† jour, il va donc cr√©er une nouvelle table avec le nouveau nom.\nEnsuite, l‚Äôancienne table (celle qui n‚Äôexiste plus ‚Äúdans le template‚Äù) est vue comme une ressource √† supprimer.\nComme DeletionPolicy n‚Äôa pas √©t√© d√©fini, la valeur par d√©faut est ‚ÄúDelete‚Äù : la ressource est supprim√©e quand elle est retir√©e du stack.\nDonc l‚Äôancienne table DynamoDB sera supprim√©e, ce qui peut entra√Æner une perte de donn√©es.\nC‚Äôest pourquoi la bonne r√©ponse est : cr√©er une nouvelle table et supprimer l‚Äôancienne (A).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un prof qui g√®re la liste des √©l√®ves d‚Äôun club avec une fiche ‚Äúofficielle‚Äù. Le nom du club sur la fiche sert d‚Äôidentifiant pour savoir de quel club on parle.**\n\nConcept : CloudFormation, c‚Äôest comme ce prof : il suit une ‚Äúfiche‚Äù (le template) et met √† jour ce qui existe d√©j√†. Si tu changes le nom d‚Äôun objet, il croit que c‚Äôest un nouvel objet.\nIci, la table DynamoDB = le classeur avec toutes les infos des √©l√®ves. En changeant son nom dans la fiche, CloudFormation pense : ‚ÄúAh, l‚Äôancienne table n‚Äôest plus demand√©e, et une nouvelle table est demand√©e.‚Äù\nDeletionPolicy par d√©faut = si un objet n‚Äôest plus dans la fiche, le prof le jette √† la poubelle (il le supprime).\nDonc il cr√©e une nouvelle table (nouveau classeur) et supprime l‚Äôancienne table (ancien classeur).\nR√©sultat : A est correct.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:527:cc5f0fe345d8eb41",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 527,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing an application in AWS Lambda. To simplify testing and deployments, the developer needs the database connection string to be easily changed without modifying the Lambda code.How can this requirement be met?",
      "choices": {
        "A": "Store the connection string as a secret in AWS Secrets Manager.",
        "B": "Store the connection string in an IAM user account.",
        "C": "Store the connection string in AWS KMS.",
        "D": "Store the connection string as a Lambda layer."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156678-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:46 a.m.",
      "textHash": "cc5f0fe345d8eb41",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "On veut pouvoir changer une ‚Äúconnection string‚Äù (adresse + identifiants/param√®tres pour se connecter √† une base) sans toucher au code Lambda.\nAWS Lambda ex√©cute du code sans serveur, mais le code ne doit pas contenir des secrets en dur (risque de fuite et d√©ploiements inutiles).\nAWS Secrets Manager est un service fait pour stocker et g√©rer des informations sensibles (mots de passe, cha√Ænes de connexion) de fa√ßon s√©curis√©e.\nL‚Äôapplication Lambda peut lire le secret au d√©marrage via l‚ÄôAPI AWS : si la cha√Æne change, on met √† jour le secret, pas le code.\nCela simplifie les tests et les d√©ploiements : m√™me code, valeurs diff√©rentes selon l‚Äôenvironnement (dev/test/prod).\nUn compte IAM (B) sert √† g√©rer des permissions, pas √† stocker des valeurs.\nAWS KMS (C) sert surtout √† chiffrer/d√©chiffrer des donn√©es, pas √† g√©rer des secrets avec versioning/rotation.\nUne Lambda layer (D) sert √† partager des biblioth√®ques/fichiers, pas √† stocker des secrets modifiables facilement.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que ta classe joue une partie de jeu vid√©o en √©quipe, et que pour entrer dans une salle secr√®te il faut un code d‚Äôacc√®s. Tu veux pouvoir changer ce code souvent sans devoir reprogrammer le jeu.**\n\nLe ‚Äúconnection string‚Äù, c‚Äôest comme ce code d‚Äôacc√®s : il dit √† ton appli o√π est la base de donn√©es et comment y entrer. Si tu le mets dans le code de Lambda (le ‚Äúpetit robot‚Äù qui ex√©cute ton programme), √† chaque changement tu dois modifier et red√©ployer le robot. AWS Secrets Manager, c‚Äôest un coffre-fort fait pour stocker des infos sensibles (mots de passe, codes) et les changer facilement. Ton robot va juste lire le code dans le coffre au moment o√π il en a besoin. Donc tu peux tester et d√©ployer sans toucher au code, juste en changeant le contenu du coffre. IAM user (B) c‚Äôest un badge d‚Äôidentit√©, pas un endroit pour stocker un code. KMS (C) c‚Äôest plut√¥t un cadenas pour chiffrer, pas un coffre o√π g√©rer des secrets. Une Lambda layer (D) c‚Äôest un pack d‚Äôoutils/code partag√©, pas fait pour des codes secrets qui changent souvent.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:524:291852b6db295256",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 524,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is launching a photo sharing application on AWS. Users use the application to upload images to an Amazon S3 bucket. When users upload images, an AWS Lambda function creates thumbnail versions of the images and stores the thumbnail versions in another S3 bucket.During development, a developer notices that the Lambda function takes more than 2 minutes to complete the thumbnail process. The company needs alll images to be processed in less than 30 seconds.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Increase the virtual CPUs (vCPUs) for the Lambda function to use 10 vCPUs.",
        "B": "Change Lambda function instance type to use m6a.4xlarge.",
        "C": "Configure the Lambda function to increase the amount of memory.",
        "D": "Configure burstable performance for the Lambda function."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156677-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:44 a.m.",
      "textHash": "291852b6db295256",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:d609bfb9",
      "frExplanation": "Ici, l‚Äôapplication met des photos dans Amazon S3 (un stockage de fichiers). Un √©v√©nement S3 d√©clenche AWS Lambda (un service qui ex√©cute du code sans serveur) pour cr√©er des miniatures et les remettre dans un autre bucket S3.\nSi le traitement est trop lent, il faut comprendre comment ‚Äúacc√©l√©rer‚Äù Lambda : on ne choisit pas un type d‚Äôinstance (comme m6a.4xlarge) et on ne r√®gle pas des vCPU directement, car Lambda n‚Äôest pas une VM.\nDans Lambda, la puissance CPU et le d√©bit r√©seau/disque augmentent automatiquement quand on augmente la m√©moire allou√©e √† la fonction.\nDonc, donner plus de m√©moire √† la fonction lui donne aussi plus de CPU, ce qui acc√©l√®re le redimensionnement d‚Äôimages (t√¢che tr√®s CPU).\nC‚Äôest l‚Äôaction la plus simple et pr√©vue par AWS pour r√©duire le temps d‚Äôex√©cution et viser < 30 secondes.\nLes options A, B et D ne correspondent pas au mod√®le Lambda (pas de choix d‚Äôinstance, pas de ‚Äúburstable‚Äù comme sur EC2).",
      "domainKey": "development",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:520:5527ebfd59c522db",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 520,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application that is running on Amazon EC2 instances stores data in an Amazon S3 bucket. All the data must be encrypted in transit.How can a developer ensure that all traffic to the S3 bucket is encrypted?",
      "choices": {
        "A": "Install certificates on the EC2 instances.",
        "B": "Create a private VPC endpoint.",
        "C": "Configure the S3 bucket with server-side encryption with AWS KMS managed encryption keys (SSE-KMS).",
        "D": "Create an S3 bucket policy that denies traffic when the value for the aws:SecureTransport condition key is false."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156676-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:43 a.m.",
      "textHash": "5527ebfd59c522db",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, on veut chiffrer les donn√©es ¬´ en transit ¬ª, c‚Äôest-√†-dire pendant le trajet r√©seau entre les instances EC2 (serveurs) et Amazon S3 (stockage d‚Äôobjets). Pour garantir cela, il faut forcer l‚Äôutilisation de HTTPS/TLS (et refuser HTTP). La meilleure fa√ßon c√¥t√© S3 est une policy de bucket qui refuse toute requ√™te si elle n‚Äôutilise pas un transport s√©curis√©. La condition aws:SecureTransport = false correspond aux appels faits en HTTP (non chiffr√©) : la policy les bloque, donc seuls les appels en HTTPS passent. SSE-KMS (option C) chiffre ¬´ au repos ¬ª dans S3, pas le trajet r√©seau. Un VPC endpoint (B) rend l‚Äôacc√®s priv√© dans le r√©seau AWS, mais ne garantit pas √† lui seul que le protocole est chiffr√©. Installer des certificats sur EC2 (A) n‚Äôimpose pas que les appels vers S3 utilisent HTTPS : c‚Äôest le client et la policy S3 qui doivent l‚Äôexiger. Donc la bonne r√©ponse est D.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un casier au lyc√©e (le ‚Äúbucket S3‚Äù) o√π tu ranges des copies, et des √©l√®ves (les serveurs EC2) qui viennent d√©poser/chercher des documents.**\n\nConcept : ‚Äúchiffr√© en transit‚Äù = pendant le trajet dans le couloir, le document est dans une enveloppe ferm√©e (connexion HTTPS). Sans √ßa, quelqu‚Äôun peut lire en passant.\nPourquoi D : une ‚Äúbucket policy‚Äù c‚Äôest le r√®glement du casier. Avec la r√®gle aws:SecureTransport, tu dis : ‚ÄúSi tu n‚Äôutilises pas l‚Äôenveloppe ferm√©e (SecureTransport=false), alors acc√®s refus√©.‚Äù Donc TOUTES les entr√©es/sorties vers le casier doivent passer en HTTPS.\nPourquoi pas C : SSE-KMS chiffre surtout ce qui est stock√© DANS le casier (au repos), pas forc√©ment le trajet.\nPourquoi pas A : mettre des certificats sur les √©l√®ves ne force pas tout le monde √† utiliser l‚Äôenveloppe.\nPourquoi pas B : un chemin priv√© aide, mais ne garantit pas que le document est dans une enveloppe chiffr√©e.\nDonc D est la seule option qui bloque automatiquement tout trafic non chiffr√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:515:21d8556956a890d8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 515,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn AWS Lambda function that handles application requests uses the default Lambda logging mechanism to log the timestamp, processing time, and status of requests.A developer needs to create Amazon CloudWatch metrics based on the logs. The developer needs to write the metrics to a custom CloudWatch metrics namespace.Which solution will meet these requirements?",
      "choices": {
        "A": "Use Amazon CloudWatch Logs Insights to generate custom metrics from the logs by using CloudWatch embedded metric format (EMF).",
        "B": "Use Amazon CloudWatch RUM to generate custom metrics from the logs by using CloudWatch embedded metric format (EMF).",
        "C": "Use Amazon CloudWatch Logs Insights to generate custom metrics from the logs by using JSON format.",
        "D": "Use the CloudWatch embedded metric format (EMF) for the structure of the log statements to generate custom CloudWatch metrics."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156674-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:40 a.m.",
      "textHash": "21d8556956a890d8",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "AWS Lambda ex√©cute du code sans serveur et √©crit automatiquement ses logs dans CloudWatch Logs (un service de stockage/consultation de journaux).\nLe besoin est de cr√©er des m√©triques CloudWatch (compteurs/mesures) √† partir de ces logs, et de les publier dans un namespace personnalis√© (un ‚Äúdossier‚Äù logique de m√©triques).\nLa fa√ßon la plus directe est d‚Äô√©crire les logs dans un format sp√©cial que CloudWatch sait convertir en m√©triques : l‚ÄôEmbedded Metric Format (EMF).\nAvec EMF, votre fonction Lambda logge une ligne JSON structur√©e qui contient √† la fois le message et la d√©finition des m√©triques (nom, unit√©, dimensions, namespace).\nCloudWatch lit ces logs EMF et cr√©e automatiquement les m√©triques correspondantes dans le namespace que vous indiquez.\nLes options avec Logs Insights ne sont pas n√©cessaires pour publier des m√©triques EMF : Insights sert surtout √† interroger/analyser des logs, pas √† imposer la cr√©ation automatique.\nCloudWatch RUM concerne la surveillance ‚ÄúReal User Monitoring‚Äù c√¥t√© navigateur, pas les logs Lambda.\nDonc la bonne r√©ponse est d‚Äôutiliser EMF dans la structure des logs pour g√©n√©rer des m√©triques personnalis√©es (D).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le cahier d‚Äôappel de ta classe : chaque ligne note l‚Äôheure, combien de temps un √©l√®ve a mis pour rendre un devoir, et si c‚Äô√©tait OK ou pas. Mais le proviseur veut des graphiques (retards, temps moyen, taux de r√©ussite) dans un classeur sp√©cial avec un nom de rubrique perso.**\n\nConcept : les ‚Äúlogs‚Äù de Lambda, c‚Äôest comme des lignes √©crites dans un cahier. Les ‚Äúm√©triques‚Äù, c‚Äôest comme des compteurs et des graphiques automatiques. Pour que le proviseur fasse les graphiques, il faut √©crire les lignes dans un format bien rang√©.\nPourquoi D : EMF (Embedded Metric Format), c‚Äôest un mod√®le d‚Äô√©criture dans le cahier qui dit clairement : ‚Äúceci est un compteur‚Äù, ‚Äúvoici son nom‚Äù, ‚Äúvoici la valeur‚Äù, et ‚Äúmets-le dans telle rubrique (namespace)‚Äù. CloudWatch lit ces lignes et cr√©e tout seul les m√©triques dans ton namespace personnalis√©.\nPourquoi pas A/C : Logs Insights, c‚Äôest plut√¥t un outil pour fouiller/chercher dans le cahier apr√®s coup, pas la m√©thode principale pour transformer automatiquement chaque ligne en m√©trique.\nPourquoi pas B : RUM sert √† mesurer l‚Äôexp√©rience des utilisateurs sur un site/app (comme le ressenti des √©l√®ves), pas √† convertir des logs de Lambda.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:513:ca91d9d6e0591f08",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 513,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a three-tier web application that should be able to handle a minimum of 5000 requests per minute. Requirements state that the web tier should be completely stateless while the application maintains session state for the users.How can session data be externalized, keeping latency at the LOWEST possible value?",
      "choices": {
        "A": "Create an Amazon RDS instance, then implement session handling at the application level to leverage a database inside the RDS database instance for session data storage.",
        "B": "Implement a shared file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the shared file system for session data storage.",
        "C": "Create an Amazon ElastiCache (Memcached) cluster, then implement session handling at the application level to leverage the cluster for session data storage.",
        "D": "Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156673-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:34 a.m.",
      "textHash": "ca91d9d6e0591f08",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:dc8cb425",
      "frExplanation": "On veut un web tier ¬´ stateless ¬ª : aucune session ne doit √™tre stock√©e sur les serveurs web, sinon on ne peut pas facilement ajouter/retirer des instances.\nIl faut donc mettre les donn√©es de session dans un service externe, tr√®s rapide, accessible par toutes les instances.\nAmazon ElastiCache (Memcached) est un cache en m√©moire (RAM) g√©r√© par AWS : lire/√©crire en RAM est beaucoup plus rapide qu‚Äôun disque ou une base relationnelle.\nPour 5000 requ√™tes/min, la latence la plus faible vient g√©n√©ralement d‚Äôun stockage en m√©moire partag√©.\nRDS (A) est une base relationnelle sur disque : plus de latence et plus de charge pour des lectures/√©critures de session fr√©quentes.\nUn syst√®me de fichiers partag√© (B) implique du stockage r√©seau : plus lent et plus complexe √† g√©rer.\nDynamoDB (D) est tr√®s performant, mais reste une base NoSQL avec appels r√©seau et latence g√©n√©ralement sup√©rieure √† un cache en m√©moire pour des sessions.\nDonc la meilleure option pour la latence la plus basse est ElastiCache Memcached (C).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o en ligne : les serveurs qui affichent le jeu (web tier) doivent √™tre ‚Äúsans m√©moire‚Äù, mais chaque joueur a une partie en cours (session) qu‚Äôil faut retrouver instantan√©ment.**\n\nConcept : si les serveurs n‚Äôont pas le droit de garder la m√©moire des joueurs, on met la m√©moire dans un ‚Äúcasier commun‚Äù accessible √† tous.\nPour la latence la plus basse, ce casier doit √™tre ultra rapide, comme une m√©moire vive, pas un cahier d‚Äôarchives.\nC (ElastiCache Memcached) = un casier en RAM : on lit/√©crit la session tr√®s vite, parfait pour 5000 requ√™tes/min.\nA (RDS) = comme noter dans un classeur au secr√©tariat : fiable mais plus lent √† consulter.\nB (fichiers partag√©s) = comme un classeur partag√© : √ßa marche, mais ce n‚Äôest pas le plus rapide.\nD (DynamoDB) = comme un registre tr√®s bien organis√© : rapide, mais g√©n√©ralement moins ‚Äúinstantan√©‚Äù que la RAM.\nDonc pour garder le web tier sans m√©moire et avoir le minimum de d√©lai, on choisit C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:511:b50bdb193657e21d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 511,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building a social media application. A developer is modifying an AWS Lambda function that updates a database with data that tracks each user's online activity. A web application server uses the AWS SDK to invoke the Lambda function.The developer has tested the new Lambda code and is ready to deploy the code into production. However, the developer wants to allow only a small percentage of the invocations from the AWS SDK to call the new code.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure a Lambda version that has a specific weight value for the updated Lambda function.",
        "B": "Create an alias for the Lambda function. Configure a specific weight value for the updated version.",
        "C": "Create an Application Load Balancer. Specify weighted target groups for the original Lambda function and the updated Lambda function.",
        "D": "Create a Network Load Balancer. Specify weighted target groups for the original Lambda function and the updated Lambda function."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156672-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:33 a.m.",
      "textHash": "b50bdb193657e21d",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Objectif : d√©ployer une nouvelle version d‚Äôune fonction AWS Lambda (code ex√©cut√© √† la demande) sans l‚Äôenvoyer √† 100% des appels tout de suite.\nLambda permet de publier des ¬´ versions ¬ª (snapshots immuables du code) et de cr√©er un ¬´ alias ¬ª (un nom stable comme prod) qui pointe vers une version.\nUn alias peut faire du routage pond√©r√© : par exemple 95% des invocations vont vers l‚Äôancienne version, 5% vers la nouvelle.\nComme le serveur web appelle Lambda via le SDK, il suffit qu‚Äôil invoque l‚Äôalias (ex: prod) : AWS r√©partit automatiquement selon les poids.\nC‚Äôest exactement un d√©ploiement progressif (canary) pour limiter le risque en production.\nPourquoi pas A : on ne met pas de ‚Äúpoids‚Äù directement sur une version seule ; le poids se configure sur un alias entre deux versions.\nPourquoi pas C/D : les Load Balancers et leurs target groups ne sont pas n√©cessaires ici ; Lambda a d√©j√† le m√©canisme natif via alias + versions.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que la cantine teste une nouvelle recette de p√¢tes. Tu veux que seulement 5% des √©l√®ves la re√ßoivent, et les autres gardent l‚Äôancienne recette, sans changer toute l‚Äôorganisation.**\n\nDans AWS, la fonction Lambda, c‚Äôest la ‚Äúrecette‚Äù qui s‚Äôex√©cute quand l‚Äôappli l‚Äôappelle. Une ‚Äúversion‚Äù, c‚Äôest une recette fig√©e (ancienne ou nouvelle). Un ‚Äúalias‚Äù, c‚Äôest l‚Äô√©tiquette sur le menu: ‚ÄúP√¢tes du jour‚Äù. Avec l‚Äôalias, tu peux dire: 95% des plateaux vont vers l‚Äôancienne version, 5% vers la nouvelle (poids/weight). Comme √ßa, tu testes en vrai avec peu de monde, et si √ßa bug, tu remets 100% sur l‚Äôancienne vite. A est faux car le pourcentage se r√®gle via un alias qui pointe vers des versions, pas juste une version seule. C et D, c‚Äôest comme ajouter des portiques et des files d‚Äôattente √©normes juste pour choisir une recette: trop compliqu√© et pas le bon outil. Donc B.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:461:cb7700ed5ff7da4c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 461,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer published a change to a new version of an AWS Lambda function. To test the change, the developer must route 50% of the traffic to the new version and 60% of the traffic to the current version.What is the MOST operationally efficient way to meet this requirement?",
      "choices": {
        "A": "Create two Amazon Route 53 records that use a simple routing policy to route traffic to the different versions of the Lambda function. Create another Route 53 record that uses a weighted routing policy to route 50% of the traffic to each simple routing record. Test the Lambda function by using the weighted routing record.",
        "B": "Create an Amazon API Gateway API with a POST method that is integrated with the Lambda function. Add a stage variable that includes the version of the Lambda function. Add a canary release that will override the version variable 50% of the time. Deploy and test the Lambda function through the API Gateway stage.",
        "C": "Create a Lambda function alias. Set the weight to 50% for the current version and 50% for the new version. Set the event source mappings for the Lambda function to point to the alias.",
        "D": "Update the event source mappings for the Lambda function. In the mappings, set the weight to 50% for the current version and 50% for the new version."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156669-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:22 a.m.",
      "textHash": "cb7700ed5ff7da4c",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "AWS Lambda ex√©cute du code ¬´ sans serveur ¬ª et chaque publication cr√©e une version num√©rot√©e immuable (ex: v1, v2). Pour envoyer une partie du trafic vers une version pr√©cise, on utilise un alias Lambda : c‚Äôest un nom stable (ex: \"prod\") qui pointe vers une version. Un alias peut faire du ¬´ traffic shifting ¬ª en r√©partissant un pourcentage vers une seconde version (routing weights). Donc on met l‚Äôalias sur la version actuelle et on ajoute 50% vers la nouvelle version : Lambda r√©partit automatiquement les invocations. Ensuite, on configure les sources d‚Äô√©v√©nements (event source mappings : SQS, Kinesis, DynamoDB Streams, etc.) pour appeler l‚Äôalias, pas une version fixe. C‚Äôest le plus simple √† op√©rer : pas besoin de Route 53 ni d‚ÄôAPI Gateway, et les event source mappings ne supportent pas eux-m√™mes des poids. M√™me si l‚Äô√©nonc√© mentionne 50% et 60% (incoh√©rent), la bonne approche pour du pourcentage est bien l‚Äôalias Lambda.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:500:5a34a71a983de016",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 500,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an Amazon DynamoDB table that contains records of users that have signed up for a trial of the company‚Äôs product. The company is using a spreadsheet to track data about the product trial. The company needs to ensure the spreadsheet is automatically updated with the latest information when individual trials begin, are updated, or finish.Which solution will meet these requirements?",
      "choices": {
        "A": "Create a DynamoDB Accelerator (DAX) cluster from the table. Set the view type to old image. Create an AWS Lambda function that uses the cluster data to update the spreadsheet. Subscribe the Lambda function to the cluster.",
        "B": "Create a DynamoDB Accelerator (DAX) cluster from the table. Set the view type to new image. Create an AWS Lambda function that uses the cluster data to update the spreadsheet. Subscribe the Lambda function to the cluster.",
        "C": "Enable a DynamoDB stream for the table. Set the view type to new image. Create an AWS Lambda function that uses the stream data to update the spreadsheet. Subscribe the Lambda function to the stream.",
        "D": "Enable a DynamoDB stream for the table. Set the view type to old image. Create an AWS Lambda function that uses the stream data to update the spreadsheet. Subscribe the Lambda function to the stream."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156668-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:18 a.m.",
      "textHash": "5a34a71a983de016",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:dynamodb:bc1cba07",
      "frExplanation": "Pour mettre √† jour automatiquement un fichier (tableur) quand des lignes changent dans DynamoDB, il faut un m√©canisme qui ‚Äúd√©clenche‚Äù une action √† chaque √©criture.\nDynamoDB Streams est justement un journal des modifications (insert, update, delete) d‚Äôune table, que l‚Äôon peut connecter √† AWS Lambda.\nAWS Lambda est un service qui ex√©cute du code automatiquement quand un √©v√©nement arrive (ici, un changement dans la table).\nEn activant le stream et en choisissant la vue ‚Äúnew image‚Äù, chaque √©v√©nement contient la nouvelle version compl√®te de l‚Äô√©l√©ment apr√®s la modification.\nC‚Äôest id√©al pour mettre √† jour le tableur avec l‚Äô√©tat le plus r√©cent d‚Äôun essai (d√©but, mise √† jour, fin).\nDAX ne sert pas √† d√©clencher des √©v√©nements : c‚Äôest un cache pour acc√©l√©rer les lectures, donc A et B ne r√©pondent pas au besoin.\n‚ÄúOld image‚Äù (D) donnerait l‚Äôancienne valeur, moins pratique pour √©crire directement la derni√®re info dans le tableur.\nDonc la bonne solution est d‚Äôactiver DynamoDB Streams (new image) et de d√©clencher une Lambda : choix C.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un tableau d‚Äôaffichage au lyc√©e avec la liste des √©l√®ves en p√©riode d‚Äôessai pour un club. √Ä chaque fois qu‚Äôun √©l√®ve s‚Äôinscrit, change d‚Äôinfo, ou termine, un ‚Äújournal des changements‚Äù note automatiquement ce qui vient d‚Äô√™tre modifi√©. Un d√©l√©gu√© lit ce journal et recopie la derni√®re version dans un tableur.**\n\nIci, la table DynamoDB = la liste des √©l√®ves. Le tableur = ton fichier de suivi. Pour mettre √† jour automatiquement, il faut un ‚Äújournal des changements‚Äù : c‚Äôest DynamoDB Streams (un flux qui enregistre chaque ajout/modif/suppression). On choisit ‚Äúnew image‚Äù = la nouvelle fiche apr√®s le changement, comme lire la version √† jour de l‚Äô√©l√®ve. Ensuite, Lambda = le d√©l√©gu√© automatique : d√®s qu‚Äôune ligne change dans le journal, il se d√©clenche et met √† jour le tableur. DAX, lui, c‚Äôest plut√¥t un ‚Äúacc√©l√©rateur de lecture‚Äù (comme une copie rapide), pas un journal d‚Äô√©v√©nements, donc il ne sert pas √† d√©clencher des mises √† jour. Donc la bonne r√©ponse est C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:498:2d155de3f93f5a3a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 498,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer created an AWS Lambda function named ProcessMessages. The Lambda function is invoked asynchronously when a message is published to an Amazon Simple Notification Service (Amazon SNS) topic named InputTopic. The developer uses a second SNS topic named ErrorTopic to handle alerts of failures for other services.The developer wants to receive notifications from the ErrorTopic SNS topic when the ProcessMessages Lambda function fails to process a message.Which solution will meet this requirement?",
      "choices": {
        "A": "Configure a subscription for the ErrorTopic SNS topic. Configure a filter policy for failures. Specify the ProcessMessages Lambda function as the endpoint.",
        "B": "Configure a failure destination for the ProcessMessages Lambda function. Specify the Amazon Resource Name (ARN) of the ErrorTopic SNS topic as the destination ARN.",
        "C": "Configure a trigger for the ProcessMessages Lambda function. Specify the ErrorTopic SNS topic as the trigger topic. Configure a filter policy on the topic for failures",
        "D": "Configure a delivery policy on the ErrorTopic SNS topic. Configure a filter policy for failures. Specify the Lambda function as the input endpoint."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156666-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:16 a.m.",
      "textHash": "2d155de3f93f5a3a",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Ici, une fonction AWS Lambda (ProcessMessages) est appel√©e de fa√ßon asynchrone par un message SNS (InputTopic). ¬´ Asynchrone ¬ª signifie que SNS envoie l‚Äô√©v√©nement et n‚Äôattend pas la r√©ponse : si Lambda √©choue, il faut un m√©canisme sp√©cial pour r√©cup√©rer l‚Äô√©chec.\nLambda propose justement les ¬´ destinations ¬ª (Destinations) pour les invocations asynchrones : on peut envoyer automatiquement les √©v√©nements en √©chec vers une cible.\nLa solution correcte est donc de configurer une destination d‚Äô√©chec (onFailure) sur la fonction Lambda.\nEn mettant comme destination l‚ÄôARN du topic SNS ErrorTopic, chaque √©chec de traitement publiera un message dans ErrorTopic.\nEnsuite, vous pouvez vous abonner √† ErrorTopic (email, SMS, HTTP, etc.) pour recevoir les alertes.\nLes autres choix parlent de filtres ou de politiques SNS, mais ils ne capturent pas automatiquement les √©checs d‚Äôex√©cution Lambda : ils filtrent des messages SNS, pas les erreurs internes de Lambda.\nDonc B r√©pond exactement au besoin : relayer les √©checs de Lambda vers ErrorTopic.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine : InputTopic est le micro qui annonce une commande, ProcessMessages est l‚Äô√©l√®ve qui va chercher le plateau, et ErrorTopic est le groupe WhatsApp ‚ÄúProbl√®mes‚Äù o√π on pr√©vient quand √ßa se passe mal.**\n\nConcept : SNS (les ‚Äútopics‚Äù) sert √† diffuser un message √† des abonn√©s, comme une annonce au micro. Lambda est un petit ‚Äúrobot/√©l√®ve‚Äù qui fait une t√¢che quand il re√ßoit l‚Äôannonce. Si la t√¢che rate, il faut un plan B pour pr√©venir.\nPourquoi B : on r√®gle directement sur l‚Äô√©l√®ve ProcessMessages une ‚Äúdestination d‚Äô√©chec‚Äù : si il n‚Äôarrive pas √† traiter une commande, il envoie automatiquement un message dans le groupe WhatsApp ErrorTopic (son ARN = l‚Äôadresse exacte du groupe).\nLes autres choix : A et D essaient de filtrer/param√©trer ErrorTopic comme si ErrorTopic recevait d√©j√† les erreurs, mais personne ne les envoie. C met ErrorTopic comme d√©clencheur, √ßa ferait juste lancer ProcessMessages quand on parle d‚Äôerreurs, pas pr√©venir quand il √©choue.\nDonc B est la bonne r√©ponse : on dit √† ProcessMessages o√π envoyer ses √©checs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:497:1a7975fdd1a365c4",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 497,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an application that uses an AWS Lambda function to transform and load data from an Amazon S3 bucket. When the developer tests the application, the developer finds that some invocations of the Lambda function are slower than others.The developer needs to update the Lambda function to have predictable invocation durations that run with low latency. Any initialization activities, such as loading libraries and instantiating clients, must run during allocation time rather than during actual function invocations.Which combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Create a schedule group in Amazon EventBridge Scheduler to invoke the Lambda function.",
        "B": "Configure provisioned concurrency for the Lambda function to have the necessary number of execution environments.",
        "C": "Use the $LATEST version of the Lambda function.",
        "D": "Configure reserved concurrency for the Lambda function to have the necessary number of execution environments.",
        "E": "Deploy changes, and publish a new version of the Lambda function."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156665-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:15 a.m.",
      "textHash": "1a7975fdd1a365c4",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Le probl√®me vient des ¬´ cold starts ¬ª : parfois AWS Lambda doit cr√©er un nouvel environnement d‚Äôex√©cution, ce qui ajoute du temps (chargement des biblioth√®ques, cr√©ation de clients AWS, etc.).\nPour avoir une latence faible et surtout pr√©visible, il faut que des environnements soient d√©j√† pr√™ts avant l‚Äôappel.\nLa ¬´ provisioned concurrency ¬ª (concurrence provisionn√©e) garde un nombre d√©fini d‚Äôenvironnements Lambda pr√©-initialis√©s et chauds.\nAinsi, l‚Äôinitialisation se fait au moment de l‚Äôallocation (quand AWS pr√©pare ces environnements), pas pendant l‚Äôinvocation r√©elle.\nC‚Äôest exactement ce que demande l‚Äô√©nonc√© : ex√©cutions r√©guli√®res, faible latence, et init hors du chemin critique.\nLes autres options ne garantissent pas cela : une planification (EventBridge) ne rend pas l‚Äôex√©cution pr√©visible, et la reserved concurrency limite surtout le nombre d‚Äôex√©cutions mais ne pr√©chauffe pas.\nDonc la bonne action est de configurer la provisioned concurrency (B).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : parfois tu arrives et ton plateau est d√©j√† pr√™t, parfois il faut d‚Äôabord sortir les plateaux, allumer les machines et pr√©parer les couverts.**\n\nIci, la fonction AWS Lambda, c‚Äôest comme un ‚Äú√©l√®ve-cuisinier‚Äù qui pr√©pare un plat √† chaque commande.\nQuand c‚Äôest lent, c‚Äôest souvent parce qu‚Äôil doit d‚Äôabord ‚Äúouvrir la cuisine‚Äù (charger des outils, pr√©parer les ustensiles) avant de cuisiner : √ßa fait une attente impr√©visible.\nLa solution B (provisioned concurrency), c‚Äôest payer pour garder un certain nombre de cuisines d√©j√† ouvertes et pr√™tes.\nDonc quand une commande arrive, on cuisine tout de suite : temps plus r√©gulier et faible latence.\nEt surtout, les pr√©paratifs (charger les biblioth√®ques, pr√©parer les clients) se font pendant que la cuisine est mise en place, pas au moment de servir.\nLes autres choix ne garantissent pas des cuisines d√©j√† chaudes : ils planifient, limitent, ou g√®rent des versions, mais ne suppriment pas l‚Äôattente de d√©marrage.\nDonc la bonne r√©ponse est B.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:496:7142951817e96320",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 496,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company offers a business-to-business software service that runs on dedicated infrastructure deployed in each customer‚Äôs AWS account. Before a feature release, the company needs to run integration tests on real AWS test infrastructure. The test infrastructure consists of Amazon EC2 instances and an Amazon RDS database.A developer must set up a continuous delivery process that will provision the test infrastructure across the different AWS accounts. The developer then must run the integration tests.Which solution will meet these requirements with the LEAST administrative effort?",
      "choices": {
        "A": "Use AWS CodeDeploy with AWS CloudFormation StackSets to deploy the infrastructure. Use Amazon CodeGuru to run the tests.",
        "B": "Use AWS CodePipeline with AWS CloudFormation StackSets to deploy the infrastructure. Use AWS CodeBuild to run the tests.",
        "C": "Use AWS CodePipeline with AWS CloudFormation change sets to deploy the infrastructure. Use a CloudFormation custom resource to run the tests.",
        "D": "Use AWS Serverless Application Model (AWS SAM) templates with AWS CloudFormation change sets to deploy the infrastructure. Use AWS CodeDeploy to run the tests."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153523-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 27, 2024, 10:24 p.m.",
      "textHash": "7142951817e96320",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici, il faut d√©ployer automatiquement une ‚Äúinfrastructure de test‚Äù (EC2 + RDS) dans plusieurs comptes AWS, puis lancer des tests d‚Äôint√©gration, avec le moins d‚Äôadministration possible.\nAWS CodePipeline est un service qui orchestre un workflow de livraison continue (√©tapes: d√©ployer, tester, valider).\nAWS CloudFormation permet de d√©crire l‚Äôinfrastructure en ‚Äútemplate‚Äù (Infrastructure as Code). StackSets est la fonction qui d√©ploie le m√™me template dans plusieurs comptes et r√©gions, id√©al pour un mod√®le ‚Äúun client = un compte‚Äù.\nAWS CodeBuild ex√©cute des commandes de build/test dans un environnement g√©r√© (pas besoin de g√©rer des serveurs de test), donc parfait pour lancer des tests d‚Äôint√©gration.\nLa r√©ponse B combine exactement ces besoins: CodePipeline pilote le processus, StackSets provisionne EC2/RDS dans chaque compte, puis CodeBuild lance les tests.\nA est faux car CodeGuru sert surtout √† l‚Äôanalyse de code/performance, pas √† ex√©cuter des tests d‚Äôint√©gration.\nC et D ajoutent de la complexit√© (change sets + custom resource, ou SAM/CodeDeploy) sans avantage pour des tests sur EC2/RDS multi-comptes.\nDonc B est la solution la plus simple √† administrer et la plus adapt√©e au multi-compte.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:495:e93ba9785e61e3e4",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 495,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building an application to accept data from customers. The data must be encrypted at rest and in transit.The application uses an Amazon API Gateway API that resolves to AWS Lambda functions. The Lambda functions store the data in an Amazon Aurora MySQL DB cluster. The application worked properly during testing.A developer configured an Amazon CloudFront distribution with field-level encryption that uses an AWS Key Management Service (AWS KMS) key. After the configuration of the distribution, the application behaved unexpectedly. All the data in the database changed from plaintext to ciphertext.The developer must ensure that the data is not stored in the database as the ciphertext from the CloudFront field-level encryption.Which solution will meet this requirement?",
      "choices": {
        "A": "Change the CloudFront Viewer protocol policy from ‚ÄúHTTP and HTTPS‚Äù to ‚ÄúHTTPS only.‚Äù",
        "B": "Add a Lambda function that uses the KMS key to decrypt the data fields before saving the data to the database.",
        "C": "Enable encryption on the DB cluster by using the same KMS key that is used in CloudFront.",
        "D": "Request and deploy a new SSL certificate to use with the CloudFront distribution."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156664-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2025, 10:12 a.m.",
      "textHash": "e93ba9785e61e3e4",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "CloudFront est un CDN (r√©seau de diffusion) qui peut chiffrer certains champs d‚Äôune requ√™te avec le ‚Äúfield-level encryption‚Äù. Cela chiffre les champs c√¥t√© client/edge avant qu‚Äôils n‚Äôarrivent √† votre API.\nAPI Gateway et Lambda re√ßoivent donc d√©j√† des valeurs chiffr√©es (ciphertext). Si Lambda les enregistre telles quelles dans Aurora MySQL, la base stocke du ciphertext au lieu du texte lisible.\nLe chiffrement ‚Äúau repos‚Äù d‚ÄôAurora (option KMS) prot√®ge les fichiers de la base sur disque, mais ne change pas ce que l‚Äôapplication √©crit dans les colonnes : si vous √©crivez du ciphertext, vous stockez du ciphertext.\nPour retrouver du texte clair en base, il faut d√©chiffrer avant l‚Äôinsertion, dans la logique applicative.\nLa solution est donc d‚Äôajouter dans Lambda une √©tape de d√©chiffrement avec AWS KMS (service qui g√®re des cl√©s et permet encrypt/decrypt) puis de sauvegarder le r√©sultat en base.\nForcer HTTPS, changer de certificat SSL, ou utiliser la m√™me cl√© KMS pour Aurora ne d√©chiffrera pas les champs chiffr√©s par CloudFront.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une cantine o√π les √©l√®ves d√©posent des fiches avec leurs allergies. Pour prot√©ger, ils mettent certaines infos dans une enveloppe scell√©e avant de la donner au surveillant.**\n\nConcept : ‚Äúchiffrer‚Äù = mettre un message dans une enveloppe cod√©e. ‚ÄúEn transit‚Äù = pendant le trajet. ‚ÄúAu repos‚Äù = quand c‚Äôest rang√© dans un placard. CloudFront a commenc√© √† sceller certains champs (field-level encryption) avant qu‚Äôils arrivent √† l‚Äôappli. Du coup, la base de donn√©es (le placard) a rang√© les enveloppes scell√©es telles quelles : elle voit du charabia (ciphertext) au lieu du texte normal. Pour que la base stocke du texte lisible, il faut ouvrir l‚Äôenveloppe juste avant de ranger : ajouter une Lambda qui d√©chiffre avec la cl√© KMS, puis sauvegarde (B). A et D ne font que s√©curiser le trajet (HTTPS/certificat), √ßa n‚Äôouvre pas l‚Äôenveloppe. C chiffre le placard, mais ne transforme pas le charabia en texte normal.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:123:a211a4295f8a7e4f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 123,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company must deploy all its Amazon RDS DB instances by using AWS CloudFormation templates as part of AWS CodePipeline continuous integration and continuous delivery (CI/CD) automation. The primary password for the DB instance must be automatically generated as part of the deployment process.Which solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Create an AWS Lambda-backed CloudFormation custom resource. Write Lambda code that generates a secure string. Return the value of the secure string as a data field of the custom resource response object. Use the CloudFormation Fn::GetAtt intrinsic function to get the value of the secure string. Use the value to create the DB instance.",
        "B": "Use the AWS CodeBuild action of CodePipeline to generate a secure string by using the following AWS CLI command: aws secretsmanager get-random-password. Pass the generated secure string as a CloudFormation parameter with the NoEcho attribute set to true. Use the parameter reference to create the DB instance.",
        "C": "Create an AWS Lambda-backed CloudFormation custom resource. Write Lambda code that generates a secure string. Return the value of the secure string as a data field of the custom resource response object. Use the CloudFormation Fn::GetAtt intrinsic function to get a value of the secure string. Create secrets in AWS Secrets Manager. Use the secretsmanager dynamic reference to use the value stored in the secret to create the DB instance.",
        "D": "Use the AWS::SecretsManager::Secret resource to generate a secure string. Store the secure string as a secret in AWS Secrets Manager. Use the secretsmanager dynamic reference to use the value stored in the secret to create the DB instance."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107059-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 11:54 p.m.",
      "textHash": "a211a4295f8a7e4f",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici, on veut d√©ployer des bases Amazon RDS via CloudFormation (outil qui d√©crit l‚Äôinfrastructure en ‚Äútemplate‚Äù) dans un pipeline CI/CD (CodePipeline), et le mot de passe doit √™tre cr√©√© automatiquement.\nLa solution la plus simple est d‚Äôutiliser AWS Secrets Manager, un service fait pour stocker et g√©n√©rer des mots de passe/secret de fa√ßon s√©curis√©e.\nAvec CloudFormation, la ressource AWS::SecretsManager::Secret peut g√©n√©rer un mot de passe al√©atoire toute seule (pas de code √† √©crire).\nEnsuite, CloudFormation peut r√©cup√©rer ce secret au moment du d√©ploiement via une ‚Äúdynamic reference‚Äù secretsmanager, et l‚Äôutiliser comme mot de passe du DB RDS.\nCela √©vite de coder une Lambda (options A et C) et donc r√©duit le d√©veloppement et la maintenance.\nCela √©vite aussi de g√©n√©rer le mot de passe dans CodeBuild et de le passer en param√®tre (option B), ce qui est plus fragile et moins propre c√¥t√© s√©curit√©/gestion.\nDonc D r√©pond aux exigences (g√©n√©ration automatique + d√©ploiement CloudFormation dans CI/CD) avec le moins d‚Äôeffort.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois donner un casier √† chaque nouvel √©l√®ve, et le mot de passe du cadenas doit √™tre cr√©√© automatiquement, sans que toi tu l‚Äôinventes √† la main.**\n\nConcept : CloudFormation, c‚Äôest comme une fiche d‚Äôinscription qui d√©crit tout ce qu‚Äôil faut cr√©er. La base de donn√©es (RDS), c‚Äôest le casier. Le mot de passe doit √™tre g√©n√©r√© tout seul pendant l‚Äôinscription.\nPourquoi D : avec D, tu ajoutes sur la fiche un ‚Äúcoffre-fort de l‚Äô√©cole‚Äù (Secrets Manager) qui sait fabriquer un mot de passe au hasard tout seul. Puis la fiche dit au casier : ‚Äúprends le mot de passe directement dans le coffre-fort‚Äù.\nC‚Äôest le moins d‚Äôeffort car tu n‚Äô√©cris aucun code sp√©cial.\nA et C demandent d‚Äô√©crire un petit programme (Lambda) juste pour inventer le mot de passe.\nB demande de rajouter une √©tape de fabrication dans la cha√Æne (CodeBuild) et de passer le mot de passe comme un param√®tre.\nDonc D est le plus simple : la fiche cr√©e le coffre, le coffre g√©n√®re, et le casier utilise.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:122:dd69b39dba3f0a19",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 122,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is developing an application that uses signed requests (Signature Version 4) to call other AWS services. The developer has created a canonical request, has created the string to sign, and has calculated signing information.Which methods could the developer use to complete a signed request? (Choose two.)",
      "choices": {
        "A": "Add the signature to an HTTP header that is named Authorization.",
        "B": "Add the signature to a session cookie.",
        "C": "Add the signature to an HTTP header that is named Authentication.",
        "D": "Add the signature to a query string parameter that is named X-Amz-Signature.",
        "E": "Add the signature to an HTTP header that is named WWW-Authenticate."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107057-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 11:33 p.m.",
      "textHash": "dd69b39dba3f0a19",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:8a8808d5",
      "frExplanation": "Avec AWS Signature Version 4 (SigV4), on prouve √† AWS que la requ√™te vient bien de vous en ajoutant une ‚Äúsignature‚Äù calcul√©e √† partir de la requ√™te et de vos cl√©s.\nUne fois la signature calcul√©e, AWS accepte deux fa√ßons standard de l‚Äôenvoyer :\n1) Dans l‚Äôen-t√™te HTTP Authorization : c‚Äôest le format classique ‚ÄúAuthorization: AWS4-HMAC-SHA256 ‚Ä¶ Signature=‚Ä¶‚Äù.\n2) Dans l‚ÄôURL (query string) pour une requ√™te ‚Äúpr√©-sign√©e‚Äù : le param√®tre s‚Äôappelle X-Amz-Signature.\nLes cookies de session ne font pas partie du m√©canisme SigV4.\nLes en-t√™tes ‚ÄúAuthentication‚Äù et ‚ÄúWWW-Authenticate‚Äù ne sont pas utilis√©s pour SigV4 (WWW-Authenticate sert plut√¥t aux d√©fis d‚Äôauthentification HTTP).\nDonc les bonnes m√©thodes sont A et D.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:106:d3286708fa3cae02",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 106,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is updating an application to move the backend of the application from Amazon EC2 instances to a serverless model. The application uses an Amazon RDS for MySQL DB instance and runs in a single VPC on AWS. The application and the DB instance are deployed in a private subnet in the VPC.The company needs to connect AWS Lambda functions to the DB instance.Which solution will meet these requirements?",
      "choices": {
        "A": "Create Lambda functions inside the VPC with the AWSLambdaBasicExecutionRole policy attached to the Lambda execution role. Modify the RDS security group to allow inbound access from the Lambda security group.",
        "B": "Create Lambda functions inside the VPC with the AWSLambdaVPCAccessExecutionRole policy attached to the Lambda execution role. Modify the RDS security group to allow inbound access from the Lambda security group.",
        "C": "Create Lambda functions with the AWSLambdaBasicExecutionRole policy attached to the Lambda execution role. Create an interface VPC endpoint for the Lambda functions. Configure the interface endpoint policy to allow the lambda:InvokeFunclion action for each Lambda function's Amazon Resource Name (ARN).",
        "D": "Create Lambda functions with the AWSLambdaVPCAccessExecutionRole policy attached to the Lambda execution role. Create an interface VPC endpoint for the Lambda functions. Configure the interface endpoint policy to allow the lambda:InvokeFunction action for each Lambda function's Amazon Resource Name (ARN)."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106992-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 4:40 p.m.",
      "textHash": "d3286708fa3cae02",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "La base RDS MySQL est dans un sous-r√©seau priv√© d‚Äôun VPC : elle n‚Äôest pas accessible depuis Internet.\nPar d√©faut, une fonction AWS Lambda n‚Äôest pas ‚Äúdans‚Äù votre VPC, donc elle ne peut pas joindre directement une ressource priv√©e comme RDS.\nIl faut donc configurer la Lambda pour s‚Äôex√©cuter dans le VPC (choisir les sous-r√©seaux priv√©s et un Security Group).\nQuand une Lambda est dans un VPC, elle a besoin de cr√©er et g√©rer des interfaces r√©seau (ENI) pour avoir une adresse IP dans le sous-r√©seau.\nLa politique IAM AWSLambdaVPCAccessExecutionRole donne justement les droits n√©cessaires √† Lambda pour g√©rer ces ENI.\nEnsuite, c√¥t√© s√©curit√© r√©seau, le Security Group de RDS doit autoriser en entr√©e le Security Group de la Lambda (port MySQL 3306).\nAWSLambdaBasicExecutionRole ne suffit pas : elle sert surtout aux logs CloudWatch, pas √† l‚Äôacc√®s VPC.\nUn VPC endpoint ‚ÄúLambda‚Äù sert √† appeler l‚ÄôAPI Lambda depuis un VPC, pas √† permettre √† Lambda d‚Äôacc√©der √† RDS. Donc B est la bonne r√©ponse.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une √©cole. La base de donn√©es (RDS) est un coffre dans une salle ferm√©e √† cl√© (sous-r√©seau priv√©). Les fonctions Lambda sont des √©l√®ves ‚Äúmessagers‚Äù qui doivent aller chercher/d√©poser des infos dans ce coffre, sans sortir de l‚Äô√©cole.**\n\nConcept : si le coffre est dans une salle priv√©e, le messager doit avoir un badge sp√©cial pour entrer dans les couloirs priv√©s de l‚Äô√©cole (acc√®s VPC). Sinon, il reste dehors et ne peut pas atteindre la salle.\nPourquoi B : en cr√©ant Lambda ‚Äúdans la VPC‚Äù, tu mets le messager √† l‚Äôint√©rieur de l‚Äô√©cole. Le r√¥le AWSLambdaVPCAccessExecutionRole, c‚Äôest le badge qui lui permet d‚Äôutiliser les couloirs priv√©s (r√©seau priv√©) pour atteindre la salle.\nEnsuite, on r√®gle la ‚Äúliste des personnes autoris√©es‚Äù sur la porte du coffre (security group de RDS) pour accepter les messagers (security group de Lambda).\nA est faux : AWSLambdaBasicExecutionRole, c‚Äôest juste un badge pour √©crire dans le cahier de surveillance (logs), pas pour entrer dans les couloirs priv√©s.\nC et D parlent d‚Äôune ‚Äúporte sp√©ciale‚Äù pour appeler Lambda depuis la VPC, mais ici le besoin est l‚Äôinverse : Lambda doit aller vers la base dans le priv√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:27:58144ef4d10a5061",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 27,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer wants to expand an application to run in multiple AWS Regions. The developer wants to copy Amazon Machine Images (AMIs) with the latest changes and create a new application stack in the destination Region. According to company requirements, all AMIs must be encrypted in all Regions. However, not all the AMIs that the company uses are encrypted.How can the developer expand the application to run in the destination Region while meeting the encryption requirement?",
      "choices": {
        "A": "Create new AMIs, and specify encryption parameters. Copy the encrypted AMIs to the destination Region. Delete the unencrypted AMIs.",
        "B": "Use AWS Key Management Service (AWS KMS) to enable encryption on the unencrypted AMIs. Copy the encrypted AMIs to the destination Region.",
        "C": "Use AWS Certificate Manager (ACM) to enable encryption on the unencrypted AMIs. Copy the encrypted AMIs to the destination Region.",
        "D": "Copy the unencrypted AMIs to the destination Region. Enable encryption by default in the destination Region."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102901-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 17, 2023, 9:48 a.m.",
      "textHash": "58144ef4d10a5061",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Une AMI (Amazon Machine Image) est un ‚Äúmod√®le‚Äù de machine virtuelle EC2. Pour la r√©utiliser dans une autre R√©gion AWS, on la copie ou on en recr√©e une.\nL‚Äôexigence dit : toutes les AMI doivent √™tre chiffr√©es dans toutes les R√©gions. Or certaines AMI actuelles ne le sont pas.\nOn ne peut pas ‚Äúactiver le chiffrement‚Äù directement sur une AMI d√©j√† non chiffr√©e : il faut repartir d‚Äôun snapshot/volume et cr√©er une nouvelle AMI chiffr√©e.\nLa bonne approche est donc de cr√©er de nouvelles AMI en demandant le chiffrement (avec une cl√© KMS), puis de copier ces AMI chiffr√©es vers la R√©gion cible.\nEnsuite, on supprime les anciennes AMI non chiffr√©es pour rester conforme.\nB est faux car KMS fournit les cl√©s, mais ne peut pas convertir une AMI non chiffr√©e en place.\nC est faux car ACM g√®re des certificats TLS, pas le chiffrement des disques/AMI.\nD est insuffisant : ‚Äúchiffrement par d√©faut‚Äù aide pour de nouveaux volumes, mais ne chiffre pas automatiquement une AMI non chiffr√©e d√©j√† copi√©e.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tes AMIs sont des ‚Äúphotocopies‚Äù d‚Äôun cahier de cours complet (le PC + l‚Äôappli). Tu veux envoyer ce cahier dans un autre lyc√©e (une autre R√©gion). R√®gle de l‚Äô√©cole : dans chaque lyc√©e, tous les cahiers doivent √™tre dans une pochette ferm√©e √† cl√© (chiffr√©s).**\n\nConcept : une AMI, c‚Äôest un mod√®le pr√™t √† l‚Äôemploi pour recr√©er un ‚Äúordi + appli‚Äù. Chiffrer, c‚Äôest mettre le mod√®le dans une pochette verrouill√©e pour que personne ne lise si on le vole.\nPourquoi A : si certains cahiers sont sans pochette (non chiffr√©s), tu ne peux pas juste ‚Äúajouter un cadenas‚Äù sur la photocopie d√©j√† distribu√©e. Tu dois refaire une nouvelle photocopie en la mettant directement dans une pochette verrouill√©e (cr√©er une nouvelle AMI en pr√©cisant le chiffrement).\nEnsuite tu envoies ces cahiers d√©j√† verrouill√©s dans l‚Äôautre lyc√©e (copier l‚ÄôAMI chiffr√©e vers l‚Äôautre R√©gion).\nEt tu jettes les anciennes copies non prot√©g√©es (supprimer les AMIs non chiffr√©es) pour respecter la r√®gle partout.\nB et C parlent d‚Äôoutils qui ne ‚Äútransforment‚Äù pas magiquement une vieille photocopie non prot√©g√©e en photocopie prot√©g√©e.\nD copie d‚Äôabord sans pochette, donc tu casses la r√®gle pendant le transfert.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:24:9106fb3f8f155d97",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 24,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company wants to deploy and maintain static websites on AWS. Each website's source code is hosted in one of several version control systems, including AWS CodeCommit, Bitbucket, and GitHub.The company wants to implement phased releases by using development, staging, user acceptance testing, and production environments in the AWS Cloud. Deployments to each environment must be started by code merges on the relevant Git branch. The company wants to use HTTPS for all data exchange. The company needs a solution that does not require servers to run continuously.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Host each website by using AWS Amplify with a serverless backend. Conned the repository branches that correspond to each of the desired environments. Start deployments by merging code changes to a desired branch.",
        "B": "Host each website in AWS Elastic Beanstalk with multiple environments. Use the EB CLI to link each repository branch. Integrate AWS CodePipeline to automate deployments from version control code merges.",
        "C": "Host each website in different Amazon S3 buckets for each environment. Configure AWS CodePipeline to pull source code from version control. Add an AWS CodeBuild stage to copy source code to Amazon S3.",
        "D": "Host each website on its own Amazon EC2 instance. Write a custom deployment script to bundle each website's static assets. Copy the assets to Amazon EC2. Set up a workflow to run the script when code is merged."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103646-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2023, 8:22 a.m.",
      "textHash": "9106fb3f8f155d97",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Un site statique = seulement des fichiers (HTML/CSS/JS), donc pas besoin de serveurs qui tournent en permanence.\nAWS Amplify est un service g√©r√© qui h√©berge des sites statiques et se connecte directement √† des d√©p√¥ts Git (CodeCommit, GitHub, Bitbucket).\nAmplify peut associer une branche Git √† un environnement (dev, staging, UAT, prod) : chaque branche d√©clenche automatiquement un build et un d√©ploiement quand on fusionne du code.\nAmplify fournit aussi HTTPS automatiquement (certificat et distribution), donc toutes les donn√©es passent en s√©curis√© sans configuration complexe.\nC‚Äôest ‚Äúserverless‚Äù c√¥t√© h√©bergement : pas d‚ÄôEC2 √† g√©rer, pas de patching, pas de capacit√© √† dimensionner.\nLes autres options ajoutent plus d‚Äôop√©rations : Elastic Beanstalk et EC2 impliquent des environnements/instances √† maintenir, et S3+CodePipeline+CodeBuild demande plus de configuration et de maintenance du pipeline.\nDonc Amplify (A) r√©pond √† la fois aux branches/environnements, aux d√©ploiements sur merge, √† HTTPS, et minimise l‚Äôeffort op√©rationnel.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:521:2d013eab1d7d828e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 521,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is hosting an Amazon AP! Gateway REST API that calls a single AWS Lambda function. The function is infrequently invoked by multiple clients at the same time.The code performance is optimal, but the company wants to optimize the startup time of the functionWhat can a developer do to optimize the initialization of the function?",
      "choices": {
        "A": "Enable API Gateway caching for the REST API.",
        "B": "Configure provisioned concurrency for the Lambda function.",
        "C": "Use Lambda proxy integration for the REST API.",
        "D": "Configure AWS Global Accelerator for the Lambda function."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153503-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 27, 2024, 4:16 p.m.",
      "textHash": "2d013eab1d7d828e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:76c7d5c7",
      "frExplanation": "Ici, l‚ÄôAPI REST Amazon API Gateway appelle une fonction AWS Lambda. Lambda est un code qui s‚Äôex√©cute ‚Äú√† la demande‚Äù sans serveur √† g√©rer.\nQuand la fonction est peu appel√©e, AWS peut devoir cr√©er un nouvel environnement d‚Äôex√©cution au moment de l‚Äôappel : c‚Äôest le ‚Äúcold start‚Äù (temps de d√©marrage/initialisation).\nLe probl√®me d√©crit n‚Äôest pas la vitesse du code une fois lanc√©, mais le temps de d√©marrage quand plusieurs clients appellent en m√™me temps.\nLa ‚Äúprovisioned concurrency‚Äù (concurrence provisionn√©e) garde √† l‚Äôavance un certain nombre d‚Äôinstances Lambda d√©j√† d√©marr√©es et pr√™tes.\nAinsi, les requ√™tes arrivent sur des environnements d√©j√† initialis√©s, ce qui r√©duit fortement le cold start.\nLe cache API Gateway (A) sert surtout √† √©viter d‚Äôappeler Lambda en renvoyant une r√©ponse d√©j√† calcul√©e, pas √† acc√©l√©rer l‚Äôinitialisation de Lambda.\nLambda proxy integration (C) change le format de requ√™te/r√©ponse entre API Gateway et Lambda, pas le temps de d√©marrage.\nGlobal Accelerator (D) optimise le routage r√©seau mondial, pas l‚Äôinitialisation interne de Lambda.\nDonc la meilleure action pour optimiser le startup time est de configurer la provisioned concurrency (B).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : quand personne ne vient, la cuisine est √©teinte. Si 10 √©l√®ves arrivent d‚Äôun coup, il faut d‚Äôabord allumer les plaques et sortir les ingr√©dients avant de servir.**\n\nIci, la ‚Äúfonction‚Äù (Lambda) est comme la cuisine : elle peut √™tre appel√©e rarement, mais parfois plusieurs personnes arrivent en m√™me temps. Le probl√®me n‚Äôest pas la recette (le code est d√©j√† rapide), c‚Äôest le temps de d√©marrage quand la cuisine √©tait √©teinte (initialisation). La ‚Äúprovisioned concurrency‚Äù (B) revient √† garder quelques cuisiniers et postes de cuisine d√©j√† pr√™ts et chauds, m√™me s‚Äôil n‚Äôy a pas de clients. Du coup, quand plusieurs clients arrivent, on sert tout de suite, sans attendre l‚Äôallumage. A (cache) garde des plats d√©j√† faits, mais ne r√©duit pas le temps d‚Äôallumer la cuisine pour une nouvelle commande. C change juste la fa√ßon de passer la commande, pas la pr√©paration. D aide surtout √† aller plus vite sur la route, pas √† d√©marrer la cuisine.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:523:e787e7be3f89da9c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 523,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has an application that uses AWS Security Token Service (AWS STS). The application calls the STS AssumeRole API operation to provide trusted users with temporary security credentials. The application calls AWS STS at the service's default endpoint: https://sts.amazonaws.com.The application is deployed in an Asia Pacific AWS Region. The application is experiencing errors that are related to intermittent latency when the application calls AWS STS.What should the developer do to resolve this issue?",
      "choices": {
        "A": "Update the application to use the GetSessionToken API operation.",
        "B": "Update the application to use the AssumeRoleWithSAML API operation.",
        "C": "Update the application to use a Regional STS endpoint that is closer to the application deployment.",
        "D": "Update the application to use the AssumeRoleWithWebldentity API operation. Move the STS endpoint to a global endpoint."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156021-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 6, 2025, 9:41 a.m.",
      "textHash": "e787e7be3f89da9c",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "AWS STS (Security Token Service) sert √† obtenir des identifiants temporaires (cl√©s) via des appels comme AssumeRole.\nIci, l‚Äôapplication appelle l‚Äôendpoint par d√©faut https://sts.amazonaws.com, qui est un endpoint ¬´ global ¬ª.\nComme l‚Äôapplication est d√©ploy√©e en Asie-Pacifique, appeler un endpoint global peut parfois impliquer un trajet r√©seau plus long, donc plus de latence et des erreurs intermittentes.\nLa solution logique est de r√©duire la distance r√©seau en appelant un endpoint STS r√©gional (par ex. sts.ap-southeast-1.amazonaws.com selon la r√©gion).\nCela garde la m√™me fonctionnalit√© (AssumeRole) mais am√©liore la stabilit√© et les temps de r√©ponse.\nLes autres choix changent le type d‚Äôauthentification (SAML, Web Identity) ou l‚ÄôAPI (GetSessionToken) sans traiter la cause principale: la latence r√©seau vers l‚Äôendpoint.\nDonc il faut utiliser un endpoint STS r√©gional plus proche du d√©ploiement.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:522:8b826d789dd7cb47",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 522,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a three-tier application with an Application Load Balancer (ALB), Amazon EC2 instances, and Amazon RDS. There is an alias record in Amazon Route 53 that points to the ALB. When the developer tries to access the ALB from a laptop, the request times out.Which logs should the developer investigate to verify that the request is reaching the AWS network?",
      "choices": {
        "A": "VPC Flow Logs",
        "B": "Amazon Route 53 logs",
        "C": "AWS Systems Manager Agent logs",
        "D": "Amazon CloudWatch agent logs"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156020-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 6, 2025, 9:38 a.m.",
      "textHash": "8b826d789dd7cb47",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Ici, la question est : est-ce que la requ√™te de votre laptop arrive bien dans le r√©seau AWS (au niveau du VPC) ?\nLes VPC Flow Logs sont des journaux r√©seau : ils enregistrent les m√©tadonn√©es des connexions (IP source/destination, port, protocole, action ACCEPT/REJECT) qui traversent les interfaces r√©seau dans votre VPC.\nDonc, si la requ√™te atteint AWS, vous verrez un flux correspondant vers l‚ÄôALB (ou ses interfaces) et vous saurez si c‚Äôest accept√© ou bloqu√© (ex. Security Group/NACL).\nLes logs Route 53 concernent surtout les requ√™tes DNS (r√©solution de nom), pas le trafic HTTP/HTTPS r√©el : ils ne prouvent pas que la connexion r√©seau arrive.\nLes logs Systems Manager Agent servent √† la gestion des instances (patch, commandes), sans lien direct avec l‚Äôarriv√©e d‚Äôune requ√™te web.\nLes logs CloudWatch Agent concernent des m√©triques/logs syst√®me de l‚Äôinstance (CPU, m√©moire, fichiers), pas le chemin r√©seau jusqu‚Äô√† l‚ÄôALB.\nDonc, pour v√©rifier que la requ√™te ‚Äúentre‚Äù bien dans AWS, le meilleur choix est VPC Flow Logs.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:508:114346d214a243db",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 508,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is troubleshooting the permissions of an application that needs to make changes to an Amazon RDS database. The developer has access to the IAM role that the application is using.Which command structure should the developer use to test the role permissions?",
      "choices": {
        "A": "aws sts assume-role",
        "B": "aws iam attach-role-policy",
        "C": "aws ssm resume-session",
        "D": "aws rds add-role-to-db-cluster"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156019-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 6, 2025, 9:20 a.m.",
      "textHash": "114346d214a243db",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Ici, l‚Äôapplication utilise un r√¥le IAM (Identity and Access Management) pour obtenir des autorisations AWS. Pour v√©rifier si ce r√¥le permet bien de modifier une base Amazon RDS (service de base de donn√©es g√©r√©), il faut ‚Äúse mettre √† la place‚Äù de l‚Äôapplication. La commande aws sts assume-role sert justement √† demander √† AWS STS (Security Token Service) des identifiants temporaires en empruntant ce r√¥le. Ensuite, avec ces identifiants temporaires, le d√©veloppeur peut ex√©cuter des commandes RDS et voir si elles sont autoris√©es ou refus√©es (AccessDenied). Les autres choix ne testent pas les permissions : attach-role-policy modifie le r√¥le (dangereux en d√©pannage), resume-session concerne AWS Systems Manager (sessions sur des machines), et add-role-to-db-cluster attache un r√¥le √† un cluster RDS (action de configuration, pas un test). Donc la bonne structure de commande pour tester les permissions du r√¥le est d‚Äôassumer le r√¥le via STS.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine l‚Äô√©cole : une appli, c‚Äôest un √©l√®ve. Un ‚Äúr√¥le IAM‚Äù, c‚Äôest un badge qui dit ce que l‚Äô√©l√®ve a le droit de faire (entrer au labo, modifier un cahier, etc.). La base de donn√©es RDS, c‚Äôest le cahier de notes du prof : tr√®s prot√©g√©.**\n\nConcept : pour v√©rifier des permissions, tu dois ‚Äúte mettre dans la peau‚Äù de celui qui agit, comme si tu empruntais son badge.\nCommande A (aws sts assume-role) = tu empruntes temporairement le badge du r√¥le pour tester si tu peux vraiment modifier le cahier de notes.\nC‚Äôest exactement ce qu‚Äôon veut : tester les droits du r√¥le utilis√© par l‚Äôappli.\nB (attach-role-policy) = tu ajoutes des droits au badge : ce n‚Äôest pas un test, tu changes les r√®gles.\nC (resume-session) = reprendre une session de connexion : rien √† voir avec v√©rifier des droits.\nD (add-role-to-db-cluster) = donner un r√¥le √† la base : tu modifies la config, pas un test.\nDonc A est la bonne r√©ponse : tu ‚Äúassumes‚Äù le r√¥le pour v√©rifier ses permissions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:504:283ac441f3d4e832",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 504,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on a new authorization mechanism for an application. The developer must create an Amazon API Gateway API and must test JSON Web Token (JWT) authorization on the API.The developer must use the built-in authorizer and must avoid managing the code with custom logic. The developer needs to define an API route that is available at /auth to test the authorizer configuration.Which solution will meet these requirements?",
      "choices": {
        "A": "Create a WebSocket API and the /auth route. Configure and attach the JWT authorizer to the API. Deploy the API.",
        "B": "Create a WebSocket API and the /auth route. Create and configure an AWS Lambda authorizer. Attach the Lambda authorizer to the API. Deploy the API.",
        "C": "Create an HTTP API and the /auth route. Create and configure an AWS Lambda authorizer. Attach the Lambda authorizer to the /auth route. Deploy the API.",
        "D": "Create an HTTP API and the /auth route. Configure the JWT authorizer. Attach the JWT authorizer to the /auth route. Deploy the API."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152776-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 10, 2024, 4:35 a.m.",
      "textHash": "283ac441f3d4e832",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:7ed04db0",
      "frExplanation": "API Gateway sert √† cr√©er des API (des ‚Äúportes d‚Äôentr√©e‚Äù HTTP) avec des routes comme /auth.\nUn JWT (JSON Web Token) est un jeton sign√© utilis√© pour prouver l‚Äôidentit√©/les droits d‚Äôun utilisateur.\nLe besoin dit : utiliser l‚Äôauthorizer int√©gr√© (built-in) et √©viter de g√©rer du code personnalis√©.\nDans API Gateway, l‚Äôauthorizer JWT int√©gr√© existe pour les HTTP API (pas besoin de Lambda).\nUn authorizer Lambda implique d‚Äô√©crire et maintenir du code : cela contredit la contrainte.\nLes WebSocket API sont faites pour des connexions temps r√©el, pas pour un simple test de route HTTP /auth.\nDonc il faut cr√©er une HTTP API, d√©finir la route /auth, configurer un JWT authorizer, puis l‚Äôattacher √† cette route.\nEnfin, d√©ployer l‚ÄôAPI pour pouvoir appeler /auth et v√©rifier que le JWT est bien valid√©.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine l‚Äôentr√©e du lyc√©e : il y a une porte (l‚ÄôAPI) et un surveillant (l‚Äôautorisation). Pour entrer, tu montres ton carnet/QR code (le JWT), et le surveillant sait d√©j√† le lire sans que tu inventes tes propres r√®gles.**\n\nConcept : une API, c‚Äôest une porte avec des chemins, comme /auth = ‚Äúporte test‚Äù. Un JWT, c‚Äôest un badge sign√© qui prouve qui tu es. Un ‚Äúauthorizer int√©gr√©‚Äù, c‚Äôest un surveillant d√©j√† form√© pour v√©rifier ce badge, sans que tu √©crives du code.\nPourquoi D : une HTTP API, c‚Äôest une porte simple pour des requ√™tes normales (comme ouvrir une page). Tu cr√©es la route /auth pour tester. Tu configures l‚Äôauthorizer JWT int√©gr√© et tu l‚Äôaccroches √† /auth : le surveillant contr√¥le l‚Äôentr√©e sur cette porte.\nPourquoi pas A/B : WebSocket, c‚Äôest plut√¥t une discussion en continu (comme un chat), pas juste une porte ‚Äúouvre/ferme‚Äù pour tester /auth. Pourquoi pas B/C : ‚ÄúLambda authorizer‚Äù = tu embauches un surveillant perso et tu √©cris ses r√®gles (du code), interdit ici. Donc D.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:551:9f5c3f81b6bcd1bc",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 551,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application includes an Amazon DynamoDB table that is named orders. The table has a primary partition key of id and a global secondary index (GSI) that is named an accountIndex. The GSI has a partition key of accountId and a sort key of orderDateTime.A developer needs to create an AWS Lambda function to retrieve the orders that have an accountId of 100.Which solution will meet this requirement by using the LEAST read capacity?",
      "choices": {
        "A": "Define a DynamoDB API request for the GetItem action with the following parameters:",
        "B": "Define a DynamoDB API request for the BatchGetItem action with the following parameters:",
        "C": "Define a DynamoDB API request for the Scan action with the following parameters:",
        "D": "Define a DynamoDB API request for the Query action with the following parameters:"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/156015-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 6, 2025, 8:22 a.m.",
      "textHash": "9f5c3f81b6bcd1bc",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π lire co√ªte des ‚Äúunit√©s de lecture‚Äù (RCU). Pour consommer le moins possible, il faut √©viter de parcourir toute la table.\nIci, la cl√© primaire de la table est id, donc GetItem ne peut r√©cup√©rer qu‚Äôun seul √©l√©ment si on conna√Æt exactement id (pas accountId).\nBatchGetItem permet de r√©cup√©rer plusieurs √©l√©ments, mais seulement si on conna√Æt d√©j√† toutes les cl√©s primaires (les id). Ce n‚Äôest pas le cas.\nScan lit toute la table (ou une grande partie) puis filtre : c‚Äôest le plus co√ªteux en lecture.\nUn GSI (index secondaire global) sert justement √† chercher par d‚Äôautres cl√©s : ici accountIndex permet de chercher par accountId et trier par orderDateTime.\nL‚Äôaction Query lit uniquement les √©l√©ments qui correspondent √† la cl√© de partition de l‚Äôindex (accountId = 100), donc beaucoup moins de donn√©es.\nDonc la meilleure solution avec le moins de capacit√© de lecture est de faire un Query sur le GSI accountIndex avec la condition accountId = 100.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéí Imagine ton casier au lyc√©e** : tu ranges tes affaires diff√©remment selon comment tu les cherches.\n\n**LSI (Local)** = C'est comme **des onglets DANS ton classeur** üìÅ - Cr√©√©s √† la rentr√©e, jamais modifiables, super rapides (m√™me partition).\n\n**GSI (Global)** = C'est comme **un index √† la fin du livre** üìñ - Ajoutable n'importe quand, cherche partout, mais plus lent.\n\n**üß† Mn√©motechnique :** \"**L**SI = **L**ocal, **L**imit√©\" | \"**G**SI = **G**lobal, **G**√©nial\"\n\n**Quand utiliser GSI :** Quand tu veux chercher par un autre crit√®re que la cl√© principale, sur TOUTE la table.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:138:f1f321d3e4f48f24",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 138,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nUsers are reporting errors in an application. The application consists of several microservices that are deployed on Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.Which combination of steps should a developer take to fix the errors? (Choose two.)",
      "choices": {
        "A": "Deploy AWS X-Ray as a sidecar container to the microservices. Update the task role policy to allow access to the X-Ray API.",
        "B": "Deploy AWS X-Ray as a daemonset to the Fargate cluster. Update the service role policy to allow access to the X-Ray API.",
        "C": "Instrument the application by using the AWS X-Ray SDK. Update the application to use the PutXrayTrace API call to communicate with the X-Ray API.",
        "D": "Instrument the application by using the AWS X-Ray SDK. Update the application to communicate with the X-Ray daemon.",
        "E": "Instrument the ECS task to send the stdout and stderr output to Amazon CloudWatch Logs. Update the task role policy to allow the cloudwatch:PullLogs action."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/117795-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Aug. 10, 2023, 2:07 p.m.",
      "textHash": "f1f321d3e4f48f24",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Ici, on doit diagnostiquer des erreurs dans une appli en microservices sur ECS Fargate (conteneurs g√©r√©s par AWS, sans acc√®s aux serveurs). Pour comprendre o√π √ßa casse entre services, on utilise AWS X-Ray, un service de ‚Äútracing‚Äù qui suit une requ√™te de bout en bout. Sur Fargate, on ne peut pas installer un agent au niveau du cluster comme sur des machines (donc pas de ‚Äúdaemonset‚Äù type Kubernetes) : on ajoute X-Ray dans la m√™me t√¢che ECS en ‚Äúsidecar‚Äù (un conteneur √† c√¥t√© de l‚Äôapp). Ensuite, le conteneur X-Ray doit pouvoir envoyer les traces vers le service X-Ray : il faut donc autoriser l‚Äôacc√®s √† l‚ÄôAPI X-Ray via une policy IAM sur le r√¥le de t√¢che (task role). Les autres propositions parlent d‚Äôun daemon (pas adapt√© √† Fargate) ou d‚Äôappeler directement l‚ÄôAPI depuis l‚Äôapp (pas la pratique recommand√©e : l‚Äôapp parle au daemon/sidecar). Donc la bonne combinaison est : d√©ployer X-Ray en sidecar + donner les permissions IAM n√©cessaires.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine avec plusieurs stands (p√¢tes, burgers, salades). Ton appli, c‚Äôest pareil : plein de petits stands (microservices) qui doivent bien se coordonner. Quand des √©l√®ves se plaignent, tu as besoin d‚Äôun surveillant-espion qui suit le trajet d‚Äôun plateau pour voir o√π √ßa bloque.**\n\nConcept : AWS X-Ray, c‚Äôest ce ‚Äúsurveillant‚Äù qui trace le parcours d‚Äôune demande (comme un plateau) √† travers tous les stands.\nPour que √ßa marche, il faut 2 choses : (1) que chaque stand soit ‚Äú√©quip√©‚Äù pour √™tre observ√©, (2) qu‚Äôil ait le droit d‚Äôenvoyer ses infos au surveillant.\nR√©ponse A : ‚Äúsidecar container‚Äù = tu ajoutes un petit surveillant √† c√¥t√© de chaque stand, dans la m√™me √©quipe, donc il voit tout ce qui se passe.\nEt ‚Äúupdate the task role policy‚Äù = tu donnes l‚Äôautorisation officielle √† ce stand d‚Äôenvoyer les traces √† X-Ray (sinon, il est bloqu√©).\nPourquoi pas B : sur Fargate, tu ne g√®res pas un gros surveillant commun sur les machines comme √ßa.\nPourquoi pas C/D : il manque l‚Äôid√©e cl√© ‚Äúajouter le surveillant √† c√¥t√©‚Äù + les bonnes autorisations au bon endroit.\nDonc A est la bonne combinaison.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:67:65cd912fed9c37ba",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 67,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has installed smart meters in all its customer locations. The smart meters measure power usage at 1-minute intervals and send the usage readings to a remote endpoint for collection. The company needs to create an endpoint that will receive the smart meter readings and store the readings in a database. The company wants to store the location ID and timestamp information.The company wants to give its customers low-latency access to their current usage and historical usage on demand. The company expects demand to increase significantly. The solution must not impact performance or include downtime while scaling.Which solution will meet these requirements MOST cost-effectively?",
      "choices": {
        "A": "Store the smart meter readings in an Amazon RDS database. Create an index on the location ID and timestamp columns. Use the columns to filter on the customers' data.",
        "B": "Store the smart meter readings in an Amazon DynamoDB table. Create a composite key by using the location ID and timestamp columns. Use the columns to filter on the customers' data.",
        "C": "Store the smart meter readings in Amazon ElastiCache for Redis. Create a SortedSet key by using the location ID and timestamp columns. Use the columns to filter on the customers' data.",
        "D": "Store the smart meter readings in Amazon S3. Partition the data by using the location ID and timestamp columns. Use Amazon Athena to filter on the customers' data."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107439-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 25, 2023, 2:39 p.m.",
      "textHash": "65cd912fed9c37ba",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:02596d46",
      "frExplanation": "Ici, on re√ßoit beaucoup de mesures (toutes les minutes) et la demande va fortement augmenter. Il faut donc une base qui s‚Äôadapte automatiquement sans arr√™t de service.\nAmazon DynamoDB est une base NoSQL g√©r√©e par AWS, con√ßue pour de tr√®s gros volumes et une latence faible (r√©ponses rapides), avec mise √† l‚Äô√©chelle sans downtime.\nEn cr√©ant une cl√© composite (partition key = location ID, sort key = timestamp), on peut r√©cup√©rer efficacement l‚Äôhistorique d‚Äôun lieu (toutes les mesures d‚Äôun compteur) tri√© par temps, et aussi la valeur la plus r√©cente.\nC‚Äôest exactement le mod√®le ‚Äúdonn√©es par appareil + s√©rie temporelle‚Äù, tr√®s courant pour l‚ÄôIoT.\nAmazon RDS (SQL) peut fonctionner, mais scaler sans impact est plus complexe et co√ªteux (dimensionnement, index, limites de connexions, op√©rations).\nElastiCache Redis est un cache en m√©moire : rapide mais cher pour stocker tout l‚Äôhistorique durablement, et ce n‚Äôest pas une base principale.\nS3 + Athena est √©conomique pour l‚Äôanalytique, mais pas id√©al pour un acc√®s ‚Äú√† la demande‚Äù en faible latence sur des donn√©es r√©centes.\nDonc DynamoDB avec cl√© composite est le choix le plus cost-effective et scalable pour lecture/√©criture rapides.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine o√π chaque √©l√®ve d√©pose un ticket toutes les minutes avec son num√©ro d‚Äô√©l√®ve (location ID) et l‚Äôheure (timestamp). Les parents veulent voir tout de suite ce que leur enfant a mang√© aujourd‚Äôhui, et aussi l‚Äôhistorique, m√™me si demain il y a 10√ó plus d‚Äô√©l√®ves.**\n\nLe concept : il faut un ‚Äúcahier‚Äù qui accepte √©norm√©ment de tickets tr√®s vite, et qui peut grandir sans fermer la cantine (pas de pause, pas de ralentissement).\nAvec DynamoDB (B), c‚Äôest comme un cahier ultra-rapide fait pour avaler plein de tickets et s‚Äôagrandir automatiquement.\nLa ‚Äúcl√© composite‚Äù (location ID + timestamp), c‚Äôest comme classer chaque ticket par √©l√®ve puis par heure : retrouver ‚Äúl‚Äô√©l√®ve 123 √† 14:32‚Äù est imm√©diat.\n√áa donne un acc√®s tr√®s rapide (faible latence) au dernier ticket et √† l‚Äôhistorique, √† la demande.\nRDS (A) ressemble plus √† un classeur plus fragile quand il y a trop de monde : √ßa peut ralentir et scaler est plus compliqu√©.\nRedis (C) est plut√¥t un tableau blanc pour du temporaire, pas id√©al pour tout l‚Äôhistorique.\nS3 + Athena (D) c‚Äôest comme stocker en cartons et chercher ensuite : bien pour analyser, moins pour afficher ‚Äútout de suite‚Äù l‚Äôusage actuel.\nDonc B est le plus adapt√© et le plus √©conomique pour beaucoup d‚Äô√©critures + acc√®s rapide + croissance sans coupure.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:32:6bfdf34a2d6cc1f9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 32,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to perform geographic load testing of an API. The developer must deploy resources to multiple AWS Regions to support the load testing of the API.How can the developer meet these requirements without additional application code?",
      "choices": {
        "A": "Create and deploy an AWS Lambda function in each desired Region. Configure the Lambda function to create a stack from an AWS CloudFormation template in that Region when the function is invoked.",
        "B": "Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI create-stack-set command to create a stack set in the desired Regions.",
        "C": "Create an AWS Systems Manager document that defines the resources. Use the document to create the resources in the desired Regions.",
        "D": "Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI deploy command to create a stack from the template in each Region."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103515-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 10:23 p.m.",
      "textHash": "6bfdf34a2d6cc1f9",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:366a6f7c",
      "frExplanation": "Objectif : d√©ployer les m√™mes ressources de test de charge dans plusieurs R√©gions AWS, sans √©crire de code applicatif.\nAWS CloudFormation permet de d√©crire une infrastructure (serveurs, r√¥les, etc.) dans un ¬´ template ¬ª (fichier) et de la cr√©er automatiquement.\nUn ¬´ stack ¬ª = un d√©ploiement CloudFormation dans une R√©gion. Le probl√®me : le faire proprement dans plusieurs R√©gions.\nCloudFormation StackSets est con√ßu exactement pour √ßa : d√©ployer et g√©rer le m√™me stack dans plusieurs comptes et/ou plusieurs R√©gions depuis une seule commande.\nAvec la commande AWS CLI create-stack-set, on d√©finit le StackSet puis on cible les R√©gions voulues : CloudFormation cr√©e les stacks partout.\nC‚Äôest sans code suppl√©mentaire, reproductible, et facile √† mettre √† jour ou supprimer ensuite.\nA ajoute une couche inutile (Lambda + logique de d√©ploiement). C n‚Äôest pas l‚Äôoutil standard pour ‚Äúd√©ployer une infra‚Äù multi-R√©gion. D oblige √† lancer un d√©ploiement s√©par√© par R√©gion (moins adapt√© que StackSets).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois installer exactement le m√™me stand de jeu (m√™mes consoles, m√™mes r√®gles) dans plusieurs gymnases de villes diff√©rentes, pour tester combien de joueurs peuvent jouer en m√™me temps.**\n\nConcept : tu veux copier-coller la m√™me ‚Äúinstallation‚Äù dans plusieurs endroits, sans r√©√©crire les r√®gles √† chaque fois.\nDans AWS, une ‚ÄúR√©gion‚Äù = une ville diff√©rente, et un ‚Äútemplate CloudFormation‚Äù = la fiche de montage du stand (liste de tout ce qu‚Äôil faut installer).\nLa bonne r√©ponse est B car un ‚Äústack set‚Äù = un chef d‚Äô√©quipe qui lit UNE SEULE fiche de montage et installe automatiquement le m√™me stand dans plusieurs villes en une commande.\nDonc tu d√©ploies les ressources de test dans plusieurs R√©gions sans ajouter de code √† ton appli.\nA ajoute une √©tape avec une fonction (comme embaucher un √©l√®ve pour relire la fiche), inutile.\nC n‚Äôest pas fait pour cloner facilement des installations compl√®tes partout.\nD te force √† refaire la commande R√©gion par R√©gion (comme refaire le montage ville par ville √† la main).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:137:e6775d4bd93b2b0c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 137,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is developing a serverless multi-tier application on AWS. The company will build the serverless logic tier by using Amazon API Gateway and AWS Lambda.While the company builds the logic tier, a developer who works on the frontend of the application must develop integration tests. The tests must cover both positive and negative scenarios, depending on success and error HTTP status codes.Which solution will meet these requirements with the LEAST effort?",
      "choices": {
        "A": "Set up a mock integration for API methods in API Gateway. In the integration request from Method Execution, add simple logic to return either a success or error based on HTTP status code. In the integration response, add messages that correspond to the HTTP status codes.",
        "B": "Create two mock integration resources for API methods in API Gateway. In the integration request, return a success HTTP status code for one resource and an error HTTP status code for the other resource. In the integration response, add messages that correspond to the HTTP status codes.",
        "C": "Create Lambda functions to perform tests. Add simple logic to return either success or error, based on the HTTP status codes. Build an API Gateway Lambda integration. Select appropriate Lambda functions that correspond to the HTTP status codes.",
        "D": "Create a Lambda function to perform tests. Add simple logic to return either success or error-based HTTP status codes. Create a mock integration in API Gateway. Select the Lambda function that corresponds to the HTTP status codes."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/117331-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Aug. 4, 2023, 1:38 p.m.",
      "textHash": "e6775d4bd93b2b0c",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:976018a2",
      "frExplanation": "Ici, le backend (la logique) n‚Äôest pas encore pr√™t, mais le d√©veloppeur frontend doit d√©j√† tester l‚Äôint√©gration via des codes HTTP (200, 400, 500, etc.).\nAmazon API Gateway est la ‚Äúporte d‚Äôentr√©e‚Äù HTTP de l‚Äôapplication, et AWS Lambda ex√©cute du code c√¥t√© serveur.\nPour faire des tests sans backend, API Gateway peut simuler une r√©ponse gr√¢ce √† une ‚Äúmock integration‚Äù (r√©ponse fictive).\nLa solution A demande de configurer une seule m√©thode avec une mock integration, puis d‚Äôajouter une petite logique dans la requ√™te d‚Äôint√©gration pour renvoyer succ√®s ou erreur selon le code HTTP voulu.\nEnsuite, on mappe les r√©ponses dans ‚Äúintegration response‚Äù pour retourner les bons messages et statuts.\nC‚Äôest le moins d‚Äôeffort car on ne cr√©e aucune fonction Lambda, aucun d√©ploiement de code, et on garde un seul endpoint √† tester.\nLa B cr√©e deux ressources s√©par√©es (plus de configuration et de maintenance).\nLa C et la D impliquent de cr√©er/d√©ployer des Lambda (plus long et inutile pour de simples r√©ponses de test).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un self au lyc√©e : tu veux tester le parcours ‚Äúje prends mon plateau ‚Üí je passe en caisse ‚Üí je mange‚Äù, mais la cuisine n‚Äôest pas encore construite.**\n\nConcept : pour tester le site (frontend), tu n‚Äôas pas besoin du vrai ‚Äúcuisinier‚Äù (Lambda, le petit programme qui r√©pond). Tu peux faire semblant avec un ‚Äúpanneau‚Äù √† l‚Äôentr√©e qui donne une r√©ponse pr√©vue. API Gateway, c‚Äôest comme le guichet qui re√ßoit les commandes et renvoie une r√©ponse. Une ‚Äúmock integration‚Äù, c‚Äôest un faux guichet qui renvoie des r√©ponses sans appeler la cuisine. Pourquoi A : tu configures un seul faux guichet et tu ajoutes une petite r√®gle : selon le code HTTP (comme un ticket vert = succ√®s, ticket rouge = erreur), il renvoie soit une r√©ussite soit une erreur. √áa couvre les tests positifs et n√©gatifs avec le moins d‚Äôeffort, sans cr√©er plusieurs faux stands (B) ni √©crire des fonctions Lambda juste pour tester (C, D).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:129:fa02f0ebbf301a04",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 129,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Lambda function. The Lambda function will consume messages from an Amazon Simple Queue Service (Amazon SQS) queue. The developer wants to integrate unit testing as part of the function's continuous integration and continuous delivery (CI/CD) process.How can the developer unit test the function?",
      "choices": {
        "A": "Create an AWS CloudFormation template that creates an SQS queue and deploys the Lambda function. Create a stack from the template during the CI/CD process. Invoke the deployed function. Verify the output.",
        "B": "Create an SQS event for tests. Use a test that consumes messages from the SQS queue during the function's Cl/CD process.",
        "C": "Create an SQS queue for tests. Use this SQS queue in the application's unit test. Run the unit tests during the CI/CD process.",
        "D": "Use the aws lambda invoke command with a test event during the CIICD process."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/112424-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 17, 2023, 6:32 a.m.",
      "textHash": "fa02f0ebbf301a04",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici on parle de ¬´ unit test ¬ª : on veut tester uniquement le code de la fonction Lambda, sans d√©pendre d‚Äôinfrastructures r√©elles (SQS, CloudFormation, etc.).\nAWS Lambda est un service qui ex√©cute du code √† la demande. Amazon SQS est une file de messages ; en production, SQS d√©clenche Lambda avec un √©v√©nement (payload) au format SQS.\nPour un unit test en CI/CD, on peut simuler cet √©v√©nement SQS avec un fichier JSON de test et appeler la fonction directement.\nLa commande AWS CLI ¬´ aws lambda invoke ¬ª permet d‚Äôinvoquer une Lambda en lui passant un √©v√©nement de test, puis de v√©rifier la r√©ponse (sortie, logs, code retour).\nC‚Äôest rapide, automatisable dans un pipeline, et ne n√©cessite pas de cr√©er une vraie file SQS.\nA et C cr√©ent des ressources (CloudFormation/SQS) : c‚Äôest plut√¥t de l‚Äôint√©gration/end-to-end test, plus lent et plus fragile.\nB d√©pend aussi d‚Äôune vraie consommation depuis SQS, donc ce n‚Äôest pas un test unitaire.\nDonc la meilleure approche pour un unit test dans CI/CD est d‚Äôinvoquer Lambda avec un √©v√©nement de test via ¬´ aws lambda invoke ¬ª (D).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une pizzeria : les commandes arrivent dans une bo√Æte √† tickets (la file SQS) et un cuisinier (la fonction Lambda) pr√©pare la pizza quand il re√ßoit un ticket.**\n\nConcept : un ‚Äúunit test‚Äù, c‚Äôest tester le cuisinier tout seul, pas toute la pizzeria.\nDonc tu lui donnes un faux ticket d‚Äôexemple (un ‚Äú√©v√©nement de test‚Äù) et tu regardes s‚Äôil cuisine correctement.\nPourquoi D : la commande \"aws lambda invoke\" revient √† dire : ‚Äúh√© cuisinier, voil√† un ticket de test, fais la pizza maintenant‚Äù.\nTu peux faire √ßa automatiquement √† chaque livraison de code (CI/CD), sans cr√©er une vraie bo√Æte √† tickets.\nA et C : √ßa construit une vraie pizzeria de test (trop gros, c‚Äôest plut√¥t un test complet).\nB : √ßa d√©pend d‚Äôune vraie bo√Æte √† tickets et de vrais messages, donc ce n‚Äôest plus ‚Äúle cuisinier tout seul‚Äù.\nAvec D, tu testes juste la logique de la fonction, rapidement et souvent.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:97:dd87a64bcb321b67",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 97,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has designed an application to store incoming data as JSON files in Amazon S3 objects. Custom business logic in an AWS Lambda function then transforms the objects, and the Lambda function loads the data into an Amazon DynamoDB table. Recently, the workload has experienced sudden and significant changes in traffic. The flow of data to the DynamoDB table is becoming throttled.The developer needs to implement a solution to eliminate the throttling and load the data into the DynamoDB table more consistently.Which solution will meet these requirements?",
      "choices": {
        "A": "Refactor the Lambda function into two functions. Configure one function to transform the data and one function to load the data into the DynamoDB table. Create an Amazon Simple Queue Service (Amazon SQS) queue in between the functions to hold the items as messages and to invoke the second function.",
        "B": "Turn on auto scaling for the DynamoDB table. Use Amazon CloudWatch to monitor the table's read and write capacity metrics and to track consumed capacity.",
        "C": "Create an alias for the Lambda function. Configure provisioned concurrency for the application to use.",
        "D": "Refactor the Lambda function into two functions. Configure one function to store the data in the DynamoDB table. Configure the second function to process the data and update the items after the data is stored in DynamoDB. Create a DynamoDB stream to invoke the second function after the data is stored."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106947-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 21, 2023, 9:02 p.m.",
      "textHash": "dd87a64bcb321b67",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Le probl√®me vient de DynamoDB qui ¬´ throttle ¬ª : il refuse temporairement des √©critures quand trop de requ√™tes arrivent d‚Äôun coup (pics de trafic).\nS3 stocke les fichiers JSON, puis Lambda (code sans serveur) transforme et √©crit dans DynamoDB (base NoSQL). Si Lambda envoie trop vite, DynamoDB sature.\nLa solution A ajoute une file d‚Äôattente SQS entre deux fonctions Lambda : SQS stocke les messages et lisse les pics (buffer).\nLa 1re Lambda transforme et envoie chaque √©l√©ment dans SQS ; la 2e Lambda lit SQS √† un rythme contr√¥l√© et √©crit dans DynamoDB.\nAinsi, m√™me si l‚Äôentr√©e explose, les donn√©es ne sont pas perdues et DynamoDB re√ßoit un flux plus r√©gulier, ce qui r√©duit/√©vite le throttling.\nB (auto scaling) aide mais peut r√©agir trop lentement et ne prot√®ge pas contre des pics tr√®s brusques ; C (provisioned concurrency) acc√©l√®re Lambda, ce qui peut empirer la surcharge DynamoDB.\nD (Streams) d√©clenche apr√®s √©criture : √ßa ne r√©sout pas le throttling √† l‚Äô√©tape d‚Äô√©criture initiale.\nDonc A est la meilleure approche pour absorber les variations et charger plus r√©guli√®rement.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : des √©l√®ves arrivent par vagues. Si tout le monde veut √™tre servi en m√™me temps, la caisse bloque et refuse des gens (√ßa ‚Äúthrottle‚Äù).**\n\nIci, DynamoDB = la caisse, Lambda = le cuisinier+serveur, et les fichiers JSON dans S3 = les commandes qui arrivent.\nQuand il y a un gros rush, le serveur essaie d‚Äôenvoyer trop de plateaux √† la caisse d‚Äôun coup, donc la caisse ralentit et refuse.\nSolution A : on s√©pare en 2 personnes : 1) pr√©pare les plateaux (transforme les donn√©es), 2) les passe √† la caisse (√©crit dans DynamoDB).\nEntre les deux, on met une ‚Äúfile d‚Äôattente‚Äù (SQS) = une ligne o√π les plateaux attendent sagement.\nComme √ßa, m√™me si 200 commandes arrivent d‚Äôun coup, elles se rangent dans la file.\nLa caisse re√ßoit les plateaux √† un rythme r√©gulier, donc plus de blocage et un chargement plus constant.\nB aide un peu, mais ne g√®re pas bien les gros pics instantan√©s.\nC rend le serveur pr√™t, mais la caisse peut encore bloquer.\nD traite apr√®s coup, mais n‚Äôemp√™che pas le blocage au moment d‚Äô√©crire.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:455:6726ca76b690b0dd",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 455,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application stores user data in Amazon S3 buckets in multiple AWS Regions. A developer needs to implement a solution that analyzes the user data in the S3 buckets to find sensitive information. The analysis findings from all the S3 buckets must be available in the eu-west-2 Region.Which solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Create an AWS Lambda function to generate findings. Program the Lambda function to send the findings to another S3 bucket in eu-west-2.",
        "B": "Configure Amazon Macie to generate findings. Use Amazon EventBridge to create rules that copy the findings to eu-west-2.",
        "C": "Configure Amazon Inspector to generate findings. Use Amazon EventBridge to create rules that copy the findings to eu-west-2.",
        "D": "Configure Amazon Macie to generate findings and to publish the findings to AWS CloudTrail. Use a CloudTrail trail to copy the results to eu-west-2."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/151627-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 19, 2024, 11:32 a.m.",
      "textHash": "6726ca76b690b0dd",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Ici, on veut d√©tecter des informations sensibles (ex: donn√©es personnelles, num√©ros de carte) dans des fichiers stock√©s dans Amazon S3 (service de stockage d‚Äôobjets). Le service AWS fait pour √ßa avec tr√®s peu de code est Amazon Macie : il scanne les buckets S3 et produit des ‚Äúfindings‚Äù (r√©sultats/alertes) sur les donn√©es sensibles. Comme les buckets sont dans plusieurs R√©gions, il faut centraliser les r√©sultats dans eu-west-2. Amazon EventBridge (bus d‚Äô√©v√©nements) peut recevoir les √©v√©nements de findings Macie et, via des r√®gles, les router/copier vers une cible dans eu-west-2 (centralisation). C‚Äôest moins d‚Äôeffort que d‚Äô√©crire une Lambda (A) et de g√©rer soi-m√™me l‚Äôanalyse. Amazon Inspector (C) sert surtout √† analyser des vuln√©rabilit√©s sur EC2/ECR/Lambda, pas √† classifier des donn√©es S3. CloudTrail (D) journalise des appels API, ce n‚Äôest pas l‚Äôoutil pour publier/centraliser des findings Macie de fa√ßon simple.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que tes donn√©es sont des cahiers rang√©s dans plusieurs biblioth√®ques (une par ville). Tu veux rep√©rer automatiquement les pages avec des infos sensibles (adresse, num√©ro, etc.) et avoir tous les rapports rassembl√©s dans la biblioth√®que de Londres (eu-west-2).**\n\nConcept : il te faut 1) un ‚Äúd√©tective‚Äù sp√©cialis√© pour trouver les infos sensibles dans les cahiers, et 2) un ‚Äúfacteur‚Äù qui envoie tous les rapports au m√™me endroit.\nAmazon Macie = le d√©tective fait expr√®s pour chercher des donn√©es sensibles dans des fichiers stock√©s (comme tes cahiers en biblioth√®que).\nAmazon EventBridge = le facteur/standard qui r√©cup√®re chaque rapport et l‚Äôenvoie vers la bonne ville (eu-west-2).\nDonc B : tu actives Macie pour produire les rapports, puis EventBridge pour les copier/centraliser √† Londres.\nA demande d‚Äô√©crire toi-m√™me le d√©tective (Lambda = petit robot que tu programmes), donc plus d‚Äôeffort.\nC est faux : Inspector, c‚Äôest plut√¥t un contr√¥leur de s√©curit√© pour des ‚Äúmachines/applications‚Äù, pas pour fouiller des fichiers S3.\nD ajoute un d√©tour inutile (CloudTrail = journal ‚Äúqui a fait quoi‚Äù), pas le plus simple pour centraliser les rapports.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:48:7bed3384a697a026",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 48,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Lambda function that needs credentials to connect to an Amazon RDS for MySQL database. An Amazon S3 bucket currently stores the credentials. The developer needs to improve the existing solution by implementing credential rotation and secure storage. The developer also needs to provide integration with the Lambda function.Which solution should the developer use to store and retrieve the credentials with the LEAST management overhead?",
      "choices": {
        "A": "Store the credentials in AWS Systems Manager Parameter Store. Select the database that the parameter will access. Use the default AWS Key Management Service (AWS KMS) key to encrypt the parameter. Enable automatic rotation for the parameter. Use the parameter from Parameter Store on the Lambda function to connect to the database.",
        "B": "Encrypt the credentials with the default AWS Key Management Service (AWS KMS) key. Store the credentials as environment variables for the Lambda function. Create a second Lambda function to generate new credentials and to rotate the credentials by updating the environment variables of the first Lambda function. Invoke the second Lambda function by using an Amazon EventBridge rule that runs on a schedule. Update the database to use the new credentials. On the first Lambda function, retrieve the credentials from the environment variables. Decrypt the credentials by using AWS KMS, Connect to the database.",
        "C": "Store the credentials in AWS Secrets Manager. Set the secret type to Credentials for Amazon RDS database. Select the database that the secret will access. Use the default AWS Key Management Service (AWS KMS) key to encrypt the secret. Enable automatic rotation for the secret. Use the secret from Secrets Manager on the Lambda function to connect to the database.",
        "D": "Encrypt the credentials by using AWS Key Management Service (AWS KMS). Store the credentials in an Amazon DynamoDB table. Create a second Lambda function to rotate the credentials. Invoke the second Lambda function by using an Amazon EventBridge rule that runs on a schedule. Update the DynamoDB table. Update the database to use the generated credentials. Retrieve the credentials from DynamoDB with the first Lambda function. Connect to the database."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103918-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 26, 2023, 2:24 a.m.",
      "textHash": "7bed3384a697a026",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Ici, la fonction AWS Lambda (code ex√©cut√© sans serveur) doit se connecter √† une base Amazon RDS MySQL, donc elle a besoin d‚Äôun identifiant/mot de passe. Les stocker dans S3 n‚Äôest pas id√©al et ne g√®re pas la rotation automatiquement.\nAWS Secrets Manager est con√ßu pr√©cis√©ment pour stocker des ‚Äúsecrets‚Äù (mots de passe, cl√©s API) de fa√ßon chiffr√©e et contr√¥l√©e.\nEn choisissant le type ‚ÄúCredentials for Amazon RDS‚Äù, Secrets Manager sait comment se connecter √† RDS et peut automatiser la rotation (changer le mot de passe r√©guli√®rement) sans que vous √©criviez beaucoup de code.\nLe chiffrement avec AWS KMS prot√®ge le secret au repos, et les permissions IAM limitent qui peut le lire.\nLambda peut r√©cup√©rer le secret √† l‚Äôex√©cution via l‚ÄôAPI/SDK AWS, donc pas besoin de variables d‚Äôenvironnement √† mettre √† jour manuellement.\nLes autres options demandent de cr√©er et maintenir une Lambda de rotation + planification + mise √† jour du stockage (DynamoDB/env vars), donc plus d‚Äôadministration.\nParameter Store ne fournit pas la rotation automatique g√©r√©e pour des identifiants RDS comme Secrets Manager.\nDonc la solution avec le moins de gestion et la meilleure int√©gration est Secrets Manager avec rotation automatique.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:442:17bdf3ca01d8476f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 442,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on an ecommerce application that stores data in an Amazon RDS for MySQL cluster. The developer needs to implement a caching layer for the application to retrieve information about the most viewed products.Which solution will meet these requirements?",
      "choices": {
        "A": "Edit the RDS for MySQL cluster by adding a cache node. Configure the cache endpoint instead of the cluster endpoint in the application.",
        "B": "Create an Amazon ElastiCache for Redis cluster. Update the application code to use the ElastiCache for Redis cluster endpoint.",
        "C": "Create an Amazon DynamoDB Accelerator (DAX) cluster in front of the RDS for MySQL cluster. Configure the application to connect to the DAX endpoint instead of the RDS endpoint.",
        "D": "Configure the RDS for MySQL cluster to add a standby instance in a different Availability Zone. Configure the application to read the data from the standby instance."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153821-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Jan. 2, 2025, 5:58 p.m.",
      "textHash": "17bdf3ca01d8476f",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:319bdb50",
      "frExplanation": "Ici, on veut une ¬´ couche de cache ¬ª : un stockage tr√®s rapide en m√©moire pour √©viter de relire sans cesse la base MySQL (Amazon RDS) quand on affiche souvent les m√™mes produits.\nAmazon ElastiCache est justement le service AWS fait pour √ßa (Redis ou Memcached). Redis est un cache en m√©moire tr√®s performant, id√©al pour des classements comme ¬´ produits les plus vus ¬ª.\nLa bonne approche est donc de cr√©er un cluster ElastiCache for Redis et de modifier l‚Äôapplication pour lire/√©crire ces donn√©es via l‚Äôendpoint Redis (B).\nA est faux : on ne peut pas ‚Äúajouter un cache node‚Äù directement dans RDS MySQL ; RDS est une base de donn√©es, pas un service de cache.\nC est faux : DAX est un cache uniquement pour DynamoDB, pas pour RDS/MySQL.\nD est faux : une instance standby sert √† la haute disponibilit√© (failover), pas √† acc√©l√©rer les lectures ; on ne l‚Äôutilise pas comme cache.\nEn pratique : l‚Äôapp met √† jour le compteur de vues et stocke le top produits dans Redis, puis lit Redis pour afficher rapidement la liste.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : la cuisine (la base de donn√©es) pr√©pare tous les plats, mais √† midi tout le monde demande les m√™mes (les produits les plus vus). On met alors un comptoir ‚Äúsnacks pr√™ts‚Äù juste devant la cuisine pour servir vite.**\n\nConcept : un cache, c‚Äôest une petite r√©serve tr√®s rapide qui garde en m√©moire les infos les plus demand√©es, pour √©viter de retourner tout le temps en cuisine.\nIci, les ‚Äúproduits les plus vus‚Äù sont demand√©s sans arr√™t, donc on les met dans ce comptoir rapide.\nB est bon : ElastiCache Redis = le comptoir ‚Äúsnacks pr√™ts‚Äù, et l‚Äôappli va le consulter d‚Äôabord via son adresse (endpoint).\nA est faux : on ne ‚Äúrajoute pas un cache‚Äù directement dans RDS, RDS c‚Äôest la cuisine, pas le comptoir.\nC est faux : DAX est un acc√©l√©rateur fait pour DynamoDB (un autre type de stockage), pas pour MySQL/RDS.\nD est faux : une instance standby, c‚Äôest une cuisine de secours en cas de panne, pas un comptoir rapide pour servir plus vite.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:441:c30296d49bc78f26",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 441,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer hosts a static website on Amazon S3 and connects the website to an Amazon CloudFront distribution. The website uses a custom domain name that points to the CloudFront URL.The developer has set up a continuous integration and continuous delivery (CI/CD) pipeline. The pipeline automatically runs when changes occur in an AWS CodeCommit repository. The pipeline has a source stage and then a build stage. The build stage invokes an AWS CodeBuild project that references a buildspec.yml file. The buildspec.yml file builds the code and deploys the static files to the S3 bucket.The pipeline runs successfully, and the latest website files are visible in the S3 bucket and at the S3 website URL. However, when the developer accesses the website through the CloudFront domain, the updates are not reflected on the website.What should the developer configure the buildspec.yml file to do to resolve this issue?",
      "choices": {
        "A": "Properly synchronize the objects in the S3 bucket with new files from the source stage.",
        "B": "Delete the previous website files in the S3 bucket and redeploy the website files.",
        "C": "Invalidate the file caches for the primary CloudFront distribution.",
        "D": "Modify the cross-origin resource sharing (CORS) policy of the S3 bucket and redeploy the website files."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148959-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 10, 2024, 8:08 a.m.",
      "textHash": "c30296d49bc78f26",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Amazon S3 stocke les fichiers du site (HTML/CSS/JS). Amazon CloudFront est un CDN : il met en cache ces fichiers pr√®s des utilisateurs pour acc√©l√©rer l‚Äôaffichage.\nIci, le pipeline d√©ploie bien les nouveaux fichiers dans S3 (on les voit via l‚ÄôURL du site S3), donc le d√©ploiement est correct.\nSi CloudFront affiche encore l‚Äôancienne version, c‚Äôest presque toujours parce qu‚Äôil sert une copie en cache (ancienne) au lieu de relire S3.\nLa solution est de demander √† CloudFront d‚Äôoublier son cache pour les fichiers concern√©s : une ‚Äúinvalidation‚Äù.\nDans buildspec.yml, on ajoute une commande (souvent via AWS CLI) pour cr√©er une invalidation sur la distribution CloudFront, par exemple sur /* ou sur les chemins modifi√©s.\nAinsi, CloudFront r√©cup√®re les nouveaux fichiers depuis S3 et les utilisateurs voient imm√©diatement la mise √† jour.\nSynchroniser ou supprimer dans S3 ne force pas CloudFront √† rafra√Æchir son cache, et CORS n‚Äôa rien √† voir avec l‚Äôaffichage d‚Äôune version ancienne.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine le site web comme le menu du self au lyc√©e. Le dossier S3, c‚Äôest le tableau officiel o√π le menu est √©crit. CloudFront, c‚Äôest des photocopies du menu affich√©es dans les couloirs pour que tout le monde le voie vite.**\n\nConcept : CloudFront garde des copies (un ‚Äúcache‚Äù) pour aller plus vite, comme des photocopies d√©j√† pr√™tes. Quand tu changes le menu sur le tableau officiel (S3), les photocopies dans les couloirs peuvent rester anciennes. Ici, la pipeline met bien √† jour S3 (le tableau), donc le menu est bon √† l‚ÄôURL S3. Mais CloudFront montre encore l‚Äôancienne copie, donc le domaine CloudFront n‚Äôaffiche pas la mise √† jour. La solution est de dire dans buildspec.yml : ‚Äújette les anciennes photocopies et refais-les‚Äù = invalider le cache CloudFront. Donc la bonne r√©ponse est C : invalider les caches de la distribution CloudFront.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:435:946696ca93fb2548",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 435,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a serverless application that uses an Amazon API Gateway API to invoke an AWS Lambda function. A developer creates a fix for a defect in the Lambda function code. The developer wants to deploy this fix to the production environment.To test the changes, the developer needs to send 10% of the live production traffic to the updated Lambda function version.Which combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Publish a new version of the Lambda function that contains the updated code.",
        "B": "Set up a new stage in API Gateway with a new Lambda function version. Enable weighted routing in API Gateway stages.",
        "C": "Create an alias for the Lambda function. Configure weighted routing on the alias. Specify a 10% weight for the new Lambda function version.",
        "D": "Set up a routing policy on a Network Load Balancer. Configure 10% of the traffic to go to the new Lambda function version.",
        "E": "Set up a weighted routing policy by using Amazon Route 53. Configure 10% of the traffic to go to the new Lambda function version."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/150942-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 7, 2024, 2:36 p.m.",
      "textHash": "946696ca93fb2548",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Pour envoyer seulement 10% du trafic vers un nouveau code Lambda, il faut d‚Äôabord avoir une ¬´ version ¬ª distincte de la fonction. AWS Lambda permet de publier des versions immuables (num√©rot√©es) : c‚Äôest la base pour faire du d√©ploiement progressif.\nDonc l‚Äô√©tape indispensable est de publier une nouvelle version contenant le correctif (r√©ponse A).\nEnsuite, pour r√©partir le trafic entre l‚Äôancienne et la nouvelle version, on utilise un alias Lambda (un nom stable comme ‚Äúprod‚Äù) qui peut pointer vers une version et faire du ‚Äúweighted routing‚Äù (par ex. 90% vers l‚Äôancienne, 10% vers la nouvelle).\nAPI Gateway appelle g√©n√©ralement l‚Äôalias (ou l‚ÄôARN de la fonction) : en changeant l‚Äôalias, on teste en production sans changer l‚ÄôAPI.\nAPI Gateway n‚Äôest pas l‚Äôoutil standard pour faire du poids entre versions Lambda, et NLB/Route 53 ne routent pas directement vers des versions Lambda.\nEn pratique : publier la version (A), puis configurer l‚Äôalias avec 10% vers la nouvelle version (C).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:433:aade1b517dcffaa8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 433,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer uses Amazon S3 Event Notifications to invoke AWS Lambda functions. The Lambda functions process images after the images are uploaded to S3 buckets. The developer has set up a development S3 bucket, a production S3 bucket, a development Lambda function, and a production Lambda function in the same AWS account.The developer notices that uploads to the development S3 bucket wrongly invoke the production Lambda function. The developer must prevent development data from affecting the production Lambda function.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Update the execution role for the production Lambda function. Add a policy that allows the execution role to read from only the production S3 bucket.",
        "B": "Update the S3 bucket policy for the production S3 bucket to invoke the production Lambda function. Update the S3 bucket policy for the development S3 bucket to invoke the development Lambda function.",
        "C": "Separate the development environment and the production environment into their own AWS accounts. Update the execution role for each Lambda function. Add a policy that allows the execution role to read from only the S3 bucket that is in the same account.",
        "D": "Separate the development environment and the production environment into their own AWS accounts. Add a resource policy to the Lambda functions to allow only S3 bucket events in the same account to invoke the functions."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148956-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 10, 2024, 7:43 a.m.",
      "textHash": "aade1b517dcffaa8",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Le probl√®me vient de qui a le droit d‚ÄôINVOQUER (d√©clencher) la fonction Lambda, pas de qui a le droit de LIRE dans S3.\nS3 Event Notifications envoie un √©v√©nement √† Lambda quand un objet est ajout√©. Si Lambda accepte ces appels, elle s‚Äôex√©cute, m√™me si les donn√©es viennent du mauvais bucket.\nLe ‚Äúexecution role‚Äù de Lambda sert surtout quand Lambda appelle d‚Äôautres services (ex: lire S3). Il ne bloque pas l‚Äôappel initial de S3 vers Lambda.\nLa bonne pratique est d‚Äôisoler dev et prod dans des comptes AWS s√©par√©s pour √©viter les m√©langes accidentels.\nEnsuite, on met une ‚Äúresource policy‚Äù sur chaque Lambda (politique attach√©e √† la fonction) pour n‚Äôautoriser l‚Äôinvocation que depuis des √©v√©nements S3 du m√™me compte.\nAinsi, m√™me si quelqu‚Äôun configure par erreur une notification S3, le compte dev ne pourra pas d√©clencher la Lambda de prod.\nC‚Äôest exactement ce que propose D : s√©paration par comptes + politique de ressource Lambda limitant l‚Äôinvocation au m√™me compte.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine deux cuisines dans un m√™me grand b√¢timent : une cuisine ‚Äúentra√Ænement‚Äù (dev) et une cuisine ‚Äúservice du soir‚Äù (prod). Chaque cuisine a sa sonnette : quand une pizza arrive, la sonnette appelle un cuisinier pr√©cis.**\n\nConcept : S3 (le d√©p√¥t) envoie une ‚Äúsonnette‚Äù (notification) pour appeler Lambda (le cuisinier) quand une image (pizza) arrive.\nProbl√®me : la sonnette de la cuisine entra√Ænement appelle parfois le cuisinier du service du soir. Donc des tests perturbent le vrai service.\nSolution D : s√©parer dev et prod dans deux b√¢timents diff√©rents (deux comptes AWS). Comme √ßa, les sonnettes d‚Äôun b√¢timent ne peuvent pas appeler les cuisiniers de l‚Äôautre.\nEt on ajoute une r√®gle sur chaque cuisinier (resource policy) : ‚Äúje n‚Äôaccepte que les sonnettes venant de mon b√¢timent‚Äù.\nPourquoi pas A : limiter ce que le cuisinier peut lire n‚Äôemp√™che pas qu‚Äôon le d√©range (il est quand m√™me appel√©).\nPourquoi pas B : les r√®gles du d√©p√¥t ne sont pas le bon verrou pour emp√™cher l‚Äôappel au mauvais cuisinier.\nPourquoi pas C : m√™me s√©par√©, changer seulement les droits de lecture ne bloque pas l‚Äôinvocation.\nDonc D emp√™che vraiment dev d‚Äôappeler prod.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:429:b6282eb1ecd6a942",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 429,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn ecommerce startup is preparing for an annual sales event. As the traffic to the company's application increases, the development team wants to be notified when the Amazon EC2 instance's CPU utilization exceeds 80%.Which solution will meet this requirement?",
      "choices": {
        "A": "Create a custom Amazon CloudWatch alarm that sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%.",
        "B": "Create a custom AWS CloudTrail alarm that sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%.",
        "C": "Create a cron job on the EC2 instance that invokes the --describe-instance-information command on the host instance every 15 minutes and sends the results to an Amazon SNS topic.",
        "D": "Create an AWS Lambda function that queries the AWS CloudTrail logs for the CPUUtilization metric every 15 minutes and sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/150695-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 4, 2024, 1:13 a.m.",
      "textHash": "b6282eb1ecd6a942",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Pour √™tre alert√© quand le CPU d‚Äôune instance EC2 d√©passe 80%, il faut surveiller une m√©trique de performance.\nAmazon EC2 est un serveur dans le cloud, et ¬´ CPUUtilization ¬ª est une m√©trique standard qui indique le pourcentage d‚Äôutilisation du processeur.\nAmazon CloudWatch est le service AWS fait pour collecter des m√©triques (CPU, r√©seau, disque) et d√©clencher des alarmes selon des seuils.\nUne alarme CloudWatch peut √™tre configur√©e pour se d√©clencher si CPUUtilization > 80% pendant une p√©riode donn√©e.\nAmazon SNS (Simple Notification Service) sert √† envoyer des notifications (email, SMS, webhook) via un ‚Äútopic‚Äù.\nDonc la bonne solution est : cr√©er une alarme CloudWatch sur la m√©trique CPU et la connecter √† un topic SNS.\nCloudTrail, lui, enregistre des actions/API (qui a fait quoi), pas des m√©triques de CPU, donc B et D ne conviennent pas.\nUn cron job sur l‚Äôinstance (C) est inutilement complexe et moins fiable que CloudWatch, qui est con√ßu pour √ßa.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la salle des serveurs comme une cuisine de cantine le jour o√π tout le lyc√©e d√©barque. Le CPU, c‚Äôest le cuisinier: plus il est d√©bord√©, plus il ‚Äúchauffe‚Äù.**\n\nTu veux √™tre pr√©venu d√®s que le cuisinier travaille √† plus de 80% de sa capacit√©. CloudWatch, c‚Äôest le surveillant qui regarde en continu l‚Äôeffort du cuisinier (le CPU). Une ‚Äúalarm‚Äù CloudWatch, c‚Äôest une r√®gle: ‚Äúsi √ßa d√©passe 80%, d√©clenche une alerte‚Äù. SNS, c‚Äôest le m√©gaphone/groupe de messages qui envoie la notif √† l‚Äô√©quipe. Donc A = surveillant + r√®gle + message automatique, exactement ce qu‚Äôon demande. CloudTrail (B et D) c‚Äôest plut√¥t le journal ‚Äúqui a fait quoi‚Äù (comme un cahier de discipline), pas un capteur d‚Äôeffort du cuisinier. Le cron job (C) c‚Äôest demander √† un √©l√®ve de v√©rifier toutes les 15 min: moins fiable et pas ‚Äúen continu‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:427:7413483e6b4bcd5a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 427,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that uses an Amazon API Gateway API to invoke an AWS Lambda function. The application is latency sensitive.A developer needs to configure the Lambda function to reduce the cold start time that is associated with default scaling.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Publish a new version of the Lambda function. Configure provisioned concurrency. Set the provisioned concurrency limit to meet the company requirements.",
        "B": "Increase the Lambda function's memory to the maximum amount. Increase the Lambda function's reserved concurrency limit.",
        "C": "Increase the reserved concurrency of the Lambda function to a number that matches the current production load.",
        "D": "Use Service Quotas to request an increase in the Lambda function's concurrency limit for the AWS account where the function is deployed."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/150694-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 4, 2024, 1:09 a.m.",
      "textHash": "7413483e6b4bcd5a",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:7ccebe71",
      "frExplanation": "Une API Amazon API Gateway appelle une fonction AWS Lambda (code ex√©cut√© √† la demande). Quand Lambda n‚Äôa pas d‚Äôinstance ‚Äúchaude‚Äù pr√™te, il doit d√©marrer un nouvel environnement d‚Äôex√©cution : c‚Äôest le ‚Äúcold start‚Äù, qui ajoute de la latence.\nPour une application sensible au temps de r√©ponse, il faut garder des environnements d√©j√† initialis√©s.\nLa fonctionnalit√© faite pour √ßa est la ‚Äúprovisioned concurrency‚Äù : AWS maintient un nombre d√©fini d‚Äôinstances Lambda pr√™tes √† r√©pondre imm√©diatement.\nPour l‚Äôactiver, on publie une version (ou un alias) de la fonction, puis on configure la provisioned concurrency sur cette version.\nOn choisit une valeur (le nombre d‚Äôinstances pr√©chauff√©es) qui couvre le trafic attendu afin de r√©duire fortement les cold starts.\nAugmenter la m√©moire ou la ‚Äúreserved concurrency‚Äù limite seulement le nombre d‚Äôex√©cutions simultan√©es, mais ne garantit pas des instances pr√©chauff√©es.\nDemander une hausse de quota de compte augmente la capacit√© globale, pas la latence de d√©marrage.\nDonc la bonne r√©ponse est de publier une version et configurer la provisioned concurrency (A).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine une pizzeria qui re√ßoit des commandes via une appli. Quand il n‚Äôy a pas eu de commande depuis un moment, le four est √©teint et l‚Äô√©quipe n‚Äôest pas pr√™te : la premi√®re pizza met longtemps (cold start).**\n\nConcept : API Gateway = l‚Äôappli qui prend la commande. Lambda = le cuisinier automatique. Latency sensitive = les clients veulent la pizza tout de suite.\nLe ‚Äúcold start‚Äù, c‚Äôest quand AWS doit ‚Äúr√©veiller‚Äù un cuisinier et pr√©parer la cuisine avant de commencer.\nR√©ponse A : publier une nouvelle version, c‚Äôest comme fixer une recette stable. Provisioned concurrency, c‚Äôest payer pour garder X cuisiniers d√©j√† en cuisine, four chaud, pr√™ts √† d√©marrer.\nDonc la premi√®re commande n‚Äôattend pas : pas de gros d√©lai au d√©but.\nB et C (m√©moire/r√©serv√©) g√®rent surtout la quantit√© de commandes possibles, pas le fait d‚Äôavoir des cuisiniers d√©j√† pr√™ts.\nD augmente le plafond global, mais ne garde pas la cuisine ‚Äúchaude‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:7:f9571fdd2c392c2d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 7,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has an application that makes batch requests directly to Amazon DynamoDB by using the BatchGetItem low-level API operation. The responses frequently return values in the UnprocessedKeys element.Which actions should the developer take to increase the resiliency of the application when the batch response includes values in UnprocessedKeys? (Choose two.)",
      "choices": {
        "A": "Retry the batch operation immediately.",
        "B": "Retry the batch operation with exponential backoff and randomized delay.",
        "C": "Update the application to use an AWS software development kit (AWS SDK) to make the requests.",
        "D": "Increase the provisioned read capacity of the DynamoDB tables that the operation accesses.",
        "E": "Increase the provisioned write capacity of the DynamoDB tables that the operation accesses."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102785-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 16, 2023, 10 a.m.",
      "textHash": "f9571fdd2c392c2d",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:dynamodb:65861584",
      "frExplanation": "BatchGetItem est une op√©ration DynamoDB qui lit plusieurs √©l√©ments en une seule requ√™te. Si DynamoDB est trop occup√© (limites de d√©bit, \"throttling\"), il peut renvoyer une partie des r√©sultats et mettre le reste dans UnprocessedKeys (cl√©s non trait√©es). Cela ne veut pas dire que les donn√©es n‚Äôexistent pas, juste que le service n‚Äôa pas pu tout traiter √† ce moment-l√†. Pour rendre l‚Äôapplication plus robuste, il faut r√©essayer uniquement ces cl√©s non trait√©es. Le bon r√©flexe est d‚Äôattendre un peu avant de r√©essayer, puis d‚Äôaugmenter progressivement l‚Äôattente (exponential backoff) et d‚Äôajouter un d√©lai al√©atoire (jitter) pour √©viter que tous les clients r√©essaient en m√™me temps. R√©essayer imm√©diatement (sans attente) aggrave souvent le throttling. Augmenter la capacit√© de lecture peut aider, mais la question demande surtout la r√©silience c√¥t√© application face √† UnprocessedKeys, donc la strat√©gie de retry avec backoff est la bonne r√©ponse.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : tu arrives avec une liste de 30 plats √† r√©cup√©rer d‚Äôun coup. Parfois, la cantine est d√©bord√©e et te dit : ‚ÄúJe t‚Äôai donn√© 20 plats, mais pour les 10 restants, reviens plus tard.‚Äù**\n\nDans DynamoDB, ‚ÄúUnprocessedKeys‚Äù = les plats que la cantine n‚Äôa pas eu le temps de te donner. Le concept : quand le service est trop occup√©, il refuse une partie de ta demande pour √©viter de planter. Si tu r√©essaies tout de suite (A), tu reviens en courant au m√™me moment o√π c‚Äôest encore la foule, donc √ßa √©choue encore. La bonne strat√©gie (B) : tu attends un peu, puis un peu plus si √ßa recommence (exponential backoff), et tu ajoutes un petit d√©lai au hasard (randomized delay) pour que tout le monde ne revienne pas en m√™me temps. Comme √ßa, tu laisses la cantine respirer et tu r√©cup√®res le reste plus s√ªrement. Augmenter la ‚Äúcapacit√©‚Äù (D/E) c‚Äôest comme embaucher plus de personnel, mais la question demande surtout comment rendre l‚Äôapp plus solide quand √ßa arrive : c‚Äôest B.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:509:ff5ab595a0dbaf48",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 509,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA gaming company has deployed a web portal on AWS Elastic Beanstalk. The company sometimes needs to deploy new versions three or four times in a day. The company needs to deploy new features for all users as quickly as possible. The solution must minimize performance impact and must maximize availability.What solution will meet these requirements?",
      "choices": {
        "A": "Use a rolling deployment policy to deploy to Amazon EC2 instances.",
        "B": "Use an immutable deployment policy to deploy to Amazon EC2 instances.",
        "C": "Use an all-at-once deployment policy to deploy to Amazon EC2 instances.",
        "D": "Use a-canary deployment strategy to deploy changes to Amazon EC2 instances."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153213-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 20, 2024, 12:46 a.m.",
      "textHash": "ff5ab595a0dbaf48",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Elastic Beanstalk est un service AWS qui d√©ploie et g√®re automatiquement une application web sur des instances Amazon EC2 (des serveurs). Ici, on veut livrer tr√®s souvent, vite, avec un impact minimal sur les performances et une disponibilit√© maximale. Le d√©ploiement ¬´ immutable ¬ª cr√©e d‚Äôabord un nouveau groupe d‚Äôinstances EC2 avec la nouvelle version, sans toucher aux serveurs en production. Une fois que ces nouvelles instances sont pr√™tes et v√©rifi√©es, Beanstalk bascule le trafic vers elles (puis supprime les anciennes). R√©sultat : presque pas de baisse de performance et tr√®s peu de risque de panne, car l‚Äôancienne version reste disponible tant que la nouvelle n‚Äôest pas valid√©e. Un rolling met √† jour par lots et peut r√©duire la capacit√© pendant la mise √† jour. All-at-once met tout √† jour d‚Äôun coup et peut provoquer une indisponibilit√©. Le canary n‚Äôest pas la politique standard de Beanstalk pour EC2 et vise surtout √† tester sur un petit pourcentage, pas √† d√©ployer ¬´ pour tous ¬ª le plus vite possible.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:436:63c14422409e436c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 436,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a video search application for a global company. The video files have an average size of 2.5 TB. The video storage system must provide instant access to the video files for the first 90 days. After the first 90 days, the video files can take more than 10 minutes to load.Which solution will meet these requirements MOST cost-effectively?",
      "choices": {
        "A": "Upload the video files to the Amazon Elastic File System (Amazon EFS) Standard storage class for the first 90 days. After 90 days, transition the video files to the EFS Standard-Infrequent Access (Standard-IA) storage class.",
        "B": "Upload the video files to Amazon S3. Use the S3 Glacier Deep Archive storage class for the first 90 days. After 90 days, transition the video file to the S3 Glacier Flexible Retrieval storage class.",
        "C": "Use Amazon Elastic Block Store (Amazon EBS) to store the video files for the first 90 days. After 90 days, transition the video files to the Amazon S3 Glacier Deep Archive storage class.",
        "D": "Upload the video files to Amazon S3. Use the S3 Glacier Instant Retrieval storage class for the first 90 days. After 90 days, transition the video files to the S3 Glacier Flexible Retrieval storage class."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/151546-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 18, 2024, 12:46 p.m.",
      "textHash": "63c14422409e436c",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:d54df0cf",
      "frExplanation": "On doit stocker de tr√®s gros fichiers vid√©o (2,5 To) avec acc√®s imm√©diat pendant 90 jours, puis on accepte un d√©lai > 10 minutes apr√®s 90 jours.\nAmazon S3 est un stockage d‚Äôobjets tr√®s √©conomique et adapt√© aux fichiers volumineux, avec des ‚Äúclasses‚Äù de stockage selon la fr√©quence d‚Äôacc√®s.\nPendant les 90 premiers jours, il faut un acc√®s instantan√© : S3 Glacier Instant Retrieval est une classe d‚Äôarchivage qui permet une r√©cup√©ration en millisecondes (comme du ‚Äúpresque en ligne‚Äù), mais moins ch√®re que S3 Standard.\nApr√®s 90 jours, on peut tol√©rer une attente : S3 Glacier Flexible Retrieval permet des r√©cup√©rations en minutes √† heures, donc compatible avec ‚Äúplus de 10 minutes‚Äù, et co√ªte moins cher.\nA (EFS) est un syst√®me de fichiers r√©seau, souvent plus co√ªteux et inutile ici (pas besoin de POSIX/partage de fichiers).\nB est invers√© : Deep Archive n‚Äôest pas instantan√© (r√©cup√©ration tr√®s lente), donc ne respecte pas les 90 jours.\nC utilise EBS (disques pour serveurs) : cher et peu adapt√© √† l‚Äôarchivage massif, puis migration complexe.\nDonc la solution la plus rentable et conforme est S3 Glacier Instant Retrieval puis transition vers S3 Glacier Flexible Retrieval (D).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une m√©diath√®que du lyc√©e avec des films √©normes. Pendant 90 jours, les √©l√®ves doivent pouvoir prendre un film tout de suite. Apr√®s 90 jours, on peut accepter d‚Äôattendre longtemps car c‚Äôest rarement demand√©.**\n\nConcept : il existe des ‚Äúrayons‚Äù plus ou moins chers. Rayon devant = acc√®s imm√©diat mais co√ªte plus. R√©serve au sous-sol = moins cher mais il faut du temps pour r√©cup√©rer.\nIci, Amazon S3 = la m√©diath√®que o√π on stocke les vid√©os. ‚ÄúGlacier Instant Retrieval‚Äù = rayon devant : tu prends le film instantan√©ment, parfait pour les 90 premiers jours.\nApr√®s 90 jours, ‚ÄúGlacier Flexible Retrieval‚Äù = sous-sol : c‚Äôest moins cher et tu peux attendre plus de 10 minutes pour que le film remonte.\nPourquoi D : il combine acc√®s instantan√© au d√©but + stockage moins cher ensuite, exactement comme d√©placer les films du rayon devant vers la r√©serve.\nPourquoi pas B : Deep Archive au d√©but, c‚Äôest comme mettre direct au sous-sol tr√®s lent, donc pas ‚Äúinstant‚Äù.\nPourquoi pas A/C : EFS/EBS, c‚Äôest comme louer des √©tag√®res premium en continu, trop cher pour des vid√©os g√©antes.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:432:ca9039e9f0b797ec",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 432,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company processes incoming documents from an Amazon S3 bucket. Users upload documents to an S3 bucket using a web user interface. Upon receiving files in S3, an AWS Lambda function is invoked to process the files, but the Lambda function times out intermittently.If the Lambda function is configured with the default settings, what will happen to the S3 event when there is a timeout exception?",
      "choices": {
        "A": "Notification of a failed S3 event is sent as an email through Amazon SNS.",
        "B": "The S3 event is sent to the default Dead Letter Queue.",
        "C": "The S3 event is processed until it is successful.",
        "D": "The S3 event is discarded after the event is retried twice."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/147042-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Sept. 6, 2024, 12:23 a.m.",
      "textHash": "ca9039e9f0b797ec",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Quand un fichier est ajout√© dans Amazon S3 (stockage de fichiers), S3 envoie un ¬´ √©v√©nement ¬ª pour d√©clencher AWS Lambda (code ex√©cut√© automatiquement).\nSi la fonction Lambda d√©passe le temps maximum (timeout), l‚Äôex√©cution est consid√©r√©e comme un √©chec.\nAvec les r√©glages par d√©faut pour une invocation asynchrone depuis S3, AWS r√©essaie automatiquement l‚Äôex√©cution un nombre limit√© de fois.\nPar d√©faut, Lambda retente 2 fois apr√®s la premi√®re tentative (donc jusqu‚Äô√† 3 tentatives au total).\nSi apr√®s ces tentatives la fonction √©choue encore (ex: timeout intermittent), l‚Äô√©v√©nement n‚Äôest pas conserv√© ind√©finiment.\nIl n‚Äôy a pas d‚Äôemail SNS automatique (il faut le configurer).\nIl n‚Äôy a pas de Dead Letter Queue ¬´ par d√©faut ¬ª : une DLQ ou une destination d‚Äô√©chec doit √™tre configur√©e explicitement.\nDonc, apr√®s 2 retries, l‚Äô√©v√©nement S3 est abandonn√© (discarded), ce qui correspond √† la r√©ponse D.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine : tu d√©poses ton plateau sale sur un tapis roulant, et un √©l√®ve ‚Äúrobot‚Äù doit le laver en un temps limit√©. S‚Äôil n‚Äôa pas fini √† temps, la cantine r√©essaie un peu, mais ne peut pas bloquer toute la file.**\n\nIci, le seau S3 = le tapis o√π on d√©pose les documents. Lambda = l‚Äô√©l√®ve robot qui traite le document. Un ‚Äútimeout‚Äù = il n‚Äôa pas fini avant la sonnerie. Avec les r√©glages par d√©faut, S3 ne pr√©vient pas par email (A faux) et il n‚Äôy a pas de ‚Äúpoubelle sp√©ciale automatique‚Äù (Dead Letter Queue) sans l‚Äôavoir configur√©e (B faux). S3 ne r√©essaie pas ind√©finiment jusqu‚Äô√† r√©ussir (C faux), sinon la file serait bloqu√©e. Il r√©essaie seulement un petit nombre de fois : en pratique, 2 nouvelles tentatives. Si √ßa rate encore, l‚Äô√©v√©nement est abandonn√©. Donc D est la bonne r√©ponse.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:428:a395c12dde26ebce",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 428,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is deploying an application on Amazon EC2 instances that run in Account A. The application needs to read data from an existing Amazon Kinesis data stream in Account B.Which actions should the developer take to provide the application with access to the stream? (Choose two.)",
      "choices": {
        "A": "Update the instance profile role in Account A with stream read permissions.",
        "B": "Create an IAM role with stream read permissions in Account B.",
        "C": "Add a trust policy to the instance profile role and IAM role in Account B to allow the instance profile role to assume the IAM role.",
        "D": "Add a trust policy to the instance profile role and IAM role in Account B to allow reads from the stream.",
        "E": "Add a resource-based policy in Account B to allow read access from the instance profile role."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/146862-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Sept. 3, 2024, 6:18 p.m.",
      "textHash": "a395c12dde26ebce",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici on a un acc√®s ¬´ cross-account ¬ª : l‚Äôapplication tourne sur des EC2 dans le compte A, mais le flux Kinesis est dans le compte B. Par d√©faut, un compte AWS ne peut pas lire les ressources d‚Äôun autre compte.\nAmazon Kinesis Data Streams est un service qui stocke des √©v√©nements; pour lire, il faut des permissions IAM (ex: GetRecords, GetShardIterator).\nLa bonne pratique est de cr√©er dans le compte qui poss√®de la ressource (compte B) un r√¥le IAM qui a les permissions de lecture sur le stream : c‚Äôest l‚Äôaction B.\nEnsuite, depuis le compte A, l‚Äôapplication (via le r√¥le de l‚Äôinstance EC2, appel√© ¬´ instance profile role ¬ª) assumera ce r√¥le du compte B pour obtenir temporairement ces droits.\nMettre seulement des permissions sur le r√¥le du compte A (A) ne suffit pas, car le stream appartient au compte B.\nUne ¬´ trust policy ¬ª sert √† dire qui a le droit d‚Äôassumer un r√¥le, pas √† autoriser directement la lecture (donc D est faux).\nUne resource-based policy (E) n‚Äôest pas l‚Äôapproche attendue ici pour Kinesis Streams dans ce contexte d‚Äôexamen; on passe g√©n√©ralement par un r√¥le dans le compte B.\nDonc l‚Äôaction cl√© est de cr√©er un r√¥le IAM dans le compte B avec les permissions de lecture, puis le faire assumer par le r√¥le EC2 du compte A.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:425:5a72c458ac2f4dc4",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 425,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building various microservices for an application that will run on Amazon EC2 instances. The developer needs to monitor the end-to-end view of the requests between the microservices and debug any issues in the various microservices.What should the developer do to accomplish these tasks?",
      "choices": {
        "A": "Use Amazon CloudWatch to aggregate the microservices' logs and metrics, and build the monitoring dashboard.",
        "B": "Use AWS CloudTrail to aggregate the microservices' logs and metrics, and build the monitoring dashboard.",
        "C": "Use the AWS X-Ray SDK to add instrumentation in all the microservices, and monitor using the X-Ray service map.",
        "D": "Use AWS Health to monitor the health of all the microservices."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148938-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 9, 2024, 3:33 p.m.",
      "textHash": "5a72c458ac2f4dc4",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Ici, on veut suivre une requ√™te ¬´ de bout en bout ¬ª quand elle traverse plusieurs microservices sur EC2, et trouver o√π √ßa ralentit ou √©choue.\nAmazon CloudWatch (A) sert surtout √† collecter des m√©triques et des logs, mais ne reconstruit pas automatiquement le parcours complet d‚Äôune requ√™te entre services.\nAWS CloudTrail (B) enregistre les actions faites sur l‚ÄôAPI AWS (qui a cr√©√©/modifi√© une ressource), pas le trafic applicatif entre microservices.\nAWS Health (D) informe sur l‚Äô√©tat des services AWS et certains √©v√©nements de compte, pas sur le d√©tail des appels internes de votre application.\nAWS X-Ray est con√ßu pour le ‚Äúdistributed tracing‚Äù : il trace une requ√™te √† travers plusieurs services et montre les d√©pendances.\nEn ajoutant le SDK X-Ray dans chaque microservice, vous cr√©ez des ‚Äútraces‚Äù et des ‚Äúsegments‚Äù qui mesurent latence, erreurs et points de blocage.\nLa carte de services (service map) X-Ray donne une vue globale des appels entre microservices et aide √† isoler rapidement le service fautif.\nDonc la bonne r√©ponse est C : instrumenter tous les microservices avec le SDK X-Ray et utiliser X-Ray pour visualiser et d√©boguer.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une livraison de pizzas : la commande passe par plusieurs √©tapes (prise de commande, cuisine, four, livreur). Chaque √©tape est un ‚Äúmicroservice‚Äù.**\n\nConcept : tu veux voir le trajet COMPLET d‚Äôune commande, du d√©but √† la fin, et savoir o√π √ßa bloque.\nSi une pizza arrive en retard, tu dois savoir si c‚Äôest la cuisine, le four ou le livreur.\nR√©ponse C : AWS X-Ray, c‚Äôest comme coller un ticket de suivi sur chaque commande.\nLe ‚ÄúSDK X-Ray‚Äù = tu ajoutes ce ticket dans CHAQUE √©tape (chaque microservice).\nLa ‚Äúservice map‚Äù = une carte qui montre le chemin de la commande entre les √©tapes.\nDu coup, tu vois o√π √ßa ralentit ou o√π √ßa casse, et tu peux d√©bugger pr√©cis√©ment.\nCloudWatch (A) ressemble plut√¥t √† des compteurs et des journaux par √©tape, mais pas le suivi complet d‚Äôune commande.\nCloudTrail (B) = qui a fait quoi sur AWS, pas le trajet des requ√™tes.\nAWS Health (D) = √©tat g√©n√©ral des services AWS, pas le d√©tail entre tes microservices.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:423:f600fa5479dca294",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 423,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA development team wants to immediately build and deploy an application whenever there is a change to the source code.Which approaches could be used to trigger the deployment? (Choose two.)",
      "choices": {
        "A": "Store the source code in an Amazon S3 bucket. Configure AWS CodePipeline to start whenever a file in the bucket changes.",
        "B": "Store the source code in an encrypted Amazon EBS volume. Configure AWS CodePipeline to start whenever a file in the volume changes.",
        "C": "Store the source code in an AWS CodeCommit repository. Configure AWS CodePipeline to start whenever a change is committed to the repository.",
        "D": "Store the source code in an Amazon S3 bucket. Configure AWS CodePipeline to start every 15 minutes.",
        "E": "Store the source code in an Amazon EC2 instance‚Äôs ephemeral storage. Configure the instance to start AWS CodePipeline whenever there are changes to the source code."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/146861-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Sept. 3, 2024, 6:04 p.m.",
      "textHash": "f600fa5479dca294",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Objectif : d√©clencher automatiquement un d√©ploiement d√®s qu‚Äôil y a un changement de code (d√©clenchement ‚Äú√©v√©nementiel‚Äù).\nAWS CodePipeline est un service qui orchestre les √©tapes CI/CD (r√©cup√©rer le code, construire, tester, d√©ployer).\nUne source valide pour CodePipeline peut √™tre Amazon S3 : si le code (un zip, par exemple) est d√©pos√©/mis √† jour dans un bucket, un √©v√©nement S3 peut d√©marrer le pipeline imm√©diatement.\nC‚Äôest exactement l‚Äôoption A : changement dans S3 ‚Üí d√©clenchement automatique du pipeline.\nLes volumes EBS ou le stockage √©ph√©m√®re EC2 ne sont pas des sources ‚Äúsurveillance de fichiers‚Äù natives pour d√©clencher CodePipeline : il n‚Äôy a pas d‚Äô√©v√©nement simple int√©gr√© ‚Äúfichier modifi√©‚Äù (B et E sont donc peu r√©alistes).\nL‚Äôoption D lance toutes les 15 minutes : ce n‚Äôest pas ‚Äúimm√©diatement‚Äù et c‚Äôest du polling, donc moins adapt√©.\nEn pratique, pour un d√©clenchement imm√©diat, on utilise une source qui √©met des √©v√©nements (comme S3) reli√©s √† CodePipeline.\nDonc la bonne approche ici est A.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un devoir de groupe : vous mettez la derni√®re version du devoir dans un casier partag√© au CDI, et d√®s que quelqu‚Äôun remplace le fichier, le prof lance automatiquement l‚Äôimpression et la distribution aux √©l√®ves.**\n\nLe but : d√®s que le code change, on relance tout de suite la ‚Äúmise en ligne‚Äù (d√©ploiement).\nAmazon S3 = le casier partag√© o√π on d√©pose les fichiers.\nAWS CodePipeline = le ‚Äúrobot prof‚Äù qui surveille et lance les √©tapes (tester, pr√©parer, publier).\nAvec A : d√®s qu‚Äôun fichier dans le casier (S3) est modifi√©, le robot est pr√©venu et d√©marre imm√©diatement.\nC‚Äôest exactement ‚Äúimm√©diatement apr√®s un changement‚Äù.\nLes autres : B et E parlent de stockages pas faits pour √™tre surveill√©s comme un casier partag√©.\nD d√©marre toutes les 15 minutes : ce n‚Äôest pas imm√©diat.\nDonc l‚Äôapproche qui colle √† la consigne ‚Äúd√®s qu‚Äôil y a un changement‚Äù est A.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:422:9ecac15a333b878d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 422,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is automating a new application deployment with AWS Serverless Application Model (AWS SAM). The new application has one AWS Lambda function and one Amazon S3 bucket. The Lambda function must access the S3 bucket to only read objects.How should the developer configure AWS SAM to grant the necessary read privilege to the S3 bucket?",
      "choices": {
        "A": "Reference a second Lambda authorizer function.",
        "B": "Add a custom S3 bucket policy to the Lambda function.",
        "C": "Create an Amazon Simple Queue Service (SQS) topic for only S3 object reads. Reference the topic in the template.",
        "D": "Add the S3ReadPolicy template to the Lambda function's execution role."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/150944-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 7, 2024, 3:44 p.m.",
      "textHash": "9ecac15a333b878d",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, il faut donner √† une fonction AWS Lambda (code ex√©cut√© sans serveur) le droit de lire des fichiers dans un bucket Amazon S3 (stockage d‚Äôobjets).\nLe bon endroit pour donner des permissions √† Lambda est son ¬´ execution role ¬ª : un r√¥le IAM (identit√©) que Lambda utilise pour appeler d‚Äôautres services AWS.\nAWS SAM permet d‚Äôajouter facilement des permissions via des politiques (policies) pr√™tes √† l‚Äôemploi dans le template.\nLa policy ¬´ S3ReadPolicy ¬ª accorde uniquement les actions de lecture S3 (ex: GetObject, ListBucket) sur le bucket indiqu√©, donc principe du moindre privil√®ge.\nLes autres choix ne conviennent pas : un authorizer sert √† API Gateway, pas √† l‚Äôacc√®s S3 ; une policy de bucket se met sur S3 (pas ‚Äúsur la fonction‚Äù) et est plus complexe ; SQS n‚Äôa rien √† voir avec ‚Äúlire‚Äù des objets S3.\nDonc, dans SAM, on ajoute S3ReadPolicy √† la fonction (√† son r√¥le d‚Äôex√©cution) en ciblant le bucket.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine ton √©cole : la biblioth√®que (le bucket S3) contient des livres (les fichiers). Un √©l√®ve (la fonction Lambda) doit seulement LIRE des livres, pas en ajouter ni en d√©chirer.**\n\nConcept : dans le cloud, chaque ‚Äúpersonne‚Äù a un badge avec des droits. Sans badge, tu ne rentres pas. Avec un badge ‚Äúlecture‚Äù, tu peux juste consulter.\nIci, on doit donner √† Lambda un badge ‚Äúlecture seule‚Äù pour la biblioth√®que S3.\nDans AWS SAM, ce badge s‚Äôajoute au ‚Äúr√¥le d‚Äôex√©cution‚Äù de Lambda (son badge officiel).\nL‚Äôoption D fait exactement √ßa : elle ajoute une r√®gle pr√™te √† l‚Äôemploi ‚ÄúS3ReadPolicy‚Äù au badge de Lambda.\nDonc Lambda peut lire les objets du bucket, et rien d‚Äôautre.\nA ne sert pas √† donner des droits de lecture √† une biblioth√®que.\nB vise la biblioth√®que, mais on veut surtout √©quiper l‚Äô√©l√®ve avec le bon badge.\nC ajoute une file d‚Äôattente inutile : √ßa ne donne pas directement le droit de lire.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:420:2028905aa7bbcac3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 420,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is developing a publicly accessible single-page application. The application makes calls from a client web browser to backend services to provide a user interface to customers. The application depends on a third-party web service exposed as an HTTP API. The web client must provide an API key to the third-party web service by using the HTTP header as part of the HTTP request. The company's API key must not be exposed to the users of the web application.Which solution will meet these requirements MOST cost-effectively?",
      "choices": {
        "A": "Use Amazon API Gateway to create a private REST API. Create an HTTP integration to integrate with the third-party HTTP API. Add the company‚Äôs API key to the HTTP headers list of the integration request configuration.",
        "B": "Use Amazon API Gateway to create a private REST API. Create an AWS Lambda proxy integration. Make calls to the third-party HTTP API from the Lambda function. Pass the company's API key as an HTTP request header.",
        "C": "Use Amazon API Gateway to create a REST API. Create an HTTP integration to integrate with the third-party HTTP API. Add the company's API key to the HTTP headers list of the integration request configuration.",
        "D": "Use Amazon API Gateway to create a REST API. Create an AWS Lambda proxy integration. Make calls to the third-party HTTP API from the Lambda function. Pass the company's API key as an HTTP request header."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/146857-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Sept. 3, 2024, 5:49 p.m.",
      "textHash": "2028905aa7bbcac3",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:529682fa",
      "frExplanation": "Le probl√®me : une application web (dans le navigateur) doit appeler un service HTTP tiers qui exige une cl√© API dans un en-t√™te HTTP, mais cette cl√© ne doit jamais √™tre visible c√¥t√© utilisateur.\nSi le navigateur appelle directement le service tiers, la cl√© serait forc√©ment expos√©e (dans le code ou les requ√™tes). Il faut donc un ‚Äúinterm√©diaire‚Äù c√¥t√© serveur.\nAmazon API Gateway est un service qui expose une URL d‚ÄôAPI publique et peut ensuite appeler un autre service HTTP en arri√®re-plan.\nAvec une int√©gration HTTP, API Gateway peut relayer la requ√™te vers l‚ÄôAPI tierce et ajouter automatiquement l‚Äôen-t√™te contenant la cl√© API, sans l‚Äôenvoyer au navigateur.\nC‚Äôest plus √©conomique que d‚Äôajouter AWS Lambda, car Lambda ex√©cute du code et facture des invocations/temps d‚Äôex√©cution, alors qu‚Äôici on fait surtout du proxy HTTP.\nLes options ‚Äúprivate REST API‚Äù (A, B) ne conviennent pas : une API priv√©e n‚Äôest pas accessible publiquement depuis Internet, donc pas adapt√©e √† une SPA publique.\nDonc la meilleure solution, simple et peu co√ªteuse, est une API Gateway REST API publique avec int√©gration HTTP et ajout de l‚Äôen-t√™te : C.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une pizzeria qui livre chez toi. Pour commander une pizza sp√©ciale, il faut un ‚Äúcode secret‚Äù (API key). Tu ne veux pas donner ce code √† tous les clients, sinon ils pourraient commander √† ta place.**\n\nConcept : le navigateur du client, c‚Äôest le client au t√©l√©phone. Si tu lui donnes le code secret, il le voit et peut le r√©utiliser. Donc il faut un ‚Äúinterm√©diaire‚Äù qui parle √† la pizzeria avec le code, sans le montrer au client.\nAPI Gateway, c‚Äôest comme un standard t√©l√©phonique public : les clients appellent le standard, pas directement la pizzeria. Avec l‚Äôoption C, le standard transf√®re l‚Äôappel vers la pizzeria (HTTP API tiers) et ajoute lui-m√™me le code secret dans l‚Äôen-t√™te (le petit papier coll√© sur la commande). Le client ne voit jamais ce papier.\nPourquoi C : c‚Äôest public (pas ‚Äúprivate‚Äù, sinon les clients ne peuvent pas appeler), et c‚Äôest le moins cher/simple car on ne rajoute pas un ‚Äúemploy√©‚Äù en plus (Lambda) juste pour relayer l‚Äôappel.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:419:01d5f5c6b4d1e426",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 419,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to deploy the code for a new application on an AWS Lambda function. The application needs a dependency file that is 500 MB to run the business logic.Which solution will meet these requirements?",
      "choices": {
        "A": "Compress the application code and dependencies into a .zip file. Directly upload the .zip file as a deployment package for the Lambda function instead of copying the code.",
        "B": "Compress the application code and dependencies into a .zip file. Upload the .zip file to an Amazon S3 bucket. Configure the Lambda function to run the code from the .zip file in the S3 bucket.",
        "C": "Package the application code and dependencies into a container image. Upload the image to an Amazon S3 bucket. Configure the Lambda function to run the code in the image.",
        "D": "Package the application code and dependencies into a container image. Push the image to an Amazon Elastic Container Registry (Amazon ECR) repository. Deploy the image to the Lambda function."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148936-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 9, 2024, 3:30 p.m.",
      "textHash": "01d5f5c6b4d1e426",
      "rawFormat": "discussion-md",
      "conceptKey": "ecr_image_scanning",
      "frExplanation": "AWS Lambda ex√©cute du code sans g√©rer de serveur. Pour d√©ployer, on peut utiliser soit un fichier .zip, soit une image de conteneur (container image).\nUn package .zip Lambda a des limites de taille : un fichier de d√©pendances de 500 MB est trop gros pour un d√©ploiement classique en .zip (m√™me via S3, la limite Lambda reste un frein).\nUne image de conteneur permet d‚Äôembarquer l‚Äôapplication + toutes les d√©pendances dans un format Docker, avec une limite beaucoup plus grande (jusqu‚Äô√† plusieurs Go).\nAmazon ECR (Elastic Container Registry) est le ‚Äúregistre‚Äù AWS fait pour stocker et versionner des images de conteneurs.\nLambda sait directement tirer (pull) une image depuis ECR et l‚Äôex√©cuter.\nDonc la bonne approche est : construire l‚Äôimage, la pousser dans ECR, puis configurer Lambda pour utiliser cette image.\nLes options A et B restent en .zip (trop petit). L‚Äôoption C est incorrecte car Lambda ne lance pas une image depuis S3 : c‚Äôest ECR qui est pr√©vu pour √ßa.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois apporter un projet au prof, mais ton dossier fait 500 pages. Un simple classeur (zip) est trop petit. Tu mets tout dans une grosse valise (container) et tu la stockes dans le casier sp√©cial ‚Äúvalises‚Äù du lyc√©e (ECR), puis le prof vient prendre la valise pour corriger.**\n\nLambda, c‚Äôest comme un √©l√®ve ‚Äúrobot‚Äù qui ex√©cute ton devoir quand on lui demande. Le probl√®me: ton devoir a un √©norme manuel (d√©pendance 500 MB). Un fichier .zip, m√™me envoy√© directement ou via un ‚Äúdrive‚Äù (S3), reste limit√©: c‚Äôest comme un classeur qui ne peut pas contenir un dossier aussi gros. La solution est de mettre ton code + le gros manuel dans une ‚Äúvalise‚Äù appel√©e image de conteneur. Cette valise doit √™tre rang√©e dans l‚Äôendroit pr√©vu pour √ßa: Amazon ECR (un casier √† images), pas dans S3 (plut√¥t un drive de fichiers). Ensuite, Lambda r√©cup√®re la valise depuis ECR et peut ex√©cuter l‚Äôappli. Donc D est la bonne r√©ponse.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:450:b4203d21459eee90",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 450,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has an application that uses AWS Lambda functions and AWS CloudFormation templates. Usage of the application has increased. As a result, the Lambda functions are encountering rate limit errors when they retrieve data.The Lambda functions retrieve an advanced parameter from AWS Systems Manager Parameter Store on every call. The parameter changes only during new deployments. Because the application‚Äôs usage is unpredictable, the developer needs a way to avoid the rate limiting.Which solution will meet these requirements MOST cost-effectively?",
      "choices": {
        "A": "Configure the Lambda functions to use reserved concurrency that is equal to the last month‚Äôs average number of concurrent invocations.",
        "B": "Add a retry mechanism with exponential backoff to the call to Parameter Store.",
        "C": "Request a service quota increase for Parameter Store GetParameter API operations to match the expected usage of the Lambda functions.",
        "D": "Add an SSM dynamic reference as an environment variable to the Lambda functions resource in the CloudFormation templates."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/150773-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 5, 2024, 12:08 p.m.",
      "textHash": "b4203d21459eee90",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Le probl√®me vient du fait que chaque ex√©cution de Lambda (service qui lance du code √† la demande) appelle Parameter Store (stockage de param√®tres) pour lire le m√™me param√®tre ¬´ avanc√© ¬ª. Quand le trafic augmente, trop d‚Äôappels GetParameter arrivent en m√™me temps et AWS renvoie des erreurs de limite (rate limit).\nComme le param√®tre ne change qu‚Äôau moment des d√©ploiements, il est inutile de le relire √† chaque invocation.\nLa solution la plus √©conomique est de le charger une seule fois lors du d√©ploiement, puis de le fournir √† Lambda via une variable d‚Äôenvironnement.\nAvec une r√©f√©rence dynamique SSM dans CloudFormation (outil d‚Äôinfrastructure as code), CloudFormation lit la valeur du param√®tre pendant le d√©ploiement et la place dans l‚Äôenvironnement de la fonction.\nEnsuite, Lambda utilise la variable d‚Äôenvironnement √† chaque appel, sans requ√™te vers Parameter Store, donc plus de rate limiting.\nA ne r√®gle pas les appels √† Parameter Store, B r√©duit les erreurs mais garde beaucoup d‚Äôappels et ajoute de la latence, C peut co√ªter plus cher et ne supprime pas la cause.\nDonc D r√©pond au besoin (param√®tre stable entre d√©ploiements) et √©vite les limites au moindre co√ªt.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une classe o√π, √† chaque exercice, chaque √©l√®ve court √† la biblioth√®que demander la m√™me fiche de consignes. La fiche ne change que quand le prof distribue une nouvelle version (nouveau d√©ploiement). Quand trop d‚Äô√©l√®ves y vont en m√™me temps, la biblioth√©caire dit ‚Äústop, trop de demandes‚Äù.**\n\nConcept : si une info change rarement, tu n‚Äôas pas besoin d‚Äôaller la redemander √† chaque fois ; tu peux la garder ‚Äúdans ton sac‚Äù. Ici, les fonctions Lambda sont les √©l√®ves, et Parameter Store est la biblioth√®que. Elles vont chercher le param√®tre √† chaque appel, donc √ßa cr√©e des embouteillages (rate limit). La r√©ponse D met le param√®tre directement dans une ‚Äúnote coll√©e sur la copie‚Äù (variable d‚Äôenvironnement) au moment o√π on pr√©pare la classe avec CloudFormation (le prof distribue la fiche). Du coup, pendant les exercices, plus besoin d‚Äôaller √† la biblioth√®que : z√©ro file d‚Äôattente, et c‚Äôest le plus √©conomique. A ne r√®gle pas le probl√®me de la biblioth√®que. B et C gardent les allers-retours, donc √ßa co√ªte/attend plus.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:42:b76729afed29723e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 42,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA development team wants to build a continuous integration/continuous delivery (CI/CD) pipeline. The team is using AWS CodePipeline to automate the code build and deployment. The team wants to store the program code to prepare for the CI/CD pipeline.Which AWS service should the team use to store the program code?",
      "choices": {
        "A": "AWS CodeDeploy",
        "B": "AWS CodeArtifact",
        "C": "AWS CodeCommit",
        "D": "Amazon CodeGuru"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103914-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 26, 2023, 2:02 a.m.",
      "textHash": "b76729afed29723e",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Pour une pipeline CI/CD, il faut d‚Äôabord un endroit central o√π stocker le code source (fichiers du programme) et suivre les versions.\nAWS CodeCommit est un service AWS de d√©p√¥t Git : il h√©berge votre code, garde l‚Äôhistorique des changements (commits) et permet de travailler en √©quipe.\nCodePipeline peut ensuite ‚Äúlire‚Äù ce d√©p√¥t comme source et d√©clencher automatiquement les √©tapes de build et de d√©ploiement.\nPourquoi pas CodeDeploy ? CodeDeploy sert √† d√©ployer une application sur des serveurs/instances, pas √† stocker le code.\nPourquoi pas CodeArtifact ? CodeArtifact stocke des d√©pendances et paquets (ex: biblioth√®ques npm, Maven), pas le code source principal.\nPourquoi pas CodeGuru ? CodeGuru analyse le code (qualit√©, performance) mais ne l‚Äôh√©berge pas.\nDonc, pour stocker le programme code avant la CI/CD, le bon choix est AWS CodeCommit.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un projet de groupe au lyc√©e : vous √©crivez un expos√© et vous devez garder toutes les versions du document au m√™me endroit, avec l‚Äôhistorique (qui a chang√© quoi, et quand).**\n\nLe CI/CD, c‚Äôest comme une cha√Æne automatique : d√®s que quelqu‚Äôun modifie l‚Äôexpos√©, √ßa v√©rifie, √ßa met en forme, puis √ßa ‚Äúrend‚Äù la bonne version.\nPour que cette cha√Æne marche, il faut d‚Äôabord un endroit o√π ranger le code, comme un ‚Äúcasier‚Äù officiel pour le document.\nAWS CodeCommit (C) est ce casier : un d√©p√¥t de code, avec versions et historique.\nCodeDeploy (A) c‚Äôest plut√¥t l‚Äôaction de ‚Äúdistribuer/rendre‚Äù le travail, pas le stocker.\nCodeArtifact (B) c‚Äôest une r√©serve de ‚Äúpi√®ces‚Äù toutes faites (biblioth√®que de composants), pas le document principal.\nCodeGuru (D) c‚Äôest un prof qui relit et donne des conseils, pas un endroit de stockage.\nDonc pour stocker le programme avant la cha√Æne CI/CD : CodeCommit (C).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:386:700426f2c8864c9e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 386,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses an AWS Lambda function to transfer files from an Amazon S3 bucket to the company's SFTP server. The Lambda function connects to the SFTP server by using credentials such as username and password. The company uses Lambda environment variables to store these credentials.A developer needs to implement encrypted username and password credentials.Which solution will meet these requirements?",
      "choices": {
        "A": "Remove the user credentials from the Lambda environment. Implement IAM database authentication.",
        "B": "Move the user credentials from Lambda environment variables to AWS Systems Manager Parameter Store.",
        "C": "Move the user credentials from Lambda environment variables to AWS Key Management Service (AWS KMS).",
        "D": "Move the user credentials from the Lambda environment to an encrypted .txt file. Store the file in an S3 bucket."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143763-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 6:08 a.m.",
      "textHash": "700426f2c8864c9e",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, le probl√®me est que le nom d‚Äôutilisateur et le mot de passe sont stock√©s dans des variables d‚Äôenvironnement Lambda, ce qui n‚Äôest pas id√©al pour des secrets.\nAWS Systems Manager Parameter Store est un service qui stocke des param√®tres, dont des ‚Äúsecrets‚Äù, et peut les chiffrer.\nOn y met les identifiants comme des ‚ÄúSecureString‚Äù : ils sont chiffr√©s au repos (souvent avec une cl√© AWS KMS) et on contr√¥le qui peut les lire via IAM.\nLa fonction Lambda r√©cup√®re le secret au moment de l‚Äôex√©cution (API GetParameter), au lieu de l‚Äôavoir en clair dans sa configuration.\nC‚Äôest donc une solution simple, g√©r√©e par AWS, avec chiffrement + permissions, adapt√©e aux identifiants.\nPourquoi pas KMS (C) : KMS g√®re des cl√©s et le chiffrement, mais ce n‚Äôest pas un coffre √† secrets pratique pour stocker/versions/acc√®s.\nPourquoi pas un fichier S3 (D) : m√™me chiffr√©, c‚Äôest plus fragile (gestion du fichier, acc√®s, rotation) et moins ‚Äúsecret manager‚Äù.\nPourquoi pas IAM database authentication (A) : cela concerne l‚Äôacc√®s √† certaines bases de donn√©es AWS, pas un serveur SFTP externe.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:375:511a90e2f3d5520e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 375,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is developing an application that will be accessed through the Amazon API Gateway REST API. Registered users should be the only ones who can access certain resources of this API. The token being used should expire automatically and needs to be refreshed periodically.How can a developer meet these requirements?",
      "choices": {
        "A": "Create an Amazon Cognito identity pool, configure the Amazon Cognito Authorizer in API Gateway, and use the temporary credentials generated by the identity pool.",
        "B": "Create and maintain a database record for each user with a corresponding token and use an AWS Lambda authorizer in API Gateway.",
        "C": "Create an Amazon Cognito user pool, configure the Cognito Authorizer in API Gateway, and use the identity or access token.",
        "D": "Create an IAM user for each API user, attach an invoke permissions policy to the API, and use an IAM authorizer in API Gateway."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143757-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:44 a.m.",
      "textHash": "511a90e2f3d5520e",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, on veut que seules des personnes ‚Äúinscrites‚Äù (utilisateurs enregistr√©s) puissent appeler certaines routes d‚Äôune API REST dans Amazon API Gateway.\nLe besoin cl√© est un ‚Äútoken‚Äù qui expire automatiquement et qu‚Äôon peut renouveler : c‚Äôest exactement le fonctionnement d‚ÄôAmazon Cognito User Pool (gestion d‚Äôutilisateurs + connexion).\nUn User Pool fournit des jetons JWT (ID token / Access token) avec une dur√©e de vie, et un Refresh token pour en obtenir de nouveaux sans se reconnecter √† chaque fois.\nAPI Gateway peut v√©rifier ces jetons directement via un ‚ÄúCognito Authorizer‚Äù : si le jeton est valide, la requ√™te passe; sinon elle est refus√©e.\nDonc la solution est : cr√©er un Cognito User Pool, configurer l‚Äôauthorizer Cognito dans API Gateway, et envoyer l‚ÄôID token ou l‚ÄôAccess token dans l‚Äôen-t√™te Authorization.\nPourquoi pas A : un Identity Pool sert surtout √† donner des identifiants AWS temporaires pour acc√©der √† des services AWS, pas √† prot√©ger simplement des endpoints d‚ÄôAPI avec login.\nPourquoi pas B : g√©rer soi-m√™me des tokens en base + Lambda authorizer est plus complexe et inutile quand Cognito fait d√©j√† l‚Äôexpiration/refresh.\nPourquoi pas D : cr√©er un IAM user par utilisateur final est une mauvaise pratique (gestion lourde, pas fait pour des utilisateurs d‚Äôapplication) et ne r√©pond pas simplement au refresh de tokens.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:130:c796473952402800",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 130,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on a web application that uses Amazon DynamoDB as its data store. The application has two DynamoDB tables: one table that is named artists and one table that is named songs. The artists table has artistName as the partition key. The songs table has songName as the partition key and artistName as the sort key.The table usage patterns include the retrieval of multiple songs and artists in a single database operation from the webpage. The developer needs a way to retrieve this information with minimal network traffic and optimal application performance.Which solution will meet these requirements?",
      "choices": {
        "A": "Perform a BatchGetltem operation that returns items from the two tables. Use the list of songName/artistName keys for the songs table and the list of artistName key for the artists table.",
        "B": "Create a local secondary index (LSI) on the songs table that uses artistName as the partition key. Perform a query operation for each artistName on the songs table that filters by the list of songName. Perform a query operation for each artistName on the artists table.",
        "C": "Perform a BatchGetitem operation on the songs table that uses the songName/artistName keys. Perform a BatchGetltem operation on the artists table that uses artistName as the key.",
        "D": "Perform a Scan operation on each table that filters by the list of songName/artistName for the songs table and the list of artistName in the artists table."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/111831-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 10, 2023, 2:49 p.m.",
      "textHash": "c796473952402800",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π l‚Äôon lit des √©l√©ments (items) surtout via leurs cl√©s (partition key et sort key).\nIci, la page web doit r√©cup√©rer plusieurs chansons ET plusieurs artistes en une seule op√©ration, avec peu de trafic r√©seau.\nBatchGetItem est fait pour √ßa : il permet de demander en une requ√™te des items pr√©cis, m√™me depuis plusieurs tables.\nPour songs, il faut fournir la cl√© compl√®te (songName + artistName) car songName est la partition key et artistName la sort key.\nPour artists, il suffit de fournir artistName (partition key).\nL‚Äôoption A fait exactement une seule op√©ration BatchGetItem qui lit dans les deux tables, donc moins d‚Äôallers-retours r√©seau et meilleure performance.\nL‚Äôoption C fait deux BatchGetItem s√©par√©s (plus de trafic r√©seau).\nLes options B et D utilisent Query/Scan multiples : plus lent, plus co√ªteux, et Scan lit beaucoup de donn√©es inutiles.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e avec 2 classeurs : un classeur ‚ÄúArtistes‚Äù (rang√© par nom d‚Äôartiste) et un classeur ‚ÄúChansons‚Äù (rang√© par nom de chanson, puis par artiste). Tu veux pr√©parer une page ‚Äúplaylist‚Äù et r√©cup√©rer d‚Äôun coup plusieurs fiches d‚Äôartistes + plusieurs fiches de chansons, sans faire 50 allers-retours au bureau.**\n\nConcept : moins tu fais d‚Äôallers-retours entre toi et la biblioth√©caire, plus c‚Äôest rapide. Donc tu donnes une liste pr√©cise de fiches √† prendre, et elle te les apporte en une seule fois.\nPourquoi A : BatchGetItem, c‚Äôest exactement ‚Äúdonner une liste de fiches √† r√©cup√©rer‚Äù et recevoir tout en un seul voyage, m√™me depuis 2 classeurs diff√©rents.\nPour les chansons, tu donnes les cl√©s exactes (songName + artistName) comme ‚Äútitre + auteur‚Äù pour trouver la bonne fiche.\nPour les artistes, tu donnes juste artistName, car le classeur Artistes est rang√© par ce nom.\nB te fait demander plusieurs fois (une requ√™te par artiste) = trop d‚Äôallers-retours.\nC fait 2 voyages s√©par√©s (un pour Chansons, un pour Artistes) = plus de trafic que A.\nD c‚Äôest fouiller tout le classeur (Scan) puis trier apr√®s = lent et inutile.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:127:7afa2dc71d8a4a4f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 127,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Lambda function. The Lambda function needs an external library to connect to a third-party solution. The external library is a collection of files with a total size of 100 MB. The developer needs to make the external library available to the Lambda execution environment and reduce the Lambda package space.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Create a Lambda layer to store the external library. Configure the Lambda function to use the layer.",
        "B": "Create an Amazon S3 bucket. Upload the external library into the S3 bucket. Mount the S3 bucket folder in the Lambda function. Import the library by using the proper folder in the mount point.",
        "C": "Load the external library to the Lambda function's /tmp directory during deployment of the Lambda package. Import the library from the /tmp directory.",
        "D": "Create an Amazon Elastic File System (Amazon EFS) volume. Upload the external library to the EFS volume. Mount the EFS volume in the Lambda function. Import the library by using the proper folder in the mount point."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107064-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 23, 2023, 12:31 a.m.",
      "textHash": "7afa2dc71d8a4a4f",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:df6eaaa0",
      "frExplanation": "AWS Lambda ex√©cute du code sans g√©rer de serveur, mais le ‚Äúpackage‚Äù (zip) de la fonction a une taille limit√©e et doit rester l√©ger.\nUne biblioth√®que externe de 100 MB alourdit fortement ce package si on l‚Äôinclut directement.\nUne ‚ÄúLambda Layer‚Äù est un paquet s√©par√© (code/d√©pendances) que plusieurs fonctions peuvent r√©utiliser : Lambda le charge automatiquement dans l‚Äôenvironnement d‚Äôex√©cution.\nDonc la fonction reste petite, et la biblioth√®que est disponible au runtime sans bricolage.\nOption B est incorrecte : Lambda ne peut pas ‚Äúmonter‚Äù un bucket S3 comme un disque (S3 est du stockage objet, pas un syst√®me de fichiers montable).\nOption C est fragile : /tmp est un espace temporaire (limit√©, non garanti entre ex√©cutions) et ne r√©duit pas vraiment la gestion des d√©pendances.\nOption D (EFS) fonctionne mais ajoute plus d‚Äôop√©rations (cr√©er/configurer EFS, VPC, montages) : plus d‚Äôoverhead que les Layers.\nDonc la solution la plus simple et avec le moins d‚Äôeffort op√©rationnel est d‚Äôutiliser une Lambda Layer.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un expos√© en groupe au lyc√©e : tu as besoin d‚Äôun gros classeur de documents (100 pages) que plusieurs groupes utilisent.**\n\nConcept : au lieu de mettre le gros classeur dans le sac de chaque √©l√®ve, on le laisse au CDI et chaque groupe vient le consulter quand il en a besoin.\nDans AWS, ta fonction Lambda = l‚Äô√©l√®ve qui fait l‚Äôexpos√©, et la ‚Äúbiblioth√®que externe‚Äù = le gros classeur.\nA (Lambda Layer) = le CDI : tu ranges la biblioth√®que une seule fois dans un ‚Äúrayon‚Äù partag√©, et ta Lambda la ‚Äúbranche‚Äù dessus.\n√áa r√©duit la taille du sac (le package Lambda), car tu ne copies pas les 100 MB √† chaque fois.\nEt c‚Äôest le moins de travail au quotidien : pas de serveur √† g√©rer, pas de disque r√©seau √† maintenir.\nB est faux : on ne ‚Äúmonte‚Äù pas un dossier S3 comme un disque dans Lambda, S3 c‚Äôest plut√¥t un stockage de fichiers √† t√©l√©charger.\nC est faux : /tmp c‚Äôest un petit casier temporaire, pas fait pour stocker proprement une grosse biblioth√®que au d√©ploiement.\nD marche mais c‚Äôest comme louer une salle + armoire (EFS) : plus de gestion, donc plus d‚Äôeffort que le CDI (Layer).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:126:bf41e913aecb3675",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 126,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is integrating Amazon ElastiCache in an application. The cache will store data from a database. The cached data must populate real-time dashboards.Which caching strategy will meet these requirements?",
      "choices": {
        "A": "A read-through cache",
        "B": "A write-behind cache",
        "C": "A lazy-loading cache",
        "D": "A write-through cache"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107063-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 23, 2023, 12:25 a.m.",
      "textHash": "bf41e913aecb3675",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:543152cc",
      "frExplanation": "Amazon ElastiCache est un service de cache en m√©moire (tr√®s rapide) plac√© entre votre application et la base de donn√©es.\nIci, les tableaux de bord doivent afficher des donn√©es ‚Äúen temps r√©el‚Äù, donc le cache doit √™tre mis √† jour imm√©diatement quand les donn√©es changent.\nLa strat√©gie ‚Äúwrite-through‚Äù signifie : √† chaque √©criture, l‚Äôapplication √©crit d‚Äôabord dans le cache ET dans la base de donn√©es (ou via une couche qui fait les deux).\nR√©sultat : le cache contient toujours la derni√®re valeur, et les dashboards lisent des donn√©es fra√Æches sans attendre.\n‚ÄúWrite-behind‚Äù √©crit d‚Äôabord dans le cache puis met √† jour la base plus tard : risque de d√©calage, donc pas id√©al pour du temps r√©el.\n‚ÄúRead-through‚Äù et ‚Äúlazy-loading‚Äù remplissent surtout le cache lors des lectures (quand il manque), donc apr√®s une mise √† jour, le cache peut rester ancien jusqu‚Äô√† la prochaine lecture.\nDonc, pour des dashboards en temps r√©el, ‚Äúwrite-through cache‚Äù est le meilleur choix.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le tableau d‚Äôaffichage du lyc√©e (dashboard) qui doit montrer en temps r√©el les absences. La base de donn√©es, c‚Äôest le registre officiel au secr√©tariat. Le cache, c‚Äôest un petit carnet pos√© √† l‚Äôaccueil, super rapide √† consulter.**\n\nConcept : un cache sert √† lire vite, mais il doit rester √† jour. Pour un dashboard ‚Äúen temps r√©el‚Äù, il faut que le carnet soit mis √† jour d√®s qu‚Äôon √©crit dans le registre officiel.\nWrite-through (D) = √† chaque nouvelle absence, on √©crit en m√™me temps dans le registre (base) ET dans le carnet (cache). Donc le tableau d‚Äôaffichage voit tout de suite la bonne info.\nWrite-behind (B) = on √©crit d‚Äôabord dans le carnet, et le registre est mis √† jour plus tard : risque de d√©calage.\nRead-through (A) et lazy-loading (C) = on remplit le carnet seulement quand quelqu‚Äôun demande l‚Äôinfo : le dashboard peut afficher du vieux ou attendre.\nDonc D est le meilleur pour du ‚Äútemps r√©el‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:125:2416365bed8aca99",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 125,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Serverless Application Model (AWS SAM) template. The AWS SAM template contains the definition of multiple AWS Lambda functions, an Amazon S3 bucket, and an Amazon CloudFront distribution. One of the Lambda functions runs on Lambda@Edge in the CloudFront distribution. The S3 bucket is configured as an origin for the CloudFront distribution.When the developer deploys the AWS SAM template in the eu-west-1 Region, the creation of the stack fails.Which of the following could be the reason for this issue?",
      "choices": {
        "A": "CloudFront distributions can be created only in the us-east-1 Region.",
        "B": "Lambda@Edge functions can be created only in the us-east-1 Region.",
        "C": "A single AWS SAM template cannot contain multiple Lambda functions.",
        "D": "The CloudFront distribution and the S3 bucket cannot be created in the same Region."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107062-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 23, 2023, 12:06 a.m.",
      "textHash": "2416365bed8aca99",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:e5b1bb95",
      "frExplanation": "CloudFront est un service CDN ¬´ global ¬ª : on peut cr√©er une distribution depuis n‚Äôimporte quelle r√©gion, ce n‚Äôest pas limit√© √† us-east-1.\nLambda@Edge, en revanche, est sp√©cial : ce sont des fonctions Lambda copi√©es et ex√©cut√©es sur les emplacements Edge de CloudFront.\nPour des raisons d‚Äôarchitecture, la fonction Lambda ¬´ source ¬ª associ√©e √† Lambda@Edge doit obligatoirement √™tre cr√©√©e dans la r√©gion us-east-1 (N. Virginia).\nDonc si votre template SAM est d√©ploy√© en eu-west-1 et qu‚Äôil essaie de cr√©er la fonction Lambda@Edge dans cette r√©gion, la cr√©ation du stack √©choue.\nAvoir plusieurs fonctions Lambda dans un m√™me template SAM est normal, donc ce n‚Äôest pas la cause.\nUn bucket S3 peut √™tre dans la m√™me r√©gion que la configuration du stack, et CloudFront peut utiliser ce bucket comme origin sans probl√®me.\nLa bonne r√©ponse est donc B : Lambda@Edge doit √™tre cr√©√© en us-east-1.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un tournoi de jeu vid√©o en ligne. Tu as un serveur central ‚Äúsp√©cial‚Äù qui g√®re les r√®gles du tournoi pour tous les joueurs du monde, et des ‚Äúpoints relais‚Äù dans chaque pays pour que √ßa charge vite.**\n\nConcept : CloudFront, c‚Äôest comme les points relais qui distribuent le contenu pr√®s des joueurs. S3, c‚Äôest l‚Äôentrep√¥t o√π sont stock√©s les fichiers. Lambda, c‚Äôest un petit ‚Äúbot‚Äù qui ex√©cute une action. Lambda@Edge, c‚Äôest un bot plac√© directement dans les points relais pour agir au plus pr√®s des joueurs.\nPourquoi B : pour que le bot soit copi√© et utilis√© dans tous les points relais du monde, AWS impose que ce bot Lambda@Edge soit ‚Äúd√©clar√©‚Äù dans la r√©gion centrale sp√©ciale : us-east-1. Si tu d√©ploies tout en eu-west-1, le bot Lambda@Edge ne peut pas √™tre cr√©√© au bon endroit, donc le d√©ploiement √©choue. Les autres choix sont faux : CloudFront est mondial, on peut avoir plusieurs Lambda, et S3 + CloudFront peuvent √™tre dans la m√™me r√©gion.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:124:211fb22c64aac75f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 124,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn organization is storing large files in Amazon S3, and is writing a web application to display meta-data about the files to end-users. Based on the metadata a user selects an object to download. The organization needs a mechanism to index the files and provide single-digit millisecond latency retrieval for the metadata.What AWS service should be used to accomplish this?",
      "choices": {
        "A": "Amazon DynamoDB",
        "B": "Amazon EC2",
        "C": "AWS Lambda",
        "D": "Amazon RDS"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107060-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 11:57 p.m.",
      "textHash": "211fb22c64aac75f",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:s3:402138d2",
      "frExplanation": "Les fichiers sont stock√©s dans Amazon S3 (un service de stockage d‚Äôobjets). S3 est tr√®s bon pour stocker et t√©l√©charger des fichiers, mais il n‚Äôest pas fait pour faire des recherches rapides dans des m√©tadonn√©es (ex: par type, date, tags, propri√©taire).\nPour afficher une liste filtrable et retrouver les m√©tadonn√©es en quelques millisecondes, il faut une base de donn√©es tr√®s rapide en lecture/√©criture.\nAmazon DynamoDB est une base de donn√©es NoSQL g√©r√©e par AWS, con√ßue pour des acc√®s tr√®s rapides (latence en millisecondes) et pour servir des applications web √† grande √©chelle.\nOn peut y stocker, pour chaque objet S3, ses m√©tadonn√©es (cl√© S3, taille, tags, date, etc.) et cr√©er des cl√©s/index pour rechercher vite.\nAmazon EC2 est juste un serveur √† g√©rer soi-m√™me, ce n‚Äôest pas un service d‚Äôindexation/BD.\nAWS Lambda ex√©cute du code, mais ne stocke pas les donn√©es; il lui faut une BD derri√®re.\nAmazon RDS (SQL) peut fonctionner, mais la question demande une latence tr√®s faible et une indexation simple √† grande √©chelle: DynamoDB est le choix le plus adapt√©.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéí Imagine ton casier au lyc√©e** : tu ranges tes affaires diff√©remment selon comment tu les cherches.\n\n**LSI (Local)** = C'est comme **des onglets DANS ton classeur** üìÅ - Cr√©√©s √† la rentr√©e, jamais modifiables, super rapides (m√™me partition).\n\n**GSI (Global)** = C'est comme **un index √† la fin du livre** üìñ - Ajoutable n'importe quand, cherche partout, mais plus lent.\n\n**üß† Mn√©motechnique :** \"**L**SI = **L**ocal, **L**imit√©\" | \"**G**SI = **G**lobal, **G**√©nial\"\n\n**Quand utiliser GSI :** Quand tu veux chercher par un autre crit√®re que la cl√© principale, sur TOUTE la table.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:121:3062372ecd539408",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 121,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company's developer is building a static website to be deployed in Amazon S3 for a production environment. The website integrates with an Amazon Aurora PostgreSQL database by using an AWS Lambda function. The website that is deployed to production will use a Lambda alias that points to a specific version of the Lambda function.The company must rotate the database credentials every 2 weeks. Lambda functions that the company deployed previously must be able to use the most recent credentials.Which solution will meet these requirements?",
      "choices": {
        "A": "Store the database credentials in AWS Secrets Manager. Turn on rotation. Write code in the Lambda function to retrieve the credentials from Secrets Manager.",
        "B": "Include the database credentials as part of the Lambda function code. Update the credentials periodically and deploy the new Lambda function.",
        "C": "Use Lambda environment variables. Update the environment variables when new credentials are available.",
        "D": "Store the database credentials in AWS Systems Manager Parameter Store. Turn on rotation. Write code in the Lambda function to retrieve the credentials from Systems Manager Parameter Store."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107055-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 11:31 p.m.",
      "textHash": "3062372ecd539408",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Ici, le site statique (S3) appelle une fonction AWS Lambda pour acc√©der √† une base Aurora PostgreSQL. Le point cl√©: les identifiants de base de donn√©es doivent changer automatiquement toutes les 2 semaines (rotation) et les anciennes versions de Lambda (via un alias pointant vers une version) doivent toujours fonctionner avec les identifiants les plus r√©cents.\nAWS Secrets Manager est un service fait pour stocker des ‚Äúsecrets‚Äù (mots de passe, cl√©s) et g√©rer leur rotation automatique, notamment pour des bases de donn√©es comme Aurora.\nSi la Lambda r√©cup√®re les identifiants √† chaque ex√©cution depuis Secrets Manager, m√™me une ancienne version de la Lambda utilisera toujours la valeur actuelle (car le secret est mis √† jour c√¥t√© service, pas dans le code).\nB est mauvais: mettre le mot de passe dans le code oblige √† red√©ployer et les anciennes versions garderaient l‚Äôancien mot de passe.\nC est mauvais: les variables d‚Äôenvironnement sont ‚Äúfig√©es‚Äù par version; une ancienne version/alias peut conserver l‚Äôancienne valeur.\nD est moins adapt√©: Parameter Store peut stocker des param√®tres, mais la rotation automatique g√©r√©e pour des identifiants de base de donn√©es est typiquement attendue avec Secrets Manager.\nDonc la bonne solution est de stocker les identifiants dans Secrets Manager, activer la rotation, et faire lire le secret par la Lambda.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:526:4e1edabd8287821b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 526,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application that will process messages from an Amazon Simple Queue Service (Amazon SQS) standard queue. The application needs to process the messages in an Amazon Elastic Container Service (Amazon ECS) task.Which actions will result in the MOST cost-effective processing of the messages? (Choose two.)",
      "choices": {
        "A": "Use long polling to query the queue for new messages.",
        "B": "Use short polling to query the queue for new messages.",
        "C": "Use message batching to retrieve messages from the queue.",
        "D": "Use Amazon ElastiCache to cache messages in the queue.",
        "E": "Use an SQS FIFO queue to manage the messages."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153619-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 29, 2024, 6:30 p.m.",
      "textHash": "4e1edabd8287821b",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Amazon SQS est une file de messages : votre application ‚Äútire‚Äù des messages pour les traiter. Chaque appel API √† SQS co√ªte, donc moins d‚Äôappels = moins cher.\nLe long polling (A) fait attendre la requ√™te jusqu‚Äô√† ce qu‚Äôun message arrive (ou un d√©lai), au lieu de r√©pondre tout de suite ‚Äúvide‚Äù. R√©sultat : beaucoup moins de requ√™tes inutiles quand la file est souvent vide, donc plus √©conomique.\nLe short polling (B) r√©pond imm√©diatement : si la file est vide, vous refaites des appels en boucle, ce qui augmente les co√ªts.\nLe message batching (C) permet de r√©cup√©rer plusieurs messages en une seule requ√™te (jusqu‚Äô√† 10). Vous payez moins d‚Äôappels pour le m√™me volume de messages, donc c‚Äôest aussi plus rentable.\nElastiCache (D) est un cache en m√©moire : il n‚Äôa pas de sens pour ‚Äúcacher‚Äù des messages SQS et ajouterait des serveurs √† payer.\nUne file FIFO (E) sert √† garantir l‚Äôordre et l‚Äôunicit√©, pas √† r√©duire les co√ªts ; elle peut m√™me √™tre plus restrictive. Les choix les plus √©conomiques sont donc A et C.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une bo√Æte aux lettres au lyc√©e o√π les profs d√©posent des devoirs √† corriger. Toi, tu es un √©l√®ve ‚Äúassistant‚Äù (la t√¢che ECS) pay√© au temps pass√© √† attendre et √† travailler.**\n\nConcept : SQS = bo√Æte aux lettres de messages. ECS task = √©l√®ve qui vient chercher des devoirs. Chaque ‚Äúaller voir‚Äù co√ªte du temps/argent.\nLong polling (A) = tu vas √† la bo√Æte et tu attends un peu sur place : si un devoir arrive, tu le prends tout de suite. Donc moins d‚Äôallers-retours inutiles.\nShort polling (B) = tu passes toutes les 2 secondes ‚Äújuste pour v√©rifier‚Äù : plein de visites o√π il n‚Äôy a rien, donc tu gaspilles.\nMessage batching (C) = quand tu y vas, tu prends plusieurs devoirs d‚Äôun coup : moins de trajets pour la m√™me quantit√© de travail, donc moins cher.\nElastiCache (D) = comme mettre une autre bo√Æte aux lettres en plus : √ßa rajoute du co√ªt, √ßa ne r√©duit pas les trajets.\nFIFO (E) = juste une r√®gle ‚Äúdans l‚Äôordre‚Äù, pas une astuce pour payer moins.\nDonc les plus √©conomiques : A (attendre intelligemment) et C (prendre par lots).",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:120:dae7330fdff7e65a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 120,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has written code for an application and wants to share it with other developers on the team to receive feedback. The shared application code needs to be stored long-term with multiple versions and batch change tracking.Which AWS service should the developer use?",
      "choices": {
        "A": "AWS CodeBuild",
        "B": "Amazon S3",
        "C": "AWS CodeCommit",
        "D": "AWS Cloud9"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107054-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 11:26 p.m.",
      "textHash": "dae7330fdff7e65a",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Le besoin d√©crit correspond √† un ‚Äúd√©p√¥t de code‚Äù : stocker du code sur le long terme, garder plusieurs versions, et suivre les changements (qui a modifi√© quoi, quand, et pourquoi).\nAWS CodeCommit est un service AWS de gestion de code source bas√© sur Git : il permet de cr√©er des d√©p√¥ts, faire des commits, des branches, des tags et consulter l‚Äôhistorique.\nC‚Äôest exactement ce qu‚Äôon utilise pour partager du code avec une √©quipe et recevoir des retours via des revues de code et des pull requests (selon l‚Äôoutil Git utilis√©).\nAmazon S3 stocke des fichiers, mais ce n‚Äôest pas un outil de gestion de versions et de suivi de changements ‚Äúpar lot‚Äù comme Git.\nAWS CodeBuild sert √† compiler/tester le code automatiquement (CI), pas √† le stocker et versionner.\nAWS Cloud9 est un environnement de d√©veloppement en ligne (√©diteur), pas un syst√®me de contr√¥le de version.\nDonc le bon choix est AWS CodeCommit.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:118:ba7d98d3b3ba55a8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 118,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing an application for a company. The application will be deployed on Amazon EC2 and will use an Amazon RDS for Microsoft SQL Server database. The company's security team requires that database credentials are rotated at least weekly.How should the developer configure the database credentials for this application?",
      "choices": {
        "A": "Create a database user. Store the user name and password in an AWS Systems Manager Parameter Store secure string parameter. Enable rotation of the AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter.",
        "B": "Enable IAM authentication for the database. Create a database user for use with IAM authentication. Enable password rotation.",
        "C": "Create a database user. Store the user name and password in an AWS Secrets Manager secret that has daily rotation enabled.",
        "D": "Use the EC2 user data to create a database user. Provide the user name and password in environment variables to the application."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107052-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 11:16 p.m.",
      "textHash": "ba7d98d3b3ba55a8",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, l‚Äôexigence cl√© est : ¬´ les identifiants de base de donn√©es doivent √™tre chang√©s (rotated) au moins chaque semaine ¬ª.\nAmazon RDS SQL Server utilise un couple utilisateur/mot de passe ; il faut donc un service qui stocke ces secrets et peut les renouveler automatiquement.\nAWS Secrets Manager est con√ßu pour stocker des mots de passe et faire la rotation automatique selon un planning (via une fonction de rotation), puis mettre √† jour le secret.\nEn choisissant une rotation quotidienne, on respecte largement l‚Äôexigence ‚Äúau moins hebdomadaire‚Äù. L‚Äôapplication sur EC2 lit toujours le secret √† jour.\nA est faux : faire tourner la cl√© KMS ne change pas le mot de passe, √ßa ne fait que changer la cl√© de chiffrement.\nB est faux : l‚Äôauthentification IAM n‚Äôest pas la m√©thode standard pour RDS Microsoft SQL Server comme pour d‚Äôautres moteurs, et ‚Äúrotation de mot de passe‚Äù ne s‚Äôapplique pas ainsi.\nD est faux : mettre des mots de passe dans user data / variables d‚Äôenvironnement est risqu√© (exposition, logs, acc√®s admin) et ne g√®re pas la rotation automatique.\nDonc la bonne r√©ponse est de stocker les identifiants dans Secrets Manager avec rotation activ√©e.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:117:a276e017c75a127e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 117,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer maintains a critical business application that uses Amazon DynamoDB as the primary data store. The DynamoDB table contains millions of documents and receives 30-60 requests each minute. The developer needs to perform processing in near-real time on the documents when they are added or updated in the DynamoDB table.How can the developer implement this feature with the LEAST amount of change to the existing application code?",
      "choices": {
        "A": "Set up a cron job on an Amazon EC2 instance. Run a script every hour to query the table for changes and process the documents.",
        "B": "Enable a DynamoDB stream on the table. Invoke an AWS Lambda function to process the documents.",
        "C": "Update the application to send a PutEvents request to Amazon EventBridge. Create an EventBridge rule to invoke an AWS Lambda function to process the documents.",
        "D": "Update the application to synchronously process the documents directly after the DynamoDB write."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107051-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 11:13 p.m.",
      "textHash": "a276e017c75a127e",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Objectif : traiter presque en temps r√©el chaque ajout/modification dans une table DynamoDB, avec le moins de changements de code.\nDynamoDB est une base NoSQL g√©r√©e ; elle peut publier automatiquement les changements via DynamoDB Streams (un ‚Äújournal‚Äù des insertions/mises √† jour/suppressions).\nAWS Lambda est un service qui ex√©cute du code √† la demande, sans serveur √† g√©rer, et peut √™tre d√©clench√© par un √©v√©nement.\nEn activant DynamoDB Streams sur la table, chaque √©criture g√©n√®re un √©v√©nement que Lambda re√ßoit imm√©diatement pour traiter le document.\nCela ne n√©cessite g√©n√©ralement aucun changement dans l‚Äôapplication existante : elle continue d‚Äô√©crire dans DynamoDB comme avant.\nA est faux : un cron toutes les heures n‚Äôest pas ‚Äúnear-real time‚Äù et oblige √† scanner/rechercher les changements (co√ªteux et complexe).\nC demande de modifier le code pour envoyer des √©v√©nements √† EventBridge, donc plus de changements.\nD ajoute du traitement synchrone apr√®s l‚Äô√©criture : plus de code, plus de latence et risque de ralentir/faire √©chouer les √©critures.\nDonc la solution la plus simple et adapt√©e est Streams + Lambda.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:116:94e1380ad5cab176",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 116,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has hundreds of AWS Lambda functions that the company's QA team needs to test by using the Lambda function URLs. A developer needs to configure the authentication of the Lambda functions to allow access so that the QA IAM group can invoke the Lambda functions by using the public URLs.Which solution will meet these requirements?",
      "choices": {
        "A": "Create a CLI script that loops on the Lambda functions to add a Lambda function URL with the AWS_IAM auth type. Run another script to create an IAM identity-based policy that allows the lambda:InvokeFunctionUrl action to all the Lambda function Amazon Resource Names (ARNs). Attach the policy to the QA IAM group.",
        "B": "Create a CLI script that loops on the Lambda functions to add a Lambda function URL with the NONE auth type. Run another script to create an IAM resource-based policy that allows the lambda:InvokeFunctionUrl action to all the Lambda function Amazon Resource Names (ARNs). Attach the policy to the QA IAM group.",
        "C": "Create a CLI script that loops on the Lambda functions to add a Lambda function URL with the AWS_IAM auth type. Run another script to loop on the Lambda functions to create an IAM identity-based policy that allows the lambda:InvokeFunctionUrl action from the QA IAM group's Amazon Resource Name (ARN).",
        "D": "Create a CLI script that loops on the Lambda functions to add a Lambda function URL with the NONE auth type. Run another script to loop on the Lambda functions to create an IAM resource-based policy that allows the lambda:InvokeFunctionUrl action from the QA IAM group's Amazon Resource Name (ARN)."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107050-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 11:08 p.m.",
      "textHash": "94e1380ad5cab176",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Une Lambda est un petit programme ex√©cut√© dans AWS. Une ‚ÄúFunction URL‚Äù est une adresse web publique pour appeler cette Lambda.\nSi l‚ÄôURL est en auth type NONE, n‚Äôimporte qui sur Internet peut l‚Äôappeler : ce n‚Äôest pas acceptable si on veut limiter √† l‚Äô√©quipe QA.\nAvec auth type AWS_IAM, l‚Äôappel doit √™tre sign√© avec des identifiants IAM (utilisateurs/roles) : seuls ceux qui ont la permission peuvent invoquer.\nPour autoriser un groupe IAM (QA), on donne une permission via une politique IAM ‚Äúidentity-based‚Äù attach√©e au groupe.\nL‚Äôaction n√©cessaire est lambda:InvokeFunctionUrl, et il faut la permettre sur les ARN des fonctions concern√©es.\nComme il y a des centaines de fonctions, un script CLI qui ajoute l‚ÄôURL en AWS_IAM et un autre qui cr√©e/attache la politique au groupe est pratique.\nLes options avec NONE rendent l‚Äôacc√®s trop ouvert, et celles qui parlent d‚Äôautoriser ‚Äúdepuis l‚ÄôARN du groupe‚Äù ne correspondent pas au mod√®le standard (les groupes ne sont pas des principals utilisables ainsi).\nDonc la solution A est la bonne : URL en AWS_IAM + policy IAM sur le groupe QA autorisant InvokeFunctionUrl sur les fonctions.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un lyc√©e avec des centaines de salles (les fonctions). Chaque salle a une porte avec un lien public (l‚ÄôURL). Mais tu veux que seuls les √©l√®ves du club QA puissent entrer, pas tout le monde.**\n\nConcept : une URL peut √™tre ouverte √† tous (comme une porte sans serrure) ou prot√©g√©e par une carte d‚Äô√©l√®ve (auth AWS_IAM = ‚Äúmontre ta carte‚Äù).\nIci, on veut que le club QA entre via les liens, donc on met une serrure ‚Äúcarte obligatoire‚Äù sur chaque porte : AWS_IAM.\nEnsuite, il faut donner au groupe QA l‚Äôautorisation officielle d‚Äôouvrir ces portes : une r√®gle attach√©e au groupe (policy ‚Äúidentity-based‚Äù).\nLa r√©ponse A fait exactement √ßa : elle met AWS_IAM sur toutes les URLs, puis donne au groupe QA le droit ‚ÄúInvokeFunctionUrl‚Äù sur toutes les salles (ARNs).\nPourquoi pas NONE (B/D) ? Ce serait comme laisser la porte sans serrure : n‚Äôimporte qui avec le lien peut entrer.\nPourquoi pas C ? Elle parle d‚Äôune r√®gle qui ‚Äúvient du groupe‚Äù mais sans la bonne logique d‚Äôattacher une autorisation claire au groupe pour toutes les salles.\nDonc A = portes verrouill√©es + cartes QA autoris√©es, pour toutes les salles.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:480:5c972f982b8fb9fb",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 480,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs a new application on AWS Elastic Beanstalk. The company needs to deploy updates to the application. The updates must not cause any downtime for application users.The deployment must forward a specified percentage of incoming client traffic to a new application version during an evaluation period.Which deployment type will meet these requirements?",
      "choices": {
        "A": "rolling",
        "B": "traffic-splitting",
        "C": "in-place",
        "D": "immutable"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152955-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 13, 2024, 3:43 a.m.",
      "textHash": "5c972f982b8fb9fb",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:c1dc023e",
      "frExplanation": "Elastic Beanstalk est un service AWS qui d√©ploie et g√®re automatiquement votre application (serveurs, mise √† l‚Äô√©chelle, etc.). Ici, on veut deux choses : 1) aucune interruption (z√©ro downtime) et 2) envoyer seulement un pourcentage du trafic vers la nouvelle version pendant une p√©riode de test. Le d√©ploiement ¬´ traffic-splitting ¬ª fait exactement cela : Beanstalk garde l‚Äôancienne version active et dirige, par exemple, 10% des utilisateurs vers la nouvelle version pour l‚Äô√©valuer, puis on peut augmenter ou revenir en arri√®re. Comme l‚Äôancienne version continue de servir le reste du trafic, les utilisateurs ne subissent pas de coupure. Un d√©ploiement ¬´ rolling ¬ª met √† jour par lots et peut r√©duire la capacit√© ou provoquer des effets visibles. ¬´ In-place ¬ª remplace sur place et peut causer du downtime. ¬´ Immutable ¬ª cr√©e de nouveaux serveurs mais ne vise pas le partage de trafic par pourcentage pendant une p√©riode d‚Äô√©valuation.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:476:679b5b3d5d763194",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 476,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an ecommerce application. The application's API sends order data to an Amazon Simple Queue Service (Amazon SOS) queue. A developer needs to modify the application to enrich the order data before the application sends the order data to a fulfillment system.Which solution will meet this requirement with the LEAST development effort?",
      "choices": {
        "A": "Create an AWS Lambda function to poll the SOS queue. enrich the message data, and send the enriched data to the fulfilment system, Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the Lambda function to the SNS topic.",
        "B": "Create an AWS Step Functions state machine. Configure an Amazon EventBridge rule to run the state machine when an order is published to the SQS queue. Map the orders to an AWS Lambda function. Program the Lambda function to perform the data enrichment and to invoke the state machine. Configure the last step of the state machine to send the enriched data to the fulfilment system,",
        "C": "Create an Amazon EMR cluster to read messages from the SQS queue. Configure an EMR job to enrich the order data. Create and configure an Amazon S3 bucket as the output location. Adjust the order fulfilment system to retrieve the enriched files from the S3 bucket.",
        "D": "Create an Amazon EventBridge pipe that uses event enrichment. Configure the SQS queue as a source for the pipe. Set the fulfillment system as the target of the pipe."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153146-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 18, 2024, 3:26 a.m.",
      "textHash": "679b5b3d5d763194",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, l‚ÄôAPI met des commandes dans une file Amazon SQS (un service qui stocke des messages pour les traiter plus tard). On veut ¬´ enrichir ¬ª les donn√©es (ajouter/transformer des champs) avant d‚Äôenvoyer au syst√®me de pr√©paration/exp√©dition, avec le moins de code possible.\nAmazon EventBridge Pipes sert justement √† connecter une source (comme SQS) √† une cible, avec des options int√©gr√©es de filtrage, transformation et enrichissement.\nAvec un Pipe, vous configurez : SQS comme source, une √©tape d‚Äôenrichissement (sans b√¢tir toute une application), puis le syst√®me de fulfillment comme cible.\nCela √©vite d‚Äô√©crire et maintenir une Lambda qui ‚Äúpoll‚Äù la file, de g√©rer des d√©clencheurs, des erreurs et des retries vous‚Äëm√™me.\nStep Functions (B) ajoute de l‚Äôorchestration et du code inutile pour un simple enrichissement.\nEMR (C) est un cluster Big Data, beaucoup trop lourd pour enrichir des messages de commande.\nDonc D est le meilleur choix car il r√©pond au besoin avec une configuration manag√©e et tr√®s peu de d√©veloppement.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : les √©l√®ves d√©posent des tickets de commande (plat, boisson) dans une bo√Æte. Avant d‚Äôenvoyer la commande au cuisinier, quelqu‚Äôun doit ajouter des infos manquantes (allergies, classe, num√©ro de table).**\n\nLa file SQS, c‚Äôest la bo√Æte o√π arrivent les tickets de commande. Le ‚Äúbesoin‚Äù, c‚Äôest d‚Äôajouter des infos (enrichir) avant d‚Äôenvoyer au cuisinier (le syst√®me de pr√©paration). La meilleure solution avec le moins d‚Äôeffort, c‚Äôest un ‚Äútuyau automatique‚Äù qui prend chaque ticket, ajoute les infos, puis l‚Äôenvoie direct au cuisinier : c‚Äôest EventBridge Pipe avec enrichment (D). Tu n‚Äôas pas √† coder beaucoup : tu configures la source (la bo√Æte SQS), l‚Äôenrichissement (la petite √©tape qui compl√®te le ticket), et la cible (le cuisinier). Les autres choix, c‚Äôest comme embaucher des √©l√®ves pour surveiller la bo√Æte (Lambda qui poll), organiser un parcours compliqu√© en plusieurs salles (Step Functions), ou monter une usine enti√®re (EMR + fichiers) : trop de travail pour juste compl√©ter un ticket.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:458:5f2dbdd637e0d11b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 458,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has implemented a pipeline in AWS CodePipeline. The company is using a single AWS account and does not use AWS Organizations. The company needs to test its AWS CloudFormation templates in its primary AWS Region and a disaster recovery Region.Which solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "A": "In the CodePipeline pipeline, implement an AWS CodeDeploy action for each Region to deploy and test the CloudFormation templates. Update CodePipeline and AWS CodeBuild with appropriate permissions.",
        "B": "Configure CodePipeline to deploy and test the CloudFormation templates. Use CloudFormation StackSets to start deployment across both Regions.",
        "C": "Configure CodePipeline to invoke AWS CodeBuild to deploy and test the CloudFormation templates in each Region. Update CodeBuild and CloudFormation with appropriate permissions.",
        "D": "Use the Snyk action in CodePipeline to deploy and test the CloudFormation templates in each Region."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/150781-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 5, 2024, 1:46 p.m.",
      "textHash": "5f2dbdd637e0d11b",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici, on veut tester des mod√®les AWS CloudFormation (des fichiers qui d√©crivent l‚Äôinfrastructure) dans 2 R√©gions : la R√©gion principale et une R√©gion de secours.\nAWS CodePipeline orchestre les √©tapes (build, test, d√©ploiement) automatiquement.\nPour d√©ployer la m√™me pile CloudFormation dans plusieurs R√©gions de fa√ßon simple, le service adapt√© est CloudFormation StackSets : il permet de ‚Äúr√©pliquer‚Äù un m√™me template vers plusieurs R√©gions depuis une seule d√©finition.\nM√™me sans AWS Organizations, StackSets peut fonctionner dans un seul compte (d√©ploiement multi-R√©gion dans le m√™me compte), ce qui correspond exactement au contexte.\nAvec l‚Äôoption B, CodePipeline d√©clenche le d√©ploiement/test, et StackSets s‚Äôoccupe de lancer les stacks dans les deux R√©gions : moins de scripts, moins d‚Äô√©tapes dupliqu√©es, moins de maintenance.\nA est moins efficace : CodeDeploy sert surtout √† d√©ployer du code applicatif, pas √† g√©rer des stacks CloudFormation multi-R√©gion.\nC marche mais demande plus d‚Äôop√©rations (scripts CodeBuild, gestion des identit√©s/permissions et logique multi-R√©gion ‚Äú√† la main‚Äù).\nD est hors sujet : Snyk sert √† l‚Äôanalyse de s√©curit√©, pas au d√©ploiement multi-R√©gion de CloudFormation.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois afficher la m√™me affiche de club dans deux b√¢timents du lyc√©e : le b√¢timent principal (r√©gion principale) et un autre b√¢timent au cas o√π le premier ferme (r√©gion secours). Tu veux une m√©thode simple pour coller la m√™me affiche au m√™me endroit, sans refaire tout √† la main.**\n\nConcept : CodePipeline, c‚Äôest comme une cha√Æne d‚Äô√©tapes qui pr√©pare et v√©rifie ton affiche. CloudFormation, c‚Äôest le plan de collage (o√π et comment mettre les affiches). Deux r√©gions = deux b√¢timents.\nPourquoi B : StackSets, c‚Äôest comme une ‚Äúcommande group√©e‚Äù qui dit : ¬´ colle cette affiche dans les deux b√¢timents en m√™me temps, de fa√ßon identique ¬ª. Tu gardes un seul plan, et √ßa d√©ploie partout automatiquement.\nA et C : c‚Äôest comme demander √† deux √©quipes diff√©rentes de coller l‚Äôaffiche dans chaque b√¢timent, avec plus d‚Äôorganisation et de risques d‚Äôoublis.\nD : Snyk, c‚Äôest plut√¥t un ‚Äúcontr√¥le de s√©curit√©‚Äù de ton affiche, pas l‚Äô√©quipe qui la colle et la teste dans deux b√¢timents.\nDonc B est le plus efficace : une seule commande, deux r√©gions, moins de boulot au quotidien.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:110:9b7b317915ef135e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 110,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is providing read access to objects in an Amazon S3 bucket for different customers. The company uses IAM permissions to restrict access to the S3 bucket. The customers can access only their own files.Due to a regulation requirement, the company needs to enforce encryption in transit for interactions with Amazon S3.Which solution will meet these requirements?",
      "choices": {
        "A": "Add a bucket policy to the S3 bucket to deny S3 actions when the aws:SecureTransport condition is equal to false.",
        "B": "Add a bucket policy to the S3 bucket to deny S3 actions when the s3:x-amz-acl condition is equal to public-read.",
        "C": "Add an IAM policy to the IAM users to enforce the usage of the AWS SDK.",
        "D": "Add an IAM policy to the IAM users that allows S3 actions when the s3:x-amz-acl condition is equal to bucket-owner-read."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107013-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 6:23 p.m.",
      "textHash": "9b7b317915ef135e",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Objectif : forcer le chiffrement ¬´ en transit ¬ª quand les clients lisent des fichiers dans Amazon S3 (stockage d‚Äôobjets). Cela veut dire : obliger l‚Äôusage de HTTPS/TLS, pas HTTP.\nLa fa√ßon la plus fiable est de le faire c√¥t√© S3 avec une bucket policy (r√®gles attach√©es au bucket) qui bloque toute requ√™te non s√©curis√©e.\nLa condition aws:SecureTransport indique si la requ√™te arrive via HTTPS (true) ou HTTP (false).\nEn mettant une r√®gle ¬´ Deny ¬ª quand aws:SecureTransport = false, S3 refuse automatiquement toute action (GetObject, List, etc.) faite en HTTP.\nCela s‚Äôapplique √† tous les clients, m√™me s‚Äôils ont d√©j√† des permissions IAM pour n‚Äôacc√©der qu‚Äô√† leurs propres fichiers.\nLes options B et D parlent d‚ÄôACL (public-read, bucket-owner-read) : √ßa concerne le partage/publication, pas le chiffrement en transit.\nL‚Äôoption C n‚Äôimpose pas HTTPS : utiliser le SDK n‚Äôemp√™che pas quelqu‚Äôun d‚Äôappeler S3 en HTTP.\nDonc la bonne r√©ponse est A : une bucket policy qui refuse les requ√™tes sans transport s√©curis√©.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e avec des casiers par √©l√®ve. Tu peux ouvrir seulement TON casier. Mais le proviseur exige que, pour parler au biblioth√©caire, tu utilises toujours un interphone s√©curis√©, pas un couloir o√π tout le monde peut √©couter.**\n\nConcept : ‚Äúchiffrement en transit‚Äù = quand tu √©changes des infos, personne ne doit pouvoir √©couter sur le trajet (comme l‚Äôinterphone s√©curis√©). ‚ÄúSecureTransport‚Äù = connexion s√©curis√©e (HTTPS), ‚Äúfalse‚Äù = connexion non s√©curis√©e (HTTP).\nPourquoi A : une ‚Äúbucket policy‚Äù c‚Äôest le r√®glement affich√© √† l‚Äôentr√©e de la biblioth√®que (il s‚Äôapplique √† tout le monde). Avec A, le r√®glement dit : si quelqu‚Äôun essaie d‚Äôacc√©der aux fichiers sans interphone s√©curis√© (aws:SecureTransport = false), on refuse l‚Äôacc√®s.\nB parle de rendre des fichiers publics (comme afficher ton devoir sur le panneau), pas de s√©curiser le trajet.\nC force un outil (SDK) mais √ßa ne garantit pas que le trajet est s√©curis√©.\nD parle de qui ‚Äúposs√®de‚Äù le fichier, pas de la s√©curit√© pendant le transport.\nDonc A est la seule option qui oblige tout le monde √† utiliser une connexion s√©curis√©e pour S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:108:c17a30ced1e25226",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 108,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is updating several AWS Lambda functions and notices that all the Lambda functions share the same custom libraries. The developer wants to centralize all the libraries, update the libraries in a convenient way, and keep the libraries versioned.Which solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Create an AWS CodeArtifact repository that contains all the custom libraries.",
        "B": "Create a custom container image for the Lambda functions to save all the custom libraries.",
        "C": "Create a Lambda layer that contains all the custom libraries.",
        "D": "Create an Amazon Elastic File System (Amazon EFS) file system to store all the custom libraries."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107010-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 6:17 p.m.",
      "textHash": "c17a30ced1e25226",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:333e0b78",
      "frExplanation": "Ici, plusieurs fonctions AWS Lambda (du code qui s‚Äôex√©cute sans serveur) utilisent les m√™mes biblioth√®ques ‚Äúmaison‚Äù. Le besoin est de partager ces biblioth√®ques, les mettre √† jour facilement, et garder des versions.\nUne ‚ÄúLambda Layer‚Äù (couche Lambda) est justement un paquet r√©utilisable de code/d√©pendances que l‚Äôon attache √† une ou plusieurs fonctions.\nOn publie une layer, puis chaque mise √† jour cr√©e une nouvelle version de la layer : on peut choisir quelle version chaque fonction utilise (versioning simple).\nAinsi, pour mettre √† jour les biblioth√®ques, on met √† jour la layer et on pointe les fonctions vers la nouvelle version, sans recopier les fichiers partout.\nC‚Äôest l‚Äôoption avec le moins d‚Äôeffort car Lambda est con√ßu pour √ßa (partage de d√©pendances entre fonctions).\nCodeArtifact sert plut√¥t √† g√©rer des paquets pour des builds/pipelines, pas √† ‚Äúmonter‚Äù directement des libs dans Lambda.\nLes images conteneur et EFS fonctionnent, mais demandent plus de configuration/gestion et sont plus lourds que des layers pour ce cas.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:107:a5dc83b514bc8905",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 107,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a web application that runs on Amazon EC2 instances with a custom Amazon Machine Image (AMI). The company uses AWS CloudFormation to provision the application. The application runs in the us-east-1 Region, and the company needs to deploy the application to the us-west-1 Region.An attempt to create the AWS CloudFormation stack in us-west-1 fails. An error message states that the AMI ID does not exist. A developer must resolve this error with a solution that uses the least amount of operational overhead.Which solution meets these requirements?",
      "choices": {
        "A": "Change the AWS CloudFormation templates for us-east-1 and us-west-1 to use an AWS AMI. Relaunch the stack for both Regions.",
        "B": "Copy the custom AMI from us-east-1 to us-west-1. Update the AWS CloudFormation template for us-west-1 to refer to AMI ID for the copied AMI. Relaunch the stack.",
        "C": "Build the custom AMI in us-west-1. Create a new AWS CloudFormation template to launch the stack in us-west-1 with the new AMI ID.",
        "D": "Manually deploy the application outside AWS CloudFormation in us-west-1."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107007-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 6:05 p.m.",
      "textHash": "a5dc83b514bc8905",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:8814c163",
      "frExplanation": "Une AMI (Amazon Machine Image) est un ‚Äúmod√®le‚Äù de serveur EC2 (syst√®me + logiciels). Important : les IDs d‚ÄôAMI sont sp√©cifiques √† une R√©gion AWS.\nDonc, une AMI cr√©√©e en us-east-1 n‚Äôexiste pas automatiquement en us-west-1 : CloudFormation √©choue car il cherche un ID d‚ÄôAMI introuvable dans cette R√©gion.\nLa solution la plus simple avec peu d‚Äôeffort est de copier l‚ÄôAMI vers la nouvelle R√©gion (fonction ‚ÄúCopy AMI‚Äù).\nApr√®s la copie, us-west-1 aura sa propre AMI avec un nouvel ID.\nIl suffit alors de mettre √† jour le template CloudFormation de us-west-1 pour pointer vers ce nouvel ID, puis relancer la stack.\nLes autres options ajoutent plus de travail : reconstruire une AMI (C), changer d‚ÄôAMI pour les deux R√©gions (A) ou d√©ployer √† la main sans CloudFormation (D) augmente l‚Äôop√©rationnel.\nDonc la r√©ponse B est correcte.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que ton appli est une recette de pizza + un four d√©j√† r√©gl√© exactement comme tu veux (c‚Äôest l‚ÄôAMI). CloudFormation, c‚Äôest la fiche de commande qui dit : ‚Äúprends CE four-l√† (avec son num√©ro) et fais tourner la pizzeria‚Äù.**\n\nConcept : le ‚Äúnum√©ro de four‚Äù (ID d‚ÄôAMI) n‚Äôest valable que dans une ville (une R√©gion). Donc en allant de us-east-1 √† us-west-1, le num√©ro n‚Äôexiste plus.\nPourquoi B : tu copies le four r√©gl√© (copier l‚ÄôAMI) vers la nouvelle ville. Tu obtiens un nouveau num√©ro de four l√†-bas.\nEnsuite tu changes juste la fiche de commande de us-west-1 pour mettre ce nouveau num√©ro, et tu relances.\nC‚Äôest le moins de boulot : tu ne refais pas la recette ni les r√©glages, tu dupliques et tu mets √† jour un seul num√©ro.\nA change de recette pour une pizza ‚Äústandard‚Äù (pas ton four perso). C reconstruit tout. D casse l‚Äôautomatisation.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:105:e785c1c5b83728cf",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 105,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA software company is launching a multimedia application. The application will allow guest users to access sample content before the users decide if they want to create an account to gain full access. The company wants to implement an authentication process that can identify users who have already created an account. The company also needs to keep track of the number of guest users who eventually create an account.Which combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Create an Amazon Cognito user pool. Configure the user pool to allow unauthenticated users. Exchange user tokens for temporary credentials that allow authenticated users to assume a role.",
        "B": "Create an Amazon Cognito identity pool. Configure the identity pool to allow unauthenticated users. Exchange unique identity for temporary credentials that allow all users to assume a role.",
        "C": "Create an Amazon CloudFront distribution. Configure the distribution to allow unauthenticated users. Exchange user tokens for temporary credentials that allow all users to assume a role.",
        "D": "Create a role for authenticated users that allows access to all content. Create a role for unauthenticated users that allows access to only the sample content.",
        "E": "Allow all users to access the sample content by default. Create a role for authenticated users that allows access to the other content."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106989-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 4:21 p.m.",
      "textHash": "e785c1c5b83728cf",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, on veut 2 choses : (1) laisser des visiteurs (invit√©s) acc√©der √† du contenu d‚Äôessai sans compte, (2) reconna√Ætre ceux qui ont un compte et mesurer combien d‚Äôinvit√©s finissent par s‚Äôinscrire.\nAmazon Cognito Identity Pool sert √† donner une ‚Äúidentit√©‚Äù √† chaque utilisateur, m√™me invit√©, et √† fournir des identifiants temporaires AWS (credentials) via des r√¥les IAM.\nEn activant les utilisateurs non authentifi√©s dans l‚ÄôIdentity Pool, chaque invit√© re√ßoit un identifiant unique : on peut donc suivre un invit√© dans le temps et voir s‚Äôil devient ensuite authentifi√© (inscription/connexion).\nQuand l‚Äôutilisateur se connecte, Cognito lui donne des credentials temporaires associ√©s au r√¥le ‚Äúauthentifi√©‚Äù, ce qui permet de distinguer clairement invit√©s vs comptes.\nC‚Äôest exactement ce que d√©crit la r√©ponse B : Identity Pool + invit√©s autoris√©s + √©change de l‚Äôidentit√© unique contre des credentials temporaires.\nLes autres propositions ne conviennent pas : un User Pool g√®re surtout l‚Äôinscription/connexion (pas l‚Äôacc√®s invit√© direct aux r√¥les AWS), CloudFront n‚Äôest pas un service d‚Äôauthentification, et les r√¥les seuls ne suffisent pas sans un m√©canisme d‚Äôidentit√© pour suivre les invit√©s.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une m√©diath√®que au lyc√©e : tout le monde peut entrer et feuilleter des extraits, mais seuls les √©l√®ves avec une carte peuvent emprunter les contenus complets. La m√©diath√®que veut aussi compter combien de visiteurs ‚Äúsans carte‚Äù finissent par demander une carte.**\n\nConcept : il faut 2 choses : 1) donner un ‚Äúbadge visiteur‚Äù aux invit√©s (sans compte) pour les reconna√Ætre plus tard, 2) donner un ‚Äúbadge √©l√®ve‚Äù √† ceux qui ont un compte, et pouvoir comparer les deux pour savoir qui s‚Äôest inscrit.\nB est bon : un ‚ÄúCognito identity pool‚Äù, c‚Äôest comme le guichet qui distribue un identifiant unique √† chaque visiteur, m√™me sans carte (invit√©). Comme chaque invit√© a un identifiant, tu peux suivre ceux qui reviennent et ceux qui finissent par cr√©er un compte.\nEnsuite, ce guichet donne des ‚Äúdroits temporaires‚Äù (comme un bracelet) pour acc√©der √† certaines zones : extrait pour invit√©s, tout pour inscrits.\nDonc tu peux reconna√Ætre les utilisateurs d√©j√† inscrits, et compter combien d‚Äôinvit√©s deviennent inscrits gr√¢ce √† leur identifiant unique.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:104:5244920046212f26",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 104,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company stores its data in data tables in a series of Amazon S3 buckets. The company received an alert that customer credit card information might have been exposed in a data table on one of the company's public applications. A developer needs to identify all potential exposures within the application environment.Which solution will meet these requirements?",
      "choices": {
        "A": "Use Amazon Athena to run a job on the S3 buckets that contain the affected data. Filter the findings by using the SensitiveData:S3Object/Personal finding type.",
        "B": "Use Amazon Macie to run a job on the S3 buckets that contain the affected data. Filter the findings by using the SensitiveData:S3Object/Financial finding type.",
        "C": "Use Amazon Macie to run a job on the S3 buckets that contain the affected data. Filter the findings by using the SensitiveData:S3Object/Personal finding type.",
        "D": "Use Amazon Athena to run a job on the S3 buckets that contain the affected data. Filter the findings by using the SensitiveData:S3Object/Financial finding type."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106987-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 4:09 p.m.",
      "textHash": "5244920046212f26",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:s3:3e598a86",
      "frExplanation": "Ici, il faut trouver o√π des num√©ros de carte bancaire (donn√©es financi√®res sensibles) pourraient √™tre pr√©sents dans des fichiers stock√©s sur Amazon S3 (stockage d‚Äôobjets, comme des fichiers dans des ‚Äúseaux‚Äù).\nAmazon Macie est un service de s√©curit√© qui analyse automatiquement les objets S3 pour d√©tecter des donn√©es sensibles (PII, num√©ros de cartes, etc.) et produit des ‚Äúfindings‚Äù (alertes/r√©sultats).\nAmazon Athena, lui, sert surtout √† faire des requ√™tes SQL sur des fichiers dans S3, mais il ne fait pas de d√©tection de donn√©es sensibles ‚Äúpr√™te √† l‚Äôemploi‚Äù avec des types de findings.\nComme l‚Äôobjectif est d‚Äôidentifier toutes les expositions potentielles de cartes bancaires, Macie est l‚Äôoutil adapt√©.\nLe filtre ‚ÄúSensitiveData:S3Object/Financial‚Äù correspond pr√©cis√©ment aux donn√©es financi√®res (ex: num√©ros de carte de cr√©dit).\nDonc on lance un job Macie sur les buckets S3 concern√©s et on filtre les r√©sultats sur le type Financial pour rep√©rer les expositions.\nLes options avec Athena sont inadapt√©es, et ‚ÄúPersonal‚Äù vise plut√¥t des donn√©es personnelles (nom, email), pas sp√©cifiquement les cartes bancaires.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine le stockage S3 comme une √©norme biblioth√®que de l‚Äô√©cole, avec plein de cartons de feuilles (les fichiers). On te dit qu‚Äôun carton pourrait contenir des num√©ros de carte bancaire d‚Äô√©l√®ves, et tu dois v√©rifier vite s‚Äôil y en a ailleurs.**\n\nConcept : il te faut un ‚Äúsurveillant‚Äù qui sait reconna√Ætre automatiquement des infos sensibles (comme des num√©ros de carte) en lisant les cartons. Amazon Macie, c‚Äôest ce surveillant sp√©cialis√© dans la d√©tection de donn√©es priv√©es dans S3. Amazon Athena, lui, c‚Äôest plut√¥t un √©l√®ve qui fait des recherches dans des tableaux quand tu sais d√©j√† quoi chercher, mais il ne ‚Äúcomprend‚Äù pas tout seul ce qui est sensible. Pourquoi B : Macie lance un scan des buckets S3 et remonte des alertes (findings) quand il voit des donn√©es sensibles. Comme on parle de cartes bancaires, c‚Äôest de l‚Äôinfo ‚ÄúFinancial‚Äù (financi√®re), pas juste ‚ÄúPersonal‚Äù (nom, email, etc.). Donc : Macie + filtre SensitiveData:S3Object/Financial = B.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:103:d7657fdc6cb001ff",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 103,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn ecommerce website uses an AWS Lambda function and an Amazon RDS for MySQL database for an order fulfillment service. The service needs to return order confirmation immediately.During a marketing campaign that caused an increase in the number of orders, the website's operations team noticed errors for ‚Äútoo many connections‚Äù from Amazon RDS. However, the RDS DB cluster metrics are healthy. CPU and memory capacity are still available.What should a developer do to resolve the errors?",
      "choices": {
        "A": "Initialize the database connection outside the handler function. Increase the max_user_connections value on the parameter group of the DB cluster. Restart the DB cluster.",
        "B": "Initialize the database connection outside the handler function. Use RDS Proxy instead of connecting directly to the DB cluster.",
        "C": "Use Amazon Simple Queue Service (Amazon SQS) FIFO queues to queue the orders. Ingest the orders into the database. Set the Lambda function's concurrency to a value that equals the number of available database connections.",
        "D": "Use Amazon Simple Queue Service (Amazon SQS) FIFO queues to queue the orders. Ingest the orders into the database. Set the Lambda function's concurrency to a value that is less than the number of available database connections."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106984-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 3:39 p.m.",
      "textHash": "d7657fdc6cb001ff",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Le probl√®me vient du fait que AWS Lambda peut lancer beaucoup d‚Äôex√©cutions en parall√®le pendant un pic. Chaque ex√©cution ouvre souvent sa propre connexion MySQL vers Amazon RDS (la base de donn√©es g√©r√©e). M√™me si le CPU et la m√©moire sont OK, RDS a une limite de connexions simultan√©es : d‚Äôo√π ‚Äútoo many connections‚Äù.\nPour r√©duire le nombre de connexions, il faut r√©utiliser la connexion : on initialise la connexion √† la base en dehors du ‚Äúhandler‚Äù Lambda, afin qu‚Äôune m√™me instance Lambda r√©utilise la connexion entre plusieurs appels.\nMais cela ne suffit pas quand il y a beaucoup d‚Äôinstances Lambda en parall√®le : il faut aussi mutualiser/mettre en pool les connexions.\nRDS Proxy est un service AWS qui se place entre Lambda et RDS et g√®re un pool de connexions, en r√©utilisant et en limitant les connexions ouvertes vers la base.\nAinsi, Lambda peut scaler sans ouvrir une connexion MySQL par ex√©cution, et les erreurs disparaissent.\nAugmenter max_user_connections (A) ne r√®gle pas la cause et peut √™tre risqu√©; SQS + limiter la concurrence (C/D) change l‚Äôarchitecture et peut retarder la confirmation imm√©diate.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : la cuisine (la base de donn√©es) peut faire plein de repas, mais il n‚Äôy a qu‚Äôun nombre limit√© de caisses ouvertes pour encaisser (connexions). Pendant une promo, trop d‚Äô√©l√®ves arrivent d‚Äôun coup et chacun veut sa caisse perso.**\n\nIci, le site utilise une ‚Äúmachine qui ex√©cute une t√¢che‚Äù (Lambda) et une ‚Äúgrande armoire √† commandes‚Äù (RDS MySQL). Quand il y a un pic, Lambda se multiplie et chaque copie ouvre sa propre ‚Äúcaisse‚Äù vers RDS. R√©sultat : ‚Äútoo many connections‚Äù, m√™me si la cuisine a encore de la place (CPU/RAM OK). La bonne id√©e est B : cr√©er la connexion en dehors du moment o√π on sert (√©viter de rouvrir une caisse √† chaque √©l√®ve) et surtout utiliser RDS Proxy, comme un surveillant qui regroupe les √©l√®ves et les fait passer par un petit nombre de caisses partag√©es. √áa limite le nombre de connexions ouvertes, √©vite les erreurs, et l‚Äô√©l√®ve (client) re√ßoit sa confirmation tout de suite.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:102:926aad426205d757",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 102,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA social media application uses the AWS SDK for JavaScript on the frontend to get user credentials from AWS Security Token Service (AWS STS). The application stores its assets in an Amazon S3 bucket. The application serves its content by using an Amazon CloudFront distribution with the origin set to the S3 bucket.The credentials for the role that the application assumes to make the SDK calls are stored in plaintext in a JSON file within the application code. The developer needs to implement a solution that will allow the application to get user credentials without having any credentials hardcoded in the application code.Which solution will meet these requirements?",
      "choices": {
        "A": "Add a Lambda@Edge function to the distribution. Invoke the function on viewer request. Add permissions to the function's execution role to allow the function to access AWS STS. Move all SDK calls from the frontend into the function.",
        "B": "Add a CloudFront function to the distribution. Invoke the function on viewer request. Add permissions to the function's execution role to allow the function to access AWS STS. Move all SDK calls from the frontend into the function.",
        "C": "Add a Lambda@Edge function to the distribution. Invoke the function on viewer request. Move the credentials from the JSON file into the function. Move all SDK calls from the frontend into the function.",
        "D": "Add a CloudFront function to the distribution. Invoke the function on viewer request. Move the credentials from the JSON file into the function. Move all SDK calls from the frontend into the function."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106981-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 3:30 p.m.",
      "textHash": "926aad426205d757",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Le probl√®me : des identifiants AWS (cl√©s) sont √©crits en clair dans le code frontend. Tout utilisateur peut les lire, les copier et appeler AWS √† votre place.\nObjectif : ne plus avoir de secrets dans le navigateur, tout en pouvant obtenir des jetons temporaires via AWS STS (service qui d√©livre des identifiants temporaires et limit√©s).\nLa bonne approche est de d√©placer les appels AWS (SDK) c√¥t√© serveur/edge, l√† o√π on peut utiliser un r√¥le IAM sans exposer de cl√©s.\nLambda@Edge ex√©cute du code Node.js au plus pr√®s des utilisateurs dans CloudFront et peut utiliser un r√¥le d‚Äôex√©cution (IAM role) avec des permissions.\nOn donne √† ce r√¥le le droit d‚Äôappeler STS, puis la fonction fait les appels AWS √† la place du frontend et renvoie seulement le r√©sultat n√©cessaire.\nAinsi, aucune cl√© n‚Äôest embarqu√©e dans l‚Äôapplication, et l‚Äôacc√®s est contr√¥l√© par IAM.\nCloudFront Functions ne convient pas ici car c‚Äôest tr√®s limit√© (pas d‚Äôacc√®s r√©seau/SDK AWS complet), donc ne peut pas appeler STS.\nLes options C et D sont mauvaises car elles d√©placent juste les m√™mes identifiants en clair dans la fonction au lieu de les supprimer.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:99:c79259a664456220",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 99,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an AWS Lambda function that processes incoming requests from an Amazon API Gateway API. The API calls the Lambda function by using a Lambda alias. A developer updated the Lambda function code to handle more details related to the incoming requests. The developer wants to deploy the new Lambda function for more testing by other developers with no impact to customers that use the API.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Create a new version of the Lambda function. Create a new stage on API Gateway with integration to the new Lambda version. Use the new API Gateway stage to test the Lambda function.",
        "B": "Update the existing Lambda alias used by API Gateway to a weighted alias. Add the new Lambda version as an additional Lambda function with a weight of 10%. Use the existing API Gateway stage for testing.",
        "C": "Create a new version of the Lambda function. Create and deploy a second Lambda function to filter incoming requests from API Gateway. If the filtering Lambda function detects a test request, the filtering Lambda function will invoke the new Lambda version of the code. For other requests, the filtering Lambda function will invoke the old Lambda version. Update the API Gateway API to use the filtering Lambda function.",
        "D": "Create a new version of the Lambda function. Create a new API Gateway API for testing purposes. Update the integration of the new API with the new Lambda version. Use the new API for testing."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/109229-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 14, 2023, 1:08 p.m.",
      "textHash": "c79259a664456220",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:f4fa0f52",
      "frExplanation": "API Gateway est la ‚Äúporte d‚Äôentr√©e‚Äù HTTP, et AWS Lambda ex√©cute votre code. Un alias Lambda est un nom (ex: ‚Äúprod‚Äù) qui pointe vers une version pr√©cise du code.\nIci, on veut tester une nouvelle version sans impacter les clients existants : il faut donc s√©parer le trafic de test du trafic client.\nLa solution A cr√©e une nouvelle version de la fonction (immuable) puis un nouveau ‚Äústage‚Äù API Gateway (ex: /test) qui appelle cette nouvelle version.\nLes clients continuent d‚Äôutiliser le stage actuel (ex: /prod) qui reste branch√© sur l‚Äôancienne version via l‚Äôalias, donc aucun impact.\nLes d√©veloppeurs testent via l‚ÄôURL du nouveau stage uniquement.\nC‚Äôest peu d‚Äôop√©rations : pas besoin de nouvelle API compl√®te ni de logique de routage suppl√©mentaire.\nB enverrait 10% du trafic r√©el vers la nouvelle version (risque pour les clients) et ne cr√©e pas un environnement de test isol√©.\nC et D ajoutent plus de composants (fonction de filtrage ou nouvelle API compl√®te), donc plus d‚Äôoverhead.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:98:fba73cdf31462dd3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 98,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Lambda function in VPC mode. An Amazon S3 event will invoke the Lambda function when an object is uploaded into an S3 bucket. The Lambda function will process the object and produce some analytic results that will be recorded into a file. Each processed object will also generate a log entry that will be recorded into a file.Other Lambda functions, AWS services, and on-premises resources must have access to the result files and log file. Each log entry must also be appended to the same shared log file. The developer needs a solution that can share files and append results into an existing file.Which solution should the developer use to meet these requirements?",
      "choices": {
        "A": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in Lambda. Store the result files and log file in the mount point. Append the log entries to the log file.",
        "B": "Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach enabled volume. Attach the EBS volume to all Lambda functions. Update the Lambda function code to download the log file, append the log entries, and upload the modified log file to Amazon EBS.",
        "C": "Create a reference to the /tmp local directory. Store the result files and log file by using the directory reference. Append the log entry to the log file.",
        "D": "Create a reference to the /opt storage directory. Store the result files and log file by using the directory reference. Append the log entry to the log file."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/109646-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 18, 2023, 1:50 p.m.",
      "textHash": "fba73cdf31462dd3",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:aa7eeab2",
      "frExplanation": "Ici, la fonction AWS Lambda (code ex√©cut√© √† la demande) tourne dans un VPC et doit √©crire des fichiers de r√©sultats + un fichier de logs partag√©, avec ajout (append) dans le m√™me fichier.\nIl faut donc un stockage de fichiers ¬´ r√©seau ¬ª accessible en m√™me temps par plusieurs fonctions Lambda, d‚Äôautres services AWS et m√™me des ressources on‚Äëpremises.\nAmazon EFS (Elastic File System) est un syst√®me de fichiers NFS manag√© : on le ‚Äúmonte‚Äù comme un disque partag√©, et plusieurs clients peuvent lire/√©crire dessus.\nEFS permet d‚Äô√©crire plusieurs fichiers et surtout d‚Äôajouter des lignes √† un fichier existant (append) sans devoir le ret√©l√©charger/reuploader.\nEBS est un disque bloc pour EC2, pas fait pour √™tre mont√© directement par Lambda, et Multi-Attach ne correspond pas √† ce besoin (et reste limit√©/complexe).\n/tmp dans Lambda est un stockage local temporaire : non partag√© entre ex√©cutions/fonctions et peut √™tre effac√©.\n/opt correspond aux couches (Lambda Layers) et est en lecture seule au runtime : on ne peut pas y √©crire des logs.\nDonc la bonne solution est de monter un EFS dans Lambda et d‚Äôy stocker r√©sultats et fichier de logs partag√©.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:96:046b7a12a07be44c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 96,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a service that uses an Amazon S3 bucket for image uploads. The service will use an AWS Lambda function to create a thumbnail of each image. Each time an image is uploaded, the service needs to send an email notification and create the thumbnail. The developer needs to configure the image processing and email notifications setup.Which solution will meet these requirements?",
      "choices": {
        "A": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure S3 event notifications with a destination of the SNS topic. Subscribe the Lambda function to the SNS topic. Create an email notification subscription to the SNS topic.",
        "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure S3 event notifications with a destination of the SNS topic. Subscribe the Lambda function to the SNS topic. Create an Amazon Simple Queue Service (Amazon SQS) queue. Subscribe the SQS queue to the SNS topic. Create an email notification subscription to the SQS queue.",
        "C": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure S3 event notifications with a destination of the SQS queue. Subscribe the Lambda function to the SQS queue. Create an email notification subscription to the SQS queue.",
        "D": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Send S3 event notifications to Amazon EventBridge. Create an EventBridge rule that runs the Lambda function when images are uploaded to the S3 bucket. Create an EventBridge rule that sends notifications to the SQS queue. Create an email notification subscription to the SQS queue."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106946-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 21, 2023, 8:33 p.m.",
      "textHash": "046b7a12a07be44c",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Amazon S3 est un stockage de fichiers : quand une image est d√©pos√©e dans un bucket, S3 peut √©mettre un ¬´ √©v√©nement ¬ª.\nAWS Lambda est une fonction qui s‚Äôex√©cute automatiquement pour traiter l‚Äôimage (ici cr√©er une miniature).\nAmazon SNS est un service de diffusion de messages (pub/sub) : un m√™me √©v√©nement peut √™tre envoy√© √† plusieurs ¬´ abonn√©s ¬ª.\nLa solution A configure S3 pour publier un √©v√©nement vers un topic SNS √† chaque upload.\nEnsuite, on abonne Lambda au topic SNS : Lambda est d√©clench√©e et g√©n√®re la miniature.\nOn ajoute aussi un abonnement e-mail au m√™me topic SNS : SNS envoie directement l‚Äôe-mail de notification.\nAinsi, un seul √©v√©nement d‚Äôupload d√©clenche deux actions en parall√®le (traitement + e-mail) sans complexit√©.\nLes autres choix compliquent inutilement avec SQS (file d‚Äôattente) et/ou EventBridge, et SQS ne sait pas envoyer d‚Äôe-mails directement.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une bo√Æte aux lettres au coll√®ge o√π les √©l√®ves d√©posent des photos pour le journal de l‚Äô√©cole. D√®s qu‚Äôune photo arrive, on veut 2 choses en m√™me temps : 1) le club photo fait une mini-version (thumbnail), 2) le CPE envoie un mail pour pr√©venir que c‚Äôest arriv√©. On utilise un ‚Äúpanneau d‚Äôannonce‚Äù central qui pr√©vient tout le monde d‚Äôun coup.**\n\nConcept : S3 = la bo√Æte o√π on d√©pose les images. Quand une image arrive, S3 envoie un message. SNS = le panneau d‚Äôannonce qui peut pr√©venir plusieurs personnes √† la fois. Lambda = le ‚Äúclub photo‚Äù qui re√ßoit l‚Äôannonce et fabrique la miniature. Email = un autre ‚Äúabonn√©‚Äù au panneau qui re√ßoit aussi l‚Äôannonce. Pourquoi A : S3 annonce l‚Äôarriv√©e au panneau (SNS). Le club photo (Lambda) est abonn√©, donc il cr√©e le thumbnail. L‚Äôemail est aussi abonn√©, donc il envoie la notification. Une seule annonce, deux actions automatiques. Les autres choix ajoutent une ‚Äúfile d‚Äôattente‚Äù (SQS) inutile ici, et SQS n‚Äôenvoie pas directement des emails.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:95:cfbfde4c910c0fab",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 95,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is leveraging a Border Gateway Protocol (BGP)-based AWS VPN connection to connect from on-premises to Amazon EC2 instances in the developer's account. The developer is able to access an EC2 instance in subnet A, but is unable to access an EC2 instance in subnet B in the same VPC.Which logs can the developer use to verify whether the traffic is reaching subnet B?",
      "choices": {
        "A": "VPN logs",
        "B": "BGP logs",
        "C": "VPC Flow Logs",
        "D": "AWS CloudTrail logs"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/108742-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 8, 2023, 2:07 p.m.",
      "textHash": "cfbfde4c910c0fab",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:49ce193f",
      "frExplanation": "Ici, on veut savoir si le trafic r√©seau arrive bien dans le sous-r√©seau B (subnet B) √† l‚Äôint√©rieur du VPC.\nUn VPC est un r√©seau priv√© AWS, et un subnet est une ‚Äúportion‚Äù de ce r√©seau o√π se trouvent des instances EC2.\nLes VPC Flow Logs enregistrent les m√©tadonn√©es des flux r√©seau (IP source/destination, ports, protocole, action ACCEPT/REJECT) pour une interface r√©seau, un subnet ou tout le VPC.\nDonc, en activant les Flow Logs sur le subnet B (ou sur l‚ÄôENI de l‚Äôinstance EC2 en subnet B), on peut v√©rifier si des paquets arrivent et s‚Äôils sont accept√©s ou rejet√©s.\nLes VPN logs/BGP logs concernent surtout l‚Äô√©tat du tunnel et l‚Äô√©change de routes, pas la preuve que les paquets atteignent un subnet pr√©cis.\nCloudTrail trace les appels API AWS (qui a cr√©√©/modifi√© des ressources), pas le trafic r√©seau entre on-premises et EC2.\nAinsi, VPC Flow Logs est le bon outil pour confirmer si le trafic ‚Äúatteint‚Äù subnet B et o√π il est bloqu√©.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:363:28992e49f9659691",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 363,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a web application for a school that stores data in Amazon DynamoDB. The ExamScores table has the following attributes: student_id, subject_name, and top_score.Each item in the ExamScores table is identified with student_id as the partition key and subject_name as the sort key. The web application needs to display the student _id for the top scores for each school subject. The developer needs to increase the speed of the queries to retrieve the student_id for the top scorer for each school subject.Which solution will meet these requirements?",
      "choices": {
        "A": "Create a local secondary index (LSI) with subject_name as the partition key and top_score as the sort key.",
        "B": "Create a local secondary index (LSI) with top_score as the partition key and student_id as the sort key.",
        "C": "Create a global secondary index (GSI) with subject_name as the partition key and top_score as the sort key.",
        "D": "Create a global secondary index (GSI) with subject_name as the partition key and student_id as the sort key."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143747-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:20 a.m.",
      "textHash": "28992e49f9659691",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π l‚Äôon lit vite si l‚Äôon interroge avec une cl√© adapt√©e. Ici, la table est organis√©e par student_id (partition key) puis subject_name (sort key), donc elle est optimis√©e pour chercher les notes d‚Äôun √©l√®ve, pas pour trouver le meilleur √©l√®ve par mati√®re.\nPour afficher le top score par mati√®re, il faut pouvoir requ√™ter par subject_name et trier par top_score pour prendre le premier (le plus grand).\nUn index secondaire sert √† cr√©er une ‚Äúautre vue‚Äù des m√™mes donn√©es avec d‚Äôautres cl√©s de recherche.\nUn LSI garde la m√™me partition key que la table (donc student_id) : il ne peut pas changer la partition key vers subject_name, donc A et B sont impossibles/inadapt√©s.\nUn GSI permet de choisir une nouvelle partition key et une nouvelle sort key : on peut mettre subject_name en partition key.\nEn mettant top_score en sort key dans le GSI, on peut faire une Query par mati√®re et r√©cup√©rer rapidement l‚Äô√©l√©ment avec le score le plus √©lev√© (en tri d√©croissant, limit 1).\nL‚Äôoption D ne permet pas de trier par score (sort key = student_id), donc elle n‚Äôaide pas √† trouver le ‚Äútop score‚Äù.\nDonc la bonne solution est de cr√©er un GSI avec subject_name (partition) et top_score (tri) : r√©ponse C.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un tableau d‚Äôaffichage au lyc√©e : pour chaque mati√®re (maths, anglais‚Ä¶), tu veux voir rapidement le meilleur score et le nom de l‚Äô√©l√®ve.**\n\nDans DynamoDB, la table est rang√©e d‚Äôabord par √©l√®ve (student_id), puis par mati√®re (subject_name). Donc chercher ‚Äúle meilleur en maths‚Äù oblige √† fouiller chez plein d‚Äô√©l√®ves : c‚Äôest lent.\nUn GSI, c‚Äôest comme cr√©er un 2e tableau d‚Äôaffichage rang√© autrement, sans toucher au premier.\nAvec C, tu ranges ce 2e tableau par mati√®re (subject_name), puis tu tries par score (top_score).\nDu coup, pour ‚Äúmaths‚Äù, tu vas direct √† la section maths et tu prends le score le plus haut : tu r√©cup√®res vite le student_id du meilleur.\nLes LSI (A/B) gardent le m√™me rangement principal par √©l√®ve, donc √ßa n‚Äôaide pas pour ‚Äúpar mati√®re‚Äù.\nD ne trie pas par score, donc tu ne peux pas trouver le meilleur rapidement.\nDonc la bonne r√©ponse est C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:94:16f8767d82f06c8c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 94,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has written an application that runs on Amazon EC2 instances. The developer is adding functionality for the application to write objects to an Amazon S3 bucket.Which policy must the developer modify to allow the instances to write these objects?",
      "choices": {
        "A": "The IAM policy that is attached to the EC2 instance profile role",
        "B": "The session policy that is applied to the EC2 instance role session",
        "C": "The AWS Key Management Service (AWS KMS) key policy that is attached to the EC2 instance profile role",
        "D": "The Amazon VPC endpoint policy"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/109241-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 14, 2023, 7:56 p.m.",
      "textHash": "16f8767d82f06c8c",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Pour qu‚Äôune application sur une instance EC2 acc√®de √† d‚Äôautres services AWS, elle utilise des identit√©s et permissions IAM.\nSur EC2, ces permissions sont fournies via un ¬´ r√¥le IAM ¬ª attach√© √† l‚Äôinstance par un ¬´ instance profile ¬ª (profil d‚Äôinstance).\nSi l‚Äôapplication doit √©crire des objets dans un bucket Amazon S3, il faut autoriser l‚Äôaction S3 correspondante (ex: s3:PutObject) sur le bucket.\nLa bonne place pour ajouter cette autorisation est la politique IAM attach√©e au r√¥le du profil d‚Äôinstance EC2.\nUne ¬´ session policy ¬ª est un cas particulier (souvent avec STS/assume role) et n‚Äôest pas ce qu‚Äôon modifie habituellement pour des instances EC2.\nUne policy KMS ne sert que si les objets S3 sont chiffr√©s avec une cl√© KMS et ne remplace pas les permissions S3.\nLa policy d‚Äôun VPC endpoint contr√¥le le trafic via l‚Äôendpoint, mais ne donne pas d‚Äôidentit√©/permission √† l‚Äôinstance.\nDonc il faut modifier la policy IAM du r√¥le attach√© au profil d‚Äôinstance EC2.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une √©cole : les √©l√®ves (les serveurs EC2) veulent d√©poser des devoirs dans une bo√Æte de collecte (le dossier S3). Pour avoir le droit, ils ont besoin d‚Äôune autorisation √©crite sur leur carnet (leur r√¥le).**\n\nConcept : EC2 = un √©l√®ve qui agit tout seul. S3 = la bo√Æte o√π on d√©pose des fichiers. Pour d√©poser, il faut une r√®gle d‚Äôautorisation.\nLa bonne r√®gle est celle attach√©e au ‚Äúr√¥le‚Äù donn√© √† l‚Äô√©l√®ve via son ‚Äúprofil‚Äù (instance profile) : c‚Äôest comme le carnet officiel qui dit ce qu‚Äôil a le droit de faire.\nDonc on modifie la politique IAM attach√©e au r√¥le du profil de l‚Äôinstance (A) pour autoriser ‚Äú√©crire dans S3‚Äù.\nB, c‚Äôest une autorisation temporaire de s√©ance, comme un pass d‚Äôune heure : pas le r√©glage principal.\nC, c‚Äôest la r√®gle du cadenas si les devoirs sont chiffr√©s : utile seulement si on verrouille avec une cl√©.\nD, c‚Äôest la r√®gle du couloir d‚Äôacc√®s (r√©seau) vers la bo√Æte : √ßa ne donne pas l‚Äôautorisation de d√©poser.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:92:df0eb32c47d1f6b2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 92,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn e-commerce web application that shares session state on-premises is being migrated to AWS. The application must be fault tolerant, natively highly scalable, and any service interruption should not affect the user experience.What is the best option to store the session state?",
      "choices": {
        "A": "Store the session state in Amazon ElastiCache.",
        "B": "Store the session state in Amazon CloudFront.",
        "C": "Store the session state in Amazon S3.",
        "D": "Enable session stickiness using elastic load balancers."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/108741-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 8, 2023, 2:02 p.m.",
      "textHash": "df0eb32c47d1f6b2",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:066a471d",
      "frExplanation": "Une ‚Äúsession‚Äù contient des infos temporaires sur l‚Äôutilisateur (panier, connexion) que plusieurs serveurs web doivent pouvoir lire/√©crire.\nSur AWS, si l‚Äôapplication doit √™tre tr√®s scalable et tol√©rante aux pannes, il faut √©viter de garder la session sur un seul serveur.\nAmazon ElastiCache est un service de cache en m√©moire (Redis/Memcached) : tr√®s rapide, partag√© par plusieurs instances, et peut √™tre configur√© en haute disponibilit√©.\nAinsi, n‚Äôimporte quel serveur web peut r√©cup√©rer la session, m√™me si un serveur tombe ou si on en ajoute/en retire automatiquement.\nCloudFront est un CDN pour mettre en cache du contenu statique pr√®s des utilisateurs, pas pour stocker des sessions dynamiques.\nS3 est un stockage d‚Äôobjets durable mais plus lent et pas adapt√© √† des lectures/√©critures fr√©quentes de petites donn√©es de session.\nLa ‚Äústickiness‚Äù du load balancer force un utilisateur √† revenir sur le m√™me serveur : si ce serveur tombe, la session est perdue et l‚Äôexp√©rience se d√©grade.\nDonc la meilleure option pour une session partag√©e, rapide, scalable et tol√©rante aux pannes est ElastiCache.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o en ligne : ta ‚Äúsession‚Äù c‚Äôest ta partie en cours (niveau, inventaire, panier d‚Äôachats). Il faut la garder quelque part pour que tu puisses continuer m√™me si un serveur ‚Äúmeurt‚Äù.**\n\nConcept : la session, c‚Äôest une petite m√©moire rapide sur toi pendant que tu navigues (qui tu es, ce que tu as mis au panier). Si elle est perdue, tu recommences.\nA (ElastiCache) = un ‚Äúcasier ultra-rapide‚Äù partag√© par tous les serveurs : n‚Äôimporte quel serveur peut reprendre ta partie sans que tu voies une coupure, et √ßa supporte beaucoup de joueurs en m√™me temps.\nB (CloudFront) sert surtout √† livrer des fichiers (images/vid√©os) vite, pas √† stocker une m√©moire qui change tout le temps.\nC (S3) est un grand ‚Äúentrep√¥t‚Äù de fichiers : fiable mais moins adapt√© pour une m√©moire de session qui doit √™tre tr√®s rapide.\nD (stickiness) force √† toujours retourner sur le m√™me serveur : si ce serveur tombe, ta session tombe aussi, donc pas tol√©rant aux pannes.\nDonc la meilleure option pour une session rapide, partag√©e et r√©sistante aux pannes, c‚Äôest A.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:91:61919c264fb2ecf5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 91,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company needs to develop a proof of concept for a web service application. The application will show the weather forecast for one of the company's office locations. The application will provide a REST endpoint that clients can call. Where possible, the application should use caching features provided by AWS to limit the number of requests to the backend service. The application backend will receive a small amount of traffic only during testing.Which approach should the developer take to provide the REST endpoint MOST cost-effectively?",
      "choices": {
        "A": "Create a container image. Deploy the container image by using Amazon Elastic Kubernetes Service (Amazon EKS). Expose the functionality by using Amazon API Gateway.",
        "B": "Create an AWS Lambda function by using the AWS Serverless Application Model (AWS SAM). Expose the Lambda functionality by using Amazon API Gateway.",
        "C": "Create a container image. Deploy the container image by using Amazon Elastic Container Service (Amazon ECS). Expose the functionality by using Amazon API Gateway.",
        "D": "Create a microservices application. Deploy the application to AWS Elastic Beanstalk. Expose the AWS Lambda functionality by using an Application Load Balancer."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/109210-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 14, 2023, 7:59 a.m.",
      "textHash": "61919c264fb2ecf5",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Pour un proof of concept avec peu de trafic, il faut √©viter de payer des serveurs qui tournent en permanence.\nAWS Lambda ex√©cute votre code ‚Äú√† la demande‚Äù : vous payez surtout quand une requ√™te arrive, ce qui est tr√®s √©conomique pour des tests.\nAmazon API Gateway fournit facilement un endpoint REST public (URL) qui d√©clenche la fonction Lambda.\nAPI Gateway peut aussi faire du cache int√©gr√© : il garde en m√©moire des r√©ponses pendant un temps d√©fini, ce qui r√©duit les appels au backend (Lambda ou service m√©t√©o).\nAWS SAM est un outil pour d√©finir et d√©ployer simplement une appli serverless (Lambda + API Gateway) sans g√©rer d‚Äôinfrastructure.\nEKS/ECS/Elastic Beanstalk impliquent des ressources de calcul (clusters/instances) √† maintenir et souvent pay√©es m√™me sans trafic, donc moins rentable ici.\nDonc l‚Äôoption B est la plus simple et la plus cost-effective, tout en utilisant le cache AWS via API Gateway.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:356:a0f372b2e602ea2e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 356,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is implementing an AWS Lambda function that will be invoked when an object is uploaded to Amazon S3. The developer wants to test the Lambda function in a local development machine before publishing the function to a production AWS account.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Upload an object to Amazon S3 by using the aws s3api put-object CLI command. Wait for the local Lambda invocation from the S3 event.",
        "B": "Create a sample JSON text file for a put object S3 event. Invoke the Lambda function locally. Use the aws lambda invoke CLI command with the JSON file and Lambda function name as arguments.",
        "C": "Use the sam local start-lambda CLI command to start Lambda. Use the sam local generate-event s3 put CLI command to create the Lambda test JSON file. Use the sam local invoke CLI command with the JSON file as the argument to invoke the Lambda function.",
        "D": "Create a JSON string for the put object S3 event. In the AWS Management Console, use the JSON string to create a test event for the local Lambda function. Perform the test."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/139677-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 26, 2024, 10:18 p.m.",
      "textHash": "a0f372b2e602ea2e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:0c41eaee",
      "frExplanation": "On veut tester une fonction AWS Lambda (code ex√©cut√© √† la demande) d√©clench√©e par un d√©p√¥t de fichier dans Amazon S3 (stockage d‚Äôobjets), mais depuis une machine locale, avant de d√©ployer en production.\nLa solution avec le moins d‚Äôeffort op√©rationnel est d‚Äôutiliser AWS SAM (Serverless Application Model), un outil qui simule Lambda localement via Docker.\nAvec \"sam local start-lambda\", on d√©marre un environnement Lambda local sans cr√©er de ressources AWS.\nPuis \"sam local generate-event s3 put\" g√©n√®re automatiquement un √©v√©nement S3 r√©aliste (le JSON que S3 enverrait √† Lambda).\nEnfin \"sam local invoke\" ex√©cute la fonction localement avec cet √©v√©nement, comme si S3 venait d‚Äôuploader un objet.\nA n√©cessite un vrai S3 et n‚Äôinvoque pas une Lambda locale.\nB utilise la CLI AWS et suppose une Lambda d√©ploy√©e dans AWS (pas local), donc plus de d√©pendances.\nD parle de console AWS et d‚Äô√©v√©nement de test, mais ce n‚Äôest pas une ex√©cution locale.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu cr√©es un mini-jeu vid√©o qui doit r√©agir quand un joueur ramasse un objet (comme une pi√®ce). Tu veux tester √ßa sur ton PC avant de l‚Äôenvoyer sur le serveur officiel du jeu.**\n\nIci, S3 = le coffre o√π on d√©pose un fichier, et Lambda = le petit ‚Äúscript‚Äù qui se d√©clenche quand un fichier arrive. Pour tester avec le moins de gal√®re, tu veux un simulateur sur ton PC. La r√©ponse C utilise SAM, un outil qui fait ‚Äúmini-serveur Lambda‚Äù en local (sam local start-lambda), puis fabrique automatiquement un faux message ‚Äúun fichier vient d‚Äô√™tre upload‚Äù (generate-event s3 put), puis lance ton script avec ce message (sam local invoke). C‚Äôest comme appuyer sur ‚ÄúPlay‚Äù dans l‚Äô√©diteur du jeu avec un √©v√©nement d√©j√† pr√™t, sans devoir vraiment envoyer un fichier sur Internet ni configurer un vrai serveur. A et B demandent de passer par AWS et des commandes plus lourdes, et D parle de console AWS (pas vraiment local). Donc C = test local + √©v√©nement S3 r√©aliste + moins d‚Äôop√©rations.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:88:a8fe41585846f031",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 88,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company caches session information for a web application in an Amazon DynamoDB table. The company wants an automated way to delete old items from the table.What is the simplest way to do this?",
      "choices": {
        "A": "Write a script that deletes old records; schedule the script as a cron job on an Amazon EC2 instance.",
        "B": "Add an attribute with the expiration time; enable the Time To Live feature based on that attribute.",
        "C": "Each day, create a new table to hold session data; delete the previous day's table.",
        "D": "Add an attribute with the expiration time; name the attribute ItemExpiration."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/108740-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 8, 2023, 1:47 p.m.",
      "textHash": "a8fe41585846f031",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:dynamodb:d5f33ce6",
      "frExplanation": "Ici, on stocke des sessions (donn√©es temporaires) dans Amazon DynamoDB, une base NoSQL g√©r√©e par AWS.\nLe besoin est de supprimer automatiquement les anciennes sessions, sans serveur √† g√©rer ni script √† maintenir.\nLa fonction la plus simple est DynamoDB Time To Live (TTL) : on ajoute un attribut (champ) qui contient une date/heure d‚Äôexpiration (timestamp).\nEnsuite, on active TTL sur la table en indiquant quel attribut contient cette date.\nDynamoDB supprimera alors automatiquement les √©l√©ments expir√©s en arri√®re-plan (pas instantan√© √† la seconde, mais automatique).\nA est plus complexe (EC2 + cron + maintenance + risques d‚Äô√©chec).\nC est lourd et co√ªteux (cr√©er/supprimer des tables chaque jour) et complique l‚Äôacc√®s aux donn√©es.\nD est faux car le nom de l‚Äôattribut n‚Äôa pas besoin d‚Äô√™tre ‚ÄúItemExpiration‚Äù : on choisit n‚Äôimporte quel nom, puis on le configure dans TTL.\nDonc la bonne r√©ponse est B.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le casier des objets trouv√©s au coll√®ge : chaque objet a une √©tiquette avec une date limite. Apr√®s cette date, le surveillant jette automatiquement l‚Äôobjet pour lib√©rer de la place.**\n\nIci, la table DynamoDB est comme l‚Äôobjet trouv√© : elle garde des ‚Äúsessions‚Äù (des infos temporaires quand tu es connect√©). Ces infos deviennent vite inutiles, comme un ticket de cantine p√©rim√©. La solution la plus simple est de mettre sur chaque session une ‚Äúdate d‚Äôexpiration‚Äù (un attribut avec l‚Äôheure/ date limite). Puis on active la fonction Time To Live (TTL) : c‚Äôest le ‚Äúsurveillant automatique‚Äù qui supprime tout seul les sessions p√©rim√©es. Pas besoin d‚Äôun ordi allum√© qui lance un script (A), ni de recr√©er une nouvelle bo√Æte chaque jour (C). Et le nom exact ‚ÄúItemExpiration‚Äù n‚Äôest pas obligatoire (D) : l‚Äôimportant est d‚Äôindiquer la date et de dire √† TTL quel champ regarder. Donc B.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:87:d243cd57b1b7f7e6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 87,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company's new mobile app uses Amazon API Gateway. As the development team completes a new release of its APIs, a developer must safely and transparently roll out the API change.What is the SIMPLEST solution for the developer to use for rolling out the new API version to a limited number of users through API Gateway?",
      "choices": {
        "A": "Create a new API in API Gateway. Direct a portion of the traffic to the new API using an Amazon Route 53 weighted routing policy.",
        "B": "Validate the new API version and promote it to production during the window of lowest expected utilization.",
        "C": "Implement an Amazon CloudWatch alarm to trigger a rollback if the observed HTTP 500 status code rate exceeds a predetermined threshold.",
        "D": "Use the canary release deployment option in API Gateway. Direct a percentage of the API traffic using the canarySettings setting."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/108739-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 8, 2023, 1:45 p.m.",
      "textHash": "d243cd57b1b7f7e6",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Objectif : d√©ployer une nouvelle version d‚ÄôAPI √† un petit groupe d‚Äôutilisateurs, sans interrompre tout le monde.\nAmazon API Gateway est le service qui expose vos API (endpoints) aux applis mobiles et route les requ√™tes vers votre backend.\nLa solution la plus simple est d‚Äôutiliser une ‚Äúcanary release‚Äù : API Gateway garde la version actuelle (stable) et envoie seulement un pourcentage du trafic vers la nouvelle version.\nAvec canarySettings, vous d√©finissez par exemple 5% du trafic vers la nouvelle version, 95% restent sur l‚Äôancienne.\nC‚Äôest ‚Äútransparent‚Äù pour les utilisateurs : m√™me URL, m√™me API, juste un routage interne diff√©rent.\nC‚Äôest ‚Äús√ªr‚Äù : si un probl√®me appara√Æt, vous remettez le pourcentage √† 0% (retour imm√©diat √† l‚Äôancienne version).\nA est plus complexe (nouvelle API + Route 53) et change l‚Äôarchitecture.\nB n‚Äôest pas un d√©ploiement progressif (tout le monde bascule d‚Äôun coup).\nC aide √† d√©tecter/alerter, mais ne fait pas le d√©ploiement progressif lui-m√™me.\nDonc D est la m√©thode la plus simple et adapt√©e pour un rollout limit√© via API Gateway.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:86:af4fc534ca69d0c8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 86,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is implementing an AWS Cloud Development Kit (AWS CDK) serverless application. The developer will provision several AWS Lambda functions and Amazon API Gateway APIs during AWS CloudFormation stack creation. The developer's workstation has the AWS Serverless Application Model (AWS SAM) and the AWS CDK installed locally.How can the developer test a specific Lambda function locally?",
      "choices": {
        "A": "Run the sam package and sam deploy commands. Create a Lambda test event from the AWS Management Console. Test the Lambda function.",
        "B": "Run the cdk synth and cdk deploy commands. Create a Lambda test event from the AWS Management Console. Test the Lambda function.",
        "C": "Run the cdk synth and sam local invoke commands with the function construct identifier and the path to the synthesized CloudFormation template.",
        "D": "Run the cdk synth and sam local start-lambda commands with the function construct identifier and the path to the synthesized CloudFormation template."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106939-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 21, 2023, 5:28 p.m.",
      "textHash": "af4fc534ca69d0c8",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:561a288d",
      "frExplanation": "Pour tester une fonction Lambda ¬´ en local ¬ª, il faut l‚Äôex√©cuter sur votre machine sans la d√©ployer sur AWS.\nAWS CDK sert √† d√©crire l‚Äôinfrastructure en code, puis √† g√©n√©rer un mod√®le CloudFormation (un fichier template) avec la commande cdk synth.\nAWS SAM fournit des commandes ¬´ sam local ¬ª qui utilisent Docker pour simuler l‚Äôenvironnement Lambda sur votre poste.\nLa commande sam local invoke permet d‚Äôex√©cuter UNE fonction pr√©cise localement, √† partir d‚Äôun template CloudFormation/SAM.\nDonc on fait d‚Äôabord cdk synth pour produire le template, puis sam local invoke en indiquant l‚Äôidentifiant du construct (la fonction) et le chemin du template g√©n√©r√©.\nLes options A et B testent via la console AWS, donc apr√®s d√©ploiement (pas local).\nL‚Äôoption D (sam local start-lambda) d√©marre un service local pour recevoir des appels, mais ce n‚Äôest pas la mani√®re la plus directe pour tester une fonction sp√©cifique.\nLa bonne r√©ponse est C car elle combine la g√©n√©ration du template CDK et l‚Äôinvocation locale cibl√©e de la Lambda.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o o√π tu construis un niveau sur papier avant de le lancer. Le plan du niveau = un ‚Äúblueprint‚Äù. Une Lambda = un petit PNJ qui fait une action pr√©cise quand tu appuies sur un bouton.**\n\nAvec AWS CDK, tu √©cris ton appli comme des instructions, puis tu g√©n√®res un plan d√©taill√© (cdk synth) : c‚Äôest le blueprint CloudFormation. Pour tester localement, tu ne veux pas ‚Äúpublier le niveau sur Internet‚Äù, tu veux juste lancer un PNJ sur ton PC. AWS SAM sert justement √† ‚Äújouer le niveau en local‚Äù : sam local invoke ex√©cute UNE Lambda pr√©cise sur ta machine. Tu lui donnes l‚Äôidentifiant du PNJ (construct identifier) + le fichier blueprint g√©n√©r√©. A et B testent via la console AWS, donc pas local. D (start-lambda) lance un service local qui attend des appels, mais pour tester une fonction pr√©cise tout de suite, invoke est le bon bouton.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:85:482837147fbe64b3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 85,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a web application that is deployed on AWS. The application uses an Amazon API Gateway API and an AWS Lambda function as its backend.The application recently demonstrated unexpected behavior. A developer examines the Lambda function code, finds an error, and modifies the code to resolve the problem. Before deploying the change to production, the developer needs to run tests to validate that the application operates properly.The application has only a production environment available. The developer must create a new development environment to test the code changes. The developer must also prevent other developers from overwriting these changes during the test cycle.Which combination of steps will meet these requirements with the LEAST development effort? (Choose two.)",
      "choices": {
        "A": "Create a new resource in the current stage. Create a new method with Lambda proxy integration. Select the Lambda function. Add the hotfix alias. Redeploy the current stage. Test the backend.",
        "B": "Update the Lambda function in the API Gateway API integration request to use the hotfix alias. Deploy the API Gateway API to a new stage named hotfix. Test the backend.",
        "C": "Modify the Lambda function by fixing the code. Test the Lambda function. Create the alias hotfix. Point the alias to the $LATEST version.",
        "D": "Modify the Lambda function by fixing the code. Test the Lambda function. When the Lambda function is working as expected, publish the Lambda function as a new version. Create the alias hotfix. Point the alias to the new version.",
        "E": "Create a new API Gateway API for the development environment. Add a resource and method with Lambda integration. Choose the Lambda function and the hotfix alias. Deploy to a new stage. Test the backend."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/108738-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 8, 2023, 1:28 p.m.",
      "textHash": "482837147fbe64b3",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:7937f5e2",
      "frExplanation": "Objectif : tester un correctif sans toucher la prod et sans que d‚Äôautres √©crasent votre travail. API Gateway sert de ‚Äúporte d‚Äôentr√©e‚Äù HTTP, et Lambda ex√©cute le code.\nLa solution la plus simple est de cr√©er un nouvel environnement c√¥t√© API Gateway : un ‚Äústage‚Äù (ex : prod, dev, hotfix) est comme une copie de configuration accessible via une URL diff√©rente.\nAvec l‚Äôoption B, on d√©ploie l‚ÄôAPI dans un nouveau stage nomm√© hotfix : vous testez via l‚ÄôURL du stage hotfix, sans impacter le stage production.\nOn configure l‚Äôint√©gration API Gateway pour appeler l‚Äôalias Lambda ‚Äúhotfix‚Äù : un alias est un nom stable qui pointe vers une version de Lambda.\nAinsi, vous pouvez faire √©voluer le code derri√®re l‚Äôalias hotfix pendant les tests, sans modifier la prod.\nEt comme l‚ÄôAPI de prod continue d‚Äôappeler sa configuration/version actuelle, les autres d√©veloppeurs ne peuvent pas ‚Äú√©craser‚Äù votre environnement de test.\nC‚Äôest moins d‚Äôeffort que de cr√©er une nouvelle API compl√®te (E) ou de bricoler des ressources/m√©thodes suppl√©mentaires dans le m√™me stage (A).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : il y a un guichet (API Gateway) o√π les √©l√®ves commandent, et une cuisine (Lambda) qui pr√©pare les plats. Aujourd‚Äôhui, il n‚Äôexiste qu‚Äôun seul service ‚Äúofficiel‚Äù (production).**\n\nConcept : au lieu de changer la cantine officielle, tu cr√©es un ‚Äúservice test‚Äù s√©par√©, mais avec le m√™me guichet. Pour √ßa, tu fais un nouveau cr√©neau/ligne de service (un nouveau stage) et tu dis au guichet d‚Äôenvoyer les commandes vers la ‚Äúcuisine test‚Äù (un alias hotfix). B fait exactement √ßa : on pointe l‚Äôint√©gration vers l‚Äôalias hotfix, puis on d√©ploie l‚ÄôAPI dans un nouveau stage nomm√© hotfix, et on teste sans toucher le service officiel. √áa √©vite que d‚Äôautres cuisiniers modifient la cuisine officielle pendant tes tests, car ton stage hotfix utilise une cible d√©di√©e. Les autres options demandent plus de bricolage (nouvelle API compl√®te, nouvelles ressources) ou ne cr√©ent pas clairement un environnement s√©par√©. Donc B = le moins d‚Äôeffort pour un vrai environnement de test isol√©.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:84:538eca3f81c749b5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 84,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a Ruby application and needs to automate the deployment, scaling, and management of an environment without requiring knowledge of the underlying infrastructure.Which service would best accomplish this task?",
      "choices": {
        "A": "AWS CodeDeploy",
        "B": "AWS CloudFormation",
        "C": "AWS OpsWorks",
        "D": "AWS Elastic Beanstalk"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/108737-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 8, 2023, 1:20 p.m.",
      "textHash": "538eca3f81c749b5",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Le besoin est de d√©ployer une appli Ruby et de g√©rer automatiquement l‚Äôenvironnement (d√©ploiement, mont√©e/descente en charge, supervision) sans s‚Äôoccuper des serveurs, du r√©seau ou des d√©tails syst√®me.\nAWS Elastic Beanstalk est fait pour √ßa : vous envoyez votre code (Ruby support√©) et le service cr√©e pour vous l‚Äôinfrastructure n√©cessaire (instances, √©quilibrage de charge, auto-scaling) et la g√®re.\nIl fournit un environnement ‚Äúpr√™t √† l‚Äôemploi‚Äù et vous laisse vous concentrer sur l‚Äôapplication, pas sur l‚Äôinfrastructure.\nCodeDeploy (A) automatise surtout le d√©ploiement sur des serveurs d√©j√† existants, mais ne cr√©e pas l‚Äôenvironnement complet.\nCloudFormation (B) d√©crit l‚Äôinfrastructure en templates : c‚Äôest puissant, mais il faut conna√Ætre et d√©finir les ressources sous-jacentes.\nOpsWorks (C) g√®re des configurations via Chef/Puppet et demande plus de gestion et de connaissances d‚Äôinfrastructure.\nDonc la meilleure r√©ponse est Elastic Beanstalk (D) car il masque l‚Äôinfrastructure tout en automatisant d√©ploiement et scaling.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu veux lancer un serveur Minecraft pour ta classe. Tu veux juste mettre ton monde en ligne, et que √ßa tienne quand tout le monde se connecte, sans apprendre comment marche l‚Äôordinateur du serveur, le r√©seau, la m√©moire, etc.**\n\nDans le cloud, ‚Äúd√©ployer + agrandir/r√©tr√©cir + g√©rer‚Äù une appli, c‚Äôest comme ‚Äúmettre le serveur en ligne + ajouter des machines quand il y a trop de joueurs + surveiller que tout tourne‚Äù.\nAWS Elastic Beanstalk (D) est le mode ‚Äútout-en-un‚Äù : tu donnes ton code Ruby, et il s‚Äôoccupe de cr√©er l‚Äôenvironnement, le lancer, l‚Äôagrandir quand il y a plus de monde, et le g√©rer au quotidien.\nLes autres choix sont plus ‚Äúoutils s√©par√©s‚Äù : CodeDeploy = juste installer/mettre √† jour le code, CloudFormation = plan d√©taill√© pour construire l‚Äôinfra toi-m√™me, OpsWorks = gestion plus manuelle fa√ßon ‚Äúadmin du serveur‚Äù.\nDonc D est le meilleur car tu n‚Äôas pas besoin de conna√Ætre l‚Äôinfrastructure, comme un bouton ‚Äúh√©berger mon serveur automatiquement‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:83:9b90e86f1ca47819",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 83,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has created an AWS Lambda function to provide notification through Amazon Simple Notification Service (Amazon SNS) whenever a file is uploaded to Amazon S3 that is larger than 50 MB. The developer has deployed and tested the Lambda function by using the CLI. However, when the event notification is added to the S3 bucket and a 3,000 MB file is uploaded, the Lambda function does not launch.Which of the following is a possible reason for the Lambda function's inability to launch?",
      "choices": {
        "A": "The S3 event notification does not activate for files that are larger than 1,000 MB.",
        "B": "The resource-based policy for the Lambda function does not have the required permissions to be invoked by Amazon S3.",
        "C": "Lambda functions cannot be invoked directly from an S3 event.",
        "D": "The S3 bucket needs to be made public."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/109005-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 11, 2023, 11:12 p.m.",
      "textHash": "9b90e86f1ca47819",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, S3 (stockage de fichiers) doit d√©clencher une fonction Lambda (code ex√©cut√© √† la demande) quand un objet est ajout√©. Pour que S3 puisse appeler Lambda, Lambda doit autoriser explicitement S3 via une ¬´ resource-based policy ¬ª (une permission attach√©e √† la fonction). Si cette permission manque ou pointe vers le mauvais bucket/ARN, S3 essaie d‚Äôinvoquer la fonction mais l‚Äôappel est refus√©, donc la fonction ne se lance pas. La taille du fichier n‚Äôemp√™che pas S3 d‚Äôenvoyer l‚Äô√©v√©nement (3 000 MB n‚Äôest pas une limite d‚Äôactivation d‚Äô√©v√©nement). Lambda peut bien √™tre d√©clench√©e par des √©v√©nements S3, c‚Äôest un cas d‚Äôusage standard. Rendre le bucket public n‚Äôa aucun rapport : l‚Äôinvocation se fait entre services AWS, pas via Internet. Donc la cause plausible est l‚Äôabsence de permission S3 -> Lambda dans la policy de la fonction.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un lyc√©e : le ‚Äúcasier‚Äù (S3) re√ßoit des colis, un ‚Äúsurveillant‚Äù (Lambda) doit se d√©clencher quand un colis est tr√®s lourd, et il envoie un message au ‚Äúhaut-parleur‚Äù (SNS) pour pr√©venir tout le monde.**\n\nConcept : quand un gros fichier arrive dans le casier, le casier doit avoir le droit d‚Äôappeler le surveillant pour qu‚Äôil agisse. M√™me si le surveillant marche quand tu l‚Äôappelles toi-m√™me (test CLI), √ßa ne veut pas dire que le casier a l‚Äôautorisation. Ici, le fichier de 3 000 MB arrive bien, mais le casier n‚Äôa pas le ‚Äúbadge‚Äù pour r√©veiller le surveillant : c‚Äôest une r√®gle d‚Äôautorisation manquante dans la politique de la fonction (resource-based policy). Donc Lambda ne se lance pas. A est faux : le casier peut signaler des gros colis. C est faux : un casier peut d√©clencher un surveillant. D est faux : pas besoin d‚Äôouvrir le lyc√©e au public.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:82:e8021793765ec733",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 82,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a highly secure healthcare application using serverless components. This application requires writing temporary data to /tmp storage on an AWS Lambda function.How should the developer encrypt this data?",
      "choices": {
        "A": "Enable Amazon EBS volume encryption with an AWS KMS key in the Lambda function configuration so that all storage attached to the Lambda function is encrypted.",
        "B": "Set up the Lambda function with a role and key policy to access an AWS KMS key. Use the key to generate a data key used to encrypt all data prior to writing to /tmp storage.",
        "C": "Use OpenSSL to generate a symmetric encryption key on Lambda startup. Use this key to encrypt the data prior to writing to /tmp.",
        "D": "Use an on-premises hardware security module (HSM) to generate keys, where the Lambda function requests a data key from the HSM and uses that to encrypt data on all requests to the function."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107445-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 25, 2023, 4:29 p.m.",
      "textHash": "e8021793765ec733",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Dans AWS Lambda, le dossier /tmp est un stockage local temporaire (√©ph√©m√®re) utilis√© pendant l‚Äôex√©cution. Il n‚Äôest pas chiffr√© automatiquement ¬´ √† votre fa√ßon ¬ª et vous devez chiffrer les donn√©es avant de les √©crire.\nAWS KMS (Key Management Service) est le service AWS pour cr√©er et prot√©ger des cl√©s de chiffrement, avec contr√¥le d‚Äôacc√®s et audit.\nLa bonne approche est de donner √† la fonction Lambda une autorisation (IAM role) pour utiliser une cl√© KMS, puis demander √† KMS de g√©n√©rer une ¬´ data key ¬ª (cl√© de donn√©es).\nVous utilisez cette data key pour chiffrer les donn√©es en m√©moire, puis vous √©crivez seulement le contenu chiffr√© dans /tmp.\nAinsi, la cl√© ma√Ætresse reste dans KMS (plus s√©curis√©), et vous pouvez d√©chiffrer plus tard en utilisant la data key chiffr√©e renvoy√©e par KMS.\nA est faux car Lambda n‚Äôattache pas un volume EBS configurable comme une instance EC2.\nC est moins s√ªr car une cl√© g√©n√©r√©e localement au d√©marrage n‚Äôest pas g√©r√©e, audit√©e, ni prot√©g√©e par KMS.\nD est inutilement complexe pour du serverless sur AWS et ne correspond pas aux bonnes pratiques AWS (KMS est le service pr√©vu).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine l‚Äôinfirmerie du lyc√©e : tu dois noter des infos m√©dicales sur un brouillon, puis le jeter. Le brouillon, c‚Äôest le dossier /tmp (temporaire).**\n\nConcept : m√™me si c‚Äôest temporaire, quelqu‚Äôun pourrait lire le brouillon si on le trouve. Donc on chiffre avant d‚Äô√©crire.\nPourquoi B : tu demandes √† un coffre-fort du lyc√©e (AWS KMS = service qui garde les cl√©s) une ‚Äúcl√© du jour‚Äù (data key). Tu utilises cette cl√© pour coder le texte sur le brouillon, puis tu peux jeter le brouillon sans risque.\nA est faux : /tmp n‚Äôest pas un disque EBS attach√© comme un disque dur classique.\nC est risqu√© : une cl√© invent√©e au d√©marrage peut √™tre mal prot√©g√©e et pas g√©r√©e/contr√¥l√©e proprement.\nD est inutilement compliqu√© : aller chercher une cl√© dans un coffre hors du lyc√©e √† chaque fois ralentit et complique.\nDonc : configurer l‚Äôacc√®s √† KMS, g√©n√©rer une data key, chiffrer avant d‚Äô√©crire dans /tmp.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:81:eca98039c392c0af",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 81,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company hosts a batch processing application on AWS Elastic Beanstalk with instances that run the most recent version of Amazon Linux. The application sorts and processes large datasets.In recent weeks, the application's performance has decreased significantly during a peak period for traffic. A developer suspects that the application issues are related to the memory usage. The developer checks the Elastic Beanstalk console and notices that memory usage is not being tracked.How should the developer gather more information about the application performance issues?",
      "choices": {
        "A": "Configure the Amazon CloudWatch agent to push logs to Amazon CloudWatch Logs by using port 443.",
        "B": "Configure the Elastic Beanstalk .ebextensions directory to track the memory usage of the instances.",
        "C": "Configure the Amazon CloudWatch agent to track the memory usage of the instances.",
        "D": "Configure an Amazon CloudWatch dashboard to track the memory usage of the instances."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106899-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 21, 2023, 2:04 p.m.",
      "textHash": "eca98039c392c0af",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Elastic Beanstalk g√®re vos serveurs (instances EC2) et affiche des m√©triques de base (CPU, r√©seau), mais la m√©moire (RAM) n‚Äôest souvent pas collect√©e par d√©faut.\nSi vous suspectez un probl√®me de RAM, il faut d‚Äôabord mesurer la RAM utilis√©e sur chaque instance.\nAmazon CloudWatch est le service AWS qui collecte des m√©triques et permet de les consulter/alerter.\nLe CloudWatch Agent est un petit programme install√© sur l‚Äôinstance qui peut envoyer des m√©triques ‚Äúsyst√®me‚Äù avanc√©es comme la m√©moire, le swap et l‚Äôespace disque.\nDonc la bonne action est de configurer le CloudWatch Agent pour publier la m√©trique de m√©moire vers CloudWatch.\nA parle surtout d‚Äôenvoyer des logs (texte) vers CloudWatch Logs, ce n‚Äôest pas une m√©trique de RAM.\nB (.ebextensions) peut automatiser des configs, mais ce n‚Äôest pas la solution directe : il faut un agent qui mesure la m√©moire.\nD un dashboard n‚Äôinvente pas de donn√©es : il affiche seulement des m√©triques d√©j√† collect√©es.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:38:d0ec2b98cab4a00f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 38,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application that is hosted on an Amazon EC2 instance needs access to files that are stored in an Amazon S3 bucket. The application lists the objects that are stored in the S3 bucket and displays a table to the user. During testing, a developer discovers that the application does not show any objects in the list.What is the MOST secure way to resolve this issue?",
      "choices": {
        "A": "Update the IAM instance profile that is attached to the EC2 instance to include the S3:* permission for the S3 bucket.",
        "B": "Update the IAM instance profile that is attached to the EC2 instance to include the S3:ListBucket permission for the S3 bucket.",
        "C": "Update the developer's user permissions to include the S3:ListBucket permission for the S3 bucket.",
        "D": "Update the S3 bucket policy by including the S3:ListBucket permission and by setting the Principal element to specify the account number of the EC2 instance."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103522-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 10:41 p.m.",
      "textHash": "d0ec2b98cab4a00f",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "L‚Äôapplication tourne sur une instance EC2 (un serveur). Pour acc√©der √† S3 (stockage d‚Äôobjets), elle doit avoir des droits IAM via un ¬´ instance profile ¬ª (r√¥le attach√© √† l‚ÄôEC2).\nLe sympt√¥me ¬´ la liste est vide ¬ª indique souvent un manque d‚Äôautorisation pour l‚Äôaction qui liste le contenu du bucket : s3:ListBucket.\nLa solution la plus s√ªre est d‚Äôaccorder uniquement le droit minimum n√©cessaire (principe du moindre privil√®ge).\nB fait exactement cela : ajouter s3:ListBucket sur le bucket au r√¥le IAM de l‚ÄôEC2, ce qui permet √† l‚Äôapplication de lister les objets.\nA est trop large (s3:* donne presque tous les droits sur S3, y compris supprimer/√©crire), donc moins s√©curis√©.\nC modifie les droits du d√©veloppeur, mais l‚Äôapplication s‚Äôex√©cute avec le r√¥le de l‚ÄôEC2, pas avec l‚Äôutilisateur du d√©veloppeur.\nD est incorrect/risqu√© : le Principal d‚Äôune policy S3 doit viser un r√¥le/ARN (ou compte), mais ¬´ le compte de l‚ÄôEC2 ¬ª n‚Äôest pas une identit√© IAM pr√©cise; mieux vaut autoriser le r√¥le de l‚Äôinstance via IAM.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:78:80a5cc8858eb7bb0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 78,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is planning to deploy an application on AWS behind an Elastic Load Balancer. The application uses an HTTP/HTTPS listener and must access the client IP addresses.Which load-balancing solution meets these requirements?",
      "choices": {
        "A": "Use an Application Load Balancer and the X-Forwarded-For headers.",
        "B": "Use a Network Load Balancer (NLB). Enable proxy protocol support on the NLB and the target application.",
        "C": "Use an Application Load Balancer. Register the targets by the instance ID.",
        "D": "Use a Network Load Balancer and the X-Forwarded-For headers."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107444-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 25, 2023, 4:11 p.m.",
      "textHash": "80a5cc8858eb7bb0",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Un Elastic Load Balancer (ELB) re√ßoit les requ√™tes des utilisateurs et les transmet √† vos serveurs (cibles). Probl√®me : quand un load balancer relaie une requ√™te, votre application peut ne voir que l‚Äôadresse IP du load balancer, pas celle du client.\nIci, l‚Äôapplication utilise un listener HTTP/HTTPS : c‚Äôest exactement le cas d‚Äôusage d‚Äôun Application Load Balancer (ALB), qui comprend les requ√™tes web (couche 7).\nPour conserver l‚ÄôIP d‚Äôorigine du client, l‚ÄôALB ajoute automatiquement l‚Äôen-t√™te HTTP ¬´ X-Forwarded-For ¬ª contenant l‚ÄôIP du client.\nVotre application doit donc lire cet en-t√™te pour conna√Ætre la vraie IP.\nLe Network Load Balancer (NLB) est plut√¥t niveau r√©seau (TCP/UDP) et n‚Äôajoute pas d‚Äôen-t√™tes HTTP ; ¬´ X-Forwarded-For ¬ª ne s‚Äôapplique pas √† lui.\nLe ¬´ proxy protocol ¬ª est une autre m√©thode, mais elle est surtout utilis√©e avec NLB/TCP, pas n√©cessaire ici puisque ALB + X-Forwarded-For r√©pond directement au besoin HTTP/HTTPS.\nDonc la meilleure solution est : ALB + en-t√™te X-Forwarded-For (r√©ponse A).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine l‚Äôentr√©e de ton lyc√©e : un surveillant (le ‚Äúr√©partiteur‚Äù) re√ßoit tous les √©l√®ves et les envoie vers diff√©rents profs (les serveurs) pour √©viter les embouteillages.**\n\nConcept : quand le surveillant renvoie un √©l√®ve vers un prof, le prof voit surtout ‚Äúle surveillant‚Äù arriver, pas l‚Äô√©l√®ve d‚Äôorigine. Pour conna√Ætre le vrai √©l√®ve, le surveillant colle un petit mot sur le dossier : ‚Äú√©l√®ve = Paul, classe 2nde3‚Äù.\nIci, l‚Äôappli parle en HTTP/HTTPS (comme des messages web). Un Application Load Balancer est le surveillant sp√©cialis√© pour HTTP/HTTPS.\nPour garder l‚ÄôIP du client (qui est ‚Äúqui est l‚Äô√©l√®ve‚Äù), il ajoute le petit mot standard X-Forwarded-For, qui contient l‚Äôadresse du client.\nDonc A est bon : ALB + X-Forwarded-For = l‚Äôappli peut lire l‚ÄôIP r√©elle.\nB et D parlent d‚Äôun autre type de surveillant (Network Load Balancer) : il ne g√®re pas ce ‚Äúpetit mot‚Äù X-Forwarded-For pour HTTP/HTTPS.\nC ne change pas le probl√®me : dire ‚Äúquel prof‚Äù (instance ID) n‚Äôaide pas √† conna√Ætre ‚Äúquel √©l√®ve‚Äù (IP client).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:77:4a1fa154cf27a228",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 77,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application under development is required to store hundreds of video files. The data must be encrypted within the application prior to storage, with a unique key for each video file.How should the developer code the application?",
      "choices": {
        "A": "Use the KMS Encrypt API to encrypt the data. Store the encrypted data key and data.",
        "B": "Use a cryptography library to generate an encryption key for the application. Use the encryption key to encrypt the data. Store the encrypted data.",
        "C": "Use the KMS GenerateDataKey API to get a data key. Encrypt the data with the data key. Store the encrypted data key and data.",
        "D": "Upload the data to an S3 bucket using server side-encryption with an AWS KMS key."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107443-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 25, 2023, 4:01 p.m.",
      "textHash": "4a1fa154cf27a228",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "On veut chiffrer chaque vid√©o AVANT de la stocker, avec une cl√© diff√©rente par fichier.\nAWS KMS (Key Management Service) g√®re des cl√©s ma√Ætres et peut cr√©er des ¬´ cl√©s de donn√©es ¬ª uniques pour chiffrer du contenu.\nAvec GenerateDataKey, l‚Äôapplication re√ßoit 2 versions de la m√™me cl√© : une en clair (pour chiffrer la vid√©o localement) et une chiffr√©e par KMS.\nL‚Äôapplication chiffre la vid√©o avec la cl√© en clair, puis jette cette cl√© (ne la stocke pas en clair).\nElle stocke seulement : la vid√©o chiffr√©e + la cl√© de donn√©es chiffr√©e (enveloppe) √† c√¥t√©.\nPlus tard, pour d√©chiffrer, on redonne la cl√© de donn√©es chiffr√©e √† KMS (Decrypt) pour r√©cup√©rer la cl√© en clair.\nA est moins adapt√© : Encrypt sert surtout √† chiffrer de petites donn√©es, pas de gros fichiers vid√©o.\nB est risqu√© car g√©rer soi-m√™me les cl√©s (rotation, stockage, acc√®s) est complexe.\nD chiffre c√¥t√© S3 (server-side) et ne respecte pas l‚Äôexigence ¬´ chiffrer dans l‚Äôapplication avant stockage ¬ª.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e o√π tu dois ranger des centaines de vid√©os sur des cl√©s USB. Chaque vid√©o doit √™tre dans un casier ferm√© avec un cadenas diff√©rent, et tu dois garder une copie du code du cadenas dans un coffre-fort du proviseur.**\n\nConcept : tu veux un cadenas unique par vid√©o (une ‚Äúcl√©‚Äù diff√©rente), mais tu ne veux pas garder ces cadenas en vrac dans ton sac (trop risqu√©). Tu demandes donc au proviseur (KMS = coffre-fort √† cl√©s) de te donner un cadenas neuf pour chaque vid√©o.\nPourquoi C : avec GenerateDataKey, le proviseur te donne 2 choses : 1) le vrai code du cadenas √† utiliser tout de suite pour fermer la vid√©o, 2) le m√™me code mais enferm√© dans une enveloppe scell√©e (la ‚Äúcl√© chiffr√©e‚Äù). Tu fermes la vid√©o avec le vrai code, puis tu ranges la vid√©o ferm√©e + l‚Äôenveloppe scell√©e ensemble.\nA est faux : Encrypt sert surtout √† fermer un petit message, pas √† g√©rer une cl√© unique par gros fichier comme ici.\nB est risqu√© : fabriquer toi-m√™me une cl√© ‚Äúpour l‚Äôappli‚Äù revient √† utiliser le m√™me cadenas partout.\nD ne respecte pas la r√®gle : c‚Äôest le stockage qui chiffre, pas l‚Äôapplication avant d‚Äôenvoyer.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:76:703fda3407ddc76b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 76,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer deployed an application to an Amazon EC2 instance. The application needs to know the public IPv4 address of the instance.How can the application find this information?",
      "choices": {
        "A": "Query the instance metadata from http://169.254.169.254/latest/meta-data/.",
        "B": "Query the instance user data from http://169.254.169.254/latest/user-data/.",
        "C": "Query the Amazon Machine Image (AMI) information from http://169.254.169.254/latest/meta-data/ami/.",
        "D": "Check the hosts file of the operating system."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/108728-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 8, 2023, 10:54 a.m.",
      "textHash": "703fda3407ddc76b",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:8546233a",
      "frExplanation": "Sur une instance Amazon EC2 (un serveur virtuel), AWS fournit un service local appel√© ‚ÄúInstance Metadata Service‚Äù.\nCe service est accessible uniquement depuis l‚Äôinstance via l‚Äôadresse sp√©ciale 169.254.169.254 (ce n‚Äôest pas Internet).\nLes ‚Äúmetadata‚Äù contiennent des infos sur l‚Äôinstance : ID, type, zone, et aussi ses adresses IP (dont l‚ÄôIPv4 publique si elle existe).\nDonc l‚Äôapplication peut faire une requ√™te HTTP vers /latest/meta-data/ (par ex. /public-ipv4) pour r√©cup√©rer l‚ÄôIP publique.\nLes ‚Äúuser data‚Äù (choix B) servent plut√¥t √† passer un script/config au d√©marrage, pas √† lire l‚ÄôIP courante.\nLes infos AMI (choix C) d√©crivent l‚Äôimage syst√®me utilis√©e, pas l‚Äôadresse r√©seau de l‚Äôinstance.\nLe fichier hosts (choix D) mappe des noms vers des IP fixes locales, il ne conna√Æt pas l‚ÄôIP publique attribu√©e par AWS.\nAinsi, la bonne m√©thode est de consulter les m√©tadonn√©es via 169.254.169.254.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine ton appli comme un √©l√®ve dans un lyc√©e. Elle doit conna√Ætre son ‚Äúadresse publique‚Äù, comme l‚Äôadresse postale du lyc√©e pour recevoir du courrier de l‚Äôext√©rieur. Dans le lyc√©e, il y a un panneau officiel d‚Äôinfos (le tableau d‚Äôaffichage) qui donne les infos du b√¢timent.**\n\nSur une machine EC2, il existe un ‚Äútableau d‚Äôaffichage interne‚Äù accessible seulement depuis la machine elle‚Äëm√™me : l‚Äôadresse http://169.254.169.254. √áa s‚Äôappelle les ‚Äúmetadata‚Äù (infos sur la machine : son adresse publique, son nom, etc.). Donc l‚Äôappli va lire ce panneau : /latest/meta-data/ pour r√©cup√©rer l‚ÄôIPv4 publique ‚Üí r√©ponse A. Les ‚Äúuser data‚Äù (B), c‚Äôest plut√¥t un mot laiss√© au d√©part par l‚Äôadmin (consignes), pas l‚Äôadresse actuelle. L‚ÄôAMI (C), c‚Äôest le ‚Äúmod√®le de d√©part‚Äù de la machine, pas son adresse r√©seau. Le fichier hosts (D), c‚Äôest un petit carnet de surnoms locaux, pas l‚Äôadresse publique officielle.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:75:29fff718240cb2e0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 75,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing unit tests for a new application that will be deployed on AWS. The developer wants to validate all pull requests with unit tests and merge the code with the main branch only when all tests pass.The developer stores the code in AWS CodeCommit and sets up AWS CodeBuild to run the unit tests. The developer creates an AWS Lambda function to start the CodeBuild task. The developer needs to identify the CodeCommit events in an Amazon EventBridge event that can invoke the Lambda function when a pull request is created or updated.Which CodeCommit event will meet these requirements?",
      "choices": {},
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106717-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 19, 2023, 1:58 p.m.",
      "textHash": "29fff718240cb2e0",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Objectif : lancer automatiquement des tests unitaires quand une Pull Request (PR) est cr√©√©e ou mise √† jour dans CodeCommit.\nCodeCommit = d√©p√¥t Git g√©r√© par AWS (comme GitHub). Une Pull Request = demande de fusionner du code vers la branche principale.\nEventBridge = service qui re√ßoit des ‚Äú√©v√©nements‚Äù (notifications) et peut d√©clencher une action (ici une fonction Lambda).\nLambda = petit code ex√©cut√© √† la demande, ici pour d√©marrer un build.\nCodeBuild = service qui ex√©cute les tests/compilations.\nIl faut donc un √©v√©nement CodeCommit li√© aux Pull Requests, pas aux commits, branches ou tags.\nL‚Äô√©v√©nement correct est celui qui signale ‚Äúpull request created or updated‚Äù (cr√©ation ou mise √† jour d‚Äôune PR).\nAvec cet √©v√©nement, EventBridge d√©clenche Lambda √† chaque changement de PR, et Lambda d√©marre CodeBuild pour valider les tests.\nC‚Äôest exactement ce qui permet de n‚Äôautoriser la fusion vers main que si les tests passent.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un devoir de groupe √† l‚Äô√©cole sur Google Docs : chacun propose des changements, et le prof veut lancer un mini-contr√¥le automatique √† chaque fois qu‚Äôun √©l√®ve cr√©e une proposition ou la modifie.**\n\nConcept : une ‚Äúpull request‚Äù, c‚Äôest comme une proposition de modifications avant de les mettre dans la version officielle du devoir (la branche principale). On veut lancer des tests (le mini-contr√¥le) √† chaque cr√©ation ou mise √† jour de cette proposition. EventBridge, c‚Äôest le ‚Äúsurveillant‚Äù qui √©coute les √©v√©nements, et Lambda, c‚Äôest l‚Äô√©l√®ve messager qui d√©clenche CodeBuild (la salle o√π on fait le contr√¥le). Donc il faut un √©v√©nement CodeCommit qui dit : ‚Äúune pull request a √©t√© cr√©√©e ou mise √† jour‚Äù. La bonne r√©ponse C correspond √† l‚Äô√©v√©nement Pull Request (cr√©ation/mise √† jour), pas √† un simple push de code ou √† un merge final. Comme √ßa, chaque fois que la proposition change, le contr√¥le se relance, et on ne fusionne que si tout est OK.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:74:4a59e85c5b2175a9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 74,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer created an AWS Lambda function that accesses resources in a VPC. The Lambda function polls an Amazon Simple Queue Service (Amazon SQS) queue for new messages through a VPC endpoint. Then the function calculates a rolling average of the numeric values that are contained in the messages. After initial tests of the Lambda function, the developer found that the value of the rolling average that the function returned was not accurate.How can the developer ensure that the function calculates an accurate rolling average?",
      "choices": {
        "A": "Set the function's reserved concurrency to 1. Calculate the rolling average in the function. Store the calculated rolling average in Amazon ElastiCache.",
        "B": "Modify the function to store the values in Amazon ElastiCache. When the function initializes, use the previous values from the cache to calculate the rolling average.",
        "C": "Set the function's provisioned concurrency to 1. Calculate the rolling average in the function. Store the calculated rolling average in Amazon ElastiCache.",
        "D": "Modify the function to store the values in the function's layers. When the function initializes, use the previously stored values to calculate the rolling average."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106711-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 19, 2023, 1:43 p.m.",
      "textHash": "4a59e85c5b2175a9",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, le probl√®me vient du fait qu‚ÄôAWS Lambda peut ex√©cuter plusieurs instances de la m√™me fonction en parall√®le pour traiter les messages SQS plus vite. Si plusieurs ex√©cutions calculent une ‚Äúmoyenne glissante‚Äù en m√™me temps, elles peuvent lire/√©crire des valeurs dans le d√©sordre et produire une moyenne incorrecte.\nPour une moyenne glissante, il faut un √©tat partag√© et des mises √† jour s√©quentielles (une seule mise √† jour √† la fois).\nLa bonne solution est donc de limiter la fonction √† une seule ex√©cution simultan√©e avec la ‚Äúreserved concurrency = 1‚Äù. Ainsi, les messages sont trait√©s un par un, dans l‚Äôordre de traitement, sans concurrence.\nEnsuite, il faut stocker la moyenne (ou l‚Äô√©tat n√©cessaire) dans un stockage externe persistant et rapide : Amazon ElastiCache (Redis/Memcached) sert de m√©moire partag√©e entre ex√©cutions.\nA r√©pond exactement √† ces deux besoins : pas de parall√©lisme + √©tat centralis√©.\nB ne r√®gle pas le parall√©lisme : m√™me avec un cache, plusieurs Lambdas peuvent √©craser l‚Äô√©tat en m√™me temps.\nC (provisioned concurrency) garantit des instances ‚Äúpr√™tes‚Äù, mais n‚Äôemp√™che pas d‚Äôen avoir plusieurs en parall√®le.\nD est faux car les layers Lambda ne sont pas faits pour stocker des donn√©es modifiables entre ex√©cutions (c‚Äôest du code/biblioth√®ques, pas un stockage d‚Äô√©tat).",
      "domainKey": "development",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:73:c5eb172586976d45",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 73,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs an application on AWS. The company deployed the application on Amazon EC2 instances. The application stores data on Amazon Aurora.The application recently logged multiple application-specific custom DECRYP_ERROR errors to Amazon CloudWatch logs. The company did not detect the issue until the automated tests that run every 30 minutes failed. A developer must implement a solution that will monitor for the custom errors and alert a development team in real time when these errors occur in the production environment.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Configure the application to create a custom metric and to push the metric to CloudWatch. Create an AWS CloudTrail alarm. Configure the CloudTrail alarm to use an Amazon Simple Notification Service (Amazon SNS) topic to send notifications.",
        "B": "Create an AWS Lambda function to run every 5 minutes to scan the CloudWatch logs for the keyword DECRYP_ERROR. Configure the Lambda function to use Amazon Simple Notification Service (Amazon SNS) to send a notification.",
        "C": "Use Amazon CloudWatch Logs to create a metric filter that has a filter pattern for DECRYP_ERROR. Create a CloudWatch alarm on this metric for a threshold >=1. Configure the alarm to send Amazon Simple Notification Service (Amazon SNS) notifications.",
        "D": "Install the CloudWatch unified agent on the EC2 instance. Configure the application to generate a metric for the keyword DECRYP_ERROR errors. Configure the agent to send Amazon Simple Notification Service (Amazon SNS) notifications."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106708-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 19, 2023, 1:39 p.m.",
      "textHash": "c5eb172586976d45",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Objectif : √™tre alert√© en temps r√©el d√®s qu‚Äôune ligne de log contient l‚Äôerreur personnalis√©e ¬´ DECRYP_ERROR ¬ª, avec le moins d‚Äôadministration possible.\nAmazon CloudWatch Logs stocke les journaux (logs) de l‚Äôapplication. On peut y cr√©er un ¬´ metric filter ¬ª : une r√®gle qui cherche un mot/texte pr√©cis dans les logs.\nQuand le filtre trouve ¬´ DECRYP_ERROR ¬ª, il incr√©mente automatiquement une m√©trique CloudWatch (un compteur).\nEnsuite, on cr√©e une alarme CloudWatch sur cette m√©trique avec un seuil >= 1 : d√®s qu‚Äôau moins une erreur appara√Æt, l‚Äôalarme se d√©clenche.\nL‚Äôalarme envoie une notification via Amazon SNS (service d‚Äôenvoi de messages) vers email, SMS, ou un outil de chat/incident.\nC‚Äôest en temps quasi r√©el et sans serveur √† g√©rer : pas de Lambda planifi√©e, pas d‚Äôagent √† installer, pas de code sp√©cifique.\nA est incorrect car CloudTrail sert √† auditer des appels API AWS, pas √† d√©tecter des messages d‚Äôerreur applicatifs dans les logs.\nB et D ajoutent plus d‚Äôoverhead (fonction planifi√©e √† maintenir ou agent √† installer/configurer).",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine ton coll√®ge : il y a un surveillant qui √©coute les annonces au micro. D√®s qu‚Äôil entend un mot interdit (‚ÄúDECRYP_ERROR‚Äù), il d√©clenche tout de suite une alarme et envoie un message au CPE (l‚Äô√©quipe dev).**\n\nConcept : on veut rep√©rer un mot pr√©cis dans un ‚Äúcahier de messages‚Äù (les logs) et pr√©venir imm√©diatement, sans qu‚Äôun humain fasse des rondes. Avec C, CloudWatch Logs met un ‚Äúfiltre‚Äù qui rep√®re automatiquement DECRYP_ERROR, comme un d√©tecteur de mots. Ce filtre transforme chaque occurrence en compteur (une m√©trique). Puis une alarme se d√©clenche d√®s que le compteur est ‚â• 1, donc en temps r√©el. L‚Äôalarme envoie un message via SNS, comme une liste de diffusion qui pr√©vient toute l‚Äô√©quipe. C‚Äôest le moins de boulot √† g√©rer : pas de script qui tourne toutes les 5 minutes (B), pas d‚Äôoutil inutile (CloudTrail en A), pas d‚Äôagent √† installer sur les PC (D).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:72:4f92c2ffd3208029",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 72,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer creates a VPC named VPC-A that has public and private subnets. The developer also creates an Amazon RDS database inside the private subnet of VPC-A. To perform some queries, the developer creates an AWS Lambda function in the default VPC. The Lambda function has code to access the RDS database. When the Lambda function runs, an error message indicates that the function cannot connect to the RDS database.How can the developer solve this problem?",
      "choices": {
        "A": "Modify the RDS security group. Add a rule to allow traffic from all the ports from the VPC CIDR block.",
        "B": "Redeploy the Lambda function in the same subnet as the RDS instance. Ensure that the RDS security group allows traffic from the Lambda function.",
        "C": "Create a security group for the Lambda function. Add a new rule in the RDS security group to allow traffic from the new Lambda security group.",
        "D": "Create an IAM role. Attach a policy that allows access to the RDS database. Attach the role to the Lambda function."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106491-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 17, 2023, 4:23 p.m.",
      "textHash": "4f92c2ffd3208029",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Le probl√®me vient du r√©seau, pas des permissions IAM. La base Amazon RDS est dans un sous-r√©seau priv√© de VPC-A : elle n‚Äôest accessible que depuis ce VPC (et selon les r√®gles de s√©curit√©).\nUne fonction AWS Lambda lanc√©e dans le VPC par d√©faut est dans un autre r√©seau isol√© : par d√©faut, elle ne peut pas ‚Äúvoir‚Äù les ressources de VPC-A, donc la connexion √©choue.\nLa solution est de configurer Lambda pour s‚Äôex√©cuter dans VPC-A, dans des sous-r√©seaux (souvent priv√©s) qui ont une route vers RDS.\nEnsuite, il faut autoriser le trafic dans le Security Group (pare-feu) de RDS depuis le Security Group de Lambda sur le port de la base (ex: 3306 MySQL, 5432 PostgreSQL).\nPourquoi pas A : ouvrir ‚Äútous les ports‚Äù est inutile et dangereux.\nPourquoi pas C : m√™me avec des Security Groups corrects, si Lambda reste dans le VPC par d√©faut, il n‚Äôy a toujours pas de chemin r√©seau vers RDS.\nPourquoi pas D : IAM g√®re l‚Äôacc√®s aux API AWS, pas la connectivit√© r√©seau TCP vers une base dans un sous-r√©seau priv√©.\nDonc il faut red√©ployer/configurer Lambda dans le m√™me VPC et des subnets compatibles, puis ajuster les Security Groups (r√©ponse B).",
      "domainKey": "development",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:71:2ec6faa5d543883d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 71,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on an existing application that uses Amazon DynamoDB as its data store. The DynamoDB table has the following attributes: partNumber (partition key), vendor (sort key), description, productFamily, and productType. When the developer analyzes the usage patterns, the developer notices that there are application modules that frequently look for a list of products based on the productFamily and productType attributes.The developer wants to make changes to the application to improve performance of the query operations.Which solution will meet these requirements?",
      "choices": {
        "A": "Create a global secondary index (GSI) with productFamily as the partition key and productType as the sort key.",
        "B": "Create a local secondary index (LSI) with productFamily as the partition key and productType as the sort key.",
        "C": "Recreate the table. Add partNumber as the partition key and vendor as the sort key. During table creation, add a local secondary index (LSI) with productFamily as the partition key and productType as the sort key.",
        "D": "Update the queries to use Scan operations with productFamily as the partition key and productType as the sort key."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106490-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 17, 2023, 4:19 p.m.",
      "textHash": "2ec6faa5d543883d",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π les requ√™tes rapides utilisent une cl√© de partition (et parfois une cl√© de tri). Ici, la table est organis√©e par partNumber (partition) + vendor (tri), donc chercher par productFamily/productType ne peut pas utiliser directement la cl√© principale.\nSi on fait un Scan, DynamoDB lit (presque) toute la table puis filtre : c‚Äôest lent et co√ªteux.\nPour interroger efficacement sur d‚Äôautres attributs que la cl√© principale, on cr√©e un index secondaire.\nUn GSI (Global Secondary Index) permet de d√©finir une nouvelle ‚Äúcl√© principale‚Äù bas√©e sur d‚Äôautres attributs, ind√©pendamment de la cl√© de la table.\nDonc un GSI avec productFamily comme partition key et productType comme sort key permet de faire des Query rapides pour lister les produits d‚Äôune famille et d‚Äôun type.\nUn LSI (Local Secondary Index) doit garder la m√™me partition key que la table (partNumber), donc il ne convient pas ici.\nRecr√©er la table n‚Äôest pas n√©cessaire, et l‚Äôoption LSI propos√©e resterait invalide car elle change la partition key.\nLa meilleure solution est donc de cr√©er un GSI sur productFamily/productType (r√©ponse A).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e : chaque livre a un code unique (partNumber) et un √©diteur (vendor). Mais toi, tu cherches souvent ‚Äútous les mangas‚Äù (productFamily) puis ‚Äúshonen‚Äù (productType).**\n\nDans DynamoDB, la table est rang√©e surtout pour retrouver vite par partNumber + vendor (comme chercher par code + √©diteur). Si tu demandes souvent ‚Äúfamille + type‚Äù, chercher dans toute la biblioth√®que serait lent (comme parcourir tous les rayons). La bonne id√©e est de cr√©er un ‚Äúcatalogue secondaire‚Äù sp√©cial, tri√© exactement pour tes recherches : productFamily en premier (partition key) puis productType (sort key). C‚Äôest √ßa un GSI : un index s√©par√© qui permet de chercher vite avec d‚Äôautres crit√®res que ceux de la table. Un LSI ne marche pas ici car il doit garder la m√™me cl√© de d√©part que la table (partNumber), donc il ne peut pas √™tre bas√© sur productFamily. Un Scan, c‚Äôest comme lire tous les livres un par un : trop lent. Donc A est la bonne r√©ponse.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:350:5b66d3840ff5e01f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 350,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company wants to migrate applications from its on-premises servers to AWS. As a first step, the company is modifying and migrating a non-critical application to a single Amazon EC2 instance. The application will store information in an Amazon S3 bucket. The company needs to follow security best practices when deploying the application on AWS.Which approach should the company take to allow the application to interact with Amazon S3?",
      "choices": {
        "A": "Create an IAM role that has administrative access to AWS. Attach the role to the EC2 instance.",
        "B": "Create an IAM user. Attach the AdministratorAccess policy. Copy the generated access key and secret key. Within the application code, use the access key and secret key along with the AWS SDK to communicate with Amazon S3.",
        "C": "Create an IAM role that has the necessary access to Amazon S3. Attach the role to the EC2 instance.",
        "D": "Create an IAM user. Attach a policy that provides the necessary access to Amazon S3. Copy the generated access key and secret key. Within the application code, use the access key and secret key along with the AWS SDK to communicate with Amazon S3."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/141194-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 25, 2024, 5:31 a.m.",
      "textHash": "5b66d3840ff5e01f",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Pour qu‚Äôune application sur une instance EC2 (un serveur dans AWS) acc√®de √† un bucket S3 (stockage de fichiers), il faut lui donner des permissions.\nLa bonne pratique AWS est d‚Äô√©viter de mettre des cl√©s d‚Äôacc√®s (access key/secret key) dans le code ou sur le serveur : elles peuvent fuiter, √™tre copi√©es, ou rester valides trop longtemps.\n√Ä la place, on utilise un r√¥le IAM : c‚Äôest une ‚Äúidentit√©‚Äù avec des droits, que l‚Äôon attache √† l‚Äôinstance EC2.\nL‚Äôapplication r√©cup√®re alors automatiquement des identifiants temporaires via le r√¥le, sans rien stocker en dur.\nOn applique aussi le principe du moindre privil√®ge : le r√¥le ne doit avoir que les droits n√©cessaires sur S3 (ex: lire/√©crire dans un bucket pr√©cis).\nDonc C est correct : cr√©er un r√¥le IAM avec les permissions S3 n√©cessaires et l‚Äôattacher √† l‚Äôinstance EC2.\nA est mauvais car ‚Äúadministrateur‚Äù donne trop de droits.\nB et D sont mauvais car ils utilisent un utilisateur IAM et des cl√©s statiques dans le code, ce qui n‚Äôest pas s√©curis√©.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:343:f99f72133c076ace",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 343,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a new containerized application by using AWS Copilot. The developer uses the AWS Copilot command line interface (CLI) to deploy the application during development. The developer committed the application code to a new AWS CodeCommit repository. The developer must create an automated deployment process before releasing the new application to production.What should the developer do to meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "A": "Create a buildspec file that invokes the AWS Copilot CLI commands to build and deploy the application. Use the AWS Copilot CLI to create an AWS CodePipeline that uses the CodeCommit repository in the source stage and AWS CodeBuild in the build stage.",
        "B": "Use the AWS Serverless Application Model (AWS SAM) CLI to bootstrap and initialize an AWS CodePipeline configuration. Use the CodeCommit repository as the source. Invoke the AWS Copilot CLI to build and deploy the application.",
        "C": "Use the AWS Copilot CLI to define the AWS Copilot pipeline and to deploy the AWS CodePipeline. Select CodeCommit as the source for the AWS CodePipeline.",
        "D": "Define an AWS CloudFormation template for an AWS CodePipeline with CodeCommit as the source. Configure the template as an AWS Copilot CLI add-on. Use the AWS Copilot CLI to deploy the application."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136967-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 4:24 a.m.",
      "textHash": "f99f72133c076ace",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "AWS Copilot est un outil (CLI) qui simplifie le d√©ploiement d‚Äôapplications conteneuris√©es sur AWS (ECS/Fargate) et peut aussi cr√©er une cha√Æne CI/CD.\nCodeCommit est le d√©p√¥t Git o√π le code est stock√©. CodePipeline est le service qui automatise les √©tapes ‚Äúprendre le code ‚Üí construire ‚Üí d√©ployer‚Äù.\nLe besoin est d‚Äôautomatiser le d√©ploiement vers la production √† partir du d√©p√¥t, de fa√ßon la plus simple √† op√©rer.\nAvec Copilot, la m√©thode la plus efficace est d‚Äôutiliser sa fonctionnalit√© ‚Äúpipeline‚Äù : Copilot g√©n√®re et configure automatiquement CodePipeline (et les r√¥les/param√®tres n√©cessaires) pour votre application.\nEn choisissant CodeCommit comme source, chaque push d√©clenche la pipeline qui construit et d√©ploie selon les environnements d√©finis.\nLes autres options demandent plus de travail manuel (buildspec, SAM, CloudFormation) et augmentent la maintenance, alors que Copilot sait d√©j√† cr√©er la pipeline adapt√©e.\nDonc la meilleure r√©ponse est d‚Äôutiliser Copilot pour d√©finir et d√©ployer la pipeline Copilot/CodePipeline avec CodeCommit comme source (C).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o o√π tu cr√©es un niveau. Ton code, c‚Äôest le plan du niveau dans ton cahier. Tu veux qu‚Äô√† chaque fois que tu modifies le plan, le jeu reconstruise automatiquement le niveau et le mette en ligne, sans que tu refasses tout √† la main.**\n\nAWS Copilot, c‚Äôest l‚Äôassistant ‚Äúsp√©cial conteneurs‚Äù (un conteneur = une bo√Æte qui contient ton appli + tout ce qu‚Äôil lui faut). CodeCommit, c‚Äôest le cahier o√π tu ranges tes versions du plan. Une pipeline (CodePipeline), c‚Äôest la cha√Æne automatique: ‚Äúje vois un nouveau plan ‚Üí je construis ‚Üí je d√©ploie‚Äù. La r√©ponse C est la meilleure car Copilot sait d√©j√† cr√©er et configurer cette cha√Æne automatique pour une appli en conteneurs, en choisissant CodeCommit comme source. Donc tu fais le moins de r√©glages manuels et tu √©vites d‚Äô√©crire des scripts ou des mod√®les compliqu√©s. A et D te demandent de bricoler plus (scripts/mod√®les), B utilise un autre outil pas fait pour Copilot. Avec C, tu passes du mode ‚Äòje d√©ploie √† la main‚Äô √† ‚Äò√ßa se d√©ploie tout seul‚Äô avant la production.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:336:0e26551e42d6bac2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 336,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company with multiple branch locations has an analytics and reporting application. Each branch office pushes a sales report to a shared Amazon S3 bucket at a predefined time each day. The company has developed an AWS Lambda function that analyzes the reports from all branch offices in a single pass. The Lambda function stores the results in a database.The company needs to start the analysis once each day at a specific time.Which solution will meet these requirements MOST cost-effectively?",
      "choices": {
        "A": "Configure an S3 event notification to invoke the Lambda function when a branch office uploads a sales report.",
        "B": "Create an AWS Step Functions state machine that invokes the Lambda function once each day at the predefined time.",
        "C": "Configure the Lambda function to run continuously and to begin analysis only at the predefined time each day.",
        "D": "Create an Amazon EventBridge scheduled rule that invokes the Lambda function once each day at the predefined time."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136959-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 3:40 a.m.",
      "textHash": "0e26551e42d6bac2",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "On veut lancer l‚Äôanalyse une seule fois par jour √† une heure fixe, pas √† chaque fichier d√©pos√©.\nAmazon S3 est un stockage de fichiers ; une ‚Äúnotification S3‚Äù d√©clenche une action √† chaque upload.\nDonc l‚Äôoption A lancerait la Lambda plusieurs fois (une fois par agence), ce qui co√ªte plus cher et complique l‚Äôanalyse ‚Äúen une seule passe‚Äù.\nAWS Lambda ex√©cute du code √† la demande ; on peut la d√©clencher par un √©v√©nement ou un planning.\nAmazon EventBridge peut cr√©er une r√®gle planifi√©e (comme un cron) qui appelle la Lambda exactement √† l‚Äôheure voulue, une fois par jour.\nC est mauvais car une Lambda ne doit pas tourner en continu (co√ªt et limites d‚Äôex√©cution).\nB (Step Functions) peut aussi √™tre planifi√©, mais ajoute un service d‚Äôorchestration inutile ici, donc plus cher/complexe.\nLa solution la plus simple et la plus √©conomique est donc une r√®gle planifi√©e EventBridge qui invoque la Lambda chaque jour.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un lyc√©e avec plusieurs classes. Chaque classe d√©pose son devoir dans une bo√Æte commune (la bo√Æte S3) tous les jours √† une heure fixe. Le prof (Lambda) corrige tout d‚Äôun coup et note les r√©sultats dans le carnet de notes (la base de donn√©es).**\n\nConcept : tu veux que le prof commence la correction √† une heure pr√©cise, pas √† chaque d√©p√¥t. Donc il te faut une ‚Äúsonnerie programm√©e‚Äù qui d√©clenche le prof une fois par jour.\nPourquoi D : EventBridge, c‚Äôest comme une alarme/sonnerie du lyc√©e r√©gl√©e √† 18h : elle appelle le prof (Lambda) exactement une fois par jour, au bon moment, et √ßa ne co√ªte presque rien quand √ßa ne sonne pas.\nPourquoi pas A : √ßa d√©clenche √† chaque devoir d√©pos√©, donc plusieurs corrections inutiles.\nPourquoi pas B : Step Functions, c‚Äôest comme un planning compliqu√© pour plusieurs √©tapes ; ici c‚Äôest juste ‚Äúsonner puis corriger‚Äù, donc trop lourd.\nPourquoi pas C : faire tourner le prof en continu, c‚Äôest le payer √† attendre toute la journ√©e, donc cher.\n=> D est le plus simple et le moins co√ªteux.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:334:a7253522a6f2229f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 334,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company's application has an AWS Lambda function that processes messages from IoT devices. The company wants to monitor the Lambda function to ensure that the Lambda function is meeting its required service level agreement (SLA).A developer must implement a solution to determine the application's throughput in near real time. The throughput must be based on the number of messages that the Lambda function receives and processes in a given time period. The Lambda function performs initialization and post-processing steps that must not factor into the throughput measurement.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Use the Lambda function's ConcurrentExecutions metric in Amazon CloudWatch to measure the throughput.",
        "B": "Modify the application to log the calculated throughput to Amazon CloudWatch Logs. Use Amazon EventBridge to invoke a separate Lambda function to process the logs on a schedule.",
        "C": "Modify the application to publish custom Amazon CloudWatch metrics when the Lambda function receives and processes each message. Use the metrics to calculate the throughput.",
        "D": "Use the Lambda function's Invocations metric and Duration metric to calculate the throughput in Amazon CloudWatch."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136640-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 19, 2024, 3:05 p.m.",
      "textHash": "a7253522a6f2229f",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "On veut mesurer le d√©bit (throughput) : combien de messages la fonction AWS Lambda traite par p√©riode (ex. par minute), presque en temps r√©el.\nLes m√©triques natives CloudWatch de Lambda (Invocations, Duration, ConcurrentExecutions) ne savent pas distinguer ¬´ traitement du message ¬ª vs √©tapes d‚Äôinitialisation et de post-traitement : elles comptent tout.\nLa solution est donc d‚Äôajouter une mesure au bon endroit dans le code, exactement quand un message est re√ßu et quand il est r√©ellement trait√©.\nAvec des m√©triques personnalis√©es Amazon CloudWatch, la Lambda peut envoyer un compteur (ex. +1) pour chaque message trait√©, ou deux compteurs (re√ßu/trait√©) si besoin.\nCloudWatch peut ensuite agr√©ger ces compteurs par minute (Sum) pour obtenir le d√©bit en quasi temps r√©el.\nC‚Äôest simple, fiable, et √ßa respecte l‚Äôexigence ¬´ ne pas inclure l‚Äôinitialisation/post-traitement ¬ª, car vous choisissez pr√©cis√©ment o√π √©mettre la m√©trique.\nA est faux : la concurrence mesure le nombre d‚Äôex√©cutions en parall√®le, pas le nombre de messages trait√©s.\nB est inutilement complexe et moins ‚Äúnear real time‚Äù (analyse de logs planifi√©e).\nD est faux : Invocations et Duration incluent aussi les parties non d√©sir√©es et ne donnent pas un d√©bit exact bas√© sur les messages.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : des √©l√®ves (messages IoT) arrivent avec un ticket, et la cantini√®re (Lambda) doit servir chaque √©l√®ve. On veut mesurer le ‚Äúd√©bit‚Äù : combien d‚Äô√©l√®ves sont servis par minute, sans compter le temps o√π la cantini√®re ouvre la cuisine ou nettoie apr√®s.**\n\nConcept : le d√©bit = nombre de messages trait√©s pendant une p√©riode (ex: par minute). Comme √† la cantine, on compte les √©l√®ves servis, pas le temps d‚Äôouverture/fermeture.\nPourquoi C : on ajoute un ‚Äúcompteur officiel‚Äù qui s‚Äôincr√©mente √† chaque ticket re√ßu et √† chaque ticket servi (m√©triques personnalis√©es CloudWatch). Ensuite, on calcule facilement ‚Äútickets servis par minute‚Äù presque en temps r√©el.\nPourquoi pas A : compter combien de cantini√®res travaillent en m√™me temps (concurrence) ne dit pas combien d‚Äô√©l√®ves ont √©t√© servis.\nPourquoi pas B : relire les cahiers de brouillon (logs) puis faire un calcul plus tard, c‚Äôest plus lent et moins direct.\nPourquoi pas D : compter les passages (invocations) et la dur√©e m√©lange aussi l‚Äôouverture/fermeture, donc √ßa fausse le d√©bit.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:333:901136781b90707d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 333,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an event-driven application by using AWS Lambda and Amazon EventBridge. The Lambda function needs to push events to an EventBridge event bus. The developer uses an SDK to run the PutEvents EventBridge action and specifies no credentials in the code. After deploying the Lambda function, the developer notices that the function is failing and there are AccessDeniedException errors in the logs.How should the developer resolve this issue?",
      "choices": {
        "A": "Configure a VPC peering connection between the Lambda function and EventBridge.",
        "B": "Modify their AWS credentials to include permissions for the PutEvents EventBridge action.",
        "C": "Modify the Lambda function execution role to include permissions for the PutEvents EventBridge action.",
        "D": "Add a resource-based policy to the Lambda function to include permissions for the PutEvents EventBridge action."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136958-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 3:34 a.m.",
      "textHash": "901136781b90707d",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Dans AWS, une fonction Lambda s‚Äôex√©cute avec une ¬´ identit√© ¬ª appel√©e r√¥le d‚Äôex√©cution (IAM role). Ce r√¥le contient des permissions (ce que la fonction a le droit de faire).\nIci, le code appelle l‚ÄôAPI EventBridge PutEvents pour envoyer des √©v√©nements vers un bus EventBridge. Comme le code ne fournit pas d‚Äôidentifiants, le SDK utilise automatiquement les permissions du r√¥le d‚Äôex√©cution de la Lambda.\nL‚Äôerreur AccessDeniedException signifie : ¬´ cette identit√© n‚Äôa pas le droit d‚Äôappeler PutEvents ¬ª.\nLa bonne solution est donc d‚Äôajouter la permission events:PutEvents (et √©ventuellement la ressource du bus) dans la policy attach√©e au r√¥le d‚Äôex√©cution de la Lambda.\nLe VPC peering (A) concerne le r√©seau, pas les autorisations.\nChanger des identifiants dans le code (B) n‚Äôest pas recommand√© et ne s‚Äôapplique pas ici car Lambda doit utiliser son r√¥le.\nUne policy bas√©e sur la ressource de la Lambda (D) sert √† autoriser d‚Äôautres services √† invoquer la Lambda, pas √† autoriser la Lambda √† appeler EventBridge.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:70:3dd0331fae6fca50",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 70,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn engineer created an A/B test of a new feature on an Amazon CloudWatch Evidently project. The engineer configured two variations of the feature (Variation A and Variation B) for the test. The engineer wants to work exclusively with Variation A. The engineer needs to make updates so that Variation A is the only variation that appears when the engineer hits the application's endpoint.Which solution will meet this requirement?",
      "choices": {
        "A": "Add an override to the feature. Set the identifier of the override to the engineer's user ID. Set the variation to Variation A.",
        "B": "Add an override to the feature. Set the identifier of the override to Variation A. Set the variation to 100%.",
        "C": "Add an experiment to the project. Set the identifier of the experiment to Variation B. Set the variation to 0%.",
        "D": "Add an experiment to the project. Set the identifier of the experiment to the AWS account's account ISet the variation to Variation A."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106488-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 17, 2023, 4:14 p.m.",
      "textHash": "3dd0331fae6fca50",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "CloudWatch Evidently sert √† faire des tests A/B : selon un identifiant (souvent un utilisateur), l‚Äôapplication re√ßoit la Variation A ou B.\nIci, l‚Äôing√©nieur veut que, pour lui uniquement, l‚Äôendpoint renvoie toujours la Variation A, m√™me si le test continue pour les autres.\nLa bonne m√©thode est d‚Äôutiliser un ¬´ override ¬ª (for√ßage) sur la fonctionnalit√© : c‚Äôest une r√®gle qui dit ‚Äúpour tel identifiant, renvoyer telle variation‚Äù.\nEn mettant comme identifiant l‚ÄôID utilisateur de l‚Äôing√©nieur et comme variation ‚ÄúVariation A‚Äù, on garantit qu‚Äôil verra toujours A.\nLes options qui parlent de mettre l‚Äôidentifiant √† ‚ÄúVariation A‚Äù ou de mettre un pourcentage ne correspondent pas : l‚Äôidentifiant doit repr√©senter une personne/entit√©, pas une variation.\nCr√©er/modifier un ‚Äúexperiment‚Äù ne force pas un utilisateur pr√©cis ; un experiment r√©partit du trafic entre variations.\nDonc la solution A est la seule qui assure que l‚Äôing√©nieur travaille exclusivement avec Variation A.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:69:f7d10acb511c1b26",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 69,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is configuring an application's deployment environment in AWS CodePipeline. The application code is stored in a GitHub repository. The developer wants to ensure that the repository package's unit tests run in the new deployment environment. The developer has already set the pipeline's source provider to GitHub and has specified the repository and branch to use in the deployment.Which combination of steps should the developer take next to meet these requirements with the LEAST overhead? (Choose two.)",
      "choices": {
        "A": "Create an AWS CodeCommit project. Add the repository package's build and test commands to the project's buildspec.",
        "B": "Create an AWS CodeBuild project. Add the repository package's build and test commands to the project's buildspec.",
        "C": "Create an AWS CodeDeploy project. Add the repository package's build and test commands to the project's buildspec.",
        "D": "Add an action to the source stage. Specify the newly created project as the action provider. Specify the build artifact as the action's input artifact.",
        "E": "Add a new stage to the pipeline after the source stage. Add an action to the new stage. Specify the newly created project as the action provider. Specify the source artifact as the action's input artifact."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107440-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 25, 2023, 2:49 p.m.",
      "textHash": "f7d10acb511c1b26",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Pour ex√©cuter des tests unitaires automatiquement dans un pipeline, il faut une √©tape de ‚Äúbuild/test‚Äù. Dans AWS, le service fait pour compiler et lancer des commandes (build, tests, scripts) est AWS CodeBuild.\nCodePipeline orchestre les √©tapes (source, build, d√©ploiement), mais ne lance pas lui-m√™me les tests : il appelle un service comme CodeBuild.\nDonc, vous cr√©ez un projet CodeBuild et vous mettez les commandes de build et de tests dans un fichier buildspec.yml (ex: installer d√©pendances, lancer npm test / mvn test).\nEnsuite, dans CodePipeline, vous ajoutez une nouvelle √©tape (stage) juste apr√®s la source GitHub, avec une action ‚ÄúBuild‚Äù qui utilise ce projet CodeBuild.\nL‚Äôentr√©e de cette action doit √™tre l‚Äôartefact source (le code r√©cup√©r√© depuis GitHub), pour que CodeBuild teste exactement ce qui va √™tre d√©ploy√©.\nLes autres options ne conviennent pas : CodeCommit est un d√©p√¥t Git (pas pour ex√©cuter des tests), CodeDeploy sert √† d√©ployer (pas √† builder/tester), et ajouter une action dans la source stage n‚Äôest pas la bonne structure.\nAinsi, la solution avec le moins d‚Äôoverhead est : cr√©er CodeBuild + l‚Äôajouter comme stage apr√®s la source.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un devoir de groupe rendu sur GitHub comme un dossier d√©pos√© au secr√©tariat du lyc√©e. Avant de l‚Äôafficher au tableau (d√©ployer), tu veux qu‚Äôun surveillant le relise et fasse un mini-contr√¥le (tests) dans la salle o√π il sera pr√©sent√©.**\n\nConcept : CodePipeline, c‚Äôest la cha√Æne ‚Äúje r√©cup√®re le dossier ‚Üí je v√©rifie ‚Üí je mets en place‚Äù. GitHub est juste l‚Äôendroit o√π le dossier est stock√©.\nPour faire les tests automatiquement, il faut une ‚Äúsalle de contr√¥le‚Äù qui sait ex√©cuter des commandes (compiler + lancer les tests) : c‚Äôest AWS CodeBuild.\nDonc B : tu cr√©es un projet CodeBuild et tu √©cris dans le buildspec la liste des consignes, comme ‚Äú1) construire le projet 2) lancer les tests‚Äù.\nEnsuite, pour que √ßa se fasse au bon moment, tu ajoutes une √©tape apr√®s la source (apr√®s GitHub) qui envoie le dossier r√©cup√©r√© √† CodeBuild.\nPourquoi pas A : CodeCommit, c‚Äôest un autre endroit pour stocker le code, pas une salle de tests.\nPourquoi pas C : CodeDeploy sert √† installer l‚Äôappli, pas √† faire tourner les tests.\nAvec CodeBuild + une √©tape apr√®s la source, tu as le minimum d‚Äôeffort et les tests tournent dans l‚Äôenvironnement de d√©ploiement.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:68:94ff19bbb3aa4fd3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 68,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building a serverless application that uses AWS Lambda functions. The company needs to create a set of test events to test Lambda functions in a development environment. The test events will be created once and then will be used by all the developers in an IAM developer group. The test events must be editable by any of the IAM users in the IAM developer group.Which solution will meet these requirements?",
      "choices": {
        "A": "Create and store the test events in Amazon S3 as JSON objects. Allow S3 bucket access to all IAM users.",
        "B": "Create the test events. Configure the event sharing settings to make the test events shareable.",
        "C": "Create and store the test events in Amazon DynamoDB. Allow access to DynamoDB by using IAM roles.",
        "D": "Create the test events. Configure the event sharing settings to make the test events private."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/106484-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 17, 2023, 3:33 p.m.",
      "textHash": "94ff19bbb3aa4fd3",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Pour tester une fonction AWS Lambda, on utilise des ¬´ test events ¬ª : ce sont des exemples d‚Äôentr√©es (souvent du JSON) que Lambda re√ßoit.\nIci, l‚Äôobjectif est que ces √©v√©nements soient cr√©√©s une seule fois, puis r√©utilis√©s par tous les d√©veloppeurs d‚Äôun m√™me groupe IAM, et que chacun puisse les modifier.\nDans la console Lambda, il existe un r√©glage de partage des √©v√©nements de test (¬´ event sharing settings ¬ª) qui permet de rendre ces √©v√©nements partageables au niveau du compte.\nEn les rendant partageables, tous les utilisateurs IAM autoris√©s (ex. le groupe d√©veloppeurs) peuvent voir et √©diter les m√™mes √©v√©nements, sans cr√©er un stockage s√©par√©.\nStocker dans S3 ou DynamoDB (A ou C) pourrait marcher techniquement, mais ce n‚Äôest pas la m√©thode pr√©vue pour les test events Lambda et cela ajoute de la gestion (permissions, lecture/√©criture, format, outil).\nMettre les √©v√©nements en priv√© (D) contredit le besoin de partage et d‚Äô√©dition par tout le groupe.\nDonc la bonne solution est de cr√©er les √©v√©nements dans Lambda et activer le partage via les param√®tres de partage.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un devoir d‚Äôentra√Ænement en classe : le prof pr√©pare des ‚Äúsujets‚Äù une fois, et toute la classe doit pouvoir les utiliser et les modifier pour s‚Äôentra√Æner.**\n\nIci, les ‚Äúsujets‚Äù, ce sont les test events (des exemples d‚Äôentr√©es) pour tester une fonction Lambda (un petit robot qui ex√©cute une t√¢che quand on lui envoie un message).\nLe besoin : cr√©er ces sujets une seule fois, puis les partager √† tout le groupe de d√©veloppeurs, et chacun doit pouvoir les modifier.\nLa bonne solution, c‚Äôest B : activer le partage des test events.\nAvec l‚Äôanalogie : c‚Äôest comme mettre les sujets dans un dossier de classe partag√©, o√π tout le monde peut ouvrir et corriger.\nA (S3) et C (DynamoDB) reviennent √† mettre les sujets dans un entrep√¥t ou un tableau de stockage : √ßa marche, mais c‚Äôest plus lourd et pas fait sp√©cialement pour ‚Äúpartager des sujets de test‚Äù simplement.\nD (priv√©) serait comme garder les sujets dans ton sac : personne d‚Äôautre ne peut les modifier.\nDonc B r√©pond exactement : partag√© + modifiable par le groupe.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:66:59a679c77393a645",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 66,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a critical application on AWS. The application exposes an HTTP API by using Amazon API Gateway. The API is integrated with an AWS Lambda function. The application stores data in an Amazon RDS for MySQL DB instance with 2 virtual CPUs (vCPUs) and 64 GB of RAM.Customers have reported that some of the API calls return HTTP 500 Internal Server Error responses. Amazon CloudWatch Logs shows errors for ‚Äútoo many connections.‚Äù The errors occur during peak usage times that are unpredictable.The company needs to make the application resilient. The database cannot be down outside of scheduled maintenance hours.Which solution will meet these requirements?",
      "choices": {
        "A": "Decrease the number of vCPUs for the DB instance. Increase the max_connections setting.",
        "B": "Use Amazon RDS Proxy to create a proxy that connects to the DB instance. Update the Lambda function to connect to the proxy.",
        "C": "Add a CloudWatch alarm that changes the DB instance class when the number of connections increases to more than 1,000.",
        "D": "Add an Amazon EventBridge rule that increases the max_connections setting of the DB instance when CPU utilization is above 75%."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107437-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 25, 2023, 2:35 p.m.",
      "textHash": "59a679c77393a645",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Le message ‚Äútoo many connections‚Äù vient de MySQL : trop de connexions simultan√©es arrivent sur la base, donc certaines requ√™tes √©chouent et l‚ÄôAPI renvoie HTTP 500.\nAvec API Gateway + Lambda, le trafic peut monter d‚Äôun coup. Lambda peut lancer beaucoup d‚Äôex√©cutions en parall√®le, et chaque ex√©cution peut ouvrir sa propre connexion √† la base.\nUne instance RDS (base de donn√©es g√©r√©e) a une limite de connexions ; augmenter max_connections ou changer la taille ne r√®gle pas bien les pics impr√©visibles et peut impacter la stabilit√©.\nAmazon RDS Proxy est un service qui se place entre Lambda et RDS et ‚Äúpool‚Äù (r√©utilise) les connexions : au lieu d‚Äôouvrir une nouvelle connexion √† chaque appel, Lambda se connecte au proxy.\nLe proxy garde un nombre contr√¥l√© de connexions vers MySQL et absorbe les pics, ce qui √©vite les erreurs ‚Äútoo many connections‚Äù.\nCela am√©liore la r√©silience sans red√©marrer la base ni faire des changements de capacit√© en urgence (ce qui pourrait causer une indisponibilit√© hors maintenance).\nDonc la bonne solution est de cr√©er un RDS Proxy et de faire pointer Lambda vers ce proxy (B).",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : la caisse (la base de donn√©es) ne peut servir qu‚Äôun certain nombre d‚Äô√©l√®ves en m√™me temps. Aux heures de pointe impr√©visibles, trop d‚Äô√©l√®ves arrivent d‚Äôun coup et la caisse bloque.**\n\nConcept : l‚ÄôAPI + Lambda, c‚Äôest comme des surveillants qui envoient des √©l√®ves √† la caisse. Si chaque √©l√®ve ouvre sa propre ‚Äúcommande‚Äù √† la caisse, on d√©passe vite la limite de places et √ßa fait ‚Äútrop de connexions‚Äù (erreur 500).\nPourquoi B : RDS Proxy, c‚Äôest comme mettre un guichet/agent devant la caisse. Les √©l√®ves parlent au guichet, et le guichet r√©utilise un petit nombre de commandes d√©j√† ouvertes vers la caisse.\nR√©sultat : moins de connexions ouvertes en m√™me temps, donc moins de blocages pendant les pics.\nEt la caisse (la base) reste stable : pas besoin de la red√©marrer ou de la changer en urgence, donc pas de panne hors maintenance.\nA ne r√©sout pas vraiment le rush, C et D changent des r√©glages ‚Äúen plein service‚Äù et peuvent perturber la caisse au mauvais moment.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:65:8f077c748b5bfe54",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 65,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a new application on AWS. The application uses an AWS Lambda function that retrieves information from an Amazon DynamoDB table. The developer hard coded the DynamoDB table name into the Lambda function code. The table name might change over time. The developer does not want to modify the Lambda code if the table name changes.Which solution will meet these requirements MOST efficiently?",
      "choices": {
        "A": "Create a Lambda environment variable to store the table name. Use the standard method for the programming language to retrieve the variable.",
        "B": "Store the table name in a file. Store the file in the /tmp folder. Use the SDK for the programming language to retrieve the table name.",
        "C": "Create a file to store the table name. Zip the file and upload the file to the Lambda layer. Use the SDK for the programming language to retrieve the table name.",
        "D": "Create a global variable that is outside the handler in the Lambda function to store the table name."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103686-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2023, 6:36 p.m.",
      "textHash": "8f077c748b5bfe54",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:62bbbc7a",
      "frExplanation": "Le probl√®me vient du ¬´ hard coding ¬ª : le nom de la table DynamoDB est √©crit en dur dans le code Lambda, donc si le nom change il faut red√©ployer le code.\nAWS Lambda est un service qui ex√©cute du code sans g√©rer de serveur. DynamoDB est une base de donn√©es NoSQL o√π les donn√©es sont dans des tables.\nLa solution la plus simple est d‚Äôutiliser une variable d‚Äôenvironnement Lambda : un param√®tre de configuration attach√© √† la fonction, modifiable sans changer le code.\nVotre code lit cette variable (avec la m√©thode standard du langage, ex: process.env / os.environ) et utilise ce nom pour appeler DynamoDB.\nAinsi, si la table change, vous mettez √† jour la variable d‚Äôenvironnement dans la console/CLI/IaC et vous n‚Äôavez pas besoin de modifier le code.\nLes options avec fichiers (/tmp ou layer) sont plus complexes, fragiles et inutiles pour une simple valeur de configuration.\nUne variable globale dans le code reste du hard coding : il faut toujours modifier et red√©ployer le code si le nom change.\nDonc la r√©ponse A est la plus efficace et la plus adapt√©e √† une configuration qui peut √©voluer.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu dois aller chercher un livre pr√©cis √† la biblioth√®que du lyc√©e, mais le nom de la salle change parfois (Salle A devient Salle B).**\n\nConcept : le code de Lambda, c‚Äôest toi qui fais la course; DynamoDB, c‚Äôest la biblioth√®que; le ‚Äúnom de table‚Äù, c‚Äôest le nom de la salle.\nSi tu √©cris le nom de la salle directement sur ta feuille (dans le code), d√®s que la salle change tu dois r√©√©crire la feuille.\nSolution A : tu mets le nom de la salle sur un post-it coll√© sur ton badge (variable d‚Äôenvironnement). Tu peux changer le post-it sans r√©√©crire ta feuille.\nDonc tu changes juste un r√©glage, pas le code : rapide et efficace.\nB : /tmp, c‚Äôest comme poser un papier sur une table qui peut √™tre nettoy√©e √† tout moment : pas fiable.\nC : une ‚Äúlayer‚Äù, c‚Äôest comme mettre l‚Äôinfo dans un sac scell√© : pour changer, il faut refaire et redistribuer le sac.\nD : une variable globale, c‚Äôest encore √©crit dans ta feuille : si √ßa change, tu dois modifier le code.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:64:fe2f862eb14652e8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 64,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is migrating some features from a legacy monolithic application to use AWS Lambda functions instead. The application currently stores data in an Amazon Aurora DB cluster that runs in private subnets in a VPC. The AWS account has one VPC deployed. The Lambda functions and the DB cluster are deployed in the same AWS Region in the same AWS account.The developer needs to ensure that the Lambda functions can securely access the DB cluster without crossing the public internet.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure the DB cluster's public access setting to Yes.",
        "B": "Configure an Amazon RDS database proxy for he Lambda functions.",
        "C": "Configure a NAT gateway and a security group for the Lambda functions.",
        "D": "Configure the VPC, subnets, and a security group for the Lambda functions."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103687-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2023, 6:38 p.m.",
      "textHash": "fe2f862eb14652e8",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:a39ed713",
      "frExplanation": "Objectif : que des fonctions AWS Lambda (code ex√©cut√© √† la demande) acc√®dent √† une base Amazon Aurora (base relationnelle g√©r√©e) situ√©e dans des sous-r√©seaux priv√©s, sans passer par Internet.\nPour rester ‚Äúpriv√©‚Äù, Lambda doit √™tre rattach√©e au m√™me VPC que la base : on configure la Lambda avec des subnets du VPC (souvent priv√©s) et un security group.\nUn VPC est un r√©seau isol√© dans AWS ; les subnets priv√©s n‚Äôont pas de route directe vers Internet.\nLes security groups sont des pare-feu : on autorise uniquement le trafic n√©cessaire (ex : port 3306/5432) depuis le security group de Lambda vers celui d‚ÄôAurora.\nAinsi, la connexion se fait via des interfaces r√©seau dans le VPC, donc sans IP publique et sans traverser l‚ÄôInternet public.\nA est faux : rendre la base ‚Äúpublicly accessible‚Äù expose une IP publique et augmente le risque.\nC est hors sujet : un NAT Gateway sert surtout √† permettre une sortie vers Internet depuis un subnet priv√©, pas √† acc√©der √† une base priv√©e.\nB (RDS Proxy) peut aider au pooling/gestion des connexions, mais ne remplace pas la n√©cessit√© de mettre Lambda dans le VPC pour acc√©der √† une base priv√©e.\nDonc la bonne solution est de configurer VPC + subnets + security group pour les fonctions Lambda (D).",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:61:f9383d659d84254c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 61,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building a web application on AWS. When a customer sends a request, the application will generate reports and then make the reports available to the customer within one hour. Reports should be accessible to the customer for 8 hours. Some reports are larger than 1 MB. Each report is unique to the customer. The application should delete all reports that are older than 2 days.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Generate the reports and then store the reports as Amazon DynamoDB items that have a specified TTL. Generate a URL that retrieves the reports from DynamoDB. Provide the URL to customers through the web application.",
        "B": "Generate the reports and then store the reports in an Amazon S3 bucket that uses server-side encryption. Attach the reports to an Amazon Simple Notification Service (Amazon SNS) message. Subscribe the customer to email notifications from Amazon SNS.",
        "C": "Generate the reports and then store the reports in an Amazon S3 bucket that uses server-side encryption. Generate a presigned URL that contains an expiration date Provide the URL to customers through the web application. Add S3 Lifecycle configuration rules to the S3 bucket to delete old reports.",
        "D": "Generate the reports and then store the reports in an Amazon RDS database with a date stamp. Generate an URL that retrieves the reports from the RDS database. Provide the URL to customers through the web application. Schedule an hourly AWS Lambda function to delete database records that have expired date stamps."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103904-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 25, 2023, 11:52 p.m.",
      "textHash": "f9383d659d84254c",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "On veut stocker des fichiers (rapports), parfois > 1 Mo, accessibles via un lien pendant 8 heures, puis supprim√©s apr√®s 2 jours, avec peu d‚Äôadministration.\nAmazon S3 est un service de stockage d‚Äôobjets (fichiers) tr√®s simple et scalable, id√©al pour des rapports uniques par client.\nUn ¬´ presigned URL ¬ª S3 est un lien temporaire qui donne acc√®s √† un objet priv√© sans rendre le bucket public : on fixe une expiration (ici 8 heures).\nPour supprimer automatiquement les rapports anciens, les r√®gles ¬´ S3 Lifecycle ¬ª effacent les objets apr√®s un d√©lai (ici 2 jours) sans code ni t√¢ches planifi√©es.\nA est mauvais car DynamoDB est une base NoSQL, pas faite pour servir de gros fichiers (limite de taille d‚Äôitem) et ce n‚Äôest pas pratique pour des rapports > 1 Mo.\nB est mauvais car SNS/email n‚Äôest pas adapt√© pour envoyer de gros fichiers en pi√®ce jointe et complique la distribution.\nD est mauvais car RDS est une base relationnelle : stocker des fichiers dedans + Lambda de nettoyage = plus d‚Äôop√©rations et de maintenance.\nDonc C r√©pond √† l‚Äôacc√®s temporaire (8h) + stockage de fichiers + suppression automatique (2 jours) avec le minimum d‚Äôoverhead.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le CDI du lyc√©e : tu imprimes un devoir (le rapport), tu le ranges dans un casier ferm√© √† cl√©, et tu donnes √† l‚Äô√©l√®ve un ticket d‚Äôacc√®s qui marche seulement quelques heures.**\n\nConcept : on veut stocker des fichiers, donner un acc√®s temporaire, puis tout nettoyer automatiquement.\nAvec l‚Äôanalogie : Amazon S3 = le casier √† fichiers du CDI (fait pour stocker des gros documents). Le chiffrement = le cadenas.\nLe ‚Äúpresigned URL‚Äù = le ticket/QR code qui ouvre le casier sans donner la cl√© du CDI, et il expire (ici apr√®s 8 heures).\nDonc chaque client re√ßoit son ticket unique, et peut r√©cup√©rer son rapport quand il veut pendant 8h.\nLes r√®gles ‚ÄúLifecycle‚Äù = le concierge du CDI qui jette automatiquement les vieux papiers (ici tout ce qui a plus de 2 jours).\nPourquoi pas A : DynamoDB, c‚Äôest plut√¥t un cahier de notes (petites infos), pas id√©al pour des gros fichiers > 1 MB.\nPourquoi pas B : envoyer le rapport par message/email, c‚Äôest lourd et pas pratique pour de gros fichiers et l‚Äôacc√®s 8h.\nPourquoi pas D : une base RDS, c‚Äôest comme un tableau Excel g√©ant : possible, mais il faut un robot qui nettoie, donc plus de boulot.\nDonc C fait tout simple : casier + ticket temporaire + m√©nage automatique, avec le moins d‚Äôentretien.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:58:2448c39b7f99ccb4",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 58,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is testing a new file storage application that uses an Amazon CloudFront distribution to serve content from an Amazon S3 bucket. The distribution accesses the S3 bucket by using an origin access identity (OAI). The S3 bucket's permissions explicitly deny access to all other users.The application prompts users to authenticate on a login page and then uses signed cookies to allow users to access their personal storage directories. The developer has configured the distribution to use its default cache behavior with restricted viewer access and has set the origin to point to the S3 bucket. However, when the developer tries to navigate to the login page, the developer receives a 403 Forbidden error.The developer needs to implement a solution to allow unauthenticated access to the login page. The solution also must keep all private content secure.Which solution will meet these requirements?",
      "choices": {
        "A": "Add a second cache behavior to the distribution with the same origin as the default cache behavior. Set the path pattern for the second cache behavior to the path of the login page, and make viewer access unrestricted. Keep the default cache behavior's settings unchanged.",
        "B": "Add a second cache behavior to the distribution with the same origin as the default cache behavior. Set the path pattern for the second cache behavior to *, and make viewer access restricted. Change the default cache behavior's path pattern to the path of the login page, and make viewer access unrestricted.",
        "C": "Add a second origin as a failover origin to the default cache behavior. Point the failover origin to the S3 bucket. Set the path pattern for the primary origin to *, and make viewer access restricted. Set the path pattern for the failover origin to the path of the login page, and make viewer access unrestricted.",
        "D": "Add a bucket policy to the S3 bucket to allow read access. Set the resource on the policy to the Amazon Resource Name (ARN) of the login page object in the S3 bucket. Add a CloudFront function to the default cache behavior to redirect unauthorized requests to the login page's S3 URL."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/104014-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 27, 2023, 1:24 a.m.",
      "textHash": "2448c39b7f99ccb4",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "CloudFront est un CDN qui sert des fichiers depuis une origine (ici un bucket S3). Avec une OAI, seul CloudFront a le droit de lire dans S3, et le bucket refuse tout le reste.\nLe mode ¬´ restricted viewer access ¬ª signifie que CloudFront exige une preuve (signed cookies/URL) avant de laisser voir le contenu.\nIci, le comportement par d√©faut est restreint, donc m√™me la page de login est consid√©r√©e comme ‚Äúpriv√©e‚Äù et CloudFront renvoie 403 avant que l‚Äôutilisateur puisse s‚Äôauthentifier.\nLa bonne approche est de cr√©er une exception uniquement pour la page de login : un deuxi√®me cache behavior avec un path pattern qui cible l‚ÄôURL du login.\nPour ce behavior, on met l‚Äôacc√®s viewer en ‚Äúunrestricted‚Äù (pas besoin de cookies sign√©s) afin que tout le monde puisse charger la page de connexion.\nOn laisse le behavior par d√©faut inchang√© et restreint pour tout le reste, ce qui garde les r√©pertoires personnels prot√©g√©s.\nAinsi, S3 reste priv√© (toujours accessible uniquement via l‚ÄôOAI), et seule la page de login est publiquement accessible via CloudFront.\nLes autres options soit rendent trop de contenu public, soit utilisent des m√©canismes inadapt√©s (failover, redirection vers l‚ÄôURL S3 qui contourne la protection).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un lyc√©e avec une biblioth√®que. La porte principale (CloudFront) distribue les livres. La r√©serve derri√®re (S3) est ferm√©e √† cl√© et seul le biblioth√©caire (OAI) peut y entrer. Pour emprunter, tu dois d‚Äôabord passer par l‚Äôaccueil (page login) pour montrer ta carte.**\n\nConcept : CloudFront peut avoir des ‚Äúr√®gles de passage‚Äù diff√©rentes selon le chemin demand√© (cache behaviors). Comme des couloirs : certains sont libres, d‚Äôautres n√©cessitent un badge.\nIci, la r√®gle par d√©faut dit : ‚Äúbadge obligatoire‚Äù (restricted viewer access). Donc m√™me la page login est bloqu√©e ‚Üí 403.\nSolution A : on ajoute une 2e r√®gle qui vise uniquement le chemin de la page login (ex: /login.html) et on la met en acc√®s libre (unrestricted).\nTout le reste garde la r√®gle par d√©faut : badge obligatoire + cookies sign√©s pour acc√©der √† ton dossier priv√©.\nR√©sultat : tout le monde peut voir l‚Äôaccueil pour se connecter, mais personne ne peut lire les fichiers priv√©s sans autorisation.\nLes autres choix soit rendent trop de choses publiques, soit m√©langent les r√®gles et cassent la s√©curit√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:56:a4f77b68f6510820",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 56,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is using an AWS Lambda function to process records from an Amazon Kinesis data stream. The company recently observed slow processing of the records. A developer notices that the iterator age metric for the function is increasing and that the Lambda run duration is constantly above normal.Which actions should the developer take to increase the processing speed? (Choose two.)",
      "choices": {
        "A": "Increase the number of shards of the Kinesis data stream.",
        "B": "Decrease the timeout of the Lambda function.",
        "C": "Increase the memory that is allocated to the Lambda function.",
        "D": "Decrease the number of shards of the Kinesis data stream.",
        "E": "Increase the timeout of the Lambda function."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103932-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 26, 2023, 7:03 a.m.",
      "textHash": "a4f77b68f6510820",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:8607e7ab",
      "frExplanation": "Amazon Kinesis est un flux de donn√©es d√©coup√© en ¬´ shards ¬ª (comme des voies sur une autoroute). Plus il y a de shards, plus on peut lire et traiter en parall√®le.\nAWS Lambda ex√©cute votre code automatiquement. Avec Kinesis, Lambda lit les enregistrements shard par shard : si un shard re√ßoit beaucoup de donn√©es, il devient un goulot d‚Äô√©tranglement.\nLa m√©trique ¬´ iterator age ¬ª qui augmente signifie que Lambda prend du retard : les donn√©es attendent trop longtemps avant d‚Äô√™tre trait√©es.\nLa dur√©e d‚Äôex√©cution Lambda au-dessus de la normale indique que chaque traitement prend plus de temps, donc le retard s‚Äôaccumule.\nAction la plus directe pour augmenter le d√©bit global : augmenter le nombre de shards (A) afin d‚Äôaugmenter le parall√©lisme de lecture/traitement.\nDiminuer le timeout (B) ferait √©chouer plus vite les ex√©cutions, sans acc√©l√©rer le traitement.\nDiminuer le nombre de shards (D) r√©duirait le parall√©lisme et empirerait le retard.\nAugmenter le timeout (E) √©vite des erreurs de timeout mais ne rend pas le traitement plus rapide.\nAugmenter la m√©moire (C) peut parfois acc√©l√©rer (plus de CPU), mais ici la r√©ponse attendue donn√©e est A.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : des √©l√®ves arrivent en file pour √™tre servis, et des surveillants (Lambda) prennent les plateaux. Les ‚Äúshards‚Äù de Kinesis, c‚Äôest le nombre de files ouvertes.**\n\nConcept : Kinesis = des files d‚Äôattente qui re√ßoivent des messages. Lambda = des serveurs qui prennent les messages et les traitent. ‚ÄúIterator age‚Äù qui monte = la file grandit, on prend du retard. ‚ÄúDur√©e Lambda‚Äù trop longue = chaque serveur met plus de temps par plateau. Pour aller plus vite, il faut plus de files ouvertes : augmenter le nombre de shards (A). Comme √ßa, les messages se r√©partissent sur plus de files, et Lambda peut traiter plus en parall√®le. B et E (changer le temps limite) ne rend pas le service plus rapide, √ßa coupe plus t√¥t ou attend plus longtemps. D (moins de files) empire l‚Äôembouteillage. C (plus de m√©moire) peut aider parfois, mais ici la r√©ponse attendue est surtout d‚Äôaugmenter les shards pour r√©duire le retard.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:55:f7cb4e70cf4cc46a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 55,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application uses Lambda functions to extract metadata from files uploaded to an S3 bucket; the metadata is stored in Amazon DynamoDB. The application starts behaving unexpectedly, and the developer wants to examine the logs of the Lambda function code for errors.Based on this system configuration, where would the developer find the logs?",
      "choices": {
        "A": "Amazon S3",
        "B": "AWS CloudTrail",
        "C": "Amazon CloudWatch",
        "D": "Amazon DynamoDB"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103931-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 26, 2023, 6:48 a.m.",
      "textHash": "f7cb4e70cf4cc46a",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Ici, le code qui s‚Äôex√©cute est une fonction AWS Lambda (un service qui lance du code sans g√©rer de serveur).\nQuand une fonction Lambda √©crit des messages (ex: erreurs, \"print\", logs), AWS envoie automatiquement ces journaux vers Amazon CloudWatch Logs.\nAmazon CloudWatch est le service de surveillance d‚ÄôAWS : il collecte m√©triques et logs pour diagnostiquer les probl√®mes.\nS3 sert √† stocker les fichiers upload√©s, pas les logs d‚Äôex√©cution du code (sauf si on configure un export volontaire).\nAWS CloudTrail enregistre surtout les actions API (qui a cr√©√©/modifi√© une ressource), pas les erreurs internes du code Lambda.\nDynamoDB stocke les donn√©es (ici les m√©tadonn√©es), pas les journaux.\nDonc, pour voir les erreurs et traces d‚Äôex√©cution de Lambda, il faut aller dans CloudWatch (Logs) : r√©ponse C.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : des √©l√®ves d√©posent leurs plateaux, un cuisinier (automatique) lit l‚Äô√©tiquette du plateau, puis note des infos dans un cahier.**\n\nIci, S3 = l‚Äôendroit o√π on d√©pose les fichiers (comme la table o√π tu poses ton plateau).\nLambda = le cuisinier automatique qui fait le travail quand un fichier arrive.\nDynamoDB = le cahier o√π on √©crit les infos trouv√©es (les m√©tadonn√©es).\nQuand √ßa bug, tu veux lire le ‚Äújournal de bord‚Äù du cuisinier : ce qu‚Äôil a fait, et les erreurs.\nCe journal n‚Äôest pas dans la table (S3) ni dans le cahier (DynamoDB).\nCloudTrail, c‚Äôest plut√¥t le registre ‚Äúqui a ouvert quelle porte et quand‚Äù (actions), pas les erreurs du cuisinier.\nLe journal de bord des Lambda est stock√© dans CloudWatch (le panneau de surveillance avec les logs).\nDonc la bonne r√©ponse est C : Amazon CloudWatch.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:54:bcad41ed78200514",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 54,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to migrate an online retail application to AWS to handle an anticipated increase in traffic. The application currently runs on two servers: one server for the web application and another server for the database. The web server renders webpages and manages session state in memory. The database server hosts a MySQL database that contains order details. When traffic to the application is heavy, the memory usage for the web server approaches 100% and the application slows down considerably.The developer has found that most of the memory increase and performance decrease is related to the load of managing additional user sessions. For the web server migration, the developer will use Amazon EC2 instances with an Auto Scaling group behind an Application Load Balancer.Which additional set of changes should the developer make to the application to improve the application's performance?",
      "choices": {
        "A": "Use an EC2 instance to host the MySQL database. Store the session data and the application data in the MySQL database.",
        "B": "Use Amazon ElastiCache for Memcached to store and manage the session data. Use an Amazon RDS for MySQL DB instance to store the application data.",
        "C": "Use Amazon ElastiCache for Memcached to store and manage the session data and the application data.",
        "D": "Use the EC2 instance store to manage the session data. Use an Amazon RDS for MySQL DB instance to store the application data."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103757-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 24, 2023, 9:35 a.m.",
      "textHash": "bcad41ed78200514",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Le probl√®me vient des sessions utilisateurs gard√©es en m√©moire sur le serveur web : quand il y a beaucoup de visiteurs, la RAM monte √† 100% et le site ralentit.\nAvec un Auto Scaling Group derri√®re un Load Balancer, il y aura plusieurs serveurs web. Si chaque serveur garde ses sessions en m√©moire locale, un utilisateur peut tomber sur un autre serveur et ‚Äúperdre‚Äù sa session, et la RAM de chaque serveur explose.\nLa bonne pratique est d‚Äôexternaliser l‚Äô√©tat (sessions) dans un service partag√© et rapide.\nAmazon ElastiCache (Memcached) est un cache en m√©moire g√©r√© : tr√®s rapide, id√©al pour stocker des donn√©es temporaires comme les sessions, et il soulage la RAM des instances EC2.\nLes donn√©es m√©tier (commandes, d√©tails) doivent rester dans une base durable : Amazon RDS for MySQL est une base MySQL g√©r√©e, fiable, avec sauvegardes et haute disponibilit√© possibles.\nDonc : sessions dans ElastiCache + donn√©es applicatives dans RDS (r√©ponse B).\nA et D gardent trop de d√©pendance √† une machine (EC2/instance store) et ne r√©solvent pas bien le partage des sessions.\nC met aussi les donn√©es applicatives dans un cache, ce qui n‚Äôest pas durable (risque de perte de donn√©es).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e. Il y a 2 postes: 1) le comptoir qui sert les √©l√®ves (le site web) et 2) le cahier des commandes (la base MySQL). Le comptoir garde dans sa t√™te qui a pris quoi (les ‚Äúsessions‚Äù). Quand trop d‚Äô√©l√®ves arrivent, le serveur du comptoir sature car il doit se souvenir de trop de monde.**\n\nConcept: pour aller plus vite, on s√©pare ‚Äúservir‚Äù et ‚Äúse souvenir‚Äù. Le comptoir doit juste servir, et la m√©moire des √©l√®ves doit √™tre mise dans un endroit d√©di√©.\nAvec plusieurs comptoirs (plusieurs EC2 + Auto Scaling), si chaque comptoir garde les infos en t√™te, √ßa bug: un √©l√®ve change de file et on ne sait plus ce qu‚Äôil avait.\nDonc on met les sessions dans un ‚Äúcasier m√©moire‚Äù partag√© et tr√®s rapide: ElastiCache (Memcached) = casiers pour se souvenir vite.\nEt on met les vraies commandes (donn√©es importantes) dans un cahier solide g√©r√© par l‚Äô√©cole: RDS MySQL = base de donn√©es fiable.\nPourquoi B: sessions dans ElastiCache (rapide, partag√©) + commandes dans RDS (fiable). R√©sultat: moins de m√©moire sur les comptoirs, moins de lenteur, et n‚Äôimporte quel comptoir peut servir n‚Äôimporte quel √©l√®ve.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:53:1c46325e276dde86",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 53,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is using an AWS Lambda function to generate avatars for profile pictures that are uploaded to an Amazon S3 bucket. The Lambda function is automatically invoked for profile pictures that are saved under the /original/ S3 prefix. The developer notices that some pictures cause the Lambda function to time out. The developer wants to implement a fallback mechanism by using another Lambda function that resizes the profile picture.Which solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Set the image resize Lambda function as a destination of the avatar generator Lambda function for the events that fail processing.",
        "B": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Set the SQS queue as a destination with an on failure condition for the avatar generator Lambda function. Configure the image resize Lambda function to poll from the SQS queue.",
        "C": "Create an AWS Step Functions state machine that invokes the avatar generator Lambda function and uses the image resize Lambda function as a fallback. Create an Amazon EventBridge rule that matches events from the S3 bucket to invoke the state machine.",
        "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Set the SNS topic as a destination with an on failure condition for the avatar generator Lambda function. Subscribe the image resize Lambda function to the SNS topic."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103723-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 24, 2023, 3:27 a.m.",
      "textHash": "1c46325e276dde86",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Ici, une fonction AWS Lambda (code ex√©cut√© sans serveur) cr√©e des avatars quand une image arrive dans Amazon S3 (stockage de fichiers) sous le dossier /original/.\nCertaines images font ‚Äútimeout‚Äù : la Lambda n‚Äôa pas fini avant la limite de temps, donc l‚Äôex√©cution est consid√©r√©e comme un √©chec.\nOn veut un ‚Äúplan B‚Äù automatique : si la g√©n√©ration d‚Äôavatar √©choue, lancer une autre Lambda qui fait juste un redimensionnement.\nLa solution la plus simple est d‚Äôutiliser les ‚ÄúLambda Destinations‚Äù : on peut envoyer automatiquement les ex√©cutions en √©chec vers une autre cible.\nEn mettant directement la Lambda de resize comme destination ‚Äúon failure‚Äù de la Lambda g√©n√©ratrice, on n‚Äôajoute presque pas d‚Äôinfrastructure.\nPas besoin de g√©rer une file (SQS), un workflow (Step Functions) ou un syst√®me de pub/sub (SNS) : ce serait plus de configuration et de code.\nDonc A est correct : en cas d‚Äô√©chec/timeout, AWS d√©clenche automatiquement la deuxi√®me Lambda comme m√©canisme de secours.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine : un √©l√®ve (photo) arrive au comptoir ‚ÄúChef Avatar‚Äù qui doit pr√©parer un plat sp√©cial. Si le chef met trop de temps, on veut qu‚Äôun autre chef ‚ÄúChef Resize‚Äù prenne le relais pour faire une version simple et rapide.**\n\nIci, AWS Lambda = un ‚Äúmini-robot‚Äù qui fait une t√¢che automatiquement quand une photo arrive dans le dossier /original/ (comme une file d‚Äô√©l√®ves). Parfois, le robot ‚Äúg√©n√©rateur d‚Äôavatar‚Äù d√©passe le temps limite (il ‚Äútimeout‚Äù, comme un chef trop lent). La solution la plus simple est de dire : ‚ÄúSi le premier robot √©choue, envoie directement la photo au second robot.‚Äù C‚Äôest exactement l‚Äôoption A : on branche le robot Resize comme ‚Äúdestination en cas d‚Äô√©chec‚Äù du robot Avatar. Pas besoin d‚Äôajouter une nouvelle file d‚Äôattente, un syst√®me de messages, ou un chef d‚Äôorchestre compliqu√©. Donc A demande le moins de bricolage : un seul r√©glage de redirection quand √ßa rate.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:51:8d658ab5d3050433",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 51,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has a legacy application that is hosted on-premises. Other applications hosted on AWS depend on the on-premises application for proper functioning. In case of any application errors, the developer wants to be able to use Amazon CloudWatch to monitor and troubleshoot all applications from one place.How can the developer accomplish this?",
      "choices": {
        "A": "Install an AWS SDK on the on-premises server to automatically send logs to CloudWatch.",
        "B": "Download the CloudWatch agent to the on-premises server. Configure the agent to use IAM user credentials with permissions for CloudWatch.",
        "C": "Upload log files from the on-premises server to Amazon S3 and have CloudWatch read the files.",
        "D": "Upload log files from the on-premises server to an Amazon EC2 instance and have the instance forward the logs to CloudWatch."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103772-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 24, 2023, 12:27 p.m.",
      "textHash": "8d658ab5d3050433",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Pour tout surveiller dans un seul endroit, il faut envoyer les m√©triques et logs du serveur on‚Äëpremises vers Amazon CloudWatch (service AWS de monitoring et d‚Äôalertes).\nLa fa√ßon standard est d‚Äôinstaller le CloudWatch Agent sur le serveur local : c‚Äôest un petit programme qui lit les fichiers de logs et les m√©triques (CPU, m√©moire, disque) et les envoie √† CloudWatch.\nComme le serveur n‚Äôest pas dans AWS, l‚Äôagent doit s‚Äôauthentifier pour avoir le droit d‚Äô√©crire dans CloudWatch.\nOn configure donc l‚Äôagent avec des identifiants IAM (un utilisateur IAM ou des cl√©s) ayant les permissions CloudWatch Logs/CloudWatch.\nAinsi, CloudWatch centralise les logs et m√©triques de l‚Äôon‚Äëpremises et des applis AWS, ce qui facilite le d√©pannage.\nA est trop vague : un SDK ne ‚Äúpousse‚Äù pas automatiquement les logs sans code et configuration.\nC est faux : CloudWatch ne lit pas directement des fichiers dans S3 comme source de logs.\nD est inutilement complexe : pas besoin de passer par une instance EC2 si l‚Äôagent peut envoyer directement.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une √©cole o√π tu veux surveiller tous les √©l√®ves depuis une seule salle de contr√¥le. Les applis sur AWS sont des classes d√©j√† reli√©es √† l‚Äôinterphone. L‚Äôappli ‚Äúsur site‚Äù (on‚Äëpremises) est une classe dans un autre b√¢timent, sans interphone.**\n\nConcept : CloudWatch, c‚Äôest la salle de contr√¥le qui re√ßoit des ‚Äúmessages‚Äù (logs/mesures) pour comprendre ce qui ne va pas. Si une classe n‚Äôenvoie rien, tu ne peux pas la surveiller.\nPourquoi B : tu installes le ‚ÄúCloudWatch agent‚Äù sur le serveur sur site, comme si tu posais un interphone dans l‚Äôautre b√¢timent. Il envoie automatiquement les messages √† la salle de contr√¥le.\nLes ‚Äúidentifiants IAM‚Äù sont comme un badge d‚Äôacc√®s : ils prouvent √† CloudWatch que cet interphone a le droit d‚Äôenvoyer des infos.\nA : un SDK, c‚Äôest plut√¥t une bo√Æte √† outils pour coder, pas un interphone pr√™t √† envoyer des logs.\nC : mettre des fichiers dans S3, c‚Äôest comme d√©poser des papiers √† la biblioth√®que : CloudWatch ne les lit pas tout seul.\nD : passer par un autre √©l√®ve (EC2) pour porter les messages, c‚Äôest inutilement compliqu√©.\nDonc B permet de tout surveiller au m√™me endroit.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:50:2a4a8541263c9e40",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 50,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a mobile app that calls a backend service by using an Amazon API Gateway REST API. For integration testing during the development phase, the developer wants to simulate different backend responses without invoking the backend service.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Create an AWS Lambda function. Use API Gateway proxy integration to return constant HTTP responses.",
        "B": "Create an Amazon EC2 instance that serves the backend REST API by using an AWS CloudFormation template.",
        "C": "Customize the API Gateway stage to select a response type based on the request.",
        "D": "Use a request mapping template to select the mock integration response."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103619-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2023, 12:47 a.m.",
      "textHash": "2a4a8541263c9e40",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:d012c8d9",
      "frExplanation": "API Gateway est un service AWS qui re√ßoit des requ√™tes HTTP de votre app mobile et les envoie normalement √† un ‚Äúbackend‚Äù (serveur, Lambda, etc.).\nIci, on veut tester l‚Äôint√©gration sans appeler le vrai backend, donc il faut que API Gateway ‚Äúfasse semblant‚Äù de r√©pondre.\nLa solution la plus simple est d‚Äôutiliser une int√©gration de type ‚ÄúMock‚Äù dans API Gateway : API Gateway g√©n√®re la r√©ponse lui‚Äëm√™me.\nUn ‚Äúrequest mapping template‚Äù (mod√®le de mapping) permet de lire des √©l√©ments de la requ√™te (chemin, en-t√™tes, param√®tres) et de choisir quelle r√©ponse simul√©e renvoyer.\nAinsi, on peut simuler plusieurs cas (200, 400, 500, corps diff√©rent) sans d√©ployer ni maintenir de serveur.\nA (Lambda) ajoute du code et une fonction √† g√©rer; B (EC2) est beaucoup plus lourd; C n‚Äôest pas la mani√®re standard de choisir des r√©ponses simul√©es.\nDonc D r√©pond au besoin avec le moins d‚Äôexploitation (pas de backend, pas de compute √† g√©rer).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu testes une appli de commande de pizzas pour le lyc√©e. Normalement, tu appelles la vraie pizzeria (le backend) et elle r√©pond ‚Äúlivr√©‚Äù, ‚Äúen retard‚Äù, ‚Äúrupture‚Äù. Mais pendant les tests, tu veux simuler ces r√©ponses sans d√©ranger la pizzeria.**\n\nConcept : API Gateway, c‚Äôest le standard t√©l√©phonique entre l‚Äôappli et la pizzeria. Pour tester, tu peux demander au standard de r√©pondre lui‚Äëm√™me avec des messages ‚Äúfaux‚Äù (mock), sans appeler la pizzeria. La r√©ponse D dit : utiliser un ‚Äúgabarit de requ√™te‚Äù (request mapping template) pour choisir quelle r√©ponse simul√©e renvoyer. Avec l‚Äôanalogie : selon ce que tu dis au standard (‚Äútest=retard‚Äù), il te r√©pond directement ‚Äúen retard‚Äù ou ‚Äúlivr√©‚Äù. C‚Äôest le moins de boulot √† g√©rer : pas de nouveau serveur (B), pas de code √† maintenir dans une fonction (A), et pas de r√©glages compliqu√©s par √©tape (C). Donc D est la meilleure pour simuler plusieurs r√©ponses facilement, sans appeler le vrai service.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:49:52d8c2fca40a2e0a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 49,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has written the following IAM policy to provide access to an Amazon S3 bucket:Which access does the policy allow regarding the s3:GetObject and s3:PutObject actions?",
      "choices": {
        "A": "Access on all buckets except the ‚ÄúDOC-EXAMPLE-BUCKET‚Äù bucket",
        "B": "Access on all buckets that start with ‚ÄúDOC-EXAMPLE-BUCKET‚Äù except the ‚ÄúDOC-EXAMPLE-BUCKET/secrets‚Äù bucket",
        "C": "Access on all objects in the ‚ÄúDOC-EXAMPLE-BUCKET‚Äù bucket along with access to all S3 actions for objects in the ‚ÄúDOC-EXAMPLE-BUCKET‚Äù bucket that start with ‚Äúsecrets‚Äù",
        "D": "Access on all objects in the ‚ÄúDOC-EXAMPLE-BUCKET‚Äù bucket except on objects that start with ‚Äúsecrets‚Äù"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103919-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 26, 2023, 2:46 a.m.",
      "textHash": "52d8c2fca40a2e0a",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "IAM est le syst√®me d‚Äôautorisations d‚ÄôAWS : une ‚Äúpolicy‚Äù dit ce qu‚Äôun utilisateur peut faire sur quelles ressources.\nAmazon S3 stocke des fichiers (‚Äúobjets‚Äù) dans des ‚Äúbuckets‚Äù. Les actions s3:GetObject et s3:PutObject servent √† lire et √† envoyer des objets.\nDans une policy, un Allow donne l‚Äôacc√®s, mais un Deny explicite (ou une exclusion via NotResource/condition) l‚Äôemporte toujours sur un Allow.\nIci, l‚Äôintention est : autoriser GetObject/PutObject sur tous les objets du bucket DOC-EXAMPLE-BUCKET.\nMais les objets dont la cl√© (le chemin) commence par \"secrets\" (ex: secrets/plan.txt) sont exclus/refus√©s.\nDonc on peut lire/√©crire partout dans DOC-EXAMPLE-BUCKET, sauf sur les objets qui commencent par \"secrets\".\nCela correspond au choix D.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un casier au lyc√©e appel√© ¬´ DOC-EXAMPLE-BUCKET ¬ª. Dedans, il y a plein de feuilles (les fichiers). Certaines feuilles sont dans une pochette sp√©ciale qui commence par ¬´ secrets‚Ä¶ ¬ª (ex: secrets/notes.txt).**\n\nConcept : une r√®gle IAM, c‚Äôest comme un r√®glement qui dit ce que tu as le droit de faire dans le casier. ¬´ GetObject ¬ª = lire/prendre une feuille. ¬´ PutObject ¬ª = d√©poser/ajouter une feuille.\nIci, la r√®gle dit : tu peux lire et d√©poser des feuilles dans ce casier. MAIS il y a une exception : tout ce qui commence par ¬´ secrets ¬ª est interdit.\nDonc tu as acc√®s √† toutes les feuilles du casier ¬´ DOC-EXAMPLE-BUCKET ¬ª, sauf celles rang√©es dans la pochette ¬´ secrets‚Ä¶ ¬ª.\nC‚Äôest exactement le choix D : acc√®s √† tous les objets du bucket, sauf ceux dont le nom commence par ¬´ secrets ¬ª.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:281:a47e742b2e50c729",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 281,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer creates a static website for their department. The developer deploys the static assets for the website to an Amazon S3 bucket and serves the assets with Amazon CloudFront. The developer uses origin access control (OAC) on the CloudFront distribution to access the S3 bucket.The developer notices users can access the root URL and specific pages but cannot access directories without specifying a file name. For example, /products/index.html works, but /products/ returns an error. The developer needs to enable accessing directories without specifying a file name without exposing the S3 bucket publicly.Which solution will meet these requirements?",
      "choices": {
        "A": "Update the CloudFront distribution's settings to index.html as the default root object is set.",
        "B": "Update the Amazon S3 bucket settings and enable static website hosting. Specify index.html as the Index document. Update the S3 bucket policy to enable access. Update the CloudFront distribution's origin to use the S3 website endpoint.",
        "C": "Create a CloudFront function that examines the request URL and appends index.html when directories are being accessed. Add the function as a viewer request CloudFront function to the CloudFront distribution's behavior.",
        "D": "Create a custom error response on the CloudFront distribution with the HTTP error code set to the HTTP 404 Not Found response code and the response page path to /index.html. Set the HTTP response code to the HTTP 200 OK response code."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134280-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:53 a.m.",
      "textHash": "a47e742b2e50c729",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, le site est dans Amazon S3 (stockage de fichiers) et servi via CloudFront (CDN). Avec OAC, CloudFront lit le bucket S3 de fa√ßon priv√©e : le bucket ne doit pas √™tre public.\nLe probl√®me : quand l‚Äôutilisateur demande /products/, CloudFront envoie exactement cette cl√© √† S3. Or dans S3 il n‚Äôexiste pas d‚Äôobjet nomm√© \"products/\" ; il existe \"products/index.html\". Donc S3 renvoie une erreur.\nLe ‚ÄúDefault root object‚Äù de CloudFront ne s‚Äôapplique qu‚Äô√† la racine ‚Äú/‚Äù, pas √† chaque dossier.\nActiver ‚ÄúS3 static website hosting‚Äù donnerait une logique d‚Äôindex par dossier, mais cela n√©cessite l‚Äôendpoint website S3, qui ne fonctionne pas avec OAC et pousse souvent √† rendre le bucket public : interdit ici.\nLa bonne approche est donc de r√©√©crire la requ√™te avant qu‚Äôelle parte vers S3.\nUne CloudFront Function en viewer request peut d√©tecter une URL finissant par ‚Äú/‚Äù (ou sans extension) et ajouter ‚Äúindex.html‚Äù.\nAinsi /products/ devient /products/index.html, tout en gardant le bucket priv√© derri√®re CloudFront.\nLes r√©ponses d‚Äôerreur personnalis√©es (404->200) masquent l‚Äôerreur mais ne servent pas le bon fichier pour chaque dossier.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéí Imagine ton casier au lyc√©e** : tu ranges tes affaires diff√©remment selon comment tu les cherches.\n\n**LSI (Local)** = C'est comme **des onglets DANS ton classeur** üìÅ - Cr√©√©s √† la rentr√©e, jamais modifiables, super rapides (m√™me partition).\n\n**GSI (Global)** = C'est comme **un index √† la fin du livre** üìñ - Ajoutable n'importe quand, cherche partout, mais plus lent.\n\n**üß† Mn√©motechnique :** \"**L**SI = **L**ocal, **L**imit√©\" | \"**G**SI = **G**lobal, **G**√©nial\"\n\n**Quand utiliser GSI :** Quand tu veux chercher par un autre crit√®re que la cl√© principale, sur TOUTE la table.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:46:cbb2631f8826ea43",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 46,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on a serverless application that needs to process any changes to an Amazon DynamoDB table with an AWS Lambda function.How should the developer configure the Lambda function to detect changes to the DynamoDB table?",
      "choices": {
        "A": "Create an Amazon Kinesis data stream, and attach it to the DynamoDB table. Create a trigger to connect the data stream to the Lambda function.",
        "B": "Create an Amazon EventBridge rule to invoke the Lambda function on a regular schedule. Conned to the DynamoDB table from the Lambda function to detect changes.",
        "C": "Enable DynamoDB Streams on the table. Create a trigger to connect the DynamoDB stream to the Lambda function.",
        "D": "Create an Amazon Kinesis Data Firehose delivery stream, and attach it to the DynamoDB table. Configure the delivery stream destination as the Lambda function."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103917-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 26, 2023, 2:07 a.m.",
      "textHash": "cbb2631f8826ea43",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Pour d√©tecter automatiquement les changements (ajout, modification, suppression) dans une table DynamoDB, il faut un m√©canisme qui ‚Äúpublie‚Äù ces √©v√©nements.\nDynamoDB Streams est une fonctionnalit√© native de DynamoDB qui enregistre chaque changement dans un flux (stream) d‚Äô√©v√©nements, dans l‚Äôordre.\nAWS Lambda peut √™tre d√©clench√©e par ce flux : √† chaque nouvel √©v√©nement dans le stream, Lambda est appel√©e sans serveur √† g√©rer.\nDonc la bonne configuration est : activer DynamoDB Streams sur la table, puis cr√©er un trigger (d√©clencheur) entre le stream et la fonction Lambda.\nA et D parlent de Kinesis (Data Streams/Firehose) : ce n‚Äôest pas l‚Äôint√©gration standard pour capter les changements DynamoDB, et Firehose sert surtout √† livrer des donn√©es vers S3/Redshift, pas √† d√©clencher Lambda depuis DynamoDB.\nB (EventBridge planifi√©) ferait du ‚Äúpolling‚Äù r√©gulier : inefficace, co√ªteux, et peut rater des changements entre deux ex√©cutions.\nAvec Streams + trigger, la d√©tection est quasi temps r√©el et bas√©e sur les √©v√©nements, ce qui est le plus logique en serverless.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le cahier d‚Äôappel de ta classe : √† chaque fois que quelqu‚Äôun arrive, part, ou qu‚Äôun prof corrige une info, une ligne est ajout√©e dans un ‚Äújournal des changements‚Äù.**\n\nConcept : la table DynamoDB, c‚Äôest comme le cahier d‚Äôappel (les donn√©es). Pour rep√©rer les changements, il faut un ‚Äújournal‚Äù qui note chaque modification.\nDynamoDB Streams, c‚Äôest justement ce journal automatique : chaque ajout/modif/suppression est enregistr√©.\nAWS Lambda, c‚Äôest comme un surveillant robot : d√®s qu‚Äôune nouvelle ligne appara√Æt dans le journal, il r√©agit tout de suite.\nDonc la bonne config : activer DynamoDB Streams sur la table, puis brancher (trigger) ce stream √† la Lambda.\nPourquoi pas A/D : Kinesis, c‚Äôest plut√¥t un gros tuyau pour transporter des flux, pas le journal natif du cahier d‚Äôappel.\nPourquoi pas B : v√©rifier √† heures fixes, c‚Äôest comme regarder le cahier toutes les 10 minutes : tu peux rater des changements et c‚Äôest moins efficace.\nAvec C, chaque changement d√©clenche automatiquement la Lambda, sans attendre.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:44:813775713e27503b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 44,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is designing a serverless application with two AWS Lambda functions to process photos. One Lambda function stores objects in an Amazon S3 bucket and stores the associated metadata in an Amazon DynamoDB table. The other Lambda function fetches the objects from the S3 bucket by using the metadata from the DynamoDB table. Both Lambda functions use the same Python library to perform complex computations and are approaching the quota for the maximum size of zipped deployment packages.What should the developer do to reduce the size of the Lambda deployment packages with the LEAST operational overhead?",
      "choices": {
        "A": "Package each Python library in its own .zip file archive. Deploy each Lambda function with its own copy of the library.",
        "B": "Create a Lambda layer with the required Python library. Use the Lambda layer in both Lambda functions.",
        "C": "Combine the two Lambda functions into one Lambda function. Deploy the Lambda function as a single .zip file archive.",
        "D": "Download the Python library to an S3 bucket. Program the Lambda functions to reference the object URLs."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103916-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 26, 2023, 2:04 a.m.",
      "textHash": "813775713e27503b",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:28ca60ba",
      "frExplanation": "Ici, les deux fonctions AWS Lambda (code qui s‚Äôex√©cute sans serveur) utilisent la m√™me biblioth√®que Python lourde et atteignent la limite de taille du package .zip.\nUne ¬´ Lambda Layer ¬ª est un paquet r√©utilisable (biblioth√®ques, d√©pendances) que l‚Äôon attache √† une ou plusieurs fonctions Lambda.\nEn mettant la biblioth√®que Python dans une Layer, chaque fonction n‚Äôa plus besoin d‚Äôembarquer sa propre copie dans son .zip : le d√©ploiement devient plus petit.\nC‚Äôest aussi le moins d‚Äôeffort au quotidien : une seule biblioth√®que √† maintenir et mettre √† jour, partag√©e par les deux fonctions.\nA ne r√©duit pas vraiment la taille totale et duplique la biblioth√®que dans chaque fonction.\nC m√©lange deux responsabilit√©s (√©criture S3/DynamoDB et lecture) et complique le code sans r√©soudre proprement le partage de d√©pendances.\nD est inadapt√© : Lambda ne peut pas ‚Äúimporter‚Äù une biblioth√®que Python directement depuis une URL S3 au runtime comme une d√©pendance standard.\nDonc la meilleure option avec le moins d‚Äôoverhead op√©rationnel est de cr√©er une Lambda Layer et l‚Äôutiliser dans les deux fonctions.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine deux clubs au lyc√©e (photo et journal) qui font chacun un devoir. Les deux utilisent le m√™me gros manuel de maths tr√®s lourd. Le sac de chaque club devient trop lourd √† porter.**\n\nConcept : au lieu de mettre le m√™me gros manuel dans chaque sac, on met 1 seul manuel dans la biblioth√®que, et les deux clubs viennent l‚Äôutiliser quand ils en ont besoin.\nDans AWS, chaque fonction Lambda = un √©l√®ve avec son sac (son ‚Äúpaquet‚Äù de code). La grosse biblioth√®que Python = le manuel lourd.\nBonne r√©ponse B : une ‚ÄúLambda layer‚Äù, c‚Äôest comme la biblioth√®que partag√©e : tu y mets la biblioth√®que Python une seule fois, et les deux fonctions l‚Äôutilisent.\nR√©sultat : les sacs (packages) deviennent plus petits, et tu ne g√®res qu‚Äôun seul exemplaire du manuel (moins de travail).\nPourquoi pas A : √ßa fait deux copies du manuel, donc deux sacs lourds.\nPourquoi pas C : fusionner les clubs en un seul complique l‚Äôorganisation pour juste un probl√®me de sac.\nPourquoi pas D : aller chercher des pages du manuel via des liens √† chaque fois, c‚Äôest plus gal√®re et moins pratique.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:43:5339fd6bf2ab3bf7",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 43,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is designing an AWS Lambda function that creates temporary files that are less than 10 MB during invocation. The temporary files will be accessed and modified multiple times during invocation. The developer has no need to save or retrieve these files in the future.Where should the temporary files be stored?",
      "choices": {
        "A": "the /tmp directory",
        "B": "Amazon Elastic File System (Amazon EFS)",
        "C": "Amazon Elastic Block Store (Amazon EBS)",
        "D": "Amazon S3"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103915-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 26, 2023, 2:03 a.m.",
      "textHash": "5339fd6bf2ab3bf7",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:0e477869",
      "frExplanation": "Dans AWS Lambda, votre code s‚Äôex√©cute dans un environnement √©ph√©m√®re (qui peut dispara√Ætre apr√®s l‚Äôex√©cution).\nLambda fournit un espace disque local temporaire accessible via le dossier /tmp.\nCet espace est fait pour cr√©er des fichiers temporaires pendant l‚Äôinvocation, les relire et les modifier plusieurs fois rapidement.\nComme vous n‚Äôavez pas besoin de conserver ces fichiers apr√®s, /tmp est le choix logique : c‚Äôest local, simple et rapide.\nAmazon S3 est un stockage d‚Äôobjets durable (fait pour garder des fichiers), donc inutile ici.\nEFS est un syst√®me de fichiers r√©seau partag√© et persistant, utile si plusieurs fonctions/instances doivent partager des fichiers.\nEBS est un disque attach√© √† des instances EC2, pas directement adapt√© √† Lambda.\nDonc, stockez les fichiers temporaires dans /tmp.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un contr√¥le en classe : tu fais des brouillons sur une feuille pos√©e sur ta table, tu l‚Äôeffaces, tu r√©√©cris, puis √† la fin tu la jettes.**\n\nLe concept : une fonction Lambda, c‚Äôest comme un √©l√®ve qui arrive, fait une t√¢che vite, puis repart. Elle a juste besoin d‚Äôun petit espace de brouillon pendant qu‚Äôelle travaille.\nLes fichiers temporaires (<10 MB) qu‚Äôon lit et modifie plusieurs fois, c‚Äôest exactement ces brouillons : il faut que ce soit rapide et juste ‚Äúsur place‚Äù.\nDans Lambda, l‚Äôendroit pr√©vu pour √ßa s‚Äôappelle le dossier /tmp : c‚Äôest la table de l‚Äô√©l√®ve pendant le contr√¥le.\nOn n‚Äôa pas besoin de garder les fichiers apr√®s : donc pas besoin d‚Äôun ‚Äúcasier‚Äù ou d‚Äôun ‚Äúcloud‚Äù pour stocker longtemps.\nEFS/EBS/S3, c‚Äôest plut√¥t pour garder des affaires pour plus tard ou les partager, comme un casier, une armoire ou une biblioth√®que.\nDonc la bonne r√©ponse est A : stocker dans /tmp.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:41:0f93e0db354f6398",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 41,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a web application that uses Amazon API Gateway to expose an AWS Lambda function to process requests from clients. During testing, the developer notices that the API Gateway times out even though the Lambda function finishes under the set time limit.Which of the following API Gateway metrics in Amazon CloudWatch can help the developer troubleshoot the issue? (Choose two.)",
      "choices": {
        "A": "CacheHitCount",
        "B": "IntegrationLatency",
        "C": "CacheMissCount",
        "D": "Latency",
        "E": "Count"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103858-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 25, 2023, 2:01 p.m.",
      "textHash": "0f93e0db354f6398",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "API Gateway est la ‚Äúporte d‚Äôentr√©e‚Äù HTTP de votre appli, et Lambda ex√©cute le code. Un timeout peut venir non pas du temps d‚Äôex√©cution Lambda, mais du temps total c√¥t√© API Gateway.\nDans CloudWatch, la m√©trique la plus utile ici est IntegrationLatency : elle mesure le temps pass√© entre API Gateway et le service int√©gr√© (ici Lambda), incluant l‚Äôappel et la r√©ponse. Si elle est √©lev√©e, l‚Äôint√©gration (r√©seau, permissions, cold start, surcharge) ralentit.\nL‚Äôautre m√©trique cl√© est Latency : elle mesure le temps total vu par API Gateway pour traiter la requ√™te (r√©ception ‚Üí r√©ponse au client). Si Latency est √©lev√©e mais IntegrationLatency faible, le probl√®me est plut√¥t dans API Gateway (mapping, authorizer, validation, throttling, etc.).\nCacheHitCount/CacheMissCount ne servent que si le cache API Gateway est activ√©, et Count ne donne que le nombre d‚Äôappels, pas la cause d‚Äôun timeout.\nDonc, pour diagnostiquer un timeout malgr√© une Lambda ‚Äúrapide‚Äù, comparez Latency et IntegrationLatency.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine le self du lyc√©e : tu commandes un plat au guichet (API Gateway), puis la cuisine le pr√©pare (Lambda), et tu attends qu‚Äôon te serve.**\n\nLe concept : le ‚Äútimeout‚Äù peut venir soit de la cuisine qui met trop de temps, soit du temps total entre ta commande et le moment o√π tu re√ßois ton plateau.\nLe bon indicateur ici est IntegrationLatency : c‚Äôest le temps pass√© entre le guichet et la cuisine (aller-retour + pr√©paration c√¥t√© cuisine).\nSi Lambda finit vite mais que IntegrationLatency est grand, le probl√®me est sur le ‚Äúchemin‚Äù entre guichet et cuisine (connexion, attente, r√©ponse).\nLatency mesure le temps total vu par l‚Äô√©l√®ve : de la commande jusqu‚Äôau plateau.\nComparer Latency et IntegrationLatency aide √† voir si le retard vient du guichet (avant/apr√®s la cuisine) ou de l‚Äô√©change avec la cuisine.\nCacheHitCount/CacheMissCount parlent d‚Äôun ‚Äúfrigo de plats d√©j√† pr√™ts‚Äù (cache), pas du timeout ici.\nCount ne dit que ‚Äúcombien de commandes‚Äù, pas ‚Äúpourquoi c‚Äôest lent‚Äù.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:456:51f9798e65c730d8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 456,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application ingests data from an Amazon Kinesis data stream. The shards in the data stream are set for normal traffic.During tests for peak traffic, the application ingests data slowly. A developer needs to adjust the data stream to handle the peak traffic.What should the developer do to meet this requirement MOST cost-effectively?",
      "choices": {
        "A": "Install the Kinesis Producer Library (KPL) to ingest data into the data stream.",
        "B": "Switch to on-demand capacity mode for the data stream. Specify a partition key when writing data to the data stream.",
        "C": "Decrease the amount of time that data is kept in the data stream by using the DecreaseStreamRetentionPeriod API operation.",
        "D": "Increase the shard count in the data stream by using the UpdateShardCount API operation."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/151547-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 18, 2024, 1:56 p.m.",
      "textHash": "51f9798e65c730d8",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:35fcc40b",
      "frExplanation": "Amazon Kinesis Data Streams est un service qui re√ßoit des √©v√©nements en continu (logs, clics, IoT). La capacit√© d√©pend du nombre de ¬´ shards ¬ª (comme des voies sur une autoroute).\nSi l‚Äôapplication ing√®re lentement pendant un test de pic, c‚Äôest souvent que la capacit√© d‚Äô√©criture/lecture par shard est satur√©e.\nLa solution la plus directe et contr√¥lable est d‚Äôaugmenter le nombre de shards : plus de shards = plus de d√©bit total.\nL‚ÄôAPI UpdateShardCount permet de faire ce ‚Äúscaling‚Äù pour absorber le pic, puis √©ventuellement de redescendre apr√®s, ce qui est rentable.\nA (KPL) peut am√©liorer l‚Äôefficacit√© c√¥t√© producteur, mais ne r√®gle pas un manque de capacit√© du stream si les shards sont d√©j√† le goulot.\nB (on-demand) peut co√ªter plus cher et n‚Äôest pas n√©cessaire si on sait qu‚Äôil faut juste plus de capacit√© pendant les pics.\nC (r√©tention) change combien de temps les donn√©es sont conserv√©es, pas la vitesse d‚Äôingestion.\nDonc D est le choix le plus cost-effective pour g√©rer le pic : augmenter le nombre de shards.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : les ‚Äúshards‚Äù sont des caisses. En temps normal, il y a assez de caisses. Aux heures de pointe, √ßa bouchonne.**\n\nConcept : un flux Kinesis, c‚Äôest comme une file o√π arrivent des plateaux. Chaque shard (caisse) peut traiter seulement un certain nombre de plateaux par seconde.\nPendant le test ‚Äúheure de pointe‚Äù, √ßa va lentement car il n‚Äôy a pas assez de caisses ouvertes.\nDonc la solution la plus efficace est d‚Äôaugmenter le nombre de shards : tu ouvres plus de caisses, tu traites plus de donn√©es en parall√®le (D).\nA (KPL) c‚Äôest comme donner un meilleur plateau aux √©l√®ves : √ßa peut aider un peu, mais si tu n‚Äôouvres pas plus de caisses, √ßa bloque quand m√™me.\nB (on-demand) c‚Äôest comme payer une cantine qui ouvre des caisses automatiquement : pratique, mais pas forc√©ment le moins cher si tu sais juste que tu as un pic √† g√©rer.\nC (r√©duire la r√©tention) c‚Äôest comme jeter les plateaux plus vite : √ßa ne rend pas les caisses plus rapides, donc √ßa ne r√®gle pas le ralentissement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:40:a6d8e4089de41f3a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 40,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has deployed infrastructure on AWS. A development team wants to create an AWS Lambda function that will retrieve data from an Amazon Aurora database. The Amazon Aurora database is in a private subnet in company's VPC. The VPC is named VPC1. The data is relational in nature. The Lambda function needs to access the data securely.Which solution will meet these requirements?",
      "choices": {
        "A": "Create the Lambda function. Configure VPC1 access for the function. Attach a security group named SG1 to both the Lambda function and the database. Configure the security group inbound and outbound rules to allow TCP traffic on Port 3306.",
        "B": "Create and launch a Lambda function in a new public subnet that is in a new VPC named VPC2. Create a peering connection between VPC1 and VPC2.",
        "C": "Create the Lambda function. Configure VPC1 access for the function. Assign a security group named SG1 to the Lambda function. Assign a second security group named SG2 to the database. Add an inbound rule to SG1 to allow TCP traffic from Port 3306.",
        "D": "Export the data from the Aurora database to Amazon S3. Create and launch a Lambda function in VPC1. Configure the Lambda function query the data from Amazon S3."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103523-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 10:47 p.m.",
      "textHash": "a6d8e4089de41f3a",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:f58c2673",
      "frExplanation": "Aurora est une base de donn√©es relationnelle (SQL) dans un sous-r√©seau priv√© : elle n‚Äôest pas accessible depuis Internet. Pour qu‚Äôune fonction AWS Lambda puisse s‚Äôy connecter, elle doit √™tre ¬´ rattach√©e ¬ª au m√™me VPC (VPC1) afin d‚Äôobtenir des interfaces r√©seau dans les sous-r√©seaux priv√©s. Ensuite, on contr√¥le l‚Äôacc√®s avec des Security Groups (pare-feu). La bonne pratique simple est d‚Äôutiliser un m√™me Security Group (SG1) pour Lambda et pour Aurora, puis d‚Äôautoriser le trafic TCP sur le port de la base (3306 pour MySQL/Aurora MySQL) entre ressources du m√™me SG. Ainsi, seule la Lambda (et toute ressource avec SG1) peut joindre la base, sans exposition publique. Les autres choix soit compliquent inutilement (VPC peering), soit configurent mal les r√®gles (r√®gle entrante sur le mauvais SG), soit changent le besoin (exporter vers S3 au lieu d‚Äôinterroger la base).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:15:5c1c2621b5af14dc",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 15,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is deploying an AWS Lambda function The developer wants the ability to return to older versions of the function quickly and seamlessly.How can the developer achieve this goal with the LEAST operational overhead?",
      "choices": {
        "A": "Use AWS OpsWorks to perform blue/green deployments.",
        "B": "Use a function alias with different versions.",
        "C": "Maintain deployment packages for older versions in Amazon S3.",
        "D": "Use AWS CodePipeline for deployments and rollbacks."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102742-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 15, 2023, 11:35 p.m.",
      "textHash": "5c1c2621b5af14dc",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "AWS Lambda est un service qui ex√©cute votre code sans g√©rer de serveurs. Chaque fois que vous publiez une nouvelle version d‚Äôune fonction Lambda, AWS peut cr√©er une ¬´ version ¬ª num√©rot√©e (immuable). Un ¬´ alias ¬ª est un nom stable (ex: prod) qui pointe vers une version pr√©cise. Pour revenir rapidement √† une ancienne version, il suffit de changer l‚Äôalias pour qu‚Äôil pointe vers la version pr√©c√©dente : c‚Äôest imm√©diat et transparent pour les appels. Cela demande tr√®s peu d‚Äôop√©rations car tout est g√©r√© dans Lambda, sans outils externes. OpsWorks et CodePipeline ajoutent une cha√Æne de d√©ploiement plus lourde √† maintenir. Stocker des packages dans S3 aide √† archiver, mais ne permet pas un basculement instantan√© sans red√©ployer. Donc l‚Äôoption la plus simple et avec le moins d‚Äôoverhead est d‚Äôutiliser un alias Lambda vers diff√©rentes versions.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:14:a9e6b3786c87a6a6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 14,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA financial company must store original customer records for 10 years for legal reasons. A complete record contains personally identifiable information (PII). According to local regulations, PII is available to only certain people in the company and must not be shared with third parties. The company needs to make the records available to third-party organizations for statistical analysis without sharing the PII.A developer wants to store the original immutable record in Amazon S3. Depending on who accesses the S3 document, the document should be returned as is or with all the PII removed. The developer has written an AWS Lambda function to remove the PII from the document. The function is named removePii.What should the developer do so that the company can meet the PII requirements while maintaining only one copy of the document?",
      "choices": {
        "A": "Set up an S3 event notification that invokes the removePii function when an S3 GET request is made. Call Amazon S3 by using a GET request to access the object without PII.",
        "B": "Set up an S3 event notification that invokes the removePii function when an S3 PUT request is made. Call Amazon S3 by using a PUT request to access the object without PII.",
        "C": "Create an S3 Object Lambda access point from the S3 console. Select the removePii function. Use S3 Access Points to access the object without PII.",
        "D": "Create an S3 access point from the S3 console. Use the access point name to call the GetObjectLegalHold S3 API function. Pass in the removePii function name to access the object without PII."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102741-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 15, 2023, 11:34 p.m.",
      "textHash": "a9e6b3786c87a6a6",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:2d37acf0",
      "frExplanation": "Ici, on veut garder un seul fichier ‚Äúoriginal‚Äù dans Amazon S3 (stockage d‚Äôobjets) pendant 10 ans, sans le modifier (immuable), mais fournir une version ‚Äúnettoy√©e‚Äù (sans PII) √† certains lecteurs (tiers).\nLes notifications S3 (options A/B) d√©clenchent des √©v√©nements sur PUT/GET, mais elles ne transforment pas automatiquement la r√©ponse renvoy√©e au client : elles servent surtout √† lancer un traitement en arri√®re-plan, ce qui pousserait souvent √† cr√©er une 2e copie nettoy√©e.\nLa bonne approche est Amazon S3 Object Lambda : c‚Äôest un point d‚Äôacc√®s S3 qui intercepte la requ√™te GET et appelle une fonction AWS Lambda pour modifier le contenu ‚Äú√† la vol√©e‚Äù avant de le renvoyer.\nDonc l‚Äôobjet stock√© reste unique et inchang√© dans le bucket, et selon le point d‚Äôacc√®s utilis√© (et les droits IAM), on renvoie soit l‚Äôoriginal, soit la version sans PII.\nOn cr√©e un S3 Object Lambda Access Point et on l‚Äôassocie √† la fonction removePii : les tiers utilisent cet access point pour obtenir automatiquement le document sans PII.\nLes options D est incorrecte : GetObjectLegalHold concerne le verrouillage l√©gal, pas la transformation de contenu, et un access point standard ne peut pas ex√©cuter Lambda sur la r√©ponse.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e qui garde un dossier √©l√®ve ‚Äúofficiel‚Äù pendant 10 ans. Ce dossier contient des infos priv√©es (adresse, t√©l√©phone). Des chercheurs externes veulent juste des stats, sans infos priv√©es.**\n\nConcept : on veut UNE seule copie du dossier, mais selon qui le lit, on montre la version compl√®te ou une version ‚Äúcaviard√©e‚Äù (infos priv√©es masqu√©es). Dans AWS, S3 est l‚Äô√©tag√®re o√π le fichier est stock√©, et la fonction Lambda removePii est le ‚Äúsurligneur noir‚Äù qui cache les infos. La bonne solution est C : S3 Object Lambda, c‚Äôest comme un guichet sp√©cial de la biblioth√®que. Quand un externe demande le dossier, le guichet appelle removePii et renvoie une version nettoy√©e, sans modifier l‚Äôoriginal. Quand un employ√© autoris√© demande, il peut recevoir l‚Äôoriginal. A et B ne marchent pas : une notification S3 sert surtout √† r√©agir apr√®s un d√©p√¥t (PUT) ou certains √©v√©nements, pas √† transformer proprement chaque lecture (GET) selon la personne. D parle d‚Äôun truc (LegalHold) qui sert √† bloquer la suppression, pas √† enlever des infos. Donc C permet bien une seule copie + une version filtr√©e √† la demande.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:159:1c3bb52ef9bf0675",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 159,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer migrated a legacy application to an AWS Lambda function. The function uses a third-party service to pull data with a series of API calls at the end of each month. The function then processes the data to generate the monthly reports. The function has been working with no issues so far.The third-party service recently issued a restriction to allow a fixed number of API calls each minute and each day. If the API calls exceed the limit for each minute or each day, then the service will produce errors. The API also provides the minute limit and daily limit in the response header. This restriction might extend the overall process to multiple days because the process is consuming more API calls than the available limit.What is the MOST operationally efficient way to refactor the serverless application to accommodate this change?",
      "choices": {
        "A": "Use an AWS Step Functions state machine to monitor API failures. Use the Wait state to delay calling the Lambda function.",
        "B": "Use an Amazon Simple Queue Service (Amazon SQS) queue to hold the API calls. Configure the Lambda function to poll the queue within the API threshold limits.",
        "C": "Use an Amazon CloudWatch Logs metric to count the number of API calls. Configure an Amazon CloudWatch alarm that stops the currently running instance of the Lambda function when the metric exceeds the API threshold limits.",
        "D": "Use Amazon Kinesis Data Firehose to batch the API calls and deliver them to an Amazon S3 bucket with an event notification to invoke the Lambda function."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122578-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:03 a.m.",
      "textHash": "1c3bb52ef9bf0675",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Le probl√®me vient d‚Äôune limite de d√©bit (quota) impos√©e par le service tiers : X appels par minute et par jour, sinon il renvoie des erreurs. Une seule ex√©cution Lambda qui boucle et appelle l‚ÄôAPI trop vite va d√©passer ces quotas, et comme le traitement peut durer plusieurs jours, il faut pouvoir ¬´ reprendre ¬ª proprement. AWS Step Functions sert √† orchestrer un workflow : encha√Æner des √©tapes, g√©rer les erreurs/retry, et surtout attendre entre deux actions. Avec un state machine, on peut appeler Lambda, lire les en-t√™tes de r√©ponse (limites minute/jour), puis utiliser un √©tat Wait pour temporiser avant de continuer, sans garder Lambda en ex√©cution. C‚Äôest op√©rationnellement efficace car Step Functions g√®re l‚Äô√©tat, les reprises et la dur√©e longue, alors que Lambda est fait pour des ex√©cutions courtes. SQS (B) ne sait pas naturellement respecter un quota minute/jour bas√© sur des en-t√™tes et demander des pauses pr√©cises. CloudWatch (C) ne peut pas ¬´ arr√™ter ¬ª proprement une ex√©cution Lambda en cours et r√©agirait trop tard. Kinesis Firehose (D) est pour l‚Äôingestion/streaming vers S3, pas pour piloter un quota d‚Äôappels API.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu dois emprunter des livres √† la biblioth√®que pour faire un gros expos√© de fin de mois. La biblioth√®que te limite √† X emprunts par minute et Y par jour. Si tu d√©passes, le biblioth√©caire te refuse (erreur).**\n\nConcept : ta fonction Lambda, c‚Äôest toi qui vas chercher des ‚Äúlivres‚Äù (donn√©es) via des appels API. La nouvelle r√®gle, c‚Äôest un quota minute/jour, donc tu dois apprendre √† attendre et reprendre plus tard.\nPourquoi A : Step Functions, c‚Äôest comme un planning/chef d‚Äôorchestre qui suit les √©tapes. Quand la biblioth√®que dit ‚Äústop, quota atteint‚Äù (√©chec), le planning le voit et ajoute une √©tape ‚Äúattendre‚Äù (Wait) avant de r√©essayer.\nComme l‚ÄôAPI te donne les limites dans l‚Äôen-t√™te, le planning peut d√©cider combien de temps patienter et continuer sans que tu surveilles √† la main.\nB ferait une file d‚Äôattente, mais √ßa ne g√®re pas aussi proprement le ‚Äúattendre puis reprendre‚Äù selon les erreurs et les quotas minute/jour.\nC arr√™terait brutalement au lieu d‚Äôorganiser la reprise, et D n‚Äôa rien √† voir avec respecter un quota d‚Äôappels.\nDonc A est le plus simple √† op√©rer : un workflow qui attend quand il faut et reprend automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:12:fe430b264a23a58d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 12,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an application that will give users the ability to store photos from their cellphones in the cloud. The application needs to support tens of thousands of users. The application uses an Amazon API Gateway REST API that is integrated with AWS Lambda functions to process the photos. The application stores details about the photos in Amazon DynamoDB.Users need to create an account to access the application. In the application, users must be able to upload photos and retrieve previously uploaded photos. The photos will range in size from 300 KB to 5 MB.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos and details in the DynamoDB table. Retrieve previously uploaded photos directly from the DynamoDB table.",
        "B": "Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.",
        "C": "Create an IAM user for each user of the application during the sign-up process. Use IAM authentication to access the API Gateway API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.",
        "D": "Create a users table in DynamoDB. Use the table to manage user accounts. Create a Lambda authorizer that validates user credentials against the users table. Integrate the Lambda authorizer with API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as par of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103439-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 6:35 a.m.",
      "textHash": "fe430b264a23a58d",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "La solution la plus simple √† exploiter est d‚Äôutiliser des services g√©r√©s faits pour ce besoin : comptes utilisateurs + stockage de fichiers.\nAmazon Cognito User Pools g√®re l‚Äôinscription, la connexion, les mots de passe et les jetons (pas besoin de coder ou d‚Äôadministrer une base d‚Äôutilisateurs).\nUn authorizer Cognito dans API Gateway prot√®ge automatiquement l‚ÄôAPI : seules les personnes connect√©es peuvent appeler les endpoints.\nPour des photos (300 KB √† 5 MB), Amazon S3 est le service de stockage d‚Äôobjets con√ßu pour des fichiers, tr√®s scalable et peu d‚Äôadministration.\nDynamoDB est id√©al pour stocker des m√©tadonn√©es (date, utilisateur, titre, etc.), mais pas pour stocker les fichiers eux-m√™mes.\nLe bon pattern est donc : Lambda met la photo dans S3, puis enregistre dans DynamoDB la ‚Äúcl√© S3‚Äù (le chemin/identifiant du fichier) avec les d√©tails.\nPour r√©cup√©rer une photo, l‚Äôapp interroge DynamoDB pour obtenir la cl√© S3, puis lit l‚Äôobjet dans S3.\nLes autres choix augmentent l‚Äôoverhead : cr√©er un IAM user par utilisateur (C) est ing√©rable √† grande √©chelle, et g√©rer soi-m√™me l‚Äôauth (D) ou stocker les photos via DynamoDB/Lambda (A) n‚Äôest pas adapt√©.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e : tu dois cr√©er une carte d‚Äôabonn√© pour entrer, tu d√©poses tes gros livres dans la r√©serve, et tu notes sur une fiche o√π ils sont rang√©s.**\n\nConcept : pour une appli photo, il faut 1) g√©rer les comptes, 2) stocker les grosses photos, 3) garder une ‚Äúfiche‚Äù avec les infos (date, nom, o√π c‚Äôest rang√©).\nPourquoi B : Amazon Cognito = la carte d‚Äôabonn√© (cr√©ation de compte + connexion) sans que tu g√®res √ßa toi-m√™me.\nAPI Gateway + authorizer Cognito = le vigile qui laisse passer seulement les abonn√©s.\nAmazon S3 = la r√©serve g√©ante faite pour stocker des fichiers (photos de 300 KB √† 5 MB) pour des dizaines de milliers d‚Äô√©l√®ves.\nDynamoDB = les fiches de catalogue : on y met les d√©tails + la ‚Äúcl√© S3‚Äù (le num√©ro d‚Äô√©tag√®re).\nPour retrouver une photo : tu lis la fiche (DynamoDB) pour obtenir le num√©ro d‚Äô√©tag√®re (cl√© S3), puis tu vas chercher le livre en r√©serve (S3).\nLes autres : A met des photos dans DynamoDB (mauvais pour gros fichiers), C/D te font g√©rer les comptes toi-m√™me (trop de boulot).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:10:1b0bfe0935719a8b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 10,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is deploying a new application to Amazon Elastic Container Service (Amazon ECS). The developer needs to securely store and retrieve different types of variables. These variables include authentication information for a remote API, the URL for the API, and credentials. The authentication information and API URL must be available to all current and future deployed versions of the application across development, testing, and production environments.How should the developer retrieve the variables with the FEWEST application changes?",
      "choices": {
        "A": "Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.",
        "B": "Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment.",
        "C": "Update the application to retrieve the variables from an encrypted file that is stored with the application. Store the API URL and credentials in unique files for each environment.",
        "D": "Update the application to retrieve the variables from each of the deployed environments. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103335-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 20, 2023, 7:24 a.m.",
      "textHash": "1b0bfe0935719a8b",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "On veut stocker des variables (URL d‚ÄôAPI, infos d‚Äôauthentification, identifiants) de fa√ßon s√©curis√©e et r√©utilisable par toutes les versions de l‚Äôapp, dans plusieurs environnements (dev/test/prod), avec peu de changements de code.\nAWS Systems Manager Parameter Store est un ‚Äúcoffre‚Äù pour param√®tres de configuration (ex: URL, tokens non ultra-sensibles) que l‚Äôapplication peut lire via l‚ÄôAPI AWS, sans les mettre dans le code.\nOn peut organiser les param√®tres par chemins, par exemple /dev/api/url, /test/api/url, /prod/api/url : c‚Äôest simple √† g√©rer et √©vite les erreurs entre environnements.\nAWS Secrets Manager est con√ßu pour les secrets sensibles (mots de passe, cl√©s d‚ÄôAPI, credentials) avec rotation et contr√¥le d‚Äôacc√®s fin : c‚Äôest plus adapt√© que Parameter Store pour les identifiants.\nAvec ECS, l‚Äôapplication n‚Äôa qu‚Äô√† lire les param√®tres/secrets au d√©marrage (ou via variables d‚Äôenvironnement inject√©es), donc peu de modifications.\nPourquoi pas KMS (B) : KMS sert √† g√©rer des cl√©s de chiffrement, pas √† stocker directement des valeurs de configuration.\nPourquoi pas fichier chiffr√© (C) : il faut g√©rer des fichiers par environnement et les d√©ployer, ce qui complique et augmente le risque de fuite.\nPourquoi pas task definition (D) : mettre des valeurs dans la d√©finition de t√¢che lie la config au d√©ploiement et n‚Äôest pas id√©al pour secrets; cela devient difficile √† maintenir pour toutes les versions et environnements.\nDonc la meilleure approche est Parameter Store pour URL/infos partag√©es + Secrets Manager pour credentials, s√©par√©s par environnement.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:8:6d6b20734b45b046",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 8,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is running a custom application on a set of on-premises Linux servers that are accessed using Amazon API Gateway. AWS X-Ray tracing has been enabled on the API test stage.How can a developer enable X-Ray tracing on the on-premises servers with the LEAST amount of configuration?",
      "choices": {
        "A": "Install and run the X-Ray SDK on the on-premises servers to capture and relay the data to the X-Ray service.",
        "B": "Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service.",
        "C": "Capture incoming requests on-premises and configure an AWS Lambda function to pull, process, and relay relevant data to X-Ray using the PutTraceSegments API call.",
        "D": "Capture incoming requests on-premises and configure an AWS Lambda function to pull, process, and relay relevant data to X-Ray using the PutTelemetryRecords API call."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102786-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 16, 2023, 10 a.m.",
      "textHash": "6d6b20734b45b046",
      "rawFormat": "discussion-md",
      "conceptKey": "xray_tracing",
      "frExplanation": "API Gateway peut envoyer des informations de tra√ßage (X-Ray) quand on active le tracing sur un stage. Pour voir le ‚Äúbout‚Äù on‚Äëpremises, il faut que vos serveurs Linux envoient aussi des segments de trace √† AWS X-Ray.\nLe moyen le plus simple est d‚Äôinstaller le X-Ray daemon : c‚Äôest un petit agent qui tourne en arri√®re‚Äëplan, re√ßoit les donn√©es de trace de l‚Äôapplication et les transmet au service AWS X-Ray.\nCela demande peu de changements applicatifs : on configure surtout l‚Äôagent (adresse, permissions r√©seau/credentials) et il relaie les traces.\nLe SDK X-Ray (choix A) implique g√©n√©ralement d‚Äôinstrumenter/modifier le code de l‚Äôapplication pour cr√©er des sous-segments, donc plus de configuration.\nLes options avec Lambda (C, D) ajoutent une architecture inutile et des appels API manuels ; PutTelemetryRecords n‚Äôest pas fait pour envoyer des segments de trace.\nDonc, pour le minimum de configuration c√¥t√© serveurs on‚Äëpremises, on installe et ex√©cute le X-Ray daemon.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un match de foot au lyc√©e : l‚Äôarbitre (API Gateway) note d√©j√† tout ce qui se passe au d√©but du match. Maintenant tu veux aussi savoir ce qui se passe sur le terrain (tes serveurs Linux sur place) avec le minimum d‚Äôinstallation.**\n\nX-Ray, c‚Äôest comme un ‚Äúreplay‚Äù qui suit une action de bout en bout.\nAPI Gateway a d√©j√† le replay activ√©, mais sur tes serveurs il manque quelqu‚Äôun pour envoyer les infos.\nLe plus simple est d‚Äôinstaller le ‚Äúpetit reporter‚Äù X-Ray daemon : il √©coute ce qui se passe sur le serveur et envoie les traces √† X-Ray.\nC‚Äôest peu de config : tu ajoutes juste ce programme qui fait le relais.\nLe SDK (A) serait comme demander √† chaque joueur d‚Äô√©crire un rapport d√©taill√© : plus de changements dans l‚Äôapp.\nLes options avec Lambda (C/D) ajoutent un interm√©diaire inutile, comme faire passer les notes par un autre prof.\nDonc B est la bonne r√©ponse : installer le daemon sur les serveurs on-premises.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:3:3e1d5526f9578adf",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 3,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application is using Amazon Cognito user pools and identity pools for secure access. A developer wants to integrate the user-specific file upload and download features in the application with Amazon S3. The developer must ensure that the files are saved and retrieved in a secure manner and that users can access only their own files. The file sizes range from 3 KB to 300 MB.Which option will meet these requirements with the HIGHEST level of security?",
      "choices": {
        "A": "Use S3 Event Notifications to validate the file upload and download requests and update the user interface (UI).",
        "B": "Save the details of the uploaded files in a separate Amazon DynamoDB table. Filter the list of files in the user interface (UI) by comparing the current user ID with the user ID associated with the file in the table.",
        "C": "Use Amazon API Gateway and an AWS Lambda function to upload and download files. Validate each request in the Lambda function before performing the requested operation.",
        "D": "Use an IAM policy within the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102788-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 16, 2023, 10:16 a.m.",
      "textHash": "3e1d5526f9578adf",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Amazon Cognito (user pools + identity pools) permet d‚Äôauthentifier un utilisateur puis de lui donner des identifiants AWS temporaires (r√¥le IAM) pour acc√©der √† des services comme Amazon S3 (stockage de fichiers).\nL‚Äôoption la plus s√ªre est de contr√¥ler l‚Äôacc√®s directement au niveau S3 avec une politique IAM li√©e √† l‚Äôidentit√© Cognito.\nAvec une policy utilisant le pr√©fixe d‚Äôidentit√© (ex: dossier s3://bucket/${cognito-identity-id}/), chaque utilisateur n‚Äôa le droit de lire/√©crire que dans ‚Äúson‚Äù dossier.\nAinsi, m√™me si l‚Äôinterface ou l‚Äôapplication est modifi√©e ou attaqu√©e, S3 refusera l‚Äôacc√®s aux fichiers des autres.\nLes autres options filtrent c√¥t√© application (UI, DynamoDB) ou ajoutent une couche (API/Lambda) mais ne garantissent pas, √† elles seules, que S3 bloque l‚Äôacc√®s non autoris√©.\nDe plus, laisser l‚Äôutilisateur envoyer/t√©l√©charger directement vers S3 avec des droits minimaux est efficace pour des fichiers jusqu‚Äô√† 300 MB.\nDonc D offre le contr√¥le le plus fort: autorisation au niveau IAM/S3, bas√©e sur l‚Äôidentit√© de l‚Äôutilisateur.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:4:81a2d63248b3b199",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 4,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building a scalable data management solution by using AWS services to improve the speed and agility of development. The solution will ingest large volumes of data from various sources and will process this data through multiple business rules and transformations.The solution requires business rules to run in sequence and to handle reprocessing of data if errors occur when the business rules run. The company needs the solution to be scalable and to require the least possible maintenance.Which AWS service should the company use to manage and automate the orchestration of the data flows to meet these requirements?",
      "choices": {
        "A": "AWS Batch",
        "B": "AWS Step Functions",
        "C": "AWS Glue",
        "D": "AWS Lambda"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102789-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 16, 2023, 10:17 a.m.",
      "textHash": "81a2d63248b3b199",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:9205a236",
      "frExplanation": "Ici, il faut ¬´ orchestrer ¬ª un flux de traitement : plusieurs r√®gles m√©tier doivent s‚Äôex√©cuter dans un ordre pr√©cis (s√©quence) et il faut pouvoir relancer (reprocess) si une √©tape √©choue.\nAWS Step Functions est un service qui permet de d√©finir un workflow (machine d‚Äô√©tats) : √©tape 1 ‚Üí √©tape 2 ‚Üí √©tape 3, avec des conditions, des boucles, des d√©lais et des branches.\nIl g√®re automatiquement les erreurs : retries (r√©essais), gestion d‚Äôexceptions, et reprise √† l‚Äô√©tape qui a √©chou√©, sans coder toute la logique de contr√¥le.\nC‚Äôest scalable (g√®re beaucoup d‚Äôex√©cutions en parall√®le) et ¬´ low maintenance ¬ª car AWS g√®re l‚Äôinfrastructure.\nAWS Lambda ex√©cute du code, mais ne fournit pas √† elle seule l‚Äôorchestration compl√®te et la gestion de s√©quence/retry entre plusieurs √©tapes.\nAWS Batch sert surtout √† lancer des jobs de calcul par lots sur des ressources de calcul, pas √† orchestrer des r√®gles m√©tier en cha√Æne.\nAWS Glue est orient√© ETL (extraction/transform/load) et catalogage de donn√©es ; utile pour transformer, mais moins adapt√© comme orchestrateur g√©n√©ral avec logique de reprise fine.\nDonc Step Functions est le meilleur choix pour encha√Æner des r√®gles, g√©rer les erreurs et automatiser le flux de bout en bout.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un tournoi de sport au lyc√©e avec plusieurs √©preuves √† faire dans un ordre pr√©cis (√©chauffement ‚Üí match ‚Üí remise des prix).**\n\nLe concept : il faut un ‚Äúcoach/organisateur‚Äù qui dit quelle √©tape commence, dans quel ordre, et quoi faire si une √©tape rate.\nIci, les ‚Äúr√®gles m√©tier‚Äù sont comme des √©preuves : elles doivent s‚Äôencha√Æner (s√©quence) et parfois recommencer si erreur (reprocessing).\nAWS Step Functions, c‚Äôest l‚Äôorganisateur du tournoi : il encha√Æne les √©tapes, attend la fin de chacune, et peut relancer une √©tape si elle √©choue.\nIl g√®re aussi les cas ‚Äúsi √ßa rate, alors refais‚Äù sans que tu surveilles tout le temps (peu de maintenance).\nAWS Lambda, c‚Äôest plut√¥t un seul joueur qui fait une petite action, pas l‚Äôorganisateur de tout le tournoi.\nAWS Batch, c‚Äôest pour lancer de gros ‚Äúdevoirs‚Äù de calcul en lot, pas pour g√©rer un encha√Ænement d‚Äô√©tapes avec reprises.\nAWS Glue sert surtout √† pr√©parer/d√©placer des donn√©es, mais l‚Äôoutil ‚Äúchef d‚Äôorchestre‚Äù des √©tapes, c‚Äôest Step Functions.\nDonc la bonne r√©ponse est B : AWS Step Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:39:176e88d408dbdc7f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 39,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is planning to securely manage one-time fixed license keys in AWS. The company's development team needs to access the license keys in automaton scripts that run in Amazon EC2 instances and in AWS CloudFormation stacks.Which solution will meet these requirements MOST cost-effectively?",
      "choices": {
        "A": "Amazon S3 with encrypted files prefixed with ‚Äúconfig‚Äù",
        "B": "AWS Secrets Manager secrets with a tag that is named SecretString",
        "C": "AWS Systems Manager Parameter Store SecureString parameters",
        "D": "CloudFormation NoEcho parameters"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103913-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 26, 2023, 1:53 a.m.",
      "textHash": "176e88d408dbdc7f",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "On veut stocker des cl√©s de licence (valeurs sensibles) et les lire automatiquement depuis des scripts sur des instances EC2 et depuis des mod√®les CloudFormation.\nAWS Systems Manager Parameter Store est un ‚Äúcoffre‚Äù de param√®tres. Le type SecureString chiffre la valeur (avec KMS) et permet de la r√©cup√©rer via API/CLI/SDK, donc parfait pour des scripts.\nCloudFormation peut aussi lire des param√®tres SecureString (r√©f√©rence dynamique), ce qui √©vite d‚Äô√©crire la cl√© en clair dans le template.\nC‚Äôest g√©n√©ralement le plus √©conomique pour des secrets simples et fixes (pas besoin de rotation avanc√©e).\nSecrets Manager (B) est plus cher et surtout utile quand on veut rotation automatique et fonctionnalit√©s d√©di√©es aux secrets.\nS3 (A) n‚Äôest pas adapt√© : il faut g√©rer fichiers, acc√®s, et le risque d‚Äôexposition est plus √©lev√©.\nNoEcho (D) masque l‚Äôaffichage dans CloudFormation, mais ne stocke pas la cl√© de fa√ßon s√©curis√©e et r√©utilisable pour EC2/scripts.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:36:1eed96bcb9533036",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 36,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA development team maintains a web application by using a single AWS CloudFormation template. The template defines web servers and an Amazon RDS database. The team uses the Cloud Formation template to deploy the Cloud Formation stack to different environments.During a recent application deployment, a developer caused the primary development database to be dropped and recreated. The result of this incident was a loss of data. The team needs to avoid accidental database deletion in the future.Which solutions will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Add a CloudFormation Deletion Policy attribute with the Retain value to the database resource.",
        "B": "Update the CloudFormation stack policy to prevent updates to the database.",
        "C": "Modify the database to use a Multi-AZ deployment.",
        "D": "Create a CloudFormation stack set for the web application and database deployments.",
        "E": "Add a Cloud Formation DeletionPolicy attribute with the Retain value to the stack."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103521-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 10:35 p.m.",
      "textHash": "1eed96bcb9533036",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Le probl√®me vient de CloudFormation (un outil AWS qui cr√©e/modifie/supprime des ressources √† partir d‚Äôun mod√®le). Si quelqu‚Äôun met √† jour le mod√®le et que la base RDS (service de base de donn√©es g√©r√©) est remplac√©e, CloudFormation peut supprimer l‚Äôancienne base et en recr√©er une, ce qui efface les donn√©es.\nLa solution la plus directe est d‚Äôajouter sur la ressource RDS l‚Äôattribut DeletionPolicy: Retain (choix A). Cela dit √† CloudFormation : ¬´ m√™me si la stack est supprim√©e ou si la ressource doit √™tre remplac√©e, ne supprime pas la base existante ¬ª.\nUne autre protection utile est une stack policy (choix B) : elle peut bloquer les mises √† jour sur la ressource base de donn√©es, emp√™chant un changement accidentel qui d√©clencherait un remplacement/suppression.\nMulti-AZ (C) am√©liore la disponibilit√©, pas la protection contre une suppression logique.\nStackSets (D) sert √† d√©ployer sur plusieurs comptes/r√©gions, pas √† emp√™cher la suppression.\nMettre Retain sur la stack (E) n‚Äôexiste pas : DeletionPolicy s‚Äôapplique aux ressources, pas √† la stack enti√®re.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un projet de classe: tu as une fiche ‚Äúplan‚Äù qui dit quoi mettre dans la salle (tables = serveurs, cahier de notes = base de donn√©es). Quand tu refais le plan, certains objets peuvent √™tre jet√©s et remplac√©s.**\n\nCloudFormation, c‚Äôest cette fiche ‚Äúplan‚Äù qui construit tout pareil √† chaque fois.\nLe probl√®me: un √©l√®ve a refait le plan et a ‚Äújet√© puis rachet√©‚Äù le cahier de notes (la base), donc toutes les notes ont disparu.\nLa solution A, c‚Äôest coller une √©tiquette sur le cahier: ‚ÄúNE PAS JETER, m√™me si on refait la salle‚Äù.\nDans AWS, cette √©tiquette s‚Äôappelle DeletionPolicy = Retain sur la ressource base de donn√©es.\nR√©sultat: m√™me si quelqu‚Äôun supprime ou recr√©e le reste, la base n‚Äôest pas effac√©e.\nB bloque surtout les modifications, mais le risque principal ici est la suppression/recr√©ation.\nC (Multi-AZ) aide si une salle ferme, pas si quelqu‚Äôun jette le cahier.\nD et E ne prot√®gent pas directement la base contre une suppression accidentelle.\nDonc la bonne r√©ponse est A.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:34:4c87b316cf895b6b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 34,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a template that uses AWS CloudFormation to deploy an application. The application is serverless and uses Amazon API Gateway, Amazon DynamoDB, and AWS Lambda.Which AWS service or tool should the developer use to define serverless resources in YAML?",
      "choices": {
        "A": "CloudFormation serverless intrinsic functions",
        "B": "AWS Elastic Beanstalk",
        "C": "AWS Serverless Application Model (AWS SAM)",
        "D": "AWS Cloud Development Kit (AWS CDK)"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103517-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 10:29 p.m.",
      "textHash": "4c87b316cf895b6b",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:9a00bafb",
      "frExplanation": "Pour d√©ployer une appli ¬´ serverless ¬ª (sans g√©rer de serveurs), on d√©crit l‚Äôinfrastructure dans un fichier YAML.\nCloudFormation est l‚Äôoutil AWS qui lit ce YAML et cr√©e les ressources (API Gateway, Lambda, DynamoDB).\nLe probl√®me : √©crire du CloudFormation ‚Äúpur‚Äù pour du serverless est verbeux (beaucoup de lignes pour une simple fonction Lambda + API).\nAWS SAM (Serverless Application Model) est une extension de CloudFormation sp√©cialement faite pour le serverless.\nAvec SAM, on utilise des types simples comme AWS::Serverless::Function et AWS::Serverless::Api, puis SAM les transforme en ressources CloudFormation compl√®tes.\nDonc SAM permet de d√©finir facilement en YAML les ressources serverless demand√©es.\nLes autres choix : Elastic Beanstalk vise surtout des applis avec serveurs; les ‚Äúintrinsic functions‚Äù CloudFormation ne sont pas un mod√®le serverless; CDK est du code (TypeScript/Python) plut√¥t que du YAML.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu dois organiser un tournoi de jeux vid√©o au lyc√©e. Tu veux une ‚Äúfiche d‚Äôorganisation‚Äù √©crite en liste (YAML), qui d√©crit tout : l‚Äôaccueil des joueurs, le tableau des scores, et les arbitres automatiques.**\n\nConcept : CloudFormation, c‚Äôest la fiche officielle pour ‚Äúconstruire‚Äù ton √©v√©nement automatiquement. En ‚Äúserverless‚Äù, tu n‚Äôinstalles pas de PC/serveurs : tu d√©cris juste les r√¥les. API Gateway = l‚Äôaccueil/porte d‚Äôentr√©e, DynamoDB = le cahier des scores, Lambda = les arbitres qui ex√©cutent des actions. Pour √©crire √ßa facilement en YAML, tu utilises AWS SAM : c‚Äôest un mod√®le sp√©cial, comme une fiche simplifi√©e faite expr√®s pour les tournois ‚Äúautomatiques‚Äù. Donc C est bon car AWS SAM te permet de d√©finir les ressources serverless en YAML sans tout d√©tailler √† la main. A n‚Äôest pas un vrai outil complet, B (Elastic Beanstalk) c‚Äôest plut√¥t pour des applis avec serveurs g√©r√©s, D (CDK) c‚Äôest √©crire en code (pas juste YAML).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:31:a1936c8d67cdc88f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 31,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket the developer must encrypt these objects at rest by using server-side encryption with Amazon S3 managed keys (SSE-S3).Which solution will meet this requirement?",
      "choices": {
        "A": "Create an AWS Key Management Service (AWS KMS) key. Assign the KMS key to the S3 bucket.",
        "B": "Set the x-amz-server-side-encryption header when invoking the PutObject API operation.",
        "C": "Provide the encryption key in the HTTP header of every request.",
        "D": "Apply TLS to encrypt the traffic to the S3 bucket."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103513-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 10:20 p.m.",
      "textHash": "a1936c8d67cdc88f",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici on veut chiffrer les objets ¬´ au repos ¬ª dans Amazon S3 (stockage de fichiers). ¬´ SSE-S3 ¬ª signifie que S3 chiffre automatiquement les objets avec des cl√©s g√©r√©es par S3, sans que l‚Äôapplication ne g√®re de cl√©.\nQuand on envoie un fichier avec l‚ÄôAPI PutObject, on peut demander ce chiffrement en ajoutant un en-t√™te HTTP sp√©cifique.\nL‚Äôen-t√™te x-amz-server-side-encryption indique √† S3 d‚Äôactiver le chiffrement c√¥t√© serveur (par exemple la valeur AES256 pour SSE-S3).\nDonc la bonne solution est de d√©finir cet en-t√™te lors de l‚Äôappel PutObject.\nA est faux car cr√©er une cl√© KMS correspond √† SSE-KMS (cl√©s g√©r√©es par KMS), pas √† SSE-S3.\nC est faux car SSE-S3 ne demande pas de fournir une cl√© dans les requ√™tes (ce serait plut√¥t SSE-C, cl√© fournie par le client).\nD est faux car TLS chiffre seulement le trafic en transit, pas le stockage ¬´ au repos ¬ª dans S3.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un casier au lyc√©e (le bucket S3) o√π tu ranges des feuilles (les fichiers). Tu veux que chaque feuille soit mise dans une enveloppe scell√©e automatiquement par le lyc√©e (chiffrement ‚Äúau repos‚Äù g√©r√© par S3).**\n\nConcept : ‚Äúchiffrer au repos‚Äù = une fois la feuille dans le casier, elle est stock√©e en version illisible, sauf pour ceux qui ont le droit. SSE-S3 = c‚Äôest le lyc√©e qui fournit et g√®re les enveloppes et les cl√©s, pas toi.\nPour √™tre s√ªr que chaque nouvelle feuille est bien mise dans l‚Äôenveloppe SSE-S3, tu dois le demander au moment o√π tu la d√©poses.\nB fait exactement √ßa : le header x-amz-server-side-encryption, c‚Äôest comme √©crire sur la feuille ‚Äúmerci de la sceller avec l‚Äôenveloppe du lyc√©e‚Äù.\nA parle d‚Äôune cl√© sp√©ciale (KMS) : ce n‚Äôest plus ‚Äúenveloppe g√©r√©e par S3‚Äù mais une autre gestion de cl√©s.\nC te demande d‚Äôapporter ta propre cl√© √† chaque fois : ce n‚Äôest pas ‚Äúg√©r√© par S3‚Äù.\nD (TLS) prot√®ge le trajet dans le couloir, pas la feuille une fois dans le casier.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:29:d6626b7ed6a8d11d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 29,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application is processing clickstream data using Amazon Kinesis. The clickstream data feed into Kinesis experiences periodic spikes. The PutRecords API call occasionally fails and the logs show that the failed call returns the response shown below:Which techniques will help mitigate this exception? (Choose two.)",
      "choices": {
        "A": "Implement retries with exponential backoff.",
        "B": "Use a PutRecord API instead of PutRecords.",
        "C": "Reduce the frequency and/or size of the requests.",
        "D": "Use Amazon SNS instead of Kinesis.",
        "E": "Reduce the number of KCL consumers."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102903-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 17, 2023, 9:50 a.m.",
      "textHash": "d6626b7ed6a8d11d",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Amazon Kinesis sert √† ing√©rer des flux de donn√©es (ex: clics) en temps r√©el. Quand il y a des pics, l‚Äôapplication peut envoyer trop de donn√©es trop vite et Kinesis ‚Äúthrottle‚Äù (limite) les requ√™tes : certaines √©critures √©chouent temporairement (souvent avec un message de d√©bit d√©pass√©). La bonne pratique est de r√©essayer, car l‚Äôerreur est g√©n√©ralement passag√®re. Les ‚Äúretries avec exponential backoff‚Äù (A) attendent un peu, puis un peu plus longtemps √† chaque tentative, ce qui r√©duit la pression sur Kinesis et augmente les chances de succ√®s. Une autre technique utile est de diminuer la charge envoy√©e (C) : r√©duire la fr√©quence des appels et/ou la taille des lots PutRecords pour rester sous les limites de d√©bit par shard. Passer √† PutRecord (B) ne r√©sout pas le throttling, √ßa peut m√™me augmenter le nombre d‚Äôappels. Changer de service (D) n‚Äôest pas une mitigation directe. R√©duire les consommateurs KCL (E) concerne la lecture, pas l‚Äô√©criture.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : √† certains moments (fin de cours), tout le monde arrive d‚Äôun coup et la file explose.**\n\nKinesis, c‚Äôest comme la cantine qui re√ßoit des ‚Äúplateaux‚Äù (les clics) en continu. Quand il y a un pic, le guichet est satur√© et refuse parfois des plateaux : c‚Äôest l‚Äôerreur. La bonne technique est A : ‚Äúr√©essayer avec un d√©lai qui grandit‚Äù (exponential backoff). Comme √† la cantine : si on te dit ‚Äúreviens plus tard‚Äù, tu ne retentes pas 50 fois tout de suite ; tu attends un peu, puis un peu plus, jusqu‚Äô√† ce que la file baisse. √áa √©vite d‚Äôaggraver l‚Äôembouteillage et augmente les chances que √ßa passe au prochain essai. B ne r√®gle pas la foule (changer la fa√ßon de poser le plateau ne cr√©e pas plus de place). C peut aider mais ce n‚Äôest pas la technique principale demand√©e ici. D et E ne traitent pas le probl√®me de ‚Äútrop de monde au guichet‚Äù au moment du d√©p√¥t.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:28:2c6ea9a92c3fc5eb",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 28,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company hosts a client-side web application for one of its subsidiaries on Amazon S3. The web application can be accessed through Amazon CloudFront from https://www.example.com. After a successful rollout, the company wants to host three more client-side web applications for its remaining subsidiaries on three separate S3 buckets.To achieve this goal, a developer moves all the common JavaScript files and web fonts to a central S3 bucket that serves the web applications. However, during testing, the developer notices that the browser blocks the JavaScript files and web fonts.What should the developer do to prevent the browser from blocking the JavaScript files and web fonts?",
      "choices": {
        "A": "Create four access points that allow access to the central S3 bucket. Assign an access point to each web application bucket.",
        "B": "Create a bucket policy that allows access to the central S3 bucket. Attach the bucket policy to the central S3 bucket",
        "C": "Create a cross-origin resource sharing (CORS) configuration that allows access to the central S3 bucket. Add the CORS configuration to the central S3 bucket.",
        "D": "Create a Content-MD5 header that provides a message integrity check for the central S3 bucket. Insert the Content-MD5 header for each web application request."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/102902-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 17, 2023, 9:49 a.m.",
      "textHash": "2c6ea9a92c3fc5eb",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Le navigateur bloque ces fichiers car ils sont charg√©s depuis un autre ¬´ origine ¬ª (domaine/bucket) que la page web. C‚Äôest une r√®gle de s√©curit√© du web appel√©e Same-Origin Policy.\nIci, chaque application est servie depuis son propre bucket S3 (via CloudFront), mais les fichiers JavaScript et polices viennent d‚Äôun bucket S3 central : c‚Äôest donc du cross-origin.\nPour autoriser explicitement ce partage, il faut configurer CORS (Cross-Origin Resource Sharing) sur le bucket qui h√©berge les ressources partag√©es (le bucket central).\nCORS permet de dire : ¬´ j‚Äôautorise les sites X, Y, Z √† t√©l√©charger mes fichiers ¬ª, en pr√©cisant les origines autoris√©es, les m√©thodes (GET) et √©ventuellement les en-t√™tes.\nUne bucket policy (B) g√®re l‚Äôautorisation c√¥t√© AWS (qui peut lire S3), mais ne suffit pas √† lever le blocage du navigateur.\nLes access points (A) ne r√©solvent pas la r√®gle de s√©curit√© du navigateur, et Content-MD5 (D) sert √† v√©rifier l‚Äôint√©grit√©, pas √† autoriser le cross-origin.\nDonc la bonne action est d‚Äôajouter une configuration CORS sur le bucket S3 central.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un lyc√©e avec 4 classes (les 4 sites web). Chaque classe a sa salle (son bucket S3). Les profs mettent les polycopi√©s communs (JS et polices) dans une seule salle de photocopies centrale (bucket central).**\n\nConcept : quand une classe veut utiliser un polycopi√© venant d‚Äôune autre salle, le surveillant dit souvent ‚Äúinterdit‚Äù par d√©faut (le navigateur bloque ce qui vient d‚Äôun autre ‚Äúendroit‚Äù).\nIci, tes 3 nouveaux sites essaient de charger des fichiers depuis le bucket central : c‚Äôest comme aller chercher des feuilles dans une autre salle, donc blocage.\nLa solution, c‚Äôest d‚Äôafficher une autorisation officielle sur la salle de photocopies : ‚Äúles √©l√®ves de telles classes ont le droit de prendre ces feuilles‚Äù.\nSur le web, cette autorisation s‚Äôappelle CORS : tu listes quels sites (origines) ont le droit de demander les fichiers.\nDonc on ajoute une configuration CORS sur le bucket central pour autoriser https://www.example.com (et les autres domaines des filiales).\nA et B parlent surtout de ‚Äúqui peut entrer‚Äù c√¥t√© stockage, mais ne disent pas au navigateur qu‚Äôil a le droit.\nD v√©rifie juste que le fichier n‚Äôest pas corrompu, √ßa ne d√©bloque pas l‚Äôacc√®s.\nDonc la bonne r√©ponse est C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:503:6b53898f7b6e2d74",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 503,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has an AWS Lambda function that needs to access an Amazon DynamoDB table named DailyOrders. The Lambda function must be able to perform read operations on the table. The Lambda function must not be able to perform write operations on the table.The developer needs to create an IAM policy to associate with the Lambda function's execution role.Which IAM policy statement will meet these requirements?",
      "choices": {},
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152775-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 10, 2024, 4:31 a.m.",
      "textHash": "6b53898f7b6e2d74",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, on parle de permissions IAM (Identity and Access Management) : c‚Äôest le syst√®me AWS qui d√©cide ce qu‚Äôun service a le droit de faire.\nUne fonction AWS Lambda s‚Äôex√©cute avec un ¬´ r√¥le d‚Äôex√©cution ¬ª (execution role) : ce r√¥le porte une politique IAM qui autorise ou refuse des actions.\nAmazon DynamoDB est une base de donn√©es NoSQL. La table s‚Äôappelle DailyOrders.\nLe besoin : autoriser uniquement la lecture (read) et interdire l‚Äô√©criture (write).\nLa bonne politique doit donc contenir des actions DynamoDB de lecture comme dynamodb:GetItem, dynamodb:Query, dynamodb:Scan (et √©ventuellement dynamodb:BatchGetItem) sur la ressource de la table DailyOrders.\nElle ne doit pas inclure des actions d‚Äô√©criture comme dynamodb:PutItem, dynamodb:UpdateItem, dynamodb:DeleteItem, BatchWriteItem.\nLe champ Resource doit viser l‚ÄôARN de la table DailyOrders (pas \"*\") pour limiter l‚Äôacc√®s √† cette table seulement.\nDonc la bonne r√©ponse (D) est celle qui fait Allow sur les actions de lecture DynamoDB et uniquement sur l‚ÄôARN de DailyOrders, sans aucune action d‚Äô√©criture.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e : le cahier ¬´ DailyOrders ¬ª est un registre pos√© sur une table. Ton d√©l√©gu√© de classe (la fonction Lambda) doit pouvoir LIRE ce registre pour v√©rifier les commandes du jour, mais il n‚Äôa pas le droit d‚Äô√©crire dedans ni de le modifier.**\n\nConcept : une ‚Äúpermission‚Äù (IAM policy) c‚Äôest comme un badge avec des r√®gles : ce que tu peux faire (actions) et sur quel objet (ressource). Ici, on veut un badge ‚Äúlecture seulement‚Äù sur le registre DailyOrders. Pourquoi D : parce que D autorise uniquement des actions de lecture sur la table (ex: lire/consulter), et ne donne aucune action d‚Äô√©criture (ex: ajouter/modifier/supprimer). Donc le d√©l√©gu√© peut regarder le cahier, mais ne peut pas y inscrire de nouvelles lignes ni en effacer. C‚Äôest exactement la consigne : lire oui, √©crire non, et seulement sur DailyOrders.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:20:8adb5beb63d2e0b3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 20,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer wants to store information about movies. Each movie has a title, release year, and genre. The movie information also can include additional properties about the cast and production crew. This additional information is inconsistent across movies. For example, one movie might have an assistant director, and another movie might have an animal trainer.The developer needs to implement a solution to support the following use cases:For a given title and release year, get all details about the movie that has that title and release year.For a given title, get all details about all movies that have that title.For a given genre, get all details about all movies in that genre.Which data store configuration will meet these requirements?",
      "choices": {
        "A": "Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the title as the partition key and the release year as the sort key. Create a global secondary index that uses the genre as the partition key and the title as the sort key.",
        "B": "Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the genre as the partition key and the release year as the sort key. Create a global secondary index that uses the title as the partition key.",
        "C": "On an Amazon RDS DB instance, create a table that contains columns for title, release year, and genre. Configure the title as the primary key.",
        "D": "On an Amazon RDS DB instance, create a table where the primary key is the title and all other data is encoded into JSON format as one additional column."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103468-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 1:30 p.m.",
      "textHash": "8adb5beb63d2e0b3",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "Ici, les donn√©es d‚Äôun film ont des champs fixes (titre, ann√©e, genre) mais aussi des champs ‚Äúvariables‚Äù (assistant r√©alisateur, dresseur d‚Äôanimaux, etc.). DynamoDB est une base NoSQL qui accepte naturellement des attributs diff√©rents selon les √©l√©ments, sans sch√©ma rigide.\nAvec une cl√© primaire compos√©e (partition key + sort key), on peut retrouver rapidement un film unique : partition key = titre, sort key = ann√©e. Donc ‚Äútitre + ann√©e‚Äù donne exactement le bon film.\nAvec la m√™me table, on peut aussi r√©cup√©rer tous les films d‚Äôun m√™me titre : on fait une requ√™te sur la partition key (titre) et on obtient toutes les ann√©es associ√©es.\nPour chercher par genre, la cl√© primaire ne suffit pas (car elle est bas√©e sur le titre). On ajoute donc un index secondaire global (GSI) avec partition key = genre.\nDans ce GSI, mettre le titre en sort key permet de trier/filtrer les r√©sultats par titre si besoin, tout en r√©cup√©rant tous les films d‚Äôun genre.\nLes options RDS (SQL) imposent un sch√©ma plus strict ou du JSON ‚Äúfourre-tout‚Äù moins pratique pour requ√™ter efficacement, et la cl√© primaire ‚Äútitre seul‚Äù ne g√®re pas plusieurs films avec le m√™me titre.\nDonc la configuration A couvre efficacement les 3 cas d‚Äôusage avec DynamoDB + GSI.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une biblioth√®que de films au CDI. Chaque fiche a toujours : titre, ann√©e, genre. Mais parfois il y a des infos en plus (dresseur d‚Äôanimaux, assistant r√©al‚Ä¶), et √ßa change selon le film.**\n\nIci, il faut un ‚Äúclasseur‚Äù qui accepte des fiches avec des champs variables : c‚Äôest DynamoDB (tu peux ajouter des infos en plus sans refaire toutes les fiches). Avec A, le tiroir principal est rang√© par Titre (partition) puis par Ann√©e (tri) : donc avec (titre+ann√©e) tu retrouves exactement 1 film, et avec juste le titre tu vois tous les films de ce titre (toutes les ann√©es). Ensuite, on ajoute un deuxi√®me plan de rangement (index) rang√© par Genre puis Titre : comme une √©tag√®re ‚ÄúAction‚Äù, ‚ÄúCom√©die‚Äù‚Ä¶ qui te montre tous les films d‚Äôun genre. B range d‚Äôabord par genre, donc chercher ‚Äútous les films d‚Äôun titre‚Äù devient moins direct. C et D (RDS) ressemblent √† un tableau tr√®s strict : les colonnes fixes ou du JSON bricol√©, moins adapt√© aux infos ‚Äúpas toujours les m√™mes‚Äù. Donc A coche les 3 recherches demand√©es.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:18:d58197f34178b777",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 18,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building a serverless application on AWS. The application uses an AWS Lambda function to process customer orders 24 hours a day, 7 days a week. The Lambda function calls an external vendor's HTTP API to process payments.During load tests, a developer discovers that the external vendor payment processing API occasionally times out and returns errors. The company expects that some payment processing API calls will return errors.The company wants the support team to receive notifications in near real time only when the payment processing external API error rate exceed 5% of the total number of transactions in an hour. Developers need to use an existing Amazon Simple Notification Service (Amazon SNS) topic that is configured to notify the support team.Which solution will meet these requirements?",
      "choices": {
        "A": "Write the results of payment processing API calls to Amazon CloudWatch. Use Amazon CloudWatch Logs Insights to query the CloudWatch logs. Schedule the Lambda function to check the CloudWatch logs and notify the existing SNS topic.",
        "B": "Publish custom metrics to CloudWatch that record the failures of the external payment processing API calls. Configure a CloudWatch alarm to notify the existing SNS topic when error rate exceeds the specified rate.",
        "C": "Publish the results of the external payment processing API calls to a new Amazon SNS topic. Subscribe the support team members to the new SNS topic.",
        "D": "Write the results of the external payment processing API calls to Amazon S3. Schedule an Amazon Athena query to run at regular intervals. Configure Athena to send notifications to the existing SNS topic when the error rate exceeds the specified rate."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103466-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 12:33 p.m.",
      "textHash": "d58197f34178b777",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, on veut alerter l‚Äô√©quipe support seulement si le taux d‚Äôerreurs de l‚ÄôAPI de paiement d√©passe 5% sur une fen√™tre d‚Äô1 heure, et le faire presque en temps r√©el.\nAWS Lambda ex√©cute du code ‚Äúsans serveur‚Äù et peut appeler une API externe, mais cette API peut parfois √©chouer (timeouts, erreurs).\nAmazon CloudWatch sert √† collecter des m√©triques (chiffres) et √† d√©clencher des alarmes automatiquement.\nLa bonne approche est donc d‚Äôenvoyer une m√©trique personnalis√©e dans CloudWatch : par exemple ‚Äúnombre total d‚Äôappels‚Äù et ‚Äúnombre d‚Äô√©checs‚Äù, ou directement un pourcentage d‚Äôerreurs.\nEnsuite, on cr√©e une alarme CloudWatch qui calcule/compare le taux d‚Äôerreur sur 1 heure et se d√©clenche si > 5%.\nQuand l‚Äôalarme se d√©clenche, CloudWatch peut publier un message vers un topic Amazon SNS existant, qui notifie le support.\nLes autres options (Logs Insights, S3/Athena) sont plus lentes, plus complexes et bas√©es sur des requ√™tes planifi√©es, donc moins ‚Äúnear real time‚Äù.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : chaque √©l√®ve qui passe √† la caisse, c‚Äôest une commande. Parfois, le terminal de paiement bug et refuse. Le CPE veut √™tre pr√©venu vite seulement si, sur 1 heure, plus de 5% des paiements √©chouent.**\n\nConcept : il faut un ‚Äúcompteur officiel‚Äù qui compte total des passages + nombre d‚Äô√©checs, puis une ‚Äúalarme‚Äù qui se d√©clenche si le pourcentage d‚Äô√©checs d√©passe 5% en 1 heure. Sur AWS, ce compteur s‚Äôappelle une m√©trique dans CloudWatch (tableau de scores), et l‚Äôalarme s‚Äôappelle un CloudWatch Alarm (surveillance). La Lambda (le caissier automatique) envoie juste un point au compteur √† chaque √©chec. CloudWatch calcule le taux d‚Äôerreur sur 1 heure et, si >5%, d√©clenche l‚Äôalarme. L‚Äôalarme envoie alors un message au SNS existant (le groupe WhatsApp du support). Les autres choix sont comme relire des tickets ou faire des requ√™tes planifi√©es : trop lent et pas ‚Äúpresque en temps r√©el‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:16:0f092f4a7428db72",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 16,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has written an AWS Lambda function. The function is CPU-bound. The developer wants to ensure that the function returns responses quickly.How can the developer improve the function's performance?",
      "choices": {
        "A": "Increase the function's CPU core count.",
        "B": "Increase the function's memory.",
        "C": "Increase the function's reserved concurrency.",
        "D": "Increase the function's timeout."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/103444-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 21, 2023, 7:20 a.m.",
      "textHash": "0f092f4a7428db72",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:1d3a074d",
      "frExplanation": "AWS Lambda ex√©cute votre code sans serveur √† g√©rer : vous choisissez surtout la m√©moire (en Mo/Go) et un d√©lai max (timeout).\nUne fonction ¬´ CPU-bound ¬ª est limit√©e par la puissance de calcul (processeur), pas par le r√©seau ou l‚Äôacc√®s disque.\nDans Lambda, la quantit√© de CPU allou√©e augmente automatiquement quand vous augmentez la m√©moire : plus de m√©moire = plus de CPU (et souvent plus de d√©bit r√©seau).\nDonc, pour r√©pondre plus vite, il faut augmenter la m√©moire afin d‚Äôobtenir plus de puissance CPU et r√©duire le temps d‚Äôex√©cution.\nOn ne peut pas choisir directement un nombre de c≈ìurs CPU (A) dans Lambda.\nLa concurrence r√©serv√©e (C) sert √† traiter plus d‚Äôappels en parall√®le, pas √† acc√©l√©rer un seul appel.\nLe timeout (D) ne rend pas la fonction plus rapide : il permet juste d‚Äôattendre plus longtemps avant d‚Äôarr√™ter l‚Äôex√©cution.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un √©l√®ve qui doit faire un gros calcul de maths pendant un contr√¥le, et tu veux qu‚Äôil finisse vite.**\n\nUne fonction AWS Lambda, c‚Äôest comme cet √©l√®ve : elle fait une t√¢che puis rend un r√©sultat.\n¬´ CPU-bound ¬ª veut dire que le probl√®me, c‚Äôest la vitesse de calcul (le cerveau), pas l‚Äôattente (comme Internet).\nDans Lambda, quand tu donnes plus de ¬´ m√©moire ¬ª, c‚Äôest comme donner √† l‚Äô√©l√®ve un meilleur cerveau + plus de place pour travailler : AWS lui donne aussi plus de puissance de calcul.\nDonc augmenter la m√©moire (B) acc√©l√®re les calculs et la r√©ponse arrive plus vite.\nA est faux : tu ne choisis pas directement ‚Äúplus de c≈ìurs CPU‚Äù dans Lambda.\nC aide surtout quand beaucoup d‚Äô√©l√®ves travaillent en m√™me temps, pas quand un √©l√®ve est lent.\nD laisse juste plus de temps avant d‚Äôabandonner, √ßa ne rend pas plus rapide.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:528:d9bbf7a7366cf8e9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 528,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an ecommerce application that uses multiple AWS Lambda functions. Each function performs a specific step in a customer order workflow, such as order processing and inventory management.The developer must ensure that the Lambda functions run in a specific order.Which solution will meet this requirement with the LEAST operational overhead?",
      "choices": {
        "A": "Configure an Amazon Simple Queue Service (Amazon SQS) queue to contain messages about each step a function must perform. Configure the Lambda functions to run sequentially based on the order of messages in the SQS queue.",
        "B": "Configure an Amazon Simple Notification Service (Amazon SNS) topic to contain notifications about each step a function must perform. Subscribe the Lambda functions to the SNS topic. Use subscription filters based on the step each function must perform.",
        "C": "Configure an AWS Step Functions state machine to invoke the Lambda functions in a specific order.",
        "D": "Configure Amazon EventBridge Scheduler schedules to invoke the Lambda functions in a specific order."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152779-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 10, 2024, 5:02 a.m.",
      "textHash": "d9bbf7a7366cf8e9",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, il faut encha√Æner plusieurs fonctions AWS Lambda (du code qui s‚Äôex√©cute ‚Äú√† la demande‚Äù sans serveur) dans un ordre pr√©cis pour traiter une commande.\nAWS Step Functions est un service fait exactement pour √ßa : il orchestre un ‚Äúworkflow‚Äù (machine d‚Äô√©tats) et appelle chaque Lambda √©tape par √©tape, dans l‚Äôordre d√©fini.\nIl g√®re aussi nativement les transitions, les erreurs, les retries et le suivi de l‚Äôex√©cution, sans que vous ayez √† coder toute la logique de coordination.\nSQS (file de messages) ne garantit pas facilement un encha√Ænement strict entre plusieurs consommateurs et demande de g√©rer la logique de s√©quencement/ack/retry.\nSNS (pub/sub) diffuse √† plusieurs abonn√©s et ne sert pas √† imposer un ordre d‚Äôex√©cution entre fonctions.\nEventBridge Scheduler sert √† d√©clencher √† des horaires, pas √† orchestrer un flux conditionnel ‚Äú√©tape 1 puis √©tape 2 puis √©tape 3‚Äù.\nDonc la solution avec le moins d‚Äôoverhead op√©rationnel pour imposer l‚Äôordre est Step Functions.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une commande √† la cantine : d‚Äôabord on encaisse, puis on pr√©pare le plateau, puis on v√©rifie le stock, puis on remet au client. Il faut un ‚Äúchef de file‚Äù qui dit qui fait quoi et dans quel ordre.**\n\nLes fonctions Lambda, c‚Äôest comme des √©l√®ves qui font chacun une t√¢che pr√©cise. Le probl√®me : sans chef, ils peuvent agir dans le d√©sordre. AWS Step Functions (C), c‚Äôest le chef d‚Äôorchestre : tu √©cris un plan avec des √©tapes 1‚Üí2‚Üí3, et il appelle chaque √©l√®ve dans le bon ordre, automatiquement. √áa demande peu d‚Äôeffort au quotidien : pas besoin de bricoler des files de messages (SQS) ou des annonces √† tout le monde (SNS) pour essayer de garder l‚Äôordre. Un planning par horaires (Scheduler) ne garantit pas ‚Äú√©tape suivante seulement quand la pr√©c√©dente est finie‚Äù. Donc C est la meilleure r√©ponse pour imposer l‚Äôordre avec le moins de gestion.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:115:68d49dca9a76d618",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 115,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to build an AWS CloudFormation template that self-populates the AWS Region variable that deploys the CloudFormation template.What is the MOST operationally efficient way to determine the Region in which the template is being deployed?",
      "choices": {
        "A": "Use the AWS::Region pseudo parameter.",
        "B": "Require the Region as a CloudFormation parameter.",
        "C": "Find the Region from the AWS::StackId pseudo parameter by using the Fn::Split intrinsic function.",
        "D": "Dynamically import the Region by referencing the relevant parameter in AWS Systems Manager Parameter Store."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107041-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 9:55 p.m.",
      "textHash": "68d49dca9a76d618",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "CloudFormation est un service AWS qui d√©ploie des ressources (serveurs, bases, etc.) √† partir d‚Äôun fichier ‚Äútemplate‚Äù.\nQuand on d√©ploie, on choisit une R√©gion AWS (ex: eu-west-1, us-east-1) : c‚Äôest l‚Äôendroit g√©ographique o√π les ressources seront cr√©√©es.\nLe besoin est que le template se ‚Äúremplisse tout seul‚Äù avec la R√©gion, sans demander d‚Äôinformation √† l‚Äôutilisateur.\nCloudFormation fournit des ‚Äúpseudo param√®tres‚Äù int√©gr√©s, dont AWS::Region, qui renvoie automatiquement la R√©gion du d√©ploiement.\nC‚Äôest le plus efficace op√©rationnellement : aucune saisie, aucun stockage externe, et √ßa marche partout.\nDemander la R√©gion en param√®tre (B) ajoute du risque d‚Äôerreur et du travail.\nExtraire la R√©gion depuis StackId (C) est compliqu√© et fragile.\nUtiliser Parameter Store (D) n√©cessite une configuration pr√©alable et n‚Äôest pas n√©cessaire.\nDonc la meilleure r√©ponse est A : AWS::Region.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu envoies une affiche de tournoi √† plusieurs lyc√©es. Sur l‚Äôaffiche, tu veux √©crire automatiquement le nom du lyc√©e o√π elle est affich√©e, sans demander √† quelqu‚Äôun de le remplir √† la main.**\n\nLe ‚ÄúRegion‚Äù AWS, c‚Äôest comme le lyc√©e/la ville o√π se d√©roule l‚Äô√©v√©nement (Paris, Lyon‚Ä¶).\nUn template CloudFormation, c‚Äôest l‚Äôaffiche qui doit s‚Äôadapter selon l‚Äôendroit o√π on la colle.\nLa m√©thode la plus simple est d‚Äôutiliser l‚Äôinfo ‚Äúd√©j√† imprim√©e‚Äù par le syst√®me : AWS::Region.\nC‚Äôest comme une √©tiquette automatique qui dit : ‚ÄúTu es au lyc√©e X‚Äù, donc l‚Äôaffiche se remplit seule.\nB demanderait √† quelqu‚Äôun d‚Äô√©crire le lyc√©e √† la main √† chaque fois (risque d‚Äôerreur, pas efficace).\nC essaie de deviner le lyc√©e en d√©coupant un long num√©ro d‚Äôaffiche (compliqu√© et fragile).\nD va chercher l‚Äôinfo dans un autre casier/registre externe (plus d‚Äô√©tapes, moins efficace).\nDonc A est le plus op√©rationnel : l‚Äôaffiche lit directement la ville/lyc√©e o√π elle est d√©ploy√©e.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:114:027fc011d95d63d2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 114,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is creating an application that processes .csv files from Amazon S3. A developer has created an S3 bucket. The developer has also created an AWS Lambda function to process the .csv files from the S3 bucket.Which combination of steps will invoke the Lambda function when a .csv file is uploaded to Amazon S3? (Choose two.)",
      "choices": {
        "A": "Create an Amazon EventBridge rule. Configure the rule with a pattern to match the S3 object created event.",
        "B": "Schedule an Amazon EventBridge rule to run a new Lambda function to scan the S3 bucket.",
        "C": "Add a trigger to the existing Lambda function. Set the trigger type to EventBridge. Select the Amazon EventBridge rule.",
        "D": "Create a new Lambda function to scan the S3 bucket for recently added S3 objects.",
        "E": "Add S3 Lifecycle rules to invoke the existing Lambda function."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107039-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 9:52 p.m.",
      "textHash": "027fc011d95d63d2",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Pour d√©clencher automatiquement une fonction AWS Lambda quand un fichier arrive dans un bucket S3, il faut un m√©canisme d‚Äô√©v√©nement. S3 peut √©mettre un √©v√©nement ‚ÄúObject Created‚Äù (objet cr√©√©) √† chaque upload. Amazon EventBridge est un service qui re√ßoit des √©v√©nements AWS et peut d√©clencher des cibles comme Lambda. Donc on cr√©e une r√®gle EventBridge (rule) avec un ‚Äúpattern‚Äù qui filtre l‚Äô√©v√©nement S3 de cr√©ation d‚Äôobjet, et on peut aussi filtrer sur l‚Äôextension .csv. Ensuite, il faut relier cette r√®gle √† la fonction Lambda (la r√®gle a une ‚Äútarget‚Äù Lambda, ou on ajoute le trigger EventBridge sur la Lambda en s√©lectionnant la r√®gle). Les options de scan planifi√© (B, D) ne sont pas √©v√©nementielles et sont moins fiables/plus co√ªteuses. Les r√®gles Lifecycle S3 (E) servent √† g√©rer la dur√©e de vie des objets (archivage/suppression), pas √† invoquer Lambda.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le casier de devoirs de ta classe : d√®s qu‚Äôun √©l√®ve d√©pose une feuille, tu veux que le prof soit pr√©venu tout de suite.**\n\nS3, c‚Äôest le casier o√π on d√©pose les fichiers .csv. Lambda, c‚Äôest le prof qui corrige automatiquement d√®s qu‚Äôil est pr√©venu. Il faut donc une ‚Äúsonnette‚Äù qui se d√©clenche quand un nouveau fichier arrive. A = cr√©er une r√®gle EventBridge, c‚Äôest installer cette sonnette et lui dire : ‚Äúquand un objet est cr√©√© dans S3, je veux un √©v√©nement‚Äù. Ensuite, cet √©v√©nement peut appeler la Lambda (le prof) imm√©diatement. B et D, c‚Äôest comme demander √† un surveillant de v√©rifier le casier toutes les heures : lent et inutile. E (Lifecycle), c‚Äôest plut√¥t pour ranger/archiver des vieux devoirs, pas pour pr√©venir √† l‚Äôinstant o√π on d√©pose. Donc la bonne √©tape cl√© est A : la r√®gle qui d√©tecte l‚Äôupload.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:113:8622288a9689eabc",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 113,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on an ecommerce website. The developer wants to review server logs without logging in to each of the application servers individually. The website runs on multiple Amazon EC2 instances, is written in Python, and needs to be highly available.How can the developer update the application to meet these requirements with MINIMUM changes?",
      "choices": {
        "A": "Rewrite the application to be cloud native and to run on AWS Lambda, where the logs can be reviewed in Amazon CloudWatch.",
        "B": "Set up centralized logging by using Amazon OpenSearch Service, Logstash, and OpenSearch Dashboards.",
        "C": "Scale down the application to one larger EC2 instance where only one instance is recording logs.",
        "D": "Install the unified Amazon CloudWatch agent on the EC2 instances. Configure the agent to push the application logs to CloudWatch."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107038-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 9:46 p.m.",
      "textHash": "8622288a9689eabc",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Le besoin est de consulter les logs (journaux) des serveurs sans se connecter √† chaque machine EC2 une par une, tout en gardant plusieurs instances pour la haute disponibilit√©.\nAmazon EC2 = des serveurs virtuels. Quand il y en a plusieurs, les logs sont dispers√©s sur chaque serveur.\nAmazon CloudWatch = service AWS qui centralise m√©triques et logs, consultables dans une console unique.\nLa solution avec le minimum de changements est d‚Äôinstaller l‚Äôagent CloudWatch (un petit programme) sur chaque EC2.\nOn configure l‚Äôagent pour envoyer automatiquement les fichiers de logs de l‚Äôapplication Python vers CloudWatch Logs.\nAinsi, tous les logs sont regroup√©s au m√™me endroit, recherchables et consultables sans SSH sur les serveurs.\nA demanderait une r√©√©criture compl√®te vers Lambda (gros changement). B ajoute une stack complexe (OpenSearch/Logstash). C casse la haute disponibilit√© en n‚Äôayant qu‚Äôune seule instance.\nDonc D r√©pond exactement au besoin avec le moins d‚Äôimpact sur l‚Äôapplication.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un lyc√©e avec plusieurs salles o√π se passent des contr√¥les en m√™me temps. Chaque salle √©crit un ‚Äúrapport‚Äù (les logs) sur ce qui s‚Äôest pass√©. Tu ne veux pas courir dans chaque salle pour lire les rapports : tu veux une bo√Æte aux lettres centrale au secr√©tariat.**\n\nConcept : les ‚Äúlogs‚Äù, c‚Äôest comme le cahier de bord qui note les erreurs et √©v√©nements. Avec plusieurs serveurs (plusieurs salles), lire les logs un par un est p√©nible. La bonne id√©e est de tout envoyer au m√™me endroit (la bo√Æte aux lettres centrale). R√©ponse D : on installe un ‚Äúfacteur‚Äù identique (CloudWatch agent) dans chaque salle/serveur EC2, et il d√©pose automatiquement les rapports dans CloudWatch (le tableau central). √áa demande peu de changements : on ne r√©√©crit pas le site, on ajoute juste l‚Äôagent. A oblige √† refaire tout le site. B ajoute une grosse usine √† gaz. C casse la haute disponibilit√© : une seule salle, si elle ferme, tout s‚Äôarr√™te.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:112:b6ec4ab7ca8c2358",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 112,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA team is developing an application that is deployed on Amazon EC2 instances. During testing, the team receives an error. The EC2 instances are unable to access an Amazon S3 bucket.Which steps should the team take to troubleshoot this issue? (Choose two.)",
      "choices": {
        "A": "Check whether the policy that is assigned to the IAM role that is attached to the EC2 instances grants access to Amazon S3.",
        "B": "Check the S3 bucket policy to validate the access permissions for the S3 bucket.",
        "C": "Check whether the policy that is assigned to the IAM user that is attached to the EC2 instances grants access to Amazon S3.",
        "D": "Check the S3 Lifecycle policy to validate the permissions that are assigned to the S3 bucket.",
        "E": "Check the security groups that are assigned to the EC2 instances. Make sure that a rule is not blocking the access to Amazon S3."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107037-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 9:40 p.m.",
      "textHash": "b6ec4ab7ca8c2358",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Sur EC2, l‚Äôapplication n‚Äôutilise pas un ¬´ utilisateur IAM ¬ª attach√© √† la machine : elle obtient des droits via un r√¥le IAM (un ensemble d‚Äôautorisations) associ√© √† l‚Äôinstance. Donc, si l‚Äôacc√®s √† S3 √©choue, la premi√®re v√©rification logique est : le r√¥le IAM de l‚ÄôEC2 a-t-il une policy qui autorise les actions S3 n√©cessaires (ex. s3:GetObject, s3:ListBucket) sur le bon bucket/chemin ? (A)\nEnsuite, m√™me si le r√¥le autorise, le bucket S3 peut aussi refuser via sa bucket policy (r√®gles c√¥t√© bucket). Il faut donc v√©rifier que la bucket policy permet bien ce r√¥le/compte et ne contient pas de ‚ÄúDeny‚Äù bloquant. (B)\nL‚Äôoption C est trompeuse : on n‚Äôattache pas un utilisateur IAM √† une instance EC2.\nLa Lifecycle policy (D) g√®re l‚Äôarchivage/suppression d‚Äôobjets, pas les permissions.\nLes security groups (E) contr√¥lent surtout le trafic r√©seau entrant/sortant ; l‚Äôacc√®s S3 se bloque rarement ici si la sortie Internet/VPC endpoint est correcte, mais le c≈ìur du probl√®me est presque toujours IAM + bucket policy.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:111:6ed5b5cf1be41737",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 111,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an image storage web application that runs on AWS. The company hosts the application on Amazon EC2 instances in an Auto Scaling group. The Auto Scaling group acts as the target group for an Application Load Balancer (ALB) and uses an Amazon S3 bucket to store the images for sale.The company wants to develop a feature to test system requests. The feature will direct requests to a separate target group that hosts a new beta version of the application.Which solution will meet this requirement with the LEAST effort?",
      "choices": {
        "A": "Create a new Auto Scaling group and target group for the beta version of the application. Update the ALB routing rule with a condition that looks for a cookie named version that has a value of beta. Update the test system code to use this cookie to test the beta version of the application.",
        "B": "Create a new ALB, Auto Scaling group, and target group for the beta version of the application. Configure an alternate Amazon Route 53 record for the new ALB endpoint. Use the alternate Route 53 endpoint in the test system requests to test the beta version of the application.",
        "C": "Create a new ALB, Auto Scaling group, and target group for the beta version of the application. Use Amazon CloudFront with Lambda@Edge to determine which specific request will go to the new ALB. Use the CloudFront endpoint to send the test system requests to test the beta version of the application.",
        "D": "Create a new Auto Scaling group and target group for the beta version of the application. Update the ALB routing rule with a condition that looks for a cookie named version that has a value of beta. Use Amazon CloudFront with Lambda@Edge to update the test system requests to add the required cookie when the requests go to the ALB."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/107028-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 22, 2023, 8:49 p.m.",
      "textHash": "6ed5b5cf1be41737",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Ici, l‚Äôapplication tourne sur des instances EC2 derri√®re un Application Load Balancer (ALB). Un ALB peut envoyer les requ√™tes vers diff√©rents ¬´ target groups ¬ª (groupes de serveurs) selon des r√®gles.\nPour tester une version b√™ta, il suffit d‚Äôajouter un deuxi√®me target group (avec un nouvel Auto Scaling group) qui h√©berge la version b√™ta.\nEnsuite, on configure une r√®gle de routage de l‚ÄôALB bas√©e sur un cookie HTTP : si le cookie s‚Äôappelle version et vaut beta, l‚ÄôALB envoie la requ√™te vers le target group b√™ta.\nLe syst√®me de test n‚Äôa qu‚Äô√† ajouter ce cookie dans ses requ√™tes : c‚Äôest simple et ne change rien pour les utilisateurs normaux.\nC‚Äôest le ¬´ least effort ¬ª car on r√©utilise le m√™me ALB et on √©vite de cr√©er un nouvel ALB, du DNS Route 53, ou CloudFront/Lambda@Edge.\nLes options B et C ajoutent beaucoup d‚Äôinfrastructure (nouvel ALB + routage DNS/CloudFront) sans n√©cessit√©.\nL‚Äôoption D ajoute CloudFront/Lambda@Edge juste pour modifier les requ√™tes, alors que le syst√®me de test peut directement envoyer le cookie.\nDonc A est la solution la plus directe et la plus l√©g√®re √† mettre en place.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:506:25a240bbf693a1bc",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 506,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is updating the code for an AWS Lambda function to add new capabilities. The Lambda function has version aliases for production and development environments that run separate versions of the function. The developer needs to configure a staging environment for the Lambda function to handle invocations to both the development version and the production version.Which solution will meet these requirements?",
      "choices": {
        "A": "Create a weighted alias that references the production version of the function and the updated version of the function.",
        "B": "Add a Network Load Balancer. Add the production version of the function and updated version of the function as targets.",
        "C": "Use AWS CodeDeploy to create a linear traffic shifting deployment",
        "D": "Create a tag for the Lambda function that contains the production version and updated version of the code."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152778-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 10, 2024, 4:37 a.m.",
      "textHash": "25a240bbf693a1bc",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "AWS Lambda ex√©cute du code sans serveur. Une ¬´ version ¬ª est un instantan√© immuable du code, et un ¬´ alias ¬ª est un nom (ex: prod, dev) qui pointe vers une version.\nOn veut un environnement de staging qui envoie des appels (invocations) √† la fois vers la version dev et la version prod.\nLa fa√ßon la plus simple est un alias ¬´ pond√©r√© ¬ª (weighted alias) : un seul alias peut r√©partir le trafic entre deux versions avec des pourcentages (ex: 50/50).\nAinsi, staging peut tester la nouvelle version tout en gardant une partie du trafic sur la version stable de production.\nB est inutile : un Network Load Balancer sert au trafic r√©seau vers des cibles, pas √† g√©rer des versions Lambda comme cela.\nC (CodeDeploy) sert surtout aux d√©ploiements progressifs (canary/linear) pour remplacer une version par une autre, pas √† maintenir un alias qui cible durablement deux versions.\nD (tags) sont juste des √©tiquettes de gestion, elles ne routent aucun trafic.\nDonc la bonne r√©ponse est de cr√©er un alias pond√©r√© entre la version prod et la version mise √† jour.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:464:646c9e5abf98024e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 464,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is using AWS CloudFormation to deploy an AWS Lambda function. The developer needs to set the Lambda function's timeout value based on the environment parameter of the template. The template contains mappings of EnvironmentData for each environment's timeout value. The environment parameter and EnvironmentData mappings are as follows:Environment parameter:EnvironmentData mappings:Which statement will meet these requirements?",
      "choices": {
        "A": "Timeout: !GetAtt [EnvironmentData, !Ref Environment, Timeout]",
        "B": "Timeout: !FindInMap [EnvironmentData, !Ref Environment, Timeout]",
        "C": "Timeout: !Select [EnvironmentData, !Ref Environment, Timeout]",
        "D": "Timeout: !ForEach[EnvironmentData, !Ref Environment, Timeout]"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152922-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 12, 2024, 7:46 p.m.",
      "textHash": "646c9e5abf98024e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:d0ba1555",
      "frExplanation": "Dans CloudFormation, un ¬´ param√®tre ¬ª (Environment) sert √† choisir une valeur au d√©ploiement (ex: dev, test, prod).\nUn ¬´ mapping ¬ª (EnvironmentData) est comme un tableau de correspondance: pour chaque environnement, on stocke des valeurs (ici, Timeout).\nLe champ Timeout d‚Äôune fonction AWS Lambda (service qui ex√©cute du code sans serveur) attend un nombre de secondes.\nPour r√©cup√©rer une valeur depuis un mapping, la fonction CloudFormation pr√©vue est !FindInMap.\n!FindInMap prend 3 √©l√©ments: le nom du mapping, la cl√© de 1er niveau (l‚Äôenvironnement choisi via !Ref Environment), puis la cl√© de 2e niveau (Timeout).\nDonc: Timeout: !FindInMap [EnvironmentData, !Ref Environment, Timeout] lit la bonne valeur selon l‚Äôenvironnement.\n!GetAtt sert √† lire un attribut d‚Äôune ressource, pas une entr√©e de mapping.\n!Select sert √† choisir un √©l√©ment dans une liste par index, et !ForEach n‚Äôest pas une fonction CloudFormation standard pour ce cas.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine ton √©cole avec un tableau affich√© √† l‚Äôentr√©e : pour chaque niveau (6e, 5e, 4e‚Ä¶), il y a une r√®gle de temps pour un contr√¥le (ex: 20 min, 30 min, 45 min).**\n\nDans CloudFormation, le ‚Äúparam√®tre Environment‚Äù c‚Äôest comme choisir ton niveau (ex: dev, test, prod). Les ‚Äúmappings EnvironmentData‚Äù c‚Äôest le tableau √† l‚Äôentr√©e : une liste qui dit, pour chaque environnement, quel ‚ÄúTimeout‚Äù (dur√©e max) appliquer. Tu veux lire la bonne dur√©e en fonction du niveau choisi. La commande faite pour ‚Äúchercher dans un tableau avec une cl√©‚Äù s‚Äôappelle !FindInMap : elle prend (le nom du tableau, la ligne = environnement choisi, la colonne = Timeout). Donc B marche : !FindInMap [EnvironmentData, !Ref Environment, Timeout] = ‚Äúva dans le tableau EnvironmentData, prends la ligne dev/test/prod, puis la valeur Timeout‚Äù. A (!GetAtt) sert √† lire une info d‚Äôun objet d√©j√† cr√©√©, pas d‚Äôun tableau. C (!Select) sert √† choisir par num√©ro dans une liste, pas par nom. D (!ForEach) n‚Äôest pas une commande standard ici.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:462:77c91b7616036e40",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 462,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application that processes a stream of user-supplied data. The data stream must be consumed by multiple Amazon EC2 based processing applications in parallel and in real time. Each processor must be able to resume without losing data if there is a service interruption. The application architect plans to add other processors in the near future, and wants to minimize the amount of data duplication involved.Which solution will satisfy these requirements?",
      "choices": {
        "A": "Publish the data to Amazon Simple Queue Service (Amazon SQS).",
        "B": "Publish the data to Amazon Data Firehose.",
        "C": "Publish the data to Amazon EventBridge.",
        "D": "Publish the data to Amazon Kinesis Data Streams."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152918-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 12, 2024, 7:21 p.m.",
      "textHash": "77c91b7616036e40",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, on a un flux de donn√©es en continu (stream) que plusieurs applications sur EC2 doivent lire en parall√®le et en temps r√©el.\nIl faut aussi pouvoir reprendre apr√®s une panne sans perdre de donn√©es : donc il faut un service qui conserve les √©v√©nements pendant un certain temps et permet de relire.\nAmazon Kinesis Data Streams est fait pour √ßa : il stocke temporairement les enregistrements (r√©tention), et plusieurs consommateurs peuvent lire le m√™me flux ind√©pendamment.\nChaque processeur peut g√©rer sa ‚Äúposition de lecture‚Äù (checkpoint) et reprendre l√† o√π il s‚Äô√©tait arr√™t√©.\nAjouter de nouveaux processeurs est simple : ils se branchent sur le m√™me stream sans que le producteur duplique les donn√©es.\nSQS est une file pour distribuer des messages √† des workers, mais un message est g√©n√©ralement consomm√© par un seul consommateur (pas id√©al pour plusieurs lectures parall√®les du m√™me √©v√©nement).\nFirehose sert surtout √† livrer des donn√©es vers S3/Redshift/OpenSearch, pas √† alimenter plusieurs consommateurs temps r√©el.\nEventBridge est un bus d‚Äô√©v√©nements pour d√©clencher des cibles, mais ce n‚Äôest pas un service de streaming avec relecture et gestion fine de la consommation.\nDonc la meilleure r√©ponse est Kinesis Data Streams.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine o√π un tapis roulant apporte des plateaux en continu, et plusieurs √©l√®ves (√©quipes) prennent chacun les plateaux pour faire une t√¢che diff√©rente en m√™me temps.**\n\nConcept : on veut un flux continu, lu en temps r√©el par plusieurs √©quipes en parall√®le, sans perdre de plateaux si une √©quipe s‚Äôarr√™te.\nKinesis Data Streams (D) = le tapis roulant qui garde les plateaux un moment : si une √©quipe a une panne, elle revient et reprend l√† o√π elle s‚Äô√©tait arr√™t√©e.\nPlusieurs √©quipes peuvent lire le m√™me flux en m√™me temps, chacune √† son rythme.\nEt si demain tu ajoutes une nouvelle √©quipe, elle peut se brancher au tapis sans que tu recopies tous les plateaux pour elle.\nPourquoi pas SQS (A) : c‚Äôest plut√¥t une file ‚Äúun plateau = un √©l√®ve‚Äù, pas fait pour que plusieurs √©quipes lisent les m√™mes donn√©es.\nPourquoi pas Firehose (B) : c‚Äôest surtout pour envoyer vers un stockage/outil final, pas pour plusieurs lecteurs qui reprennent au m√™me endroit.\nPourquoi pas EventBridge (C) : c‚Äôest pour distribuer des ‚Äú√©v√©nements‚Äù, pas pour un gros flux continu avec reprise pr√©cise.\nDonc D r√©pond : parall√®le + temps r√©el + reprise sans perte + ajout facile de nouveaux processeurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:510:92b19bb72042160d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 510,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn AWS Lambda function generates a 3 MB JSON file and then uploads it to an Amazon S3 bucket daily. The file contains sensitive information, so the developer must ensure that it is encrypted before uploading to the bucket.Which of the following modifications should the developer make to ensure that the data is encrypted before uploading it to the bucket?",
      "choices": {
        "A": "Use the default AWS Key Management Service (AWS KMS) key for Amazon S3 in the Lambda function code.",
        "B": "Use the S3 managed key and call the GenerateDataKey API to encrypt the file.",
        "C": "Use the GenerateDataKey API, then use that data key to encrypt the file in the Lambda function code.",
        "D": "Use an AWS Key Management Service (AWS KMS) customer managed key for Amazon S3 in the Lambda function code."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152782-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 10, 2024, 5:52 a.m.",
      "textHash": "92b19bb72042160d",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, on veut chiffrer (rendre illisible) le fichier AVANT de l‚Äôenvoyer sur Amazon S3 (stockage d‚Äôobjets). Donc il faut faire un chiffrement c√¥t√© client, dans le code Lambda.\nAWS KMS est un service qui g√®re des cl√©s de chiffrement. L‚ÄôAPI GenerateDataKey de KMS fournit une ¬´ cl√© de donn√©es ¬ª (sym√©trique) utilisable pour chiffrer un fichier, et renvoie aussi une version chiffr√©e de cette cl√© pour pouvoir la stocker.\nLa bonne approche est : Lambda appelle GenerateDataKey, utilise la cl√© en clair pour chiffrer le JSON localement, puis envoie sur S3 le fichier chiffr√© (et garde la cl√© de donn√©es chiffr√©e en m√©tadonn√©es ou √† c√¥t√©).\nAinsi, m√™me si quelqu‚Äôun acc√®de au fichier dans S3, il ne peut pas le lire sans repasser par KMS pour d√©chiffrer la cl√© de donn√©es.\nA et D parlent d‚Äôutiliser une cl√© KMS ¬´ pour S3 ¬ª mais cela correspond surtout au chiffrement c√¥t√© serveur (SSE-KMS) fait par S3 apr√®s l‚Äôupload, pas ‚Äúavant‚Äù.\nB est incorrect car S3 n‚Äôa pas d‚ÄôAPI GenerateDataKey : c‚Äôest KMS qui la fournit.\nDonc C est la seule option qui garantit le chiffrement avant l‚Äôupload.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que tu dois d√©poser chaque jour une copie d‚Äôun devoir secret dans un casier du lyc√©e (le ‚Äúbucket‚Äù). Avant de le mettre dans le casier, tu dois le mettre dans une enveloppe ferm√©e √† cl√©, sinon n‚Äôimporte qui qui ouvre le casier peut lire le devoir.**\n\nConcept : le casier (S3) peut stocker, mais ‚Äúchiffrer avant d‚Äôenvoyer‚Äù veut dire : tu verrouilles le fichier AVANT qu‚Äôil quitte tes mains (Lambda).\nLa ‚Äúmachine √† cl√©s‚Äù (KMS) peut te donner une cl√© unique du jour (GenerateDataKey).\nR√©ponse C : tu demandes une cl√© (GenerateDataKey), puis tu utilises cette cl√© dans ton code pour fermer l‚Äôenveloppe (chiffrer le JSON) avant de le d√©poser.\nPourquoi pas A/D : choisir une cl√© KMS ‚Äúpour S3‚Äù ressemble √† dire ‚Äúle casier a une serrure‚Äù, mais √ßa ne garantit pas que toi tu as mis le devoir dans une enveloppe avant de le d√©poser.\nPourquoi pas B : une ‚Äúcl√© g√©r√©e par S3‚Äù n‚Äôest pas faite pour que toi tu chiffrres dans ta main avec GenerateDataKey.\nDonc C = tu chiffrres toi-m√™me avant l‚Äôupload, comme demand√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:501:817592b7177fd00a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 501,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is launching a global application that delivers content to multiple countries. The developer needs to serve specific content based on the country of each user and each user‚Äôs primary language. The developer must ensure that content is served reliably and with low latency.Which solution will meet these requirements?",
      "choices": {
        "A": "Create an Amazon API Gateway REST API. Create an AWS Global Accelerator standard accelerator to resolve requests to the API. Configure endpoint groups on the accelerator. Attach listeners for each country and language.",
        "B": "Store the content in a centralized Amazon S3 bucket. Enable S3 Transfer Acceleration on the bucket. Create an Amazon Route 53 hosted zone that includes the endpoint for the S3 bucket. Create records in Route 53 that use geoproximity and geolocation routing policies.",
        "C": "Create an Amazon API Gateway REST API. Connect the REST API to AWS WAF. Use geo match statements and regex match statements to allow or deny requests based on the labels returned from web request evaluations.",
        "D": "Configure an Amazon CloudFront distribution that uses the application as an origin. Configure the distribution to forward the Accept-Language header and the CloudFront-Viewer-Country header to the origin."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152773-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 10, 2024, 4:21 a.m.",
      "textHash": "817592b7177fd00a",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:0947d867",
      "frExplanation": "Pour servir du contenu rapidement partout dans le monde, il faut un service qui met le contenu ‚Äúau plus pr√®s‚Äù des utilisateurs : Amazon CloudFront est un CDN (r√©seau de cache mondial) qui r√©duit la latence et am√©liore la fiabilit√©.\nIci, l‚Äôapplication doit renvoyer un contenu diff√©rent selon le pays et la langue de l‚Äôutilisateur.\nCloudFront peut d√©tecter le pays du visiteur et fournir l‚Äôen-t√™te (header) CloudFront-Viewer-Country.\nLe navigateur envoie aussi la langue pr√©f√©r√©e via l‚Äôen-t√™te Accept-Language.\nEn configurant CloudFront pour transmettre ces deux en-t√™tes √† l‚Äôorigine (votre application), l‚Äôapplication peut choisir la bonne version du contenu (ex: FR/CA vs EN/US).\nCloudFront garde en cache les r√©ponses, ce qui acc√©l√®re les requ√™tes suivantes et diminue la charge sur l‚Äôorigine.\nLes autres options ne g√®rent pas correctement la combinaison pays + langue pour personnaliser le contenu avec faible latence (WAF sert √† filtrer, Route 53/S3 TA ne fait pas la s√©lection par langue, Global Accelerator ne fait pas ce routage par headers).\nDonc la meilleure solution est une distribution CloudFront qui transmet Accept-Language et CloudFront-Viewer-Country √† l‚Äôorigine.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine une cha√Æne de fast-food mondiale : tu as des mini-restos dans chaque pays. Quand un client arrive, le vendeur voit son pays et la langue qu‚Äôil parle, puis lui donne le bon menu, sans le faire attendre.**\n\nConcept : pour aller vite, on met des ‚Äúmini-restos‚Äù proches des gens qui servent le contenu (√ßa √©vite un long trajet). Et on envoie au ‚Äúcuisinier central‚Äù deux infos : le pays et la langue.\nCloudFront, c‚Äôest ces mini-restos partout dans le monde (√ßa r√©duit la latence = moins d‚Äôattente).\nL‚Äôapplication ‚Äúorigin‚Äù, c‚Äôest la cuisine centrale qui a toutes les versions du contenu.\nAvec D, CloudFront transmet √† la cuisine deux √©tiquettes : le pays du client (CloudFront-Viewer-Country) et sa langue pr√©f√©r√©e (Accept-Language).\nDu coup, la cuisine choisit la bonne version (ex: fran√ßais Canada vs fran√ßais France) et CloudFront la sert vite et de fa√ßon fiable.\nPourquoi pas les autres : elles aident surtout √† diriger/filtrer, mais ne g√®rent pas aussi directement ‚Äúpays + langue‚Äù pour servir le bon contenu rapidement partout.\nDonc D r√©pond √† : bon contenu selon pays+langue + rapide + fiable.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:471:e274f8910657fa5c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 471,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that uses an Amazon Cognito user pool for authentication. A developer needs to add a new REST API that will use the user pool to authenticate requests.Which solution will meet this requirement with the LEAST development effort?",
      "choices": {
        "A": "Create a new API key and a new usage plan. Associate the API key and the REST API with the usage plan.",
        "B": "Create a Cognito authorizer for the correct user pool. Reference the header that contains the Cognito token.",
        "C": "Create an AWS Lambda token authorizer. Reference the authorization token in the event payload. Authenticate requests based on the token value.",
        "D": "Create an AWS Lambda request authorizer. Reference the authorization header in the event payload. Authenticate requests by using the header value in a request to the Cognito API."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/152946-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 13, 2024, 2:51 a.m.",
      "textHash": "e274f8910657fa5c",
      "rawFormat": "discussion-md",
      "conceptKey": "api_gw_cognito_authorizer",
      "frExplanation": "Ici, l‚Äôapplication utilise d√©j√† un ¬´ user pool ¬ª Amazon Cognito : c‚Äôest un service AWS qui g√®re des utilisateurs et √©met des jetons (tokens) apr√®s connexion.\nPour prot√©ger une nouvelle API REST, le plus simple est de laisser API Gateway v√©rifier automatiquement ces jetons.\nUn ¬´ Cognito authorizer ¬ª dans API Gateway est fait exactement pour √ßa : il valide le token JWT sign√© par Cognito et autorise/refuse la requ√™te.\nIl suffit de le lier au bon user pool et d‚Äôindiquer l‚Äôen-t√™te HTTP o√π se trouve le token (souvent Authorization: Bearer ‚Ä¶).\nA est faux : une API key/usage plan sert surtout √† limiter/compter l‚Äôusage, pas √† authentifier des utilisateurs Cognito.\nC et D demandent d‚Äô√©crire et maintenir du code Lambda pour valider le token, donc plus d‚Äôeffort et plus de risques.\nDonc la solution avec le moins de d√©veloppement est B.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine ton lyc√©e : pour entrer, tu as d√©j√† une carte d‚Äô√©l√®ve (badge) qui prouve qui tu es. Maintenant, on ouvre une nouvelle salle (la nouvelle API) et on veut que la m√™me carte marche, sans inventer un nouveau contr√¥le compliqu√©.**\n\nConcept : Amazon Cognito, c‚Äôest le ‚Äúbureau des cartes d‚Äô√©l√®ve‚Äù qui donne un badge (un jeton/token) apr√®s connexion. Une API, c‚Äôest une porte qui doit v√©rifier ce badge.\nPourquoi B : cr√©er un ‚ÄúCognito authorizer‚Äù, c‚Äôest comme installer √† la nouvelle porte un lecteur officiel qui sait d√©j√† lire les badges du lyc√©e. Il regarde le badge dans l‚Äôen-t√™te (header) de la requ√™te, comme un surveillant regarde ta carte.\nA est faux : une cl√© d‚ÄôAPI/usage plan, c‚Äôest plut√¥t un ticket d‚Äôacc√®s, pas une preuve d‚Äôidentit√© (√ßa ne remplace pas la carte d‚Äô√©l√®ve).\nC et D demandent d‚Äô√©crire du code (Lambda = un petit programme ‚Äúsur mesure‚Äù) pour v√©rifier le badge : plus de travail.\nDonc B = le plus simple : on branche la porte directement sur le syst√®me de badges existant.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:203:4bd77a58c66314ec",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 203,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn IT department uses Amazon S3 to store sensitive images. After more than 1 year, the company moves the images into archival storage. The company rarely accesses the images, but the company wants a storage solution that maximizes resiliency. The IT department needs access to the images that have been moved to archival storage within 24 hours.Which solution will meet these requirements MOST cost-effectively?",
      "choices": {
        "A": "Use S3 Standard-Infrequent Access (S3 Standard-IA) to store the images. Use S3 Glacier Deep Archive with standard retrieval to store and retrieve archived images.",
        "B": "Use S3 Standard-Infrequent Access (S3 Standard-IA) to store the images. Use S3 Glacier Deep Archive with bulk retrieval to store and retrieve archived images.",
        "C": "Use S3 Intelligent-Tiering to store the images. Use S3 Glacier Deep Archive with standard retrieval to store and retrieve archived images.",
        "D": "Use S3 One Zone-Infrequent Access (S3 One Zone-IA) to store the images. Use S3 Glacier Deep Archive with bulk retrieval to store and retrieve archived images."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122625-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3:06 a.m.",
      "textHash": "4bd77a58c66314ec",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:s3:58113ff6",
      "frExplanation": "Amazon S3 est un service de stockage d‚Äôobjets (fichiers) tr√®s durable. Pour des images ‚Äúsensibles‚Äù et pour maximiser la r√©silience, on √©vite les options ‚ÄúOne Zone‚Äù car elles ne gardent les donn√©es que dans une seule zone (moins r√©silient).\nApr√®s 1 an, on veut de l‚Äôarchivage tr√®s peu cher : S3 Glacier Deep Archive est la classe la moins co√ªteuse, mais l‚Äôacc√®s est lent.\nExigence cl√© : r√©cup√©rer les images en moins de 24 h. En Deep Archive, le mode ‚Äústandard retrieval‚Äù permet une restauration en ~12 h, donc √ßa respecte la contrainte.\nLe mode ‚Äúbulk retrieval‚Äù est moins cher mais plus lent (souvent jusqu‚Äô√† ~48 h), donc peut d√©passer 24 h : risque de ne pas respecter l‚Äôobjectif.\nPour la p√©riode avant archivage, S3 Standard-IA est adapt√© si on acc√®de rarement mais on veut une haute durabilit√© multi-zones.\nDonc : Standard-IA pour le stockage ‚Äúrarement consult√©‚Äù, puis Deep Archive + standard retrieval pour garantir l‚Äôacc√®s < 24 h au meilleur co√ªt qui respecte la contrainte.\nC (Intelligent-Tiering) ajoute des frais de monitoring inutiles si on sait d√©j√† qu‚Äôon archive apr√®s 1 an.\nD est √©cart√© car One Zone-IA r√©duit la r√©silience, contraire √† ‚Äúmaximiser la r√©silience‚Äù.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le CDI du lyc√©e : au d√©but, tes photos importantes sont dans une armoire solide et facile d‚Äôacc√®s. Apr√®s 1 an, on les met dans les archives du sous-sol, super prot√©g√©es, mais il faut du temps pour qu‚Äôun surveillant aille les chercher.**\n\nConcept : Amazon S3, c‚Äôest comme des salles de stockage. Certaines sont rapides (plus ch√®res), d‚Äôautres sont des archives (moins ch√®res mais plus lentes). Ici, on garde des images sensibles, rarement consult√©es, mais on veut que ce soit tr√®s s√ªr (r√©silient) et r√©cup√©rable en moins de 24h.\nPourquoi A : S3 Standard-IA = l‚Äôarmoire solide mais qu‚Äôon ouvre rarement (moins cher que le ‚Äútout le temps ouvert‚Äù, et tr√®s fiable). Puis S3 Glacier Deep Archive = le sous-sol d‚Äôarchives, le moins cher pour du tr√®s long terme. ‚ÄúStandard retrieval‚Äù = le mode de r√©cup√©ration ‚Äúnormal‚Äù qui rentre dans la contrainte des 24h.\nPourquoi pas B : ‚Äúbulk retrieval‚Äù = r√©cup√©ration en mode ‚Äúcamion lent‚Äù, souvent trop lent pour garantir <24h.\nPourquoi pas C : Intelligent-Tiering = un surveillant qui d√©place tout seul selon l‚Äôusage ; utile si l‚Äôacc√®s change souvent, mais ici on sait d√©j√† que c‚Äôest de l‚Äôarchive, donc √ßa co√ªte pour rien.\nPourquoi pas D : One Zone-IA = une seule salle/ b√¢timent : si le b√¢timent a un probl√®me, tu perds l‚Äôacc√®s ; moins r√©silient, donc pas OK.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:175:371636512094c0a5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 175,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application that is deployed to Amazon EC2 is using Amazon DynamoDB. The application calls the DynamoDB REST API. Periodically, the application receives a ProvisionedThroughputExceededException error when the application writes to a DynamoDB table.Which solutions will mitigate this error MOST cost-effectively? (Choose two.)",
      "choices": {
        "A": "Modify the application code to perform exponential backoff when the error is received.",
        "B": "Modify the application to use the AWS SDKs for DynamoDB.",
        "C": "Increase the read and write throughput of the DynamoDB table.",
        "D": "Create a DynamoDB Accelerator (DAX) cluster for the DynamoDB table.",
        "E": "Create a second DynamoDB table. Distribute the reads and writes between the two tables."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122595-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:28 a.m.",
      "textHash": "371636512094c0a5",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_capacity_rcu_wcu",
      "frExplanation": "L‚Äôerreur ProvisionedThroughputExceededException signifie que la table DynamoDB a atteint sa capacit√© d‚Äô√©criture autoris√©e (trop de requ√™tes en m√™me temps). DynamoDB limite le d√©bit selon la capacit√© configur√©e, donc certaines √©critures sont ‚Äúrefus√©es‚Äù temporairement. La solution la plus √©conomique est de ne pas payer plus de capacit√©, mais de mieux g√©rer les pics. Le ‚Äúexponential backoff‚Äù (A) consiste √† r√©essayer apr√®s un petit d√©lai, puis un d√©lai plus long si √ßa √©choue encore : cela laisse le temps √† DynamoDB de se ‚Äúd√©sengorger‚Äù et r√©duit les collisions. C‚Äôest une bonne pratique recommand√©e pour les API AWS quand il y a du throttling. Augmenter la capacit√© (C) peut marcher mais co√ªte plus cher en continu. DAX (D) acc√©l√®re surtout les lectures, pas les √©critures, donc ne r√®gle pas ce probl√®me. Utiliser le SDK (B) peut aider car il int√®gre souvent des retries, mais la mesure cl√© et la plus rentable reste d‚Äôimpl√©menter explicitement le backoff.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : il y a un nombre limit√© de plats que les cuisiniers peuvent servir par minute. Si trop d‚Äô√©l√®ves arrivent d‚Äôun coup, la cantine dit ¬´ trop de monde, reviens plus tard ¬ª.**\n\nConcept : DynamoDB, c‚Äôest comme la cantine, avec une capacit√© limit√©e d‚Äô√©critures (ajouter des infos) par moment. L‚Äôerreur ProvisionedThroughputExceededException veut dire : ¬´ tu demandes trop vite, je n‚Äôarrive pas √† suivre ¬ª. Solution A : l‚Äôexponential backoff, c‚Äôest comme si tu faisais la queue intelligemment : tu r√©essaies, mais en attendant un peu, puis un peu plus si √ßa bloque encore, au lieu de pousser tout le temps. √áa r√©duit les embouteillages et √ßa marche sans payer plus. Augmenter la capacit√© (C), c‚Äôest comme embaucher plus de cuisiniers : efficace mais plus cher. DAX (D) aide surtout √† lire plus vite, pas √† √©crire. Deux tables (E), c‚Äôest comme ouvrir une 2e cantine : compliqu√© et co√ªteux. Donc la solution la plus rentable est A.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:315:27c4b324ceec832a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 315,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company built an online event platform. For each event, the company organizes quizzes and generates leaderboards that are based on the quiz scores. The company stores the leaderboard data in Amazon DynamoDB and retains the data for 30 days after an event is complete. The company then uses a scheduled job to delete the old leaderboard data.The DynamoDB table is configured with a fixed write capacity. During the months when many events occur, the DynamoDB write API requests are throttled when the scheduled delete job runs.A developer must create a long-term solution that deletes the old leaderboard data and optimizes write throughput.Which solution meets these requirements?",
      "choices": {
        "A": "Configure a TTL attribute for the leaderboard data.",
        "B": "Use DynamoDB Streams to schedule and delete the leaderboard data.",
        "C": "Use AWS Step Functions to schedule and delete the leaderboard data.",
        "D": "Set a higher write capacity when the scheduled delete job runs."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134133-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 18, 2024, 12:17 p.m.",
      "textHash": "27c4b324ceec832a",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_capacity_rcu_wcu",
      "frExplanation": "DynamoDB est une base de donn√©es NoSQL. Ici, l‚Äôentreprise garde chaque ligne de ‚Äúleaderboard‚Äù 30 jours puis lance un job qui supprime en masse.\nLe probl√®me vient de la suppression en pic : beaucoup d‚Äô√©critures/suppressions d‚Äôun coup, et comme la capacit√© d‚Äô√©criture est fixe, DynamoDB ‚Äúthrottle‚Äù (bloque/ralentit) les requ√™tes.\nLa solution durable est d‚Äô√©viter ce job de suppression massif.\nTTL (Time To Live) de DynamoDB permet d‚Äôajouter un attribut ‚Äúdate d‚Äôexpiration‚Äù sur chaque √©l√©ment.\nQuand la date est d√©pass√©e, DynamoDB supprime automatiquement l‚Äô√©l√©ment en arri√®re-plan, de fa√ßon progressive.\nCela √©tale la charge dans le temps, r√©duit les pics d‚Äô√©criture et donc les throttles.\nStreams, Step Functions ou augmenter la capacit√© pendant le job gardent l‚Äôid√©e d‚Äôun traitement planifi√© et peuvent encore cr√©er des pics ou co√ªter plus cher.\nDonc il faut configurer un attribut TTL pour que les donn√©es expirent automatiquement (r√©ponse A).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le tableau d‚Äôaffichage du lyc√©e o√π on colle les scores des quiz. Chaque feuille doit rester 30 jours, puis dispara√Ætre. Au lieu d‚Äôavoir un surveillant qui vient un jour pr√©cis arracher des centaines de feuilles d‚Äôun coup, on met une r√®gle: chaque feuille a une ‚Äúdate de p√©remption‚Äù et le tableau enl√®ve les feuilles tout seul au bon moment.**\n\nConcept: DynamoDB, c‚Äôest le tableau; √©crire/supprimer, c‚Äôest coller/arracher des feuilles. La ‚Äúcapacit√© d‚Äô√©criture fixe‚Äù = le nombre max d‚Äôactions par minute. Le probl√®me: le job planifi√© arrache tout d‚Äôun coup, √ßa fait une grosse file d‚Äôattente, donc √ßa bloque (throttling). La bonne solution A (TTL) = √©crire sur chaque feuille sa date d‚Äôexpiration, et le tableau les retire automatiquement, petit √† petit, sans gros pic. Donc moins de suppressions d‚Äôun coup, moins de blocage, et √ßa marche en continu sur le long terme. B et C gardent l‚Äôid√©e d‚Äôun ‚Äúsurveillant‚Äù qui d√©clenche des actions; √ßa peut encore faire des pics. D augmente la capacit√© seulement pendant le m√©nage: √ßa co√ªte plus et ce n‚Äôest pas aussi propre qu‚Äôune r√®gle automatique.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:512:7d78c622a1ed12c9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 512,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn Amazon Data Firehose delivery stream is receiving customer data that contains personally identifiable information. A developer needs to remove pattern-based customer identifiers from the data and store the modified data in an Amazon S3 bucket.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Implement Firehose data transformation as an AWS Lambda function. Configure the function to remove the customer identifiers. Set an Amazon S3 bucket as the destination of the delivery stream.",
        "B": "Launch an Amazon EC2 instance. Set the EC2 instance as the destination of the delivery stream. Run an application on the EC2 instance to remove the customer identifiers. Store the transformed data in an Amazon S3 bucket.",
        "C": "Create an Amazon OpenSearch Service instance. Set the OpenSearch Service instance as the destination of the delivery stream. Use search and replace to remove the customer identifiers. Export the data to an Amazon S3 bucket.",
        "D": "Create an AWS Step Functions workflow to remove the customer identifiers. As the last step in the workflow, store the transformed data in an Amazon S3 bucket. Set the workflow as the destination of the delivery stream."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153149-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 18, 2024, 4:56 a.m.",
      "textHash": "7d78c622a1ed12c9",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Amazon Data Firehose (Kinesis Data Firehose) sert √† recevoir des donn√©es en continu et √† les d√©poser automatiquement dans une destination comme Amazon S3 (stockage de fichiers/objets). Ici, il faut modifier les donn√©es avant de les stocker : supprimer des identifiants clients d√©tect√©s par un motif (pattern). Firehose propose nativement une √©tape de ‚Äútransformation‚Äù via AWS Lambda, un service qui ex√©cute du code sans g√©rer de serveur. On √©crit une fonction Lambda qui lit chaque enregistrement, applique un filtre/regex pour retirer les champs sensibles, puis renvoie les donn√©es nettoy√©es √† Firehose. Ensuite Firehose √©crit directement le r√©sultat dans le bucket S3. Les autres options ajoutent des services non adapt√©s comme destination directe (EC2 √† g√©rer, OpenSearch pour recherche, Step Functions pas une destination Firehose) et compliquent inutilement.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine : un tapis roulant (Firehose) am√®ne des plateaux avec des tickets o√π il y a parfois le nom/pr√©nom des √©l√®ves (infos priv√©es). Avant de poser les plateaux dans le grand frigo de stockage (S3), tu veux qu‚Äôun surveillant efface automatiquement ces infos sur le ticket.**\n\nFirehose, c‚Äôest le tapis roulant qui transporte les donn√©es en continu.\nS3, c‚Äôest le grand frigo/entrep√¥t o√π on stocke tout.\nAWS Lambda, c‚Äôest le ‚Äúpetit robot/surveillant‚Äù qui fait une mini-t√¢che rapide quand un plateau passe.\nAvec A, tu branches Lambda sur le tapis : √† chaque arriv√©e, il rep√®re un motif (ex: un num√©ro client) et l‚Äôefface, puis Firehose d√©pose la version nettoy√©e dans S3.\nB, c‚Äôest embaucher une personne √† plein temps (EC2) juste pour gommer des tickets : trop lourd.\nC, c‚Äôest utiliser une salle de recherche (OpenSearch) pour faire un simple gommage : d√©tour inutile.\nD, c‚Äôest un parcours en plusieurs √©tapes (Step Functions) alors qu‚Äôon veut juste ‚Äúeffacer puis stocker‚Äù.\nDonc A est la bonne r√©ponse : nettoyage automatique pendant le transport, puis stockage dans S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:469:7ddae02dc50ce3ea",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 469,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an application that uses an Amazon DynamoDB table. The developer needs to develop code that reads all records that were added to the table during the previous day, creates HTML reports, and pushes the reports into third-party storage. The item size varies from 1 KB to 4 KB, and the index structure is defined with the date. The developer needs to minimize the read capacity that the application requires from the DynamoDB table.Which DynamoDB API operation should the developer use in the code to meet these requirements?",
      "choices": {
        "A": "Query",
        "B": "Scan",
        "C": "BatchGetItem",
        "D": "GetItem"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/153145-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Dec. 18, 2024, 3:16 a.m.",
      "textHash": "7ddae02dc50ce3ea",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π on lit des donn√©es via des op√©rations comme GetItem, Query ou Scan.\nIci, on veut lire uniquement les enregistrements ajout√©s ¬´ hier ¬ª et on sait que la table (ou un index) est organis√© avec une cl√© bas√©e sur la date.\nQuand on conna√Æt la cl√© de partition (et √©ventuellement une condition sur la cl√© de tri), l‚Äôop√©ration Query permet de r√©cup√©rer seulement les √©l√©ments qui correspondent √† cette date.\nQuery lit une plage cibl√©e dans l‚Äôindex/table, donc consomme beaucoup moins de capacit√© de lecture (RCU) qu‚Äôun Scan.\nScan parcourt toute la table et filtre apr√®s coup : c‚Äôest co√ªteux et inutile si on peut cibler par date.\nGetItem lit un seul √©l√©ment par cl√© exacte : impossible pour ‚Äútous les √©l√©ments d‚Äôhier‚Äù en une seule requ√™te.\nBatchGetItem lit plusieurs √©l√©ments, mais il faut d√©j√† conna√Ætre toutes les cl√©s √† l‚Äôavance, ce qui n‚Äôest pas le cas.\nDonc, pour minimiser la capacit√© de lecture et exploiter l‚Äôindex par date, il faut utiliser Query.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéí Imagine ton casier au lyc√©e** : tu ranges tes affaires diff√©remment selon comment tu les cherches.\n\n**LSI (Local)** = C'est comme **des onglets DANS ton classeur** üìÅ - Cr√©√©s √† la rentr√©e, jamais modifiables, super rapides (m√™me partition).\n\n**GSI (Global)** = C'est comme **un index √† la fin du livre** üìñ - Ajoutable n'importe quand, cherche partout, mais plus lent.\n\n**üß† Mn√©motechnique :** \"**L**SI = **L**ocal, **L**imit√©\" | \"**G**SI = **G**lobal, **G**√©nial\"\n\n**Quand utiliser GSI :** Quand tu veux chercher par un autre crit√®re que la cl√© principale, sur TOUTE la table.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:385:c009f6be17ac1e52",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 385,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn IAM role is attached to an Amazon EC2 instance that explicitly denies access to all Amazon S3 API actions. The EC2 instance credentials file specifies the IAM access key and secret access key, which allow full administrative access.Given that multiple modes of IAM access are present for this EC2 instance, which of the following is correct?",
      "choices": {
        "A": "The EC2 instance will only be able to list the S3 buckets.",
        "B": "The EC2 instance will only be able to list the contents of one S3 bucket at a time.",
        "C": "The EC2 instance will be able to perform all actions on any S3 bucket.",
        "D": "The EC2 instance will not be able to perform any S3 action on any S3 bucket."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143353-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 5, 2024, 1:14 p.m.",
      "textHash": "c009f6be17ac1e52",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Une instance EC2 peut obtenir des permissions AWS de deux fa√ßons ici : (1) via un r√¥le IAM attach√© √† l‚Äôinstance, (2) via des cl√©s d‚Äôacc√®s (access key/secret key) √©crites dans un fichier de credentials.\nAmazon S3 est le service de stockage (buckets) et ses actions sont des ‚ÄúAPI actions‚Äù (Lister, Lire, √âcrire, etc.).\nEn IAM, une r√®gle ‚ÄúDeny‚Äù explicite (refus) est prioritaire sur tout ‚ÄúAllow‚Äù, m√™me si ailleurs on a des droits admin.\nDonc, m√™me si les cl√©s dans le fichier donnent un acc√®s administrateur, le r√¥le IAM attach√© √† l‚ÄôEC2 contient un Deny sur toutes les actions S3.\nQuand AWS √©value l‚Äôautorisation, d√®s qu‚Äôun Deny explicite s‚Äôapplique, l‚Äôaction est bloqu√©e.\nR√©sultat : aucune action S3 (liste, lecture, √©criture, suppression) ne fonctionnera depuis cette instance.\nC‚Äôest pourquoi la bonne r√©ponse est D : l‚Äôinstance ne peut effectuer aucune action S3 sur aucun bucket.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:349:936d1df51f555935",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 349,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs temporary access to resources in a second account.What is the MOST secure way to achieve this?",
      "choices": {
        "A": "Use the Amazon Cognito user pools to get short-lived credentials for the second account.",
        "B": "Create a dedicated IAM access key for the second account, and send it by mail.",
        "C": "Create a cross-account access role, and use sts:AssumeRole API to get short-lived credentials.",
        "D": "Establish trust, and add an SSH key for the second account to the IAM user."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/137947-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "April 5, 2024, 8:46 a.m.",
      "textHash": "936d1df51f555935",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Le besoin est un acc√®s temporaire √† des ressources dans un autre compte AWS : on veut √©viter des identifiants permanents et faciles √† voler.\nLa solution la plus s√ªre est d‚Äôutiliser un r√¥le IAM ¬´ cross-account ¬ª : un r√¥le est une identit√© avec des permissions, mais sans mot de passe ni cl√© longue dur√©e.\nDans le compte cible (2e compte), on cr√©e un r√¥le IAM et on d√©finit une relation de confiance (trust) qui autorise le 1er compte √† l‚Äôassumer.\nEnsuite, l‚Äôapplication ou l‚Äôutilisateur appelle AWS STS (Security Token Service) avec l‚ÄôAPI sts:AssumeRole.\nSTS renvoie des identifiants temporaires (Access Key/Secret/Session Token) valables peu de temps et renouvelables, ce qui limite l‚Äôimpact en cas de fuite.\nCognito (A) sert surtout √† authentifier des utilisateurs d‚Äôapplications (web/mobile), pas √† g√©rer proprement un acc√®s inter-comptes pour un d√©veloppeur.\nEnvoyer une cl√© IAM par mail (B) cr√©e un secret permanent, difficile √† r√©voquer proprement et tr√®s risqu√©.\nUne cl√© SSH (D) n‚Äôest pas le m√©canisme standard pour acc√©der aux services AWS et ne fournit pas des permissions IAM temporaires.\nDonc la r√©ponse la plus s√©curis√©e est le r√¥le inter-comptes + AssumeRole (C).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine deux lyc√©es diff√©rents (Compte A et Compte B). Tu es √©l√®ve du lyc√©e A, mais tu dois utiliser pendant 10 minutes la salle info du lyc√©e B, sans recevoir un badge permanent.**\n\nConcept : le plus s√ªr, c‚Äôest un ‚Äúbadge visiteur‚Äù qui expire, pas une cl√© que tu gardes. Dans AWS, ce badge temporaire s‚Äôappelle des ‚Äúidentifiants temporaires‚Äù.\nPourquoi C : le lyc√©e B pr√©pare un r√¥le ‚Äúvisiteur‚Äù (un badge avec des droits pr√©cis) et dit ‚Äúje fais confiance au lyc√©e A pour demander ce badge‚Äù. Ensuite, tu ‚ÄúAssumeRole‚Äù = tu passes √† l‚Äôaccueil, on te donne un badge valable peu de temps, puis il devient inutilisable.\nPourquoi pas B : envoyer une cl√© par mail, c‚Äôest comme envoyer un double des cl√©s du lyc√©e‚Ä¶ si quelqu‚Äôun la copie, c‚Äôest dangereux et √ßa ne s‚Äôarr√™te pas tout seul.\nPourquoi pas A : Cognito sert surtout √† g√©rer des utilisateurs d‚Äôapplis (comme un login), pas le meilleur outil pour acc√©der √† un autre compte AWS.\nPourquoi pas D : une cl√© SSH, c‚Äôest pour se connecter √† des machines, pas pour donner un acc√®s propre et temporaire √† tout un compte.\nDonc la r√©ponse la plus s√©curis√©e pour un acc√®s temporaire entre comptes, c‚Äôest C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:342:f6aced168734813e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 342,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses an AWS Lambda function to call a third-party service. The third-party service has a limit of requests each minute. If the number of requests exceeds the limit, the third-party service returns rate-limiting errors.A developer needs to configure the Lambda function to avoid receiving rate limiting errors from the third-party service.Which solution will meet these requirements?",
      "choices": {
        "A": "Set the reserved concurrency on the Lambda function to match the number of concurrent requests that the third-party service allows.",
        "B": "Decrease the memory that is allocated to the Lambda function.",
        "C": "Set the provisioned concurrency on the Lambda function to match the number of concurrent requests that the third-party service allows.",
        "D": "Increase the timeout value that is specified on the Lambda function."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136966-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 4:09 a.m.",
      "textHash": "f6aced168734813e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:fed30054",
      "frExplanation": "AWS Lambda ex√©cute votre code automatiquement quand il est d√©clench√©, et peut lancer plusieurs ex√©cutions en parall√®le (concurrence).\nLe service tiers limite le nombre de requ√™tes par minute : trop d‚Äôappels en m√™me temps ou trop vite provoque des erreurs de ‚Äúrate limiting‚Äù.\nPour √©viter cela, il faut contr√¥ler combien d‚Äôex√©cutions Lambda peuvent tourner simultan√©ment, car chaque ex√©cution peut appeler le service tiers.\nLa ‚Äúreserved concurrency‚Äù (concurrence r√©serv√©e) fixe une limite maximale d‚Äôex√©cutions en parall√®le pour cette fonction.\nEn la r√©glant au niveau autoris√© par le service tiers, vous emp√™chez Lambda de d√©passer ce d√©bit, donc moins d‚Äôerreurs.\nLa ‚Äúprovisioned concurrency‚Äù sert surtout √† r√©duire la latence de d√©marrage (cold start), pas √† limiter le nombre total d‚Äôex√©cutions.\nChanger la m√©moire n‚Äôa pas de lien direct avec le nombre d‚Äôappels par minute.\nAugmenter le timeout ne r√©duit pas le d√©bit, cela laisse juste plus de temps √† une ex√©cution avant d‚Äô√™tre arr√™t√©e.\nDonc la bonne solution est de limiter la concurrence avec la reserved concurrency (A).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : il y a une r√®gle ‚Äúmax 10 √©l√®ves servis en m√™me temps‚Äù. Si 30 √©l√®ves arrivent d‚Äôun coup, la cantine refuse et dit ‚Äúrevenez plus tard‚Äù.**\n\nAWS Lambda, c‚Äôest comme des √©l√®ves ‚Äúautomatiques‚Äù qui vont demander un service √† la cantine (le service externe). Le service externe a une limite de demandes par minute, donc trop d‚Äô√©l√®ves en m√™me temps = refus (erreurs). Pour √©viter √ßa, tu dois limiter combien d‚Äô√©l√®ves peuvent partir en m√™me temps. La ‚Äúreserved concurrency‚Äù (A), c‚Äôest un quota fixe : tu dis ‚Äúau maximum X ex√©cutions Lambda en m√™me temps‚Äù. Tu r√®gles X pour coller √† la limite du service externe, donc tu n‚Äôenvoies jamais plus de demandes que ce qu‚Äôil accepte. B (m√©moire) ne change pas le nombre d‚Äô√©l√®ves. C (provisioned concurrency) sert surtout √† √™tre plus rapide au d√©marrage, pas √† limiter. D (timeout) fait juste attendre plus longtemps, mais n‚Äôemp√™che pas la foule d‚Äôarriver.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:340:498be0162e08d86c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 340,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA gaming website gives users the ability to trade game items with each other on the platform. The platform requires both users' records to be updated and persisted in one transaction. If any update fails, the transaction must roll back.Which AWS solutions can provide the transactional capability that is required for this feature? (Choose two.)",
      "choices": {
        "A": "Amazon DynamoDB with operations made with the ConsistentRead parameter set to true",
        "B": "Amazon ElastiCache for Memcached with operations made within a transaction block",
        "C": "Amazon DynamoDB with reads and writes made by using Transact* operations",
        "D": "Amazon Aurora MySQL with operations made within a transaction block",
        "E": "Amazon Athena with operations made within a transaction block"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136964-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 4:02 a.m.",
      "textHash": "498be0162e08d86c",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:296375f6",
      "frExplanation": "Ici, on veut une vraie transaction : mettre √† jour deux utilisateurs (ex: retirer un objet √† A et l‚Äôajouter √† B) de fa√ßon ¬´ tout ou rien ¬ª. Si une √©criture √©choue, tout doit √™tre annul√© (rollback).\nAmazon DynamoDB est une base NoSQL g√©r√©e. Par d√©faut, des √©critures s√©par√©es ne garantissent pas l‚Äôatomicit√© sur plusieurs √©l√©ments.\nLes op√©rations DynamoDB ¬´ Transact* ¬ª (TransactWriteItems / TransactGetItems) permettent justement des transactions ACID sur plusieurs items/tables : soit toutes les mises √† jour r√©ussissent, soit aucune n‚Äôest appliqu√©e.\nDonc C est la bonne solution pour obtenir ce comportement de transaction/rollback dans DynamoDB.\nA est faux : ConsistentRead=true ne concerne que la coh√©rence de lecture, pas une transaction multi-√©critures.\nB est faux : ElastiCache Memcached est un cache en m√©moire, sans transactions/rollback fiables.\nE est faux : Athena sert √† interroger des donn√©es dans S3 (SQL sur fichiers), pas √† faire des mises √† jour transactionnelles.\nD (Aurora MySQL) supporte aussi les transactions SQL, mais la r√©ponse fournie comme correcte ici est C.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:339:f7781d81f7ef6ba1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 339,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company stores all personally identifiable information (PII) in an Amazon DynamoDB table named PII in Account A. Developers are working on an application that is running on Amazon EC2 instances in Account B. The application in Account B requires access to the PII table.An administrator in Account A creates an IAM role named AccessPII that has permission to access the PII table. The administrator also creates a trust policy that specifies Account B as a principal that can assume the role.Which combination of steps should the developers take in Account B to allow their application to access the PII table? (Choose two.)",
      "choices": {
        "A": "Allow the EC2 IAM role the permission to assume the AccessPII role.",
        "B": "Allow the EC2 IAM role the permission to access the PII table.",
        "C": "Include the AWS API in the application code logic to obtain temporary credentials from the EC2 IAM role to access the PII table.",
        "D": "Include the AssumeRole API operation in the application code logic to obtain temporary credentials to access the PII table.",
        "E": "Include the GetSessionToken API operation in the application code logic to obtain temporary credentials to access the PII table."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136962-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 3:57 a.m.",
      "textHash": "f7781d81f7ef6ba1",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, la table DynamoDB ¬´ PII ¬ª est dans le compte A, mais l‚Äôapplication tourne sur des EC2 dans le compte B. Pour acc√©der √† des ressources d‚Äôun autre compte, on utilise un r√¥le IAM ¬´ cross-account ¬ª : le compte A a cr√©√© le r√¥le AccessPII avec les droits sur la table, et une ‚Äútrust policy‚Äù qui autorise le compte B √† l‚Äôassumer.\nC√¥t√© compte B, l‚Äôinstance EC2 utilise un r√¥le IAM (instance profile). Ce r√¥le doit avoir l‚Äôautorisation d‚Äôappeler STS:AssumeRole sur le r√¥le AccessPII (sinon l‚Äôassumption est refus√©e) : c‚Äôest l‚Äô√©tape A.\nEnsuite, l‚Äôapplication doit r√©ellement assumer ce r√¥le pour obtenir des identifiants temporaires (cl√©/secret/token) et appeler DynamoDB avec ces identifiants : c‚Äôest l‚Äô√©tape D (API AssumeRole).\nDonner directement au r√¥le EC2 l‚Äôacc√®s √† la table (B) ne marche pas car la table est dans un autre compte et on veut centraliser les droits dans AccessPII.\nGetSessionToken (E) ne sert pas √† changer de r√¥le vers un autre compte, et r√©cup√©rer les identifiants du r√¥le EC2 (C) ne suffit pas car ce r√¥le n‚Äôa pas les droits sur la table PII.\nDonc il faut : autoriser l‚ÄôEC2 role √† assumer AccessPII (A) et appeler AssumeRole dans le code (D).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:337:89c4c714ce3aa42f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 337,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has an application that asynchronously invokes an AWS Lambda function. The developer wants to store messages that resulted in failed invocations of the Lambda function so that the application can retry the call later.What should the developer do to accomplish this goal with the LEAST operational overhead?",
      "choices": {
        "A": "Set up Amazon CloudWatch Logs log groups to filter and store the messages in an Amazon S3 bucket. Import the messages in Lambda. Run the Lambda function again.",
        "B": "Configure Amazon EventBridge to send the messages to Amazon Simple Notification Service (Amazon SNS) to initiate the Lambda function again.",
        "C": "Implement a dead-letter queue for discarded messages. Set the dead-letter queue as an event source for the Lambda function.",
        "D": "Send Amazon EventBridge events to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to pull messages from the SQS queue. Run the Lambda function again."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136961-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 3:47 a.m.",
      "textHash": "89c4c714ce3aa42f",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Quand une application appelle une fonction AWS Lambda ¬´ en asynchrone ¬ª, elle envoie un √©v√©nement et ne reste pas en attente du r√©sultat. Si l‚Äôex√©cution √©choue, il faut un endroit automatique pour garder le message afin de le rejouer plus tard.\nUne Dead-Letter Queue (DLQ) est justement une ¬´ bo√Æte de secours ¬ª : Lambda y d√©pose les √©v√©nements qui n‚Äôont pas pu √™tre trait√©s apr√®s les tentatives automatiques.\nOn utilise g√©n√©ralement Amazon SQS (file d‚Äôattente) ou Amazon SNS comme DLQ. Cela demande tr√®s peu d‚Äôadministration : on active la DLQ dans la configuration Lambda.\nEnsuite, on peut relire ces messages et les renvoyer pour retenter le traitement (par un script, une autre Lambda, ou en configurant la DLQ comme source d‚Äô√©v√©nements).\nLes autres options ajoutent plus de complexit√© : filtrer des logs CloudWatch vers S3 n‚Äôest pas fait pour rejouer des √©v√©nements, et EventBridge/SNS ou EventBridge/SQS changent l‚Äôarchitecture au lieu de g√©rer simplement les √©checs.\nDonc la meilleure solution avec le moins d‚Äôoverhead est d‚Äôactiver une DLQ pour conserver automatiquement les messages √©chou√©s.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un prof qui donne des devoirs √† rendre dans une bo√Æte, mais certains devoirs se perdent ou sont refus√©s (feuille illisible).**\n\nConcept : quand tu envoies un ‚Äúmessage‚Äù √† une t√¢che (Lambda), parfois √ßa √©choue. Tu veux garder les messages rat√©s pour r√©essayer plus tard, sans surveiller tout le temps.\nSolution C : une ‚Äúdead-letter queue‚Äù, c‚Äôest comme une bo√Æte sp√©ciale ‚Äúdevoirs refus√©s‚Äù. Tout ce qui √©choue va automatiquement dedans.\nPourquoi c‚Äôest le mieux : tu ne perds rien, et tu n‚Äôas pas besoin de trier des cahiers de texte (A) ni de bricoler des renvois compliqu√©s (B, D).\nEnsuite, tu peux reprendre cette bo√Æte ‚Äúrefus√©s‚Äù et relancer les devoirs quand tu veux.\nDonc C marche avec le moins d‚Äôeffort au quotidien : AWS range tout seul les √©checs au bon endroit.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:327:c4675fb8c6b2d0e4",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 327,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is using Amazon API Gateway to invoke a new AWS Lambda function. The company has Lambda function versions in its PROD and DEV environments. In each environment, there is a Lambda function alias pointing to the corresponding Lambda function version. API Gateway has one stage that is configured to point at the PROD alias.The company wants to configure API Gateway to enable the PROD and DEV Lambda function versions to be simultaneously and distinctly available.Which solution will meet these requirements?",
      "choices": {
        "A": "Enable a Lambda authorizer for the Lambda function alias in API Gateway. Republish PROD and create a new stage for DEV. Create API Gateway stage variables for the PROD and DEV stages. Point each stage variable to the PROD Lambda authorizer to the DEV Lambda authorizer.",
        "B": "Set up a gateway response in API Gateway for the Lambda function alias. Republish PROD and create a new stage for DEV. Create gateway responses in API Gateway for PROD and DEV Lambda aliases.",
        "C": "Use an environment variable for the Lambda function alias in API Gateway. Republish PROD and create a new stage for development. Create API gateway environment variables for PROD and DEV stages. Point each stage variable to the PROD Lambda function alias to the DEV Lambda function alias.",
        "D": "Use an API Gateway stage variable to configure the Lambda function alias. Republish PROD and create a new stage for development. Create API Gateway stage variables for PROD and DEV stages. Point each stage variable to the PROD Lambda function alias and to the DEV Lambda function alias."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134142-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 18, 2024, 12:41 p.m.",
      "textHash": "c4675fb8c6b2d0e4",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:656e46be",
      "frExplanation": "API Gateway est la ‚Äúporte d‚Äôentr√©e‚Äù HTTP, et Lambda ex√©cute du code. Une Lambda peut avoir des versions (v1, v2‚Ä¶) et des alias (PROD, DEV) qui pointent vers une version pr√©cise.\nIci, l‚Äôunique stage API Gateway pointe d√©j√† vers l‚Äôalias PROD. Pour exposer PROD et DEV en m√™me temps, il faut deux stages (ex: /prod et /dev) ou au moins une configuration distincte par stage.\nLes ‚Äústage variables‚Äù d‚ÄôAPI Gateway sont des variables propres √† chaque stage, utilis√©es pour changer une valeur sans modifier le code ni dupliquer l‚ÄôAPI.\nLa bonne pratique est de mettre l‚Äôalias Lambda dans une stage variable, puis de configurer l‚Äôint√©gration Lambda avec quelque chose comme ${stageVariables.lambdaAlias}.\nAinsi, le stage PROD utilise la variable = alias PROD, et le stage DEV utilise la variable = alias DEV, tout en gardant la m√™me API.\nLes authorizers (A) servent √† l‚Äôauthentification, pas √† choisir une version de Lambda. Les gateway responses (B) g√®rent des r√©ponses d‚Äôerreur, pas le routage. Les ‚Äúenvironment variables‚Äù (C) ne sont pas une notion d‚ÄôAPI Gateway pour s√©lectionner une int√©gration.\nDonc la solution D r√©pond exactement: deux stages + stage variables pointant vers les alias PROD et DEV.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:324:0451d885855dd27a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 324,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a containerized application on AWS. The application communicates with a third-party service by using API keys. The developer needs a secure way to store the API keys and pass the API keys to the containerized application.Which solutions will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Store the API keys as a SecureString parameter in AWS Systems Manager Parameter Store. Grant the application access to retrieve the value from Parameter Store.",
        "B": "Store the API keys in AWS CloudFormation templates by using base64 encoding. Pass the API keys to the application through container definition environment variables.",
        "C": "Add a new AWS CloudFormation parameter to the CloudFormation template. Pass the API keys to the application by using the container definition environment variables.",
        "D": "Embed the API keys in the application. Build the container image on-premises. Upload the container image to Amazon Elastic Container Registry (Amazon ECR).",
        "E": "Store the API keys as a SecretString parameter in AWS Secrets Manager. Grant the application access to retrieve the value from Secrets Manager."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133634-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 12, 2024, 6:16 p.m.",
      "textHash": "0451d885855dd27a",
      "rawFormat": "discussion-md",
      "conceptKey": "ecr_image_scanning",
      "frExplanation": "Ici, on veut stocker des cl√©s API (des ‚Äúmots de passe‚Äù pour appeler un service externe) de fa√ßon s√©curis√©e et les fournir √† une application en conteneur.\nAWS Systems Manager Parameter Store peut stocker des valeurs sensibles en ‚ÄúSecureString‚Äù : la valeur est chiffr√©e et on contr√¥le qui peut la lire via des permissions IAM.\nLa bonne pratique est donc : stocker la cl√© dans Parameter Store (SecureString) et donner au r√¥le IAM du conteneur le droit de la r√©cup√©rer au d√©marrage ou √† l‚Äôex√©cution.\nC‚Äôest plus s√ªr que de mettre la cl√© dans un template CloudFormation ou dans des variables d‚Äôenvironnement en clair, car ces √©l√©ments peuvent √™tre visibles dans le code, l‚Äôhistorique, ou les logs.\nMettre la cl√© directement dans l‚Äôimage du conteneur (embed) est aussi risqu√© : toute personne ayant acc√®s √† l‚Äôimage peut extraire la cl√©.\nSecrets Manager est aussi un service de secrets, mais la r√©ponse officielle attendue ici met en avant Parameter Store SecureString comme solution conforme.\nDonc A r√©pond au besoin : stockage chiffr√© + acc√®s contr√¥l√© + r√©cup√©ration par l‚Äôapplication.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que ton appli est un √©l√®ve qui doit appeler un service externe, comme une pizzeria, et qu‚Äôil faut un ‚Äúcode secret‚Äù (API key) pour commander.**\n\nConcept : le code secret ne doit pas √™tre √©crit sur le tableau de la classe ni coll√© sur le sac, sinon tout le monde peut le copier. Il doit √™tre rang√© dans un coffre de l‚Äô√©cole, et seuls les √©l√®ves autoris√©s peuvent l‚Äôouvrir.\nA : Parameter Store en ‚ÄúSecureString‚Äù, c‚Äôest ce coffre. L‚Äôappli (l‚Äô√©l√®ve) a une autorisation pour aller chercher le code quand elle en a besoin, puis l‚Äôutiliser dans le conteneur (son ‚Äúcasier‚Äù). C‚Äôest s√©curis√© et contr√¥l√©.\nPourquoi pas B/C : √©crire le code dans un mod√®le (comme un document distribu√© √† toute la classe) ou le passer en variables visibles, √ßa risque d‚Äô√™tre lu/copier.\nPourquoi pas D : mettre le code dans l‚Äôappli, c‚Äôest comme imprimer le code sur le t-shirt de l‚Äô√©l√®ve : impossible √† cacher et dur √† changer.\nE ressemble aussi √† un coffre (Secrets Manager), mais ici la r√©ponse attendue est A : SecureString + acc√®s contr√¥l√©.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:317:6540f23227c0292a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 317,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is using Amazon API Gateway to develop an API for its application on AWS. A developer needs to test and generate API responses. Other teams are required to test the API immediately.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Set up a mock integration request in API Gateway. Configure the method's integration request and integration response to associate a response with a given status code.",
        "B": "Set up the request validators in the API's OpenAPI definition file. Import the OpenAPI definitions into API Gateway to test the API.",
        "C": "Set up a gateway response for the API in API Gateway. Configure response headers with hardcoded HTTP status codes and responses.",
        "D": "Set up a request parameter-based Lambda authorizer to control access to the API. Configure the Lambda function with the necessary mapping template."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134136-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 18, 2024, 12:20 p.m.",
      "textHash": "6540f23227c0292a",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:32ad1a7b",
      "frExplanation": "API Gateway est un service AWS qui expose des endpoints HTTP pour votre application. Ici, on veut tester l‚ÄôAPI et obtenir des r√©ponses tout de suite, m√™me si le backend (Lambda, EC2, base de donn√©es) n‚Äôest pas pr√™t.\nUne ‚Äúmock integration‚Äù dans API Gateway permet de simuler le backend : API Gateway renvoie une r√©ponse pr√©d√©finie sans appeler aucun service.\nEn configurant l‚ÄôIntegration Request et surtout l‚ÄôIntegration Response, on peut associer un code HTTP (ex: 200, 400) √† un corps de r√©ponse (JSON) fixe.\nC‚Äôest id√©al pour que d‚Äôautres √©quipes testent imm√©diatement les contrats d‚ÄôAPI (format, codes, exemples) et que le d√©veloppeur g√©n√®re des r√©ponses de test.\nB ne fait que valider la forme des requ√™tes (pas g√©n√©rer des r√©ponses). C concerne des r√©ponses d‚Äôerreur/standard d‚ÄôAPI Gateway, pas des r√©ponses m√©tier compl√®tes. D g√®re l‚Äôautorisation, pas la g√©n√©ration de r√©ponses.\nDonc la bonne solution est de cr√©er une mock integration (A).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu pr√©pares un stand de pizzas au lyc√©e, mais la cuisine n‚Äôest pas encore pr√™te. Tu veux quand m√™me que les autres classes puissent ‚Äúcommander‚Äù et recevoir une pizza factice pour s‚Äôentra√Æner.**\n\nConcept : une API, c‚Äôest comme un guichet de commande. Normalement, derri√®re, il y a la cuisine (le vrai syst√®me) qui pr√©pare la r√©ponse.\nIci, la cuisine n‚Äôest pas pr√™te, mais il faut tester tout de suite les r√©ponses.\nLa meilleure solution est de mettre des ‚Äúpizzas en plastique‚Äù : des r√©ponses invent√©es √† l‚Äôavance.\nA = ‚Äúmock integration‚Äù : API Gateway renvoie une r√©ponse simul√©e sans appeler le vrai syst√®me.\nOn peut dire : si on demande X, renvoie ‚ÄúOK‚Äù (code 200) ou ‚ÄúErreur‚Äù (code 400), comme des tickets de caisse pr√©d√©finis.\nB v√©rifie surtout si la commande est bien √©crite, pas s‚Äôil y a une r√©ponse pr√™te.\nC sert plut√¥t aux messages d‚Äôerreur g√©n√©raux, pas √† simuler tout un menu de r√©ponses.\nD parle de contr√¥le d‚Äôacc√®s (vigile √† l‚Äôentr√©e), pas de g√©n√©ration de r√©ponses.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:311:13b35a8b07696ebc",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 311,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has built a serverless application for its ecommerce website. The application includes a REST API in Amazon API Gateway that invokes an AWS Lambda function. The Lambda function processes data and stores the data in Amazon DynamoDB table. The Lambda function calls a third-party stock application API to process the order. After the ordered is processed, the Lambda function returns an HTTP 200 status code with no body to the client.During peak usage when the API calls exceeds a certain threshold, the third-party stock application sometimes fails to process the data and responds with error messages. The company needs a solution that will not overwhelm the third-party stock application.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure the REST API in API Gateway to write the requests directly into DynamoDB. Configure a DynamoDB intrinsic function to perform the transformation. Set up a DynamoDB stream to call the third-party stock application API with each new row. Delete the Lambda function.",
        "B": "Configure the REST API in API Gateway to write the requests directly into an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function with a reserved concurrency equal to the third-party stock application's threshold. Set Lambda function to process the messages from the SQS queue.",
        "C": "Configure the REST API in API Gateway to write the requests directly into an Amazon Simple Notification Service (Amazon SNS) topic. Configure the Lambda function with a provisioned concurrency equal to the third-party stock application's threshold. Set the Lambda function to process the messages from the SNS topic.",
        "D": "Configure the REST API in API Gateway to write the requests directly into Amazon Athena. Configure the transformation of the data by using SQL with multiple query result locations set up to point to the DynamoDB table and the third-party stock fulfilment application API. Delete the Lambda function."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133609-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 12, 2024, 2:10 p.m.",
      "textHash": "13b35a8b07696ebc",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Le probl√®me vient des pics : trop d‚Äôappels en m√™me temps font √©chouer l‚ÄôAPI du fournisseur (elle a une capacit√© maximale). Il faut donc ¬´ lisser ¬ª la charge.\nAmazon SQS est une file d‚Äôattente : API Gateway peut y d√©poser chaque commande, et SQS les garde m√™me si le traitement est plus lent. Ainsi, on ne perd pas de requ√™tes.\nAWS Lambda peut lire la file SQS et traiter les messages progressivement.\nLa cl√© est la ¬´ reserved concurrency ¬ª : on fixe un nombre maximum d‚Äôex√©cutions Lambda en parall√®le, √©gal au seuil support√© par l‚ÄôAPI tierce. Donc on n‚Äôenvoie jamais plus d‚Äôappels simultan√©s que ce que le fournisseur accepte.\nAvec SNS (pub/sub), on diffuse √† tous les abonn√©s et on ne contr√¥le pas aussi simplement le d√©bit ; provisioned concurrency sert surtout √† r√©duire le temps de d√©marrage, pas √† limiter la charge.\nLes options DynamoDB Streams/Athena ne sont pas adapt√©es pour contr√¥ler finement le d√©bit vers une API externe.\nDonc la meilleure solution est : API Gateway -> SQS -> Lambda avec concurrence r√©serv√©e (B).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : plein d‚Äô√©l√®ves passent commande en m√™me temps, mais la cuisine (le service stock) ne peut pr√©parer que, disons, 20 plateaux √† la fois.**\n\nConcept : si trop de commandes arrivent d‚Äôun coup, tu ne les envoies pas toutes √† la cuisine. Tu les mets dans une file d‚Äôattente (comme des tickets) et tu les traites petit √† petit.\nPourquoi B : API Gateway d√©pose chaque commande dans une ‚Äúbo√Æte de tickets‚Äù (SQS = file d‚Äôattente). Rien n‚Äôest perdu m√™me en heure de pointe.\nEnsuite, Lambda joue le r√¥le des serveurs qui prennent les tickets.\nLe ‚Äúreserved concurrency‚Äù = tu fixes le nombre max de serveurs actifs en m√™me temps, exactement au seuil que la cuisine supporte.\nDonc la cuisine (API stock externe) n‚Äôest jamais bombard√©e : elle re√ßoit un flux r√©gulier.\nC est moins adapt√© car SNS ressemble √† un m√©gaphone qui diffuse √† tous tout de suite, pas une file qui r√©gule.\nA et D ne sont pas faits pour g√©rer une file de commandes vers un service externe fragile.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:310:9bf52a283c95125a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 310,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application that invokes AWS Lambda functions asynchronously to process events. The developer notices that a Lambda function fails to process some events at random times. The developer needs to investigate the failed events and capture the events that the Lambda function fails to process.Which solution will meet these requirements?",
      "choices": {
        "A": "Add an Amazon EventBridge rule for the Lambda function. Configure the EventBridge rule to react to failed events and to store the events in an Amazon DynamoDB table.",
        "B": "Configure the Lambda function with a dead-letter queue based in Amazon Kinesis. Update the Lambda function's execution role with the required permissions.",
        "C": "Configure the Lambda function with an Amazon Simple Queue Service (Amazon SQS) dead-letter queue. Update the Lambda function's execution role with the required permissions.",
        "D": "Configure the Lambda function with an Amazon Simple Queue Service (Amazon SQS) FIFO dead-letter queue. Update the Lambda function's execution role with the required permissions."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133608-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 12, 2024, 1:31 p.m.",
      "textHash": "9bf52a283c95125a",
      "rawFormat": "discussion-md",
      "conceptKey": "sqs_dlq_redrive",
      "frExplanation": "Ici, la fonction AWS Lambda est invoqu√©e ¬´ asynchronement ¬ª : l‚Äô√©v√©nement est envoy√©, puis Lambda le traite plus tard sans que l‚Äôappelant attende la r√©ponse.\nQuand un traitement √©choue (erreur, timeout, bug), certains √©v√©nements peuvent √™tre perdus si on ne pr√©voit pas un endroit o√π les r√©cup√©rer.\nLa solution standard est une Dead-Letter Queue (DLQ) : une ¬´ bo√Æte de secours ¬ª o√π Lambda d√©pose les √©v√©nements qu‚Äôelle n‚Äôarrive pas √† traiter apr√®s ses tentatives.\nAmazon SQS est un service de file d‚Äôattente simple et fiable : on peut y stocker les messages/√©v√©nements √©chou√©s et les relire pour enqu√™te ou retraitement.\nDonc on configure la Lambda avec une DLQ SQS et on donne √† son r√¥le d‚Äôex√©cution les permissions pour envoyer des messages dans cette file.\nPourquoi pas A : EventBridge ne ‚Äúr√©cup√®re‚Äù pas automatiquement les √©v√©nements √©chou√©s d‚Äôune invocation asynchrone de Lambda comme une DLQ.\nPourquoi pas B : une DLQ Lambda ne se base pas sur Kinesis (Kinesis est un flux, pas une DLQ Lambda classique).\nPourquoi pas D : une file FIFO impose des contraintes (groupId, ordre strict) et n‚Äôest pas n√©cessaire ici ; une SQS standard suffit et correspond au comportement attendu.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine : des √©l√®ves d√©posent des tickets de commande dans une bo√Æte, et le cuisinier (Lambda) les traite plus tard, sans que l‚Äô√©l√®ve attende.**\n\nConcept : quand Lambda est appel√© ‚Äúasynchrone‚Äù, les √©v√©nements sont comme des tickets d√©pos√©s, et Lambda les traite quand il peut. Parfois, un ticket est rat√© (erreur). Il faut donc un bac sp√©cial pour r√©cup√©rer les tickets rat√©s.\nLa bonne solution est C : une ‚Äúdead-letter queue‚Äù SQS, c‚Äôest une bo√Æte de secours o√π AWS met automatiquement les √©v√©nements que Lambda n‚Äôarrive pas √† traiter apr√®s plusieurs essais.\nComme √ßa, tu peux ouvrir cette bo√Æte, voir quels tickets ont √©chou√©, et les rejouer ou enqu√™ter.\nA ne marche pas : EventBridge n‚Äôest pas le bac automatique des √©checs de Lambda.\nB ne colle pas : Kinesis est plut√¥t un ‚Äútapis roulant‚Äù de donn√©es, pas la bo√Æte standard d‚Äô√©checs.\nD (FIFO) impose un ordre strict, inutile ici : on veut surtout r√©cup√©rer les rat√©s simplement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:306:8eb33536eb35c1b7",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 306,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application on AWS. The application includes an AWS Lambda function that processes messages from an Amazon Simple Queue Service (Amazon SQS) queue.The Lambda function sometimes fails or times out. The developer needs to figure out why the Lambda function fails to process some messages.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Increase the maximum timeout of the Lambda function to 15 minutes. Check the AWS CloudTrail event history for error details.",
        "B": "Increase the visibility timeout of the SQS queue. Check logs in Amazon CloudWatch Logs for error details.",
        "C": "Create a dead-letter queue. Configure the Lambda function to send the failed messages to the dead-letter queue.",
        "D": "Create an Amazon DynamoDB table. Update the Lambda function to send the failed messages to the DynamoDB table."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134125-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 18, 2024, 10:44 a.m.",
      "textHash": "8eb33536eb35c1b7",
      "rawFormat": "discussion-md",
      "conceptKey": "sqs_visibility_timeout",
      "frExplanation": "Ici, une fonction AWS Lambda (code ex√©cut√© sans serveur) lit des messages dans une file Amazon SQS (queue). Si Lambda √©choue ou d√©passe le temps, certains messages ne sont pas trait√©s et peuvent √™tre retent√©s plusieurs fois.\nLa solution la plus simple pour comprendre ‚Äúquels messages √©chouent‚Äù et les analyser ensuite est d‚Äôutiliser une Dead-Letter Queue (DLQ) : une file sp√©ciale o√π l‚Äôon envoie automatiquement les messages qui √©chouent apr√®s plusieurs tentatives.\nAvec une DLQ, vous r√©cup√©rez exactement les messages probl√©matiques, sans modifier votre code ni mettre en place une base de donn√©es.\nC‚Äôest peu d‚Äôoverhead : on configure la DLQ et le nombre de tentatives, puis on inspecte les messages dans la DLQ pour voir les donn√©es qui causent l‚Äôerreur.\nA est insuffisant : CloudTrail trace surtout les appels API, pas les erreurs d√©taill√©es d‚Äôex√©cution de votre code.\nB aide parfois (logs CloudWatch + visibilit√©), mais ne ‚Äúmet de c√¥t√©‚Äù aucun message d√©finitivement pour analyse ; ils peuvent tourner en boucle.\nD demande de d√©velopper et maintenir une logique d‚Äô√©criture en base (plus de code, plus d‚Äôop√©rations) pour un besoin d√©j√† couvert par la DLQ.\nDonc C est la meilleure option avec le moins d‚Äôeffort op√©rationnel.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine : les √©l√®ves d√©posent des tickets de commande dans une bo√Æte (la file SQS) et un cuisinier (Lambda) prend un ticket, pr√©pare le plat, puis passe au suivant.**\n\nConcept : parfois le cuisinier est trop lent ou se trompe, donc certains tickets ne sont jamais servis. Pour comprendre lesquels posent probl√®me, il faut un endroit o√π mettre les tickets ‚Äúimpossibles‚Äù. Solution C : une ‚Äúbo√Æte des tickets rat√©s‚Äù (dead-letter queue). Si le cuisinier √©choue plusieurs fois sur le m√™me ticket, on le met automatiquement dans cette bo√Æte sp√©ciale. Comme √ßa, la file principale continue de tourner, et toi tu peux ouvrir la bo√Æte des rat√©s et voir exactement quels tickets font planter le cuisinier. C‚Äôest le moins de travail au quotidien : pas besoin de surveiller tout le temps, les tickets probl√©matiques se rangent tout seuls au bon endroit. A et B ne te donnent pas une liste claire des tickets qui √©chouent, et D (une table) demande plus de bricolage dans l‚Äôappli.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:304:50abe6ab8b849ac8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 304,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building a serverless application on AWS. The application uses Amazon API Gateway and AWS Lambda. The company wants to deploy the application to its development, test, and production environments.Which solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Use API Gateway stage variables and create Lambda aliases to reference environment-specific resources.",
        "B": "Use Amazon Elastic Container Service (Amazon ECS) to deploy the application to the environments.",
        "C": "Duplicate the code for each environment. Deploy the code to a separate API Gateway stage.",
        "D": "Use AWS Elastic Beanstalk to deploy the application to the environments."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134340-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 22, 2024, 7 a.m.",
      "textHash": "50abe6ab8b849ac8",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:6985b917",
      "frExplanation": "Ici on a une appli ¬´ serverless ¬ª : API Gateway re√ßoit les requ√™tes HTTP et Lambda ex√©cute le code sans g√©rer de serveurs.\nOn veut 3 environnements (dev/test/prod) avec le moins d‚Äôeffort de d√©veloppement.\nAPI Gateway propose des ¬´ stages ¬ª (ex: /dev, /test, /prod) et des ¬´ variables de stage ¬ª pour stocker des valeurs diff√©rentes selon l‚Äôenvironnement.\nLambda propose des ¬´ aliases ¬ª (ex: dev, test, prod) qui pointent vers une version pr√©cise de la fonction.\nEn combinant les deux, chaque stage d‚ÄôAPI Gateway peut appeler l‚Äôalias Lambda correspondant, sans dupliquer le code.\nOn garde un seul code, on change seulement la configuration (variables + alias) pour chaque environnement.\nLes options ECS et Elastic Beanstalk ajoutent de l‚Äôinfrastructure (containers/serveurs) inutile pour du serverless.\nDupliquer le code par environnement augmente le travail et les risques d‚Äôerreurs; donc A est la solution la plus simple.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o avec 3 serveurs: entra√Ænement (dev), match amical (test), tournoi (prod). Le jeu est le m√™me, mais les r√©glages et les bases de donn√©es changent selon le serveur.**\n\nConcept: tu veux garder le m√™me jeu, mais choisir automatiquement les bons r√©glages selon l‚Äôendroit o√π tu joues.\nAPI Gateway = le menu/portail qui re√ßoit les demandes des joueurs.\nLambda = le ‚Äúbot‚Äù qui ex√©cute l‚Äôaction quand tu cliques.\nA: les ‚Äústage variables‚Äù = des r√©glages du portail selon dev/test/prod (comme choisir le serveur).\nLes ‚Äúaliases Lambda‚Äù = des √©tiquettes qui pointent vers la bonne version du bot (bot-dev, bot-test, bot-prod) sans recopier le code.\nDonc tu d√©ploies une fois, et tu changes juste les √©tiquettes/r√©glages: effort minimal.\nB et D changent de ‚Äútype de console‚Äù (containers/plateforme), plus de boulot.\nC = recopier le jeu 3 fois: beaucoup d‚Äôeffort et d‚Äôerreurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:300:087c140e5b223be0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 300,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs approval from a product owner before the developer can deploy code for an application to production. The developer uses AWS CodePipeline to deploy the application. The developer configures an Amazon Simple Notification Service (Amazon SNS) topic to send notifications to the product owner.Which solution is the MOST operationally efficient way for the developer to receive approval from the product owner?",
      "choices": {
        "A": "Add a new stage to CodePipeline before the production deployment. Add a manual approval action to the new stage. Add a new notification rule in the pipeline settings. Specify manual approval as the event that initiates the notification. Specify the SNS topic's Amazon Resource Name (ARN) to notify the product owner.",
        "B": "Develop an AWS Step Functions state machine that sends a notification to the product owner and accepts an approval. Add a new stage to CodePipeline before the production deployment. Add the state machine as a Step Functions action to the new stage.",
        "C": "Add a manual approval action to the existing production deployment stage in CodePipeline. Specify the SNS topic's Amazon Resource Name (ARN) while configuring the new manual approval action.",
        "D": "Edit the settings in CodePipeline. Create a new notification rule. Specify manual approval as the event that initiates the notification. Create a new notification target. Specify the SNS topic to notify the product owner. Save the notification rule."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134336-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 22, 2024, 6:26 a.m.",
      "textHash": "087c140e5b223be0",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici, on veut que le d√©ploiement en production soit bloqu√© tant que le product owner n‚Äôa pas valid√©. AWS CodePipeline est un service qui encha√Æne des √©tapes (build, test, deploy) et peut inclure une √©tape d‚Äôapprobation manuelle. Amazon SNS est un service qui envoie des notifications (email/SMS/HTTP) √† des abonn√©s via un ‚Äútopic‚Äù. La solution la plus simple et standard est d‚Äôajouter un stage juste avant la prod avec une action ‚ÄúManual approval‚Äù : le pipeline s‚Äôarr√™te et attend un clic d‚Äôapprobation. Ensuite, on configure une r√®gle de notification du pipeline pour l‚Äô√©v√©nement ‚Äúmanual approval needed‚Äù afin d‚Äôenvoyer automatiquement un message au product owner via le topic SNS. C‚Äôest op√©rationnellement efficace car tout reste natif dans CodePipeline (pas de code √† maintenir). B est trop complexe (Step Functions inutile). C met l‚Äôapprobation dans le stage de d√©ploiement prod (moins propre/moins clair) et ne parle pas de r√®gle de notification. D envoie une notification mais ne cr√©e pas le point de blocage/approbation dans le pipeline.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un devoir de groupe √† rendre : avant de l‚Äôenvoyer au prof (la ‚Äúproduction‚Äù), tu dois avoir le ‚ÄúOK‚Äù du d√©l√©gu√© de classe (le product owner).**\n\nCodePipeline, c‚Äôest la cha√Æne d‚Äô√©tapes qui pr√©pare et rend le devoir. \nLe ‚Äúmanual approval‚Äù, c‚Äôest une √©tape ‚Äúpause‚Äù o√π quelqu‚Äôun doit valider avant d‚Äôavancer. \nSNS, c‚Äôest comme un message automatique (SMS/notification) envoy√© au d√©l√©gu√©. \nLa r√©ponse A est la meilleure car elle ajoute une √©tape d√©di√©e JUSTE avant l‚Äôenvoi final, avec un bouton ‚ÄúApprouver/Refuser‚Äù. \nEt elle r√®gle en plus une r√®gle de notification : d√®s que l‚Äô√©tape d‚Äôapprobation arrive, le d√©l√©gu√© re√ßoit le message via SNS. \nB est trop compliqu√© (cr√©er une autre machine de workflow). \nC m√©lange l‚Äôapprobation dans l‚Äô√©tape de d√©ploiement, moins propre et moins clair. \nD envoie des notifications, mais ne cr√©e pas l‚Äô√©tape ‚Äúpause + validation‚Äù dans la cha√Æne.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:299:c17efc67500b89f1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 299,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company introduced a new feature that should be accessible to only a specific group of premium customers. A developer needs the ability to turn the feature on and off in response to performance and feedback. The developer needs a solution to validate and deploy these configurations quickly without causing any disruptions.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Use AWS AppConfig to manage the feature configuration and to validate and deploy changes. Use feature flags to turn the feature on and off.",
        "B": "Use AWS Secrets Manager to securely manage and validate the feature configurations. Enable lifecycle rules to turn the feature on and off.",
        "C": "Use AWS Config to manage the feature configuration and validation. Set up AWS Config rules to turn the feature on and off based on predefined conditions.",
        "D": "Use AWS Systems Manager Parameter Store to store and validate the configuration settings for the feature. Enable lifecycle rules to turn the feature on and off."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134298-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:20 a.m.",
      "textHash": "c17efc67500b89f1",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Ici, on veut activer/d√©sactiver rapidement une nouvelle fonctionnalit√©, seulement pour certains clients ‚Äúpremium‚Äù, sans interrompre l‚Äôapplication.\nAWS AppConfig (dans AWS Systems Manager) sert justement √† g√©rer des configurations d‚Äôapplication et des ‚Äúfeature flags‚Äù (drapeaux de fonctionnalit√©) sans red√©ployer le code.\nUn feature flag permet d‚Äôallumer/√©teindre une fonction, ou de la montrer seulement √† un pourcentage d‚Äôutilisateurs ou √† un groupe pr√©cis.\nAppConfig peut valider la configuration avant application (format, r√®gles) et d√©ployer progressivement (par √©tapes) pour √©viter les incidents.\nCela r√©pond au besoin de changements rapides, contr√¥l√©s, et r√©versibles en cas de mauvais retours ou de probl√®mes de performance.\nSecrets Manager est fait pour stocker des secrets (mots de passe, cl√©s), pas pour piloter des fonctionnalit√©s.\nAWS Config sert √† auditer la conformit√© des ressources AWS, pas √† activer une feature dans une appli.\nParameter Store stocke des param√®tres, mais n‚Äôoffre pas aussi directement les m√©canismes d√©di√©s de validation et d√©ploiement progressif de feature flags comme AppConfig.\nDonc la meilleure r√©ponse est d‚Äôutiliser AWS AppConfig avec des feature flags (A).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:298:343956f1a461edbb",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 298,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an analytics application that uses an AWS Lambda function to process transaction data asynchronously. A developer notices that asynchronous invocations of the Lambda function sometimes fail. When failed Lambda function invocations occur, the developer wants to invoke a second Lambda function to handle errors and log details.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure a Lambda function destination with a failure condition. Specify Lambda function as the destination type. Specify the error-handling Lambda function's Amazon Resource Name (ARN) as the resource.",
        "B": "Enable AWS X-Ray active tracing on the initial Lambda function. Configure X-Ray to capture stack traces of the failed invocations. Invoke the error-handling Lambda function by including the stack traces in the event object.",
        "C": "Configure a Lambda function trigger with a failure condition. Specify Lambda function as the destination type. Specify the error-handling Lambda function's Amazon Resource Name (ARN) as the resource.",
        "D": "Create a status check alarm on the initial Lambda function. Configure the alarm to invoke the error-handling Lambda function when the alarm is initiated. Ensure that the alarm passes the stack trace in the event object."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134297-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:19 a.m.",
      "textHash": "343956f1a461edbb",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Ici, la fonction AWS Lambda est appel√©e ¬´ asynchronement ¬ª : l‚Äôappelant n‚Äôattend pas la r√©ponse, donc en cas d‚Äô√©chec il faut un m√©canisme automatique pour traiter l‚Äôerreur.\nAWS Lambda propose les ¬´ Destinations ¬ª pour les invocations asynchrones : on peut dire √† Lambda o√π envoyer le r√©sultat en cas de succ√®s ou d‚Äô√©chec.\nLa bonne solution est donc de configurer une destination sur l‚Äô√©v√©nement d‚Äô√©chec (onFailure).\nOn choisit comme destination une autre Lambda (la fonction de gestion d‚Äôerreurs) en indiquant son ARN (identifiant unique de ressource AWS).\nAinsi, √† chaque √©chec, Lambda d√©clenche automatiquement la seconde fonction avec des d√©tails sur l‚Äôinvocation et l‚Äôerreur, ce qui permet de journaliser et traiter.\nX-Ray sert surtout au tra√ßage et au diagnostic, pas √† d√©clencher une autre Lambda automatiquement.\nUn ¬´ trigger ¬ª n‚Äôest pas configur√© avec une condition d‚Äô√©chec de cette mani√®re pour l‚Äôasynchrone.\nUne alarme CloudWatch peut d√©tecter des erreurs globales, mais ce n‚Äôest pas fiable pour d√©clencher une action pour chaque √©chec individuel ni pour transmettre le contexte complet.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un devoir rendu en ligne : tu l‚Äôenvoies au prof, et il le corrige plus tard (pas tout de suite). Parfois, le fichier est illisible ou manque une page. Dans ce cas, tu veux que le prof envoie automatiquement le devoir au surveillant pour noter l‚Äôerreur et te dire ce qui s‚Äôest pass√©.**\n\nIci, la 1√®re Lambda = le prof qui corrige plus tard (appel ‚Äúasynchrone‚Äù). Si √ßa rate, on veut automatiquement appeler une 2e Lambda = le surveillant qui g√®re les erreurs et √©crit un rapport. La solution A fait exactement √ßa : on configure une ‚Äúdestination en cas d‚Äô√©chec‚Äù, comme une r√®gle ‚Äúsi le devoir est illisible, envoie-le au surveillant‚Äù. On donne l‚Äôadresse pr√©cise de la 2e fonction (ARN = son identifiant, comme l‚Äôemail exact du surveillant). B parle de X-Ray : c‚Äôest plut√¥t une cam√©ra pour comprendre, pas un syst√®me d‚Äôenvoi automatique. C dit ‚Äútrigger‚Äù : c‚Äôest pour d√©marrer une fonction quand quelque chose arrive, pas pour g√©rer les √©checs d‚Äôun envoi asynchrone. D parle d‚Äôalarme : √ßa pr√©vient, mais ce n‚Äôest pas fait pour transmettre automatiquement les d√©tails de chaque √©chec √† une 2e fonction.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:297:48e8ff1426e5d922",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 297,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has on-premises data centers that run an image processing service. The service consists of containerized applications that run on Kubernetes clusters. All the applications have access to the same NFS share for files and data storage.The company is running out of NFS capacity in the data centers and needs to migrate to AWS as soon as possible. The Kubernetes clusters must be highly available on AWS.Which combination of actions will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Transfer the information that is in the NFS share to an Amazon Elastic Block Store (Amazon EBS) volume. Upload the container images to Amazon Elastic Container Registry (Amazon ECR).",
        "B": "Transfer the information that is in the NFS share to an Amazon Elastic File System (Amazon EFS) volume. Upload the container images to Amazon Elastic Container Registry (Amazon ECR).",
        "C": "Create an Amazon Elastic Container Service (Amazon ECS) cluster to run the applications. Configure each node of the cluster to mount the Amazon Elastic Block Store (Amazon EBS) volume at the required path for the container images.",
        "D": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to run the applications. Configure each node of the cluster to mount the Amazon Elastic Block Store (Amazon EBS) volume at the required path for the container images.",
        "E": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to run the applications. Configure each node of the cluster to mount the Amazon Elastic File System (Amazon EFS) volume at the required path for the container images."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134296-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:17 a.m.",
      "textHash": "48e8ff1426e5d922",
      "rawFormat": "discussion-md",
      "conceptKey": "ecr_image_scanning",
      "frExplanation": "Le besoin principal est de remplacer un partage NFS (stockage de fichiers partag√©) qui manque de capacit√©, tout en gardant des clusters Kubernetes hautement disponibles sur AWS.\nSur AWS, l‚Äô√©quivalent ‚ÄúNFS manag√©‚Äù est Amazon EFS : c‚Äôest un syst√®me de fichiers partag√© accessible en m√™me temps par plusieurs serveurs/n≈ìuds, comme un NFS, et il s‚Äô√©tend automatiquement.\nAmazon EBS, lui, est un disque ‚Äúattach√©‚Äù √† une instance (plut√¥t bloc), pas un partage de fichiers multi-n≈ìuds simple comme NFS : ce n‚Äôest donc pas le bon remplacement pour un stockage commun √† toutes les applis.\nLes images de conteneurs doivent √™tre stock√©es dans un registre : Amazon ECR est le service AWS pr√©vu pour h√©berger et distribuer des images Docker.\nDonc, migrer les donn√©es du NFS vers EFS + pousser les images dans ECR r√©pond directement aux deux besoins (stockage partag√© + images de conteneurs).\nLes autres propositions parlent de monter EBS sur chaque n≈ìud, ce qui ne fournit pas un vrai partage NFS commun et complique la haute disponibilit√©.\nLa combinaison correcte est donc : transf√©rer le NFS vers EFS et utiliser ECR pour les images (r√©ponse B).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine une √©quipe au lyc√©e qui fait des montages photo. Les ‚Äúapplis‚Äù sont des √©l√®ves, et ils partagent tous le m√™me casier (NFS) pour d√©poser et prendre des fichiers. Le casier est plein, donc vous d√©m√©nagez dans une nouvelle √©cole (AWS) et il faut un casier partag√© plus grand, accessible par tous, m√™me si un couloir est ferm√© (haute dispo).**\n\nConcept : NFS = casier partag√©. Sur AWS, l‚Äô√©quivalent c‚Äôest Amazon EFS : un ‚Äúcasier‚Äù commun que plusieurs machines peuvent utiliser en m√™me temps. Amazon EBS, lui, ressemble plut√¥t √† un cahier attach√© √† un seul √©l√®ve : pas pratique si tout le monde doit √©crire dedans. Donc on transf√®re les fichiers du NFS vers EFS (choix B). Ensuite, les ‚Äúimages de conteneurs‚Äù sont comme des bo√Ætes de mat√©riel standard pour lancer l‚Äôappli : on les met dans Amazon ECR, un ‚Äúrayon de stockage‚Äù pour r√©cup√©rer facilement les bonnes bo√Ætes. B est bon car il remplace le casier partag√© par un casier partag√© AWS (EFS) + met les bo√Ætes d‚Äôapplis au bon endroit (ECR).",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:296:38a76a09402a89c9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 296,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer created an AWS Lambda function that performs a series of operations that involve multiple AWS services. The function's duration time is higher than normal. To determine the cause of the issue, the developer must investigate traffic between the services without changing the function code.Which solution will meet these requirements?",
      "choices": {
        "A": "Enable AWS X-Ray active tracing in the Lambda function. Review the logs in X-Ray.",
        "B": "Configure AWS CloudTrail. View the trail logs that are associated with the Lambda function.",
        "C": "Review the AWS Config logs in Amazon CloudWatch.",
        "D": "Review the Amazon CloudWatch logs that are associated with the Lambda function."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134295-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:17 a.m.",
      "textHash": "38a76a09402a89c9",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Ici, la fonction AWS Lambda (un code qui s‚Äôex√©cute sans serveur) appelle plusieurs services AWS et devient plus lente. On veut comprendre o√π le temps est perdu, en observant les appels r√©seau entre services, sans modifier le code. AWS X-Ray sert justement √† ‚Äútracer‚Äù une requ√™te de bout en bout : il montre chaque √©tape (appel √† DynamoDB, S3, API, etc.), la dur√©e de chaque appel et o√π se trouve la latence. En activant ‚Äúactive tracing‚Äù sur Lambda, on obtient une carte de service et des segments/temps par d√©pendance, ce qui permet d‚Äôidentifier le service ou l‚Äôappel qui ralentit. CloudTrail enregistre surtout les actions API (qui a fait quoi) mais pas une vue de performance d√©taill√©e par requ√™te. AWS Config suit les changements de configuration, pas le trafic ni les temps d‚Äôex√©cution. Les logs CloudWatch montrent ce que l‚Äôapplication √©crit, mais sans instrumentation, ils ne donnent pas une trace compl√®te des appels entre services. Donc la meilleure solution est d‚Äôactiver AWS X-Ray (A).",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un √©l√®ve qui doit faire une mission dans un jeu vid√©o : il parle au PNJ, va au magasin, puis au forgeron, puis revient. La mission est plus lente que d‚Äôhabitude, mais tu n‚Äôas pas le droit de changer la mission, juste d‚Äôobserver le trajet.**\n\nConcept : pour trouver o√π √ßa ralentit, il faut une ‚Äúcarte du trajet‚Äù avec le temps pass√© √† chaque √©tape.\nAWS Lambda = l‚Äô√©l√®ve qui fait la mission. Les services AWS = les PNJ/magasins visit√©s.\nA (AWS X-Ray active tracing) = activer un mode ‚Äúreplay + chronom√®tre‚Äù qui suit chaque aller-retour entre lieux et montre o√π √ßa bloque.\n√áa marche sans changer le code : tu actives juste le suivi, puis tu lis le parcours dans X-Ray.\nB (CloudTrail) = le registre du proviseur : qui a fait quoi, pas combien de temps √ßa a pris entre chaque √©tape.\nC (AWS Config) = l‚Äôinventaire des r√®gles et r√©glages, pas un suivi de trajet.\nD (CloudWatch logs) = le journal de l‚Äô√©l√®ve : utile, mais souvent pas assez pr√©cis pour voir le temps entre chaque service.\nDonc A est la bonne r√©ponse : c‚Äôest l‚Äôoutil fait pour ‚Äúvoir le trafic et les d√©lais‚Äù entre services.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:295:374c39307b2d5c13",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 295,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a social media application that receives large amounts of traffic. User posts and interactions are continuously updated in an Amazon RDS database. The data changes frequently, and the data types can be complex. The application must serve read requests with minimal latency.The application's current architecture struggles to deliver these rapid data updates efficiently. The company needs a solution to improve the application's performance.Which solution will meet these requirements?",
      "choices": {
        "A": "Use Amazon DynamoDB Accelerator (DAX) in front of the RDS database to provide a caching layer for the high volume of rapidly changing data.",
        "B": "Set up Amazon S3 Transfer Acceleration on the RDS database to enhance the speed of data transfer from the databases to the application.",
        "C": "Add an Amazon CloudFront distribution in front of the RDS database to provide a caching layer for the high volume of rapidly changing data.",
        "D": "Create an Amazon ElastiCache for Redis cluster. Update the application code to use a write-through caching strategy and read the data from Redis."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134294-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:14 a.m.",
      "textHash": "374c39307b2d5c13",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:e520d13e",
      "frExplanation": "Ici, la base Amazon RDS (base de donn√©es relationnelle) re√ßoit beaucoup d‚Äô√©critures et les donn√©es changent tout le temps. Le probl√®me principal est la latence en lecture : lire directement dans RDS devient trop lent sous forte charge.\nAmazon ElastiCache for Redis est un cache en m√©moire (tr√®s rapide) plac√© entre l‚Äôapplication et la base : on lit d‚Äôabord dans Redis, ce qui r√©duit fortement le temps de r√©ponse.\nAvec une strat√©gie ¬´ write-through ¬ª, quand l‚Äôapplication √©crit une donn√©e, elle l‚Äô√©crit dans le cache et le cache la stocke aussi dans RDS : le cache reste √† jour automatiquement, m√™me si les donn√©es changent souvent.\nRedis g√®re bien des structures de donn√©es vari√©es et des objets complexes, ce qui correspond au besoin.\nA est faux car DAX ne fonctionne qu‚Äôavec DynamoDB, pas avec RDS.\nB est hors sujet : S3 Transfer Acceleration acc√©l√®re les transferts vers S3, pas l‚Äôacc√®s √† une base RDS.\nC est inadapt√© : CloudFront est un CDN pour du contenu web (fichiers/HTTP), pas un cache direct pour requ√™tes de base de donn√©es tr√®s changeantes.\nDonc la meilleure solution pour des lectures rapides et des mises √† jour fr√©quentes est Redis avec write-through (D).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le mur d‚Äôaffichage du lyc√©e : tout le monde poste des messages, des likes, des commentaires, et plein d‚Äô√©l√®ves veulent juste LIRE tr√®s vite ce qui est nouveau.**\n\nLa base RDS, c‚Äôest comme le grand classeur officiel au secr√©tariat : fiable, mais si 1 000 √©l√®ves vont le consulter en m√™me temps, √ßa ralentit.\nIl faut donc une ‚Äúcopie ultra-rapide‚Äù √† c√¥t√©, comme un tableau blanc dans le couloir, mis √† jour automatiquement.\nRedis (ElastiCache) = ce tableau blanc en m√©moire : lire dessus est quasi instantan√©, donc faible latence.\nWrite-through = quand quelqu‚Äôun √©crit un nouveau post, on l‚Äô√©crit d‚Äôabord sur le tableau blanc ET on le range aussi dans le classeur officiel : les lecteurs voient tout de suite la derni√®re version.\nA est faux : DAX est un acc√©l√©rateur pour DynamoDB, pas pour RDS (mauvais outil).\nB est faux : S3 Transfer Acceleration sert √† envoyer des fichiers vers S3, pas √† acc√©l√©rer une base.\nC est faux : CloudFront cache surtout des contenus ‚Äústables‚Äù (images/vid√©os), pas des donn√©es qui changent tout le temps.\nDonc D : Redis + write-through donne des lectures tr√®s rapides avec des donn√©es √† jour.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:291:737aaef6dc0c30cf",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 291,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA data visualization company wants to strengthen the security of its core applications. The applications are deployed on AWS across its development, staging, pre-production, and production environments. The company needs to encrypt all of its stored sensitive credentials. The sensitive credentials need to be automatically rotated. A version of the sensitive credentials need to be stored for each environment.Which solution will meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "A": "Configure AWS Secrets Manager versions to store different copies of the same credentials across multiple environments.",
        "B": "Create a new parameter version in AWS Systems Manager Parameter Store for each environment. Store the environment-specific credentials in the parameter version.",
        "C": "Configure the environment variables in the application code. Use different names for each environment type.",
        "D": "Configure AWS Secrets Manager to create a new secret for each environment type. Store the environment-specific credentials in the secret."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134290-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:09 a.m.",
      "textHash": "737aaef6dc0c30cf",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Objectif : chiffrer des identifiants sensibles (mots de passe, cl√©s API), les faire tourner automatiquement, et avoir une version par environnement (dev, staging, pr√©prod, prod).\nAWS Secrets Manager est un service fait pour stocker des ‚Äúsecrets‚Äù chiffr√©s et g√©rer la rotation automatique (via int√©gration/rotation planifi√©e).\nLa solution la plus simple √† exploiter est de cr√©er un secret s√©par√© par environnement : un secret ‚Äúdb-password-dev‚Äù, un ‚Äúdb-password-prod‚Äù, etc.\nAinsi, chaque application lit uniquement le secret de son environnement, ce qui √©vite les m√©langes et r√©duit les risques.\nLa rotation peut √™tre configur√©e ind√©pendamment par environnement (fr√©quence, proc√©dure), tout en restant centralis√©e dans Secrets Manager.\nPourquoi pas A : les ‚Äúversions‚Äù d‚Äôun secret servent surtout √† l‚Äôhistorique/rotation d‚Äôun m√™me secret, pas √† s√©parer proprement des environnements.\nPourquoi pas B : Parameter Store peut chiffrer, mais la rotation automatique n‚Äôest pas aussi native/compl√®te que Secrets Manager pour des identifiants.\nPourquoi pas C : des variables d‚Äôenvironnement dans le code/config sont moins s√ªres et ne g√®rent pas la rotation automatique.\nDonc D r√©pond √† chiffrement + rotation automatique + s√©paration claire par environnement avec le moins d‚Äôop√©rations.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:289:d21ba248169dbc85",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 289,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that runs across multiple AWS Regions. The application is experiencing performance issues at irregular intervals. A developer must use AWS X-Ray to implement distributed tracing for the application to troubleshoot the root cause of the performance issues.What should the developer do to meet this requirement?",
      "choices": {
        "A": "Use the X-Ray console to add annotations for AWS services and user-defined services.",
        "B": "Use Region annotation that X-Ray adds automatically for AWS services. Add Region annotation for user-defined services.",
        "C": "Use the X-Ray daemon to add annotations for AWS services and user-defined services.",
        "D": "Use Region annotation that X-Ray adds automatically for user-defined services. Configure X-Ray to add Region annotation for AWS services."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134288-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:04 a.m.",
      "textHash": "d21ba248169dbc85",
      "rawFormat": "discussion-md",
      "conceptKey": "xray_tracing",
      "frExplanation": "AWS X-Ray sert √† ‚Äúsuivre‚Äù une requ√™te quand elle traverse plusieurs composants (API, fonctions, bases de donn√©es) pour trouver o√π √ßa ralentit.\nUne ‚Äúannotation‚Äù est une √©tiquette (cl√©/valeur) ajout√©e aux traces pour filtrer et regrouper, par exemple par R√©gion AWS.\nComme l‚Äôapplication tourne dans plusieurs R√©gions, il faut pouvoir voir si les lenteurs viennent d‚Äôune R√©gion pr√©cise.\nX-Ray ajoute d√©j√† automatiquement l‚Äôannotation de R√©gion pour les services AWS g√©r√©s (ex: API Gateway, Lambda, etc.).\nEn revanche, pour votre code/servicess maison (user-defined services), X-Ray ne peut pas deviner la R√©gion si vous ne l‚Äôajoutez pas.\nDonc la bonne approche est: utiliser l‚Äôannotation R√©gion automatique pour les services AWS, et ajouter vous-m√™me l‚Äôannotation R√©gion dans vos segments pour les services personnalis√©s.\nLes options parlant du ‚Äúdaemon‚Äù sont hors sujet: le daemon envoie les traces, il n‚Äôajoute pas les annotations √† votre place.\nLes options qui inversent ‚Äúautomatique‚Äù (user-defined) vs ‚ÄúAWS services‚Äù sont incorrectes.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une course de relais entre plusieurs stades dans diff√©rentes villes (chaque ville = une ‚ÄúR√©gion‚Äù AWS). Parfois, l‚Äô√©quipe ralentit, mais tu ne sais pas dans quelle ville √ßa coince.**\n\nLe ‚Äúdistributed tracing‚Äù avec AWS X-Ray, c‚Äôest comme mettre un bracelet GPS sur le t√©moin pour voir son trajet et o√π il perd du temps. Pour trouver la ville responsable, il faut marquer chaque √©tape avec l‚Äô√©tiquette ‚Äúville/R√©gion‚Äù. X-Ray met d√©j√† automatiquement l‚Äô√©tiquette de R√©gion pour les services AWS (comme les stades officiels). Mais pour tes propres morceaux d‚Äôappli (les coureurs que tu as recrut√©s toi-m√™me), X-Ray ne devine pas toujours la ville. Donc tu dois ajouter l‚Äôannotation R√©gion √† tes services ‚Äúmaison‚Äù pour que la trace dise clairement o√π √ßa se passe. C‚Äôest exactement le choix B : garder l‚Äôauto-√©tiquette pour AWS, et ajouter la R√©gion pour tes services √† toi.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:286:3695de4386af5c17",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 286,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is using AWS Step Functions to automate a workflow. The workflow defines each step as an AWS Lambda function task. The developer notices that runs of the Step Functions state machine fail in the GetResource task with either an IllegalArgumentException error or a TooManyRequestsException error.The developer wants the state machine to stop running when the state machine encounters an IllegalArgumentException error. The state machine needs to retry the GetResource task one additional time after 10 seconds if the state machine encounters a TooManyRequestsException error. If the second attempt fails, the developer wants the state machine to stop running.How can the developer implement the Lambda retry functionality without adding unnecessary complexity to the state machine?",
      "choices": {
        "A": "Add a Delay task after the GetResource task. Add a catcher to the GetResource task. Configure the catcher with an error type of TooManyRequestsException. Configure the next step to be the Delay task. Configure the Delay task to wait for an interval of 10 seconds. Configure the next step to be the GetResource task.",
        "B": "Add a catcher to the GetResource task. Configure the catcher with an error type of TooManyRequestsException, an interval of 10 seconds, and a maximum attempts value of 1. Configure the next step to be the GetResource task.",
        "C": "Add a retrier to the GetResource task. Configure the retrier with an error type of TooManyRequestsException, an interval of 10 seconds, and a maximum attempts value of 1.",
        "D": "Duplicate the GetResource task. Rename the new GetResource task to TryAgain. Add a catcher to the original GetResource task. Configure the catcher with an error type of TooManyRequestsException. Configure the next step to be TryAgain."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134285-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 6:52 a.m.",
      "textHash": "3695de4386af5c17",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Step Functions orchestre un workflow (machine d‚Äô√©tats) et chaque √©tape peut appeler une fonction AWS Lambda (code ex√©cut√© √† la demande). Ici, l‚Äô√©tape GetResource √©choue parfois avec deux erreurs diff√©rentes. On veut arr√™ter imm√©diatement si c‚Äôest IllegalArgumentException (erreur ‚Äúlogique‚Äù : mauvais param√®tre), donc pas de retry. En revanche, TooManyRequestsException signifie souvent ‚Äútrop d‚Äôappels / limitation‚Äù, donc un nouvel essai apr√®s une pause peut r√©ussir. Step Functions fournit nativement un m√©canisme Retry (retrier) sur une t√¢che, sans ajouter d‚Äô√©tapes suppl√©mentaires. On configure le retrier uniquement pour TooManyRequestsException, avec IntervalSeconds = 10 et MaxAttempts = 1 (donc 1 tentative en plus). Si la deuxi√®me tentative √©choue, l‚Äôex√©cution √©choue et le workflow s‚Äôarr√™te, ce qui correspond au besoin. Les catchers servent plut√¥t √† ‚Äúattraper‚Äù une erreur et bifurquer vers un autre chemin, pas √† faire un retry simple.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o avec une suite de missions (un parcours). Chaque mission est un PNJ (Lambda) que tu vas voir. La mission ‚ÄúGetResource‚Äù te donne un objet. Parfois le PNJ te dit ‚Äútu as fait n‚Äôimporte quoi‚Äù (IllegalArgumentException) et parfois ‚Äúreviens plus tard, trop de monde‚Äù (TooManyRequestsException).**\n\nConcept : Step Functions, c‚Äôest le ‚Äúsc√©nario‚Äù du jeu qui encha√Æne les missions. Un ‚Äúretry‚Äù c‚Äôest comme ‚Äúr√©essayer automatiquement‚Äù la m√™me mission. Un ‚Äúcatch‚Äù c‚Äôest comme ‚Äúsi √ßa rate, va vers un autre chemin‚Äù.\nIci, si c‚Äôest ‚Äútu as fait n‚Äôimporte quoi‚Äù, √ßa ne sert √† rien de r√©essayer : on arr√™te le sc√©nario. Donc on ne met pas de retry pour IllegalArgumentException.\nSi c‚Äôest ‚Äútrop de monde‚Äù, √ßa peut passer apr√®s une pause : on veut r√©essayer UNE fois apr√®s 10 secondes.\nLa r√©ponse C fait exactement √ßa, sans ajouter de nouvelles missions : on met un retrier sur GetResource, uniquement pour TooManyRequestsException, avec 10 secondes d‚Äôattente et 1 tentative en plus.\nSi la 2e tentative √©choue, le sc√©nario s‚Äôarr√™te tout seul (pas besoin de d√©tour compliqu√©).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:285:e752867e1f9cb5b1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 285,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn AWS Lambda function requires read access to an Amazon S3 bucket and requires read/write access to an Amazon DynamoDB table. The correct IAM policy already exists.What is the MOST secure way to grant the Lambda function access to the S3 bucket and the DynamoDB table?",
      "choices": {
        "A": "Attach the existing IAM policy to the Lambda function.",
        "B": "Create an IAM role for the Lambda function. Attach the existing IAM policy to the role. Attach the role to the Lambda function.",
        "C": "Create an IAM user with programmatic access. Attach the existing IAM policy to the user. Add the user access key ID and secret access key as environment variables in the Lambda function.",
        "D": "Add the AWS account root user access key ID and secret access key as encrypted environment variables in the Lambda function."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134284-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 6:37 a.m.",
      "textHash": "e752867e1f9cb5b1",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Le plus s√ªr est d‚Äôutiliser un r√¥le IAM, pas des cl√©s. IAM est le service qui g√®re les permissions sur AWS.\nUne fonction AWS Lambda (code ex√©cut√© sans serveur) doit obtenir des droits via un ¬´ r√¥le d‚Äôex√©cution ¬ª (IAM role) que Lambda assume automatiquement.\nAvec un r√¥le, il n‚Äôy a aucune cl√© secr√®te stock√©e dans le code ou dans des variables d‚Äôenvironnement : AWS fournit des identifiants temporaires et renouvel√©s.\nOn attache donc la politique IAM existante (qui autorise lecture S3 et lecture/√©criture DynamoDB) au r√¥le, puis on associe ce r√¥le √† la fonction Lambda.\nL‚Äôoption A est incorrecte car on n‚Äôattache pas directement une policy √† Lambda : elle s‚Äôattache √† un r√¥le (ou utilisateur/groupe).\nL‚Äôoption C est moins s√ªre : cr√©er un utilisateur IAM et stocker access key/secret dans Lambda expose des secrets et complique la rotation.\nL‚Äôoption D est la pire : utiliser le compte root est interdit en pratique (trop de privil√®ges) et dangereux m√™me si chiffr√©.\nDonc B applique le principe du moindre privil√®ge et √©vite la gestion de secrets.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une √©l√®ve ‚Äúrobot‚Äù qui doit aller √† la biblioth√®que (S3) juste pour LIRE des livres, et au secr√©tariat (DynamoDB) pour LIRE et √âCRIRE des infos. Pour faire √ßa, il lui faut un badge avec les bons droits.**\n\nConcept : en cloud, on ne donne pas des ‚Äúmots de passe‚Äù √† un programme. On lui donne un ‚Äúbadge‚Äù (un r√¥le) avec des permissions pr√©cises.\nPourquoi B : tu cr√©es un r√¥le = un badge sp√©cial pour la fonction Lambda. Tu accroches la r√®gle d√©j√† pr√™te (la policy) sur ce badge. Puis tu donnes ce badge √† Lambda.\nComme √ßa, le robot peut entrer √† la biblioth√®que en lecture seule, et au secr√©tariat en lecture/√©criture, et rien de plus.\nA est moins s√ªr car on n‚Äôattache pas directement une policy √† Lambda : Lambda porte un r√¥le.\nC et D sont dangereux : c‚Äôest comme √©crire un vrai mot de passe (cl√© secr√®te) sur un post-it dans le sac du robot. Si √ßa fuite, quelqu‚Äôun peut tout utiliser.\nDonc le plus s√©curis√© : un r√¥le d√©di√© + la policy dessus + r√¥le attach√© √† Lambda (B).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:277:b923b98719189f90",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 277,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is designing a serverless application for a game in which users register and log in through a web browser. The application makes requests on behalf of users to a set of AWS Lambda functions that run behind an Amazon API Gateway HTTP API.The developer needs to implement a solution to register and log in users on the application's sign-in page. The solution must minimize operational overhead and must minimize ongoing management of user identities.Which solution will meet these requirements?",
      "choices": {
        "A": "Create Amazon Cognito user pools for external social identity providers. Configure IAM roles for the identity pools.",
        "B": "Program the sign-in page to create users' IAM groups with the IAM roles attached to the groups.",
        "C": "Create an Amazon RDS for SQL Server DB instance to store the users and manage the permissions to the backend resources in AWS.",
        "D": "Configure the sign-in page to register and store the users and their passwords in an Amazon DynamoDB table with an attached IAM policy."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134277-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:38 a.m.",
      "textHash": "b923b98719189f90",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, il faut g√©rer l‚Äôinscription et la connexion d‚Äôutilisateurs (identit√©s) pour une appli web qui appelle des fonctions Lambda via API Gateway, avec le moins d‚Äôadministration possible.\nAmazon Cognito est un service AWS fait pour g√©rer des utilisateurs : cr√©ation de comptes, mots de passe, connexion, jetons (tokens) et int√©gration avec des fournisseurs sociaux (Google, Facebook, etc.).\nUn ‚Äúuser pool‚Äù Cognito stocke et authentifie les utilisateurs (login/mot de passe ou via un fournisseur externe).\nUn ‚Äúidentity pool‚Äù peut ensuite donner des autorisations AWS temporaires (via des r√¥les IAM) pour appeler vos API/Lambda de fa√ßon s√©curis√©e.\nC‚Äôest exactement l‚Äôobjectif : d√©l√©guer la gestion des identit√©s √† un service manag√©, sans g√©rer une base de donn√©es d‚Äôutilisateurs ni des mots de passe vous‚Äëm√™me.\nB est faux car cr√©er des groupes/roles IAM par utilisateur est lourd et non pr√©vu pour des utilisateurs finaux.\nC est faux car une base RDS implique maintenance (patching, sauvegardes) et vous g√©rez vous‚Äëm√™me l‚Äôauthentification.\nD est risqu√© et lourd : stocker des mots de passe dans DynamoDB vous oblige √† g√©rer s√©curit√©, hash, reset, MFA, etc.\nDonc la meilleure solution avec faible overhead est Cognito + r√¥les IAM (A).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un club de jeu vid√©o au lyc√©e. Pour entrer, tu peux montrer ta carte de lyc√©e OU te connecter avec ton compte Instagram/Google. Le club ne veut pas fabriquer et g√©rer des cartes pour tout le monde.**\n\nConcept : g√©rer des ‚Äúidentit√©s‚Äù = v√©rifier qui tu es et te laisser entrer. Si le club g√®re lui-m√™me les mots de passe, c‚Äôest beaucoup de boulot et de risques.\nA : Amazon Cognito, c‚Äôest le ‚Äúvideur + registre‚Äù qui sait cr√©er des comptes et aussi accepter des connexions via des r√©seaux sociaux (fournisseurs externes). Les ‚Äúpools‚Äù = la liste des membres et les r√®gles d‚Äôacc√®s. R√©sultat : presque rien √† g√©rer au quotidien.\nB : cr√©er des groupes IAM, c‚Äôest comme cr√©er une carte d‚Äôacc√®s sp√©ciale pour chaque √©l√®ve √† la main : trop de gestion.\nC : une base SQL, c‚Äôest comme tenir un √©norme classeur d‚Äôinscriptions et permissions : lourd √† maintenir.\nD : stocker mots de passe dans une table, c‚Äôest comme √©crire les mots de passe sur un cahier : dangereux et √† g√©rer soi-m√™me.\nDonc A minimise le travail et la gestion des identit√©s.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:275:1c688be257b50661",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 275,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer supports an application that accesses data in an Amazon DynamoDB table. One of the item attributes is expirationDate in the timestamp format. The application uses this attribute to find items, archive them, and remove them from the table based on the timestamp value.The application will be decommissioned soon, and the developer must find another way to implement this functionality. The developer needs a solution that will require the least amount of code to write.Which solution will meet these requirements?",
      "choices": {
        "A": "Enable TTL on the expirationDate attribute in the table. Create a DynamoDB stream. Create an AWS Lambda function to process the deleted items. Create a DynamoDB trigger for the Lambda function.",
        "B": "Create two AWS Lambda functions: one to delete the items and one to process the items. Create a DynamoDB stream. Use the DeleteItem API operation to delete the items based on the expirationDate attribute. Use the GetRecords API operation to get the items from the DynamoDB stream and process them.",
        "C": "Create two AWS Lambda functions: one to delete the items and one to process the items. Create an Amazon EventBridge scheduled rule to invoke the Lambda functions. Use the DeleteItem API operation to delete the items based on the expirationDate attribute. Use the GetRecords API operation to get the items from the DynamoDB table and process them.",
        "D": "Enable TTL on the expirationDate attribute in the table. Specify an Amazon Simple Queue Service (Amazon SQS) dead-letter queue as the target to delete the items. Create an AWS Lambda function to process the items."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134275-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:31 a.m.",
      "textHash": "1c688be257b50661",
      "rawFormat": "discussion-md",
      "conceptKey": "sqs_dlq_redrive",
      "frExplanation": "Ici, on veut automatiquement archiver puis supprimer des √©l√©ments d‚Äôune table DynamoDB quand leur date d‚Äôexpiration est d√©pass√©e, avec le moins de code possible.\nDynamoDB est une base NoSQL g√©r√©e. La fonctionnalit√© TTL (Time To Live) permet de supprimer automatiquement les items dont un attribut de type ‚Äútimestamp‚Äù est arriv√© √† √©ch√©ance.\nEn activant TTL sur l‚Äôattribut expirationDate, on n‚Äôa plus besoin d‚Äô√©crire du code pour rechercher et supprimer : DynamoDB le fait tout seul.\nMais il faut aussi ‚Äúarchiver/traiter‚Äù les items avant/apr√®s suppression : c‚Äôest l√† qu‚Äôintervient DynamoDB Streams, qui enregistre les changements (dont les suppressions).\nOn connecte le stream √† une fonction AWS Lambda (code ex√©cut√© automatiquement) via un trigger : √† chaque suppression TTL, Lambda re√ßoit l‚Äôitem supprim√© et peut l‚Äôarchiver.\nC‚Äôest exactement l‚Äôoption A : services g√©r√©s + d√©clenchement automatique, donc tr√®s peu de code.\nLes options B et C demandent d‚Äô√©crire et planifier soi-m√™me la suppression (DeleteItem, scans/lectures), donc plus de code et plus de maintenance.\nL‚Äôoption D est incorrecte : TTL ne ‚Äúcible‚Äù pas une file SQS comme destination de suppression; pour traiter les suppressions TTL, on utilise plut√¥t DynamoDB Streams + Lambda.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e : chaque livre a une date de retour (expirationDate). Quand la date est d√©pass√©e, le biblioth√©caire enl√®ve le livre du rayon et l‚Äôenvoie au ‚Äústock/archives‚Äù.**\n\nConcept : tu veux un syst√®me qui enl√®ve automatiquement les ‚Äúlivres en retard‚Äù et te pr√©vient pour les archiver, sans que tu √©crives plein de r√®gles √† la main.\nA : activer TTL, c‚Äôest comme dire √† la biblioth√®que : ‚Äú√† la date de retour, retire le livre tout seul‚Äù.\nLe Stream, c‚Äôest le journal des actions : ‚Äútel livre a √©t√© retir√©‚Äù.\nLambda, c‚Äôest un petit robot qui lit ce journal et fait l‚Äôarchivage d√®s qu‚Äôun retrait arrive.\nDonc presque pas de code : AWS retire automatiquement (TTL) + ton robot traite seulement les retraits.\nB et C : tu dois toi-m√™me chercher les livres expir√©s et les supprimer (plus de code, plus d‚Äôentretien).\nD : TTL n‚Äôenvoie pas les livres dans une file SQS comme destination directe, donc √ßa ne colle pas.\nConclusion : A fait le m√©nage automatique + te donne l‚Äôinfo pour archiver, avec le minimum de code.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:273:261652b880e354cf",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 273,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building an application on AWS. The application's backend includes an Amazon API Gateway REST API. The company's frontend application developers cannot continue work until the backend API is ready for integration. The company needs a solution that will allow the frontend application developers to continue their work.Which solution will meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "A": "Configure mock integrations for API Gateway API methods.",
        "B": "Integrate a Lambda function with API Gateway and return a mocked response.",
        "C": "Add new API endpoints to the API Gateway stage and returns a mocked response.",
        "D": "Configure a proxy resource for API Gateway API methods."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134273-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:25 a.m.",
      "textHash": "261652b880e354cf",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:f9c05295",
      "frExplanation": "Objectif : permettre aux d√©veloppeurs front-end de tester l‚Äôint√©gration avant que le vrai backend soit pr√™t.\nAmazon API Gateway est un service qui expose des endpoints HTTP (une API REST) et peut r√©pondre aux requ√™tes.\nUne ‚Äúmock integration‚Äù dans API Gateway permet de renvoyer une r√©ponse statique (ex. JSON d‚Äôexemple) sans appeler aucun backend.\nC‚Äôest le plus efficace op√©rationnellement : pas de code √† √©crire, pas de Lambda √† d√©ployer, juste une configuration par m√©thode (GET/POST).\nLes front-ends peuvent alors appeler les m√™mes URLs et recevoir des r√©ponses pr√©visibles pour d√©velopper l‚ÄôUI et la logique.\nB est moins efficace car il faut cr√©er/maintenir une fonction AWS Lambda (code, permissions, d√©ploiement).\nC n‚Äôest pas une fonctionnalit√© standard telle quelle et ajoute de la gestion de stages/endpoints inutile.\nD (proxy) sert √† relayer vers un backend r√©el, donc ne r√©sout pas l‚Äôabsence de backend.\nDonc la meilleure solution est de configurer des mock integrations sur les m√©thodes API Gateway (A).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un groupe qui pr√©pare un spectacle au coll√®ge : l‚Äô√©quipe ‚Äúd√©cors‚Äù (frontend) doit avancer, mais l‚Äô√©quipe ‚Äúsc√©nario‚Äù (backend API) n‚Äôa pas encore fini l‚Äôhistoire.**\n\nConcept : le frontend a besoin de ‚Äúfausses r√©ponses‚Äù pour tester ses √©crans, comme si on donnait un faux script provisoire aux acteurs.\nAPI Gateway, c‚Äôest la ‚Äúporte d‚Äôentr√©e‚Äù o√π le frontend vient demander des infos au backend.\nSolution A (mock integrations) = la porte r√©pond toute seule avec des r√©ponses factices, sans attendre le vrai sc√©nario.\nC‚Äôest le plus efficace : pas besoin de cr√©er une vraie √©quipe derri√®re (pas de code, pas de serveur), juste des r√©ponses pr√©-√©crites.\nB serait comme embaucher un √©l√®ve pour jouer le sc√©nariste et improviser : plus de travail √† g√©rer.\nC ajoute des ‚Äúnouvelles sc√®nes‚Äù au lieu de simuler les existantes : √ßa ne colle pas au besoin d‚Äôint√©gration.\nD (proxy) renvoie tout vers quelqu‚Äôun d‚Äôautre : √ßa n‚Äôaide pas si le sc√©nario n‚Äôexiste pas encore.\nDonc A permet aux devs frontend de continuer imm√©diatement avec des r√©ponses de test.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:272:c6d6d361abb1f2ac",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 272,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn Amazon Simple Queue Service (Amazon SQS) queue serves as an event source for an AWS Lambda function. In the SQS queue, each item corresponds to a video file that the Lambda function must convert to a smaller resolution. The Lambda function is timing out on longer video files, but the Lambda function's timeout is already configured to its maximum value.What should a developer do to avoid the timeouts without additional code changes?",
      "choices": {
        "A": "Increase the memory configuration of the Lambda function.",
        "B": "Increase the visibility timeout on the SQS queue.",
        "C": "Increase the instance size of the host that runs the Lambda function.",
        "D": "Use multi-threading for the conversion."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134272-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:23 a.m.",
      "textHash": "c6d6d361abb1f2ac",
      "rawFormat": "discussion-md",
      "conceptKey": "sqs_visibility_timeout",
      "frExplanation": "Ici, Amazon SQS est une file d‚Äôattente qui envoie des messages √† AWS Lambda (un service qui ex√©cute du code sans g√©rer de serveur). Chaque message demande de convertir une vid√©o.\nLe probl√®me : certaines vid√©os sont longues, et la fonction Lambda d√©passe le temps maximum autoris√© (timeout). On ne peut pas augmenter davantage ce timeout.\nSans changer le code, la seule ‚Äúmanette‚Äù utile est la configuration de Lambda.\nAugmenter la m√©moire de Lambda (A) augmente aussi la puissance CPU allou√©e et souvent le d√©bit disque/r√©seau : la conversion va plus vite, donc finit avant le timeout.\nAugmenter le visibility timeout SQS (B) ne rend pas le traitement plus rapide : √ßa √©vite juste qu‚Äôun message soit relu trop t√¥t pendant un traitement en cours.\nIl n‚Äôy a pas d‚Äô‚Äúinstance h√¥te‚Äù √† dimensionner pour Lambda (C) : c‚Äôest g√©r√© par AWS.\nLe multi-threading (D) n√©cessite de modifier le code, ce qui est interdit par l‚Äô√©nonc√©.\nDonc la meilleure action sans changement de code est d‚Äôaugmenter la m√©moire de la fonction Lambda.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine o√π tu dois mixer des smoothies. Chaque ticket de commande (dans une bo√Æte) correspond √† une vid√©o √† ‚Äúr√©tr√©cir‚Äù. Un √©l√®ve (Lambda) prend un ticket et doit finir avant la sonnerie.**\n\nConcept : la bo√Æte (SQS) donne des t√¢ches, l‚Äô√©l√®ve (Lambda) les fait, mais il a un temps max avant la sonnerie (timeout). Si la vid√©o est longue, il n‚Äôa pas le temps.\nPourquoi A : donner plus de ‚Äúm√©moire‚Äù √† Lambda, c‚Äôest comme donner √† l‚Äô√©l√®ve un mixeur plus puissant et plus rapide. Il finit le smoothie plus vite, donc moins de risques de d√©passer la sonnerie, sans changer la recette (pas de code).\nPourquoi pas B : augmenter le ‚Äútemps de visibilit√©‚Äù c‚Äôest juste dire ‚Äúne redonnez pas ce ticket √† un autre √©l√®ve tout de suite‚Äù. √áa ne rend pas l‚Äô√©l√®ve plus rapide, donc il peut quand m√™me d√©passer la sonnerie.\nPourquoi pas C : tu ne choisis pas la taille de la machine qui h√©berge Lambda, c‚Äôest g√©r√© par AWS.\nPourquoi pas D : le multi-threading, c‚Äôest changer la fa√ßon de travailler (du code), interdit ici.\nDonc la bonne r√©ponse est A.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:270:a8b4b6ecd01ef5da",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 270,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company notices that credentials that the company uses to connect to an external software as a service (SaaS) vendor are stored in a configuration file as plaintext.The developer needs to secure the API credentials and enforce automatic credentials rotation on a quarterly basis.Which solution will meet these requirements MOST securely?",
      "choices": {
        "A": "Use AWS Key Management Service (AWS KMS) to encrypt the configuration file. Decrypt the configuration file when users make API calls to the SaaS vendor. Enable rotation.",
        "B": "Retrieve temporary credentials from AWS Security Token Service (AWS STS) every 15 minutes. Use the temporary credentials when users make API calls to the SaaS vendor.",
        "C": "Store the credentials in AWS Secrets Manager and enable rotation. Configure the API to have Secrets Manager access.",
        "D": "Store the credentials in AWS Systems Manager Parameter Store and enable rotation. Retrieve the credentials when users make API calls to the SaaS vendor."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134270-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 4:59 a.m.",
      "textHash": "a8b4b6ecd01ef5da",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Le probl√®me : des identifiants (cl√© API / mot de passe) sont en clair dans un fichier, donc faciles √† voler si quelqu‚Äôun lit le serveur ou le d√©p√¥t de code.\nAWS Secrets Manager est un service fait pour stocker des ‚Äúsecrets‚Äù (mots de passe, cl√©s API) de fa√ßon chiffr√©e et contr√¥l√©e par des permissions IAM.\nIl permet aussi la rotation automatique : on peut configurer une rotation tous les 90 jours (trimestrielle) via une fonction (souvent AWS Lambda) qui met √† jour le secret.\nL‚Äôapplication ne garde plus le secret dans un fichier : elle le lit √† la demande depuis Secrets Manager, avec des droits limit√©s.\nC‚Äôest plus s√ªr car le secret est chiffr√©, auditable (logs), et on peut changer la valeur sans red√©ployer l‚Äôapplication.\nA (chiffrer un fichier avec KMS) prot√®ge le fichier, mais ne g√®re pas bien la rotation automatique et laisse un risque op√©rationnel (fichier √† distribuer/mettre √† jour).\nB (STS) fournit des identit√©s temporaires AWS, pas des identifiants pour un fournisseur SaaS externe.\nD (Parameter Store) peut stocker des valeurs, mais la rotation automatique native est moins adapt√©e/compl√®te que Secrets Manager pour des identifiants sensibles.\nDonc la solution la plus s√©curis√©e et conforme √† la rotation trimestrielle est : stocker dans Secrets Manager et activer la rotation (C).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:267:628d639e77d7a785",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 267,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is updating the production version of an AWS Lambda function to fix a defect. The developer has tested the updated code in a test environment. The developer wants to slowly roll out the updates to a small subset of production users before rolling out the changes to all users. Only 10% of the users should be initially exposed to the new code in production.Which solution will meet these requirements?",
      "choices": {
        "A": "Update the Lambda code and create a new version of the Lambda function. Create a Lambda function trigger. Configure the traffic weights in the trigger between the two Lambda function versions. Send 90% of the traffic to the production version, and send 10% of the traffic to the new version.",
        "B": "Create a new Lambda function that uses the updated code. Create a Lambda alias for the production Lambda function. Configure the Lambda alias to send 90% of the traffic to the production Lambda function, and send 10% of the traffic to the test Lambda function.",
        "C": "Update the Lambda code and create a new version of the Lambda function. Create a Lambda proxy integration. Configure the Lambda proxy to split traffic between the two Lambda function versions. Send 90% of the traffic to the production version, and send 10% of the traffic to the new version.",
        "D": "Update the Lambda code and create a new version of the Lambda function. Create a Lambda function alias. Configure the traffic weights in the Lambda alias between the two Lambda function versions. Send 90% of the traffic to the production version, and send 10% of the traffic to the new version."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134267-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 4:09 a.m.",
      "textHash": "628d639e77d7a785",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:b55bcbdc",
      "frExplanation": "Pour d√©ployer une correction sans risque, on veut faire un ‚Äúd√©ploiement progressif‚Äù (canary) : seulement 10% des appels vont vers le nouveau code, puis on augmente si tout va bien.\nAWS Lambda permet de publier des ‚Äúversions‚Äù (copies immuables du code) et d‚Äôutiliser un ‚Äúalias‚Äù (un nom stable comme prod) qui pointe vers une version.\nUn alias peut aussi faire du ‚Äúrouting pond√©r√©‚Äù : il envoie par exemple 90% des invocations vers l‚Äôancienne version et 10% vers la nouvelle.\nC‚Äôest exactement le besoin : m√™me fonction en production, deux versions, et un partage de trafic contr√¥l√©.\nLa r√©ponse D d√©crit : cr√©er une nouvelle version, cr√©er/configurer un alias, puis r√©gler les poids 90/10.\nA est faux car on ne r√®gle pas le split de trafic dans un ‚Äútrigger‚Äù Lambda.\nB est moins adapt√© : cr√©er une autre fonction n‚Äôest pas n√©cessaire, et le routage pond√©r√© se fait entre versions via alias, pas entre deux fonctions distinctes.\nC est faux car un ‚ÄúLambda proxy integration‚Äù (souvent via API Gateway) ne sert pas √† r√©partir le trafic entre versions de Lambda.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:266:36fd8f60011bed2c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 266,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building a compute-intensive application that will run on a fleet of Amazon EC2 instances. The application uses attached Amazon Elastic Block Store (Amazon EBS) volumes for storing data. The Amazon EBS volumes will be created at time of initial deployment. The application will process sensitive information. All of the data must be encrypted. The solution should not impact the application's performance.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure the fleet of EC2 instances to use encrypted EBS volumes to store data.",
        "B": "Configure the application to write all data to an encrypted Amazon S3 bucket.",
        "C": "Configure a custom encryption algorithm for the application that will encrypt and decrypt all data.",
        "D": "Configure an Amazon Machine Image (AMI) that has an encrypted root volume and store the data to ephemeral disks."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134266-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 4:03 a.m.",
      "textHash": "36fd8f60011bed2c",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:6922e6ac",
      "frExplanation": "Ici, on veut chiffrer toutes les donn√©es stock√©es sur les disques attach√©s aux instances EC2. EC2 = des serveurs virtuels, et EBS = des ‚Äúdisques‚Äù r√©seau persistants attach√©s √† ces serveurs.\nLa meilleure option est d‚Äôactiver le chiffrement EBS : AWS chiffre automatiquement les donn√©es ‚Äúau repos‚Äù sur le volume, et aussi les snapshots, sans que l‚Äôapplication change.\nLe chiffrement EBS est g√©r√© par AWS (souvent via AWS KMS) et est con√ßu pour avoir un impact n√©gligeable sur les performances, ce qui r√©pond √† la contrainte ‚Äúne pas impacter l‚Äôapplication‚Äù.\nB est hors sujet car l‚Äôapplication est d√©crite comme utilisant des volumes EBS attach√©s, pas S3 (S3 = stockage d‚Äôobjets, pas un disque).\nC ajoute de la complexit√© et du co√ªt CPU (chiffrer/d√©chiffrer dans l‚Äôapplication), donc risque d‚Äôimpacter les performances.\nD utilise des disques √©ph√©m√®res (instance store) qui ne sont pas persistants et ne r√©pondent pas au besoin de stockage EBS; chiffrer seulement le volume racine ne chiffre pas forc√©ment les volumes de donn√©es.\nDonc il faut configurer la flotte EC2 pour utiliser des volumes EBS chiffr√©s (A).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une salle d‚Äôexamen o√π tu dois ranger des copies tr√®s secr√®tes dans des casiers √† c√¥t√© de ton bureau.**\n\nLes EC2, c‚Äôest comme les √©l√®ves qui travaillent (ils calculent beaucoup). Les EBS, ce sont les casiers attach√©s √† chaque √©l√®ve pour stocker ses copies. On veut que tout soit chiffr√© (comme un cadenas) d√®s le d√©but, sans ralentir l‚Äô√©l√®ve. La meilleure solution est de mettre des casiers d√©j√† avec cadenas int√©gr√© (EBS chiffr√©s) : l‚Äô√©l√®ve range et reprend ses copies normalement, sans effort en plus. C‚Äôest exactement A : activer le chiffrement des volumes EBS utilis√©s par les EC2. B change de ‚Äúcasiers‚Äù vers un grand entrep√¥t (S3) et ce n‚Äôest pas ce qui est pr√©vu ici. C oblige l‚Äô√©l√®ve √† fermer/ouvrir le cadenas √† la main √† chaque fois (√ßa ralentit). D met le cadenas sur le casier principal mais demande d‚Äôutiliser des tiroirs temporaires (ephemeral) qui peuvent dispara√Ætre.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:265:968c1cad7edbe481",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 265,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application that runs on AWS receives messages from an Amazon Simple Queue Service (Amazon SQS) queue and processes the messages in batches. The application sends the data to another SQS queue to be consumed by another legacy application. The legacy system can take up to 5 minutes to process some transaction data.A developer wants to ensure that there are no out-of-order updates in the legacy system. The developer cannot alter the behavior of the legacy system.Which solution will meet these requirements?",
      "choices": {
        "A": "Use an SQS FIFO queue. Configure the visibility timeout value.",
        "B": "Use an SQS standard queue with a SendMessageBatchRequestEntry data type. Configure the DelaySeconds values.",
        "C": "Use an SQS standard queue with a SendMessageBatchRequestEntry data type. Configure the visibility timeout value.",
        "D": "Use an SQS FIFO queue. Configure the DelaySeconds value."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134265-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 3:53 a.m.",
      "textHash": "968c1cad7edbe481",
      "rawFormat": "discussion-md",
      "conceptKey": "sqs_visibility_timeout",
      "frExplanation": "Ici, le point cl√© est l‚Äôordre des messages. Une file SQS Standard peut livrer des messages ‚Äúau moins une fois‚Äù et parfois dans le d√©sordre, donc elle ne garantit pas l‚Äôordre pour le syst√®me legacy.\nUne file SQS FIFO (First-In-First-Out) garantit que les messages sont trait√©s dans l‚Äôordre d‚Äôenvoi (par groupe) et √©vite les doublons, ce qui r√©duit le risque de mises √† jour hors s√©quence.\nLe syst√®me legacy peut mettre jusqu‚Äô√† 5 minutes : pendant ce temps, il ne faut pas qu‚Äôun m√™me message redevienne visible et soit retrait√© en parall√®le.\nLe ‚Äúvisibility timeout‚Äù de SQS cache temporairement un message apr√®s sa r√©ception : si le traitement n‚Äôest pas fini avant la fin du d√©lai, le message peut r√©appara√Ætre et √™tre trait√© √† nouveau.\nDonc on r√®gle le visibility timeout √† une valeur > 5 minutes (avec marge) pour laisser le temps au traitement batch/legacy.\nDelaySeconds ne sert qu‚Äô√† retarder la livraison initiale d‚Äôun message, pas √† emp√™cher les retraits en double ni √† garantir l‚Äôordre.\nConclusion : FIFO + visibility timeout r√©pond √† la fois √† l‚Äôordre et au temps de traitement long.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine : tu √©cris des commandes sur des tickets. Un √©l√®ve (ton appli) prend un paquet de tickets, pr√©pare les plateaux, puis les passe √† un autre √©l√®ve plus lent (le vieux syst√®me) qui peut mettre 5 minutes par ticket.**\n\nUne ‚Äúfile SQS‚Äù, c‚Äôest comme une bo√Æte o√π on empile des tickets √† traiter.\nLe probl√®me : si les tickets arrivent dans le d√©sordre, le vieux syst√®me peut appliquer ‚Äúchanger menu‚Äù puis apr√®s ‚Äúannuler changement‚Äù, et √ßa fait des mises √† jour incoh√©rentes.\nUne file SQS FIFO, c‚Äôest une file ‚Äúpremier arriv√©, premier servi‚Äù : l‚Äôordre est garanti, comme une vraie queue √† la cantine.\nLe ‚Äúvisibility timeout‚Äù, c‚Äôest le temps pendant lequel un ticket pris par quelqu‚Äôun devient invisible pour les autres, pour √©viter qu‚Äôil soit trait√© deux fois.\nComme le vieux syst√®me peut prendre 5 minutes, on r√®gle ce temps assez long : pendant qu‚Äôil bosse, personne ne reprend le m√™me ticket.\nDelaySeconds, c‚Äôest juste ‚Äúattendre avant de mettre le ticket dans la file‚Äù : √ßa ne garantit pas l‚Äôordre ni l‚Äôunicit√©.\nDonc A : FIFO + visibility timeout = ordre respect√© et pas de doublons pendant les 5 minutes.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:264:e21f0b4c3b8810d8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 264,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building a new application that runs on AWS and uses Amazon API Gateway to expose APIs. Teams of developers are working on separate components of the application in parallel. The company wants to publish an API without an integrated backend so that teams that depend on the application backend can continue the development work before the API backend development is complete.Which solution will meet these requirements?",
      "choices": {
        "A": "Create API Gateway resources and set the integration type value to MOCK. Configure the method integration request and integration response to associate a response with an HTTP status code. Create an API Gateway stage and deploy the API.",
        "B": "Create an AWS Lambda function that returns mocked responses and various HTTP status codes. Create API Gateway resources and set the integration type value to AWS_PROXY. Deploy the API.",
        "C": "Create an EC2 application that returns mocked HTTP responses. Create API Gateway resources and set the integration type value to AWS. Create an API Gateway stage and deploy the API.",
        "D": "Create API Gateway resources and set the integration type value set to HTTP_PROXY. Add mapping templates and deploy the API. Create an AWS Lambda layer that returns various HTTP status codes. Associate the Lambda layer with the API deployment."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134264-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 3:44 a.m.",
      "textHash": "e21f0b4c3b8810d8",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:a000936d",
      "frExplanation": "Ici, on veut publier une API (via Amazon API Gateway) alors que le ‚Äúvrai‚Äù backend n‚Äôexiste pas encore. API Gateway est un service qui re√ßoit des requ√™tes HTTP et les route vers un backend (Lambda, HTTP, etc.).\nPour permettre aux autres √©quipes de tester et d√©velopper, il faut que l‚ÄôAPI r√©ponde quand m√™me, sans appeler aucun backend.\nLe type d‚Äôint√©gration MOCK d‚ÄôAPI Gateway sert exactement √† √ßa : API Gateway fabrique une r√©ponse ‚Äúfactice‚Äù (mock) directement.\nOn configure alors, pour chaque m√©thode (GET/POST‚Ä¶), la r√©ponse attendue et le code HTTP (200, 400, etc.) via l‚Äôintegration request/response.\nEnsuite, on d√©ploie l‚ÄôAPI sur un stage (ex: dev) pour obtenir une URL utilisable par les √©quipes.\nLes autres options cr√©ent un faux backend (Lambda ou EC2) ou utilisent des proxys, ce qui ajoute des ressources inutiles et ne r√©pond pas au besoin ‚Äúsans backend int√©gr√©‚Äù.\nDonc la bonne solution est A.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu es d√©l√©gu√© et tu annonces le menu de la cantine sur un panneau, mais la cuisine n‚Äôest pas encore pr√™te. Tu veux quand m√™me que les √©l√®ves puissent tester le ‚Äúprocess‚Äù (choisir un plat) et voir une r√©ponse (‚ÄúOK, plat r√©serv√©‚Äù ou ‚ÄúRupture‚Äù).**\n\nAPI Gateway, c‚Äôest le panneau/guichet o√π on passe commande. Le ‚Äúbackend‚Äù, c‚Äôest la cuisine qui pr√©pare vraiment les plats. Ici, on veut publier le guichet m√™me si la cuisine n‚Äôexiste pas encore, pour que les autres √©quipes puissent avancer. La solution A utilise une int√©gration MOCK : c‚Äôest comme si le guichet r√©pondait tout seul avec des messages pr√©vus √† l‚Äôavance. On configure quelles r√©ponses donner (ex: code 200 = ‚ÄúOK‚Äù, code 400 = ‚ÄúErreur‚Äù) sans appeler une vraie cuisine. Puis on ‚Äúd√©ploie‚Äù sur un stage : c‚Äôest comme afficher officiellement le panneau dans le hall. Les autres choix cr√©ent une fausse cuisine (Lambda/EC2) ou un proxy, donc ce n‚Äôest plus ‚Äúsans backend int√©gr√©‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:263:019ca078e2873948",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 263,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer manages three AWS accounts. Each account contains an Amazon RDS DB instance in a private subnet. The developer needs to define users in each database in a consistent way. The developer must ensure that the same users are created and updated later in all three accounts.Which solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "A": "Create an AWS CloudFormation template. Declare the users in the template. Attach the users to the database. Deploy the template in each account.",
        "B": "Create an AWS CloudFormation template that contains a custom resource to create the users in the database. Deploy the template in each account.",
        "C": "Write a script that creates the users. Deploy an Amazon EC2 instance in each account to run the script on the databases. Run the script in each account.",
        "D": "Implement an AWS Lambda function that creates the users in the database. Provide the function with the details of all three accounts."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134263-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 3:43 a.m.",
      "textHash": "019ca078e2873948",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:bbb2f412",
      "frExplanation": "Objectif : cr√©er les m√™mes utilisateurs (comptes) dans 3 bases Amazon RDS, chacune dans un compte AWS diff√©rent, et pouvoir les mettre √† jour plus tard facilement.\nAmazon RDS est un service de base de donn√©es g√©r√© ; les ‚Äúutilisateurs‚Äù sont des objets internes √† la base (SQL), pas des ressources AWS standard.\nAWS CloudFormation sert √† d√©ployer de l‚Äôinfrastructure ‚Äúcomme du code‚Äù de fa√ßon reproductible dans plusieurs comptes.\nMais CloudFormation ne sait pas nativement ‚Äúd√©clarer des utilisateurs SQL‚Äù dans une base RDS : il faut ex√©cuter du code qui se connecte √† la base et cr√©e/modifie les users.\nUn ‚Äúcustom resource‚Äù CloudFormation permet justement d‚Äôappeler du code (souvent une Lambda) pendant le d√©ploiement pour faire ces actions dans la base.\nAinsi, un seul template versionn√© = m√™me logique dans les 3 comptes, et une mise √† jour du template relance la cr√©ation/mise √† jour des utilisateurs partout.\nC‚Äôest plus efficace op√©rationnellement que g√©rer des EC2 et des scripts (maintenance, patching) ou une Lambda centrale multi-comptes plus complexe.\nDonc la meilleure r√©ponse est B : CloudFormation + custom resource pour cr√©er/mettre √† jour les utilisateurs dans chaque compte.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine 3 coll√®ges diff√©rents, chacun avec sa propre biblioth√®que ferm√©e (acc√®s priv√©). Tu veux que les m√™mes cartes d‚Äôabonnement (utilisateurs) existent partout, et qu‚Äôune mise √† jour se fasse pareil dans les 3.**\n\nConcept : CloudFormation, c‚Äôest comme une ‚Äúfiche de proc√©dure‚Äù officielle qui d√©crit quoi installer. Mais une fiche ne peut pas directement ‚Äúcr√©er une carte d‚Äôabonn√©‚Äù dans une biblioth√®que : il faut quelqu‚Äôun qui ex√©cute l‚Äôaction.\nB : tu ajoutes √† la fiche un ‚Äúassistant sp√©cial‚Äù (custom resource) qui, pendant l‚Äôinstallation, va vraiment entrer dans la biblioth√®que et cr√©er/mettre √† jour les cartes d‚Äôabonn√©s. Tu d√©ploies la m√™me fiche dans chaque coll√®ge : m√™mes cartes partout, m√™mes changements plus tard.\nA ne marche pas bien car la fiche seule ne sait pas g√©rer des utilisateurs √† l‚Äôint√©rieur de la base.\nC est lourd : 3 ordinateurs √† g√©rer juste pour lancer un script.\nD m√©lange les 3 coll√®ges dans une seule action, plus compliqu√© √† g√©rer et √† s√©curiser.\nDonc B est le plus simple √† op√©rer et le plus fiable pour garder les 3 identiques.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:259:4b8bdb3c1351c379",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 259,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an application that reads and writes to multiple Amazon S3 buckets. The application will be deployed to an Amazon EC2 instance. The developer wants to make secure API requests from the EC2 instances without the need to manage the security credentials for the application. The developer needs to apply the principle of least privilege.Which solution will meet these requirements?",
      "choices": {
        "A": "Create an IAM user. Create access keys and secret keys for the user. Associate the user with an IAM policy that allows s3:* permissions.",
        "B": "Associate the EC2 instance with an IAM role that has an IAM policy that allows s3:ListBucket and s3:*Object permissions for specific S3 buckets.",
        "C": "Associate the EC2 instance with an IAM role that has an AmazonS3FullAccess AWS managed policy.",
        "D": "Create a bucket policy on the S3 bucket that allows s3:ListBucket and s3:*Object permissions to the EC2 instance."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134260-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 3:09 a.m.",
      "textHash": "4b8bdb3c1351c379",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Sur AWS, pour acc√©der √† S3 (stockage d‚Äôobjets), une application doit avoir des permissions IAM.\nSur une instance EC2, la bonne pratique est d‚Äôutiliser un r√¥le IAM attach√© √† l‚Äôinstance : l‚Äôapplication re√ßoit automatiquement des identifiants temporaires, sans stocker de cl√©s.\nLe ‚Äúmoindre privil√®ge‚Äù signifie donner uniquement les actions n√©cessaires et seulement sur les ressources n√©cessaires.\nLa r√©ponse B cr√©e un r√¥le IAM pour l‚ÄôEC2 avec une politique limit√©e : lister le contenu du bucket (s3:ListBucket) et lire/√©crire des objets (s3:*Object) uniquement dans des buckets pr√©cis.\nA est mauvais car il faut g√©rer des cl√©s d‚Äôacc√®s (risque de fuite) et s3:* est trop large.\nC est mauvais car AmazonS3FullAccess donne trop de droits sur tous les buckets.\nD n‚Äôest pas id√©al : une bucket policy seule ne remplace pas une identit√© IAM pour l‚Äôapplication, et l‚Äôacc√®s ‚Äú√† l‚Äôinstance‚Äù n‚Äôest pas le mod√®le recommand√© (on autorise plut√¥t un r√¥le IAM).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:258:00892de18dd4d3df",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 258,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is migrating an application to Amazon Elastic Kubernetes Service (Amazon EKS). The developer migrates the application to Amazon Elastic Container Registry (Amazon ECR) with an EKS cluster. As part of the application migration to a new backend, the developer creates a new AWS account. The developer makes configuration changes to the application to point the application to the new AWS account and to use new backend resources. The developer successfully tests the changes within the application by deploying the pipeline.The Docker image build and the pipeline deployment are successful, but the application is still connecting to the old backend. The developer finds that the application's configuration is still referencing the original EKS cluster and not referencing the new backend resources.Which reason can explain why the application is not connecting to the new resources?",
      "choices": {
        "A": "The developer did not successfully create the new AWS account.",
        "B": "The developer added a new tag to the Docker image.",
        "C": "The developer did not update the Docker image tag to a new version.",
        "D": "The developer pushed the changes to a new Docker image tag."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134259-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 3:06 a.m.",
      "textHash": "00892de18dd4d3df",
      "rawFormat": "discussion-md",
      "conceptKey": "ecr_image_scanning",
      "frExplanation": "Dans EKS (Kubernetes manag√©), l‚Äôapplication tourne dans des conteneurs cr√©√©s √† partir d‚Äôune image Docker stock√©e dans ECR (registre d‚Äôimages). Une ‚Äúpipeline‚Äù peut reconstruire et red√©ployer, mais Kubernetes choisit quelle image ex√©cuter via son tag (ex: latest, v1, v2). Si vous modifiez la configuration (nouveau compte AWS, nouveaux endpoints) mais que vous r√©utilisez le m√™me tag d‚Äôimage, le cluster peut continuer √† utiliser l‚Äôancienne image d√©j√† en cache ou d√©j√† r√©f√©renc√©e par le d√©ploiement. R√©sultat : le d√©ploiement ‚Äúr√©ussit‚Äù, mais l‚Äôapplication reste avec l‚Äôancienne configuration et pointe vers l‚Äôancien backend/cluster. Pour forcer la prise en compte, il faut publier une nouvelle version (nouveau tag) et mettre √† jour le manifeste/deployment EKS pour utiliser ce nouveau tag. Donc la cause la plus probable est : le tag de l‚Äôimage n‚Äôa pas √©t√© mis √† jour vers une nouvelle version (C).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o o√π tu changes de serveur (nouveau monde) mais ton jeu garde en m√©moire l‚Äôancienne adresse du serveur.**\n\nConcept : une ‚Äúimage Docker‚Äù, c‚Äôest comme une cartouche/une version du jeu avec ses r√©glages dedans. Un ‚Äútag‚Äù, c‚Äôest l‚Äô√©tiquette de version (v1, v2‚Ä¶).\nTu as modifi√© les r√©glages pour viser le nouveau compte et le nouveau ‚Äúserveur‚Äù (nouveau backend), et les tests passent.\nMais si tu red√©ploies encore la m√™me version (m√™me tag), le cluster relance l‚Äôancienne cartouche, donc avec les anciens r√©glages.\nR√©sultat : l‚Äôappli continue de parler √† l‚Äôancien backend, m√™me si la livraison (pipeline) a r√©ussi.\nDonc la raison la plus logique est C : tu n‚Äôas pas mis √† jour le tag vers une nouvelle version, et l‚Äôancien contenu est r√©utilis√©.\nA ne colle pas car tu as pu tester et d√©ployer. B et D ne causent pas ce probl√®me : changer/ajouter un tag ne garantit pas que la version utilis√©e a chang√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:257:59cd900a4b2f9edd",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 257,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application that will use an Amazon API Gateway API with an AWS Lambda backend. The team that will develop the frontend requires immediate access to the API endpoints to build the UI. To prepare the backend application for integration, the developer needs to set up endpoints. The endpoints need to return predefined HTTP status codes and JSON responses for the frontend team. The developer creates a method for an API resource.Which solution will meet these requirements?",
      "choices": {
        "A": "Set the integration type to AWS_PROXY. Provision Lambda functions to return hardcoded JSON data.",
        "B": "Set the integration type to MOCK. Configure the method's integration request and integration response to associate a JSON responses with specific HTTP status codes.",
        "C": "Set the integration type to HTTP_PROXY. Configure API Gateway to pass all requests to an external placeholder API. which the team will build.",
        "D": "Set the integration type to MOCK. Use a method request to define HTTP status codes. Use an integration request to define JSON responses."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134068-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2024, 5:21 p.m.",
      "textHash": "59cd900a4b2f9edd",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:dce7325f",
      "frExplanation": "Ici, l‚Äô√©quipe frontend veut des endpoints tout de suite, m√™me si le backend (Lambda) n‚Äôest pas pr√™t. Amazon API Gateway sert √† cr√©er des routes HTTP (ex: GET /items) et √† renvoyer des r√©ponses. Le plus simple est d‚Äôutiliser une int√©gration ¬´ MOCK ¬ª : API Gateway r√©pond lui‚Äëm√™me sans appeler Lambda ni un autre service. Avec MOCK, on configure dans ¬´ Integration Response ¬ª quelles r√©ponses JSON renvoyer, et on associe chaque r√©ponse √† un code HTTP (200, 400, etc.). Cela permet de simuler l‚ÄôAPI (des ‚Äústubs‚Äù) pour que le frontend d√©veloppe l‚ÄôUI en parall√®le. L‚Äôoption A oblige √† cr√©er et d√©ployer des fonctions Lambda juste pour renvoyer du faux, ce n‚Äôest pas n√©cessaire. L‚Äôoption D est incorrecte car les codes HTTP se d√©finissent c√¥t√© r√©ponses d‚Äôint√©gration/m√©thode, pas dans la ‚Äúmethod request‚Äù. Donc B r√©pond exactement au besoin : codes HTTP + JSON pr√©d√©finis, sans backend.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu pr√©pares un stand de cantine √† l‚Äô√©cole. L‚Äô√©quipe qui fait les affiches du menu (le frontend) veut tout de suite les ‚Äúfen√™tres de commande‚Äù (les endpoints), m√™me si la cuisine (le backend/Lambda) n‚Äôest pas pr√™te.**\n\nConcept : un endpoint, c‚Äôest comme une fen√™tre o√π tu commandes. Pour s‚Äôentra√Æner, tu peux faire semblant : quand quelqu‚Äôun commande, tu r√©ponds avec un ticket d√©j√† imprim√© (code HTTP) et un petit papier avec le d√©tail (JSON). Avec l‚Äôoption MOCK, la cantine ne va pas en cuisine : elle renvoie directement des r√©ponses pr√©d√©finies. La r√©ponse B est bonne car elle dit : utiliser MOCK et configurer quelles r√©ponses JSON vont avec quels codes (ex: 200 OK, 400 Erreur). A est moins adapt√© car il faut d√©j√† √©crire et d√©ployer une ‚Äúcuisine‚Äù (Lambda) juste pour renvoyer du faux. C envoie vers une autre cantine externe, inutile et pas sous contr√¥le. D est faux car les codes et r√©ponses se r√®glent c√¥t√© ‚Äúr√©ponse‚Äù (integration response), pas juste en ‚Äúdemande‚Äù (method request).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:256:9a0f695fb456acbb",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 256,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a serverless application that runs on AWS. The developer wants to create an accelerated development workflow that deploys incremental changes to AWS for testing. The developer wants to deploy the incremental changes but does not want to fully deploy the entire application to AWS for every code commit.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Use the AWS Serverless Application Model (AWS SAM) to build the application. Use the sam sync command to deploy the incremental changes.",
        "B": "Use the AWS Serverless Application Model (AWS SAM) to build the application. Use the sam init command to deploy the incremental changes.",
        "C": "Use the AWS Cloud Development Kit (AWS CDK) to build the application. Use the cdk synth command to deploy the incremental changes.",
        "D": "Use the AWS Cloud Development Kit (AWS CDK) to build the application. Use the cdk bootstrap command to deploy the incremental changes."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134067-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 17, 2024, 5:17 p.m.",
      "textHash": "9a0f695fb456acbb",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:44d86d77",
      "frExplanation": "Ici on veut un workflow rapide pour une appli ¬´ serverless ¬ª (sans serveurs √† g√©rer, souvent avec AWS Lambda, API Gateway, etc.).\nAWS SAM est un outil pour d√©finir et d√©ployer ce type d‚Äôapplication avec des fichiers simples (template) et des commandes.\nLe besoin cl√© : envoyer seulement les changements (incr√©mentaux) √† AWS pour tester, sans red√©ployer toute l‚Äôappli √† chaque commit.\nLa commande SAM faite pour √ßa est ¬´ sam sync ¬ª : elle d√©tecte ce qui a chang√© (code, config) et met √† jour uniquement ces parties.\nDonc on gagne du temps et on √©vite un d√©ploiement complet √† chaque fois.\nPourquoi les autres sont fausses : ¬´ sam init ¬ª sert √† cr√©er un projet de d√©part, pas √† d√©ployer.\nAvec CDK, ¬´ cdk synth ¬ª g√©n√®re un template CloudFormation (pas un d√©ploiement), et ¬´ cdk bootstrap ¬ª pr√©pare l‚Äôenvironnement (r√¥les/buckets), pas des mises √† jour incr√©mentales.\nConclusion : utiliser AWS SAM et ¬´ sam sync ¬ª r√©pond exactement au besoin de d√©ploiement rapide et partiel.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o o√π tu construis une grande base. √Ä chaque petite am√©lioration (une nouvelle porte, un pi√®ge), tu veux tester vite sans reconstruire toute la base √† z√©ro.**\n\nConcept : en ‚Äúserverless‚Äù, ton appli est comme une base faite de petites pi√®ces. Tu veux envoyer seulement la pi√®ce modifi√©e pour tester, pas tout le ch√¢teau.\nAWS SAM, c‚Äôest la ‚Äúbo√Æte √† outils‚Äù sp√©ciale pour construire et g√©rer ce type d‚Äôappli.\nLa commande sam sync, c‚Äôest comme un mode ‚Äúsynchroniser‚Äù : elle rep√®re ce qui a chang√© et n‚Äôenvoie que ces petits changements sur AWS.\nDonc tu testes plus vite apr√®s chaque mini-modif, sans red√©ployer toute l‚Äôappli.\nPourquoi pas B : sam init sert √† d√©marrer un nouveau projet, pas √† envoyer des changements.\nPourquoi pas C : cdk synth fabrique un plan (un brouillon) mais ne d√©ploie pas.\nPourquoi pas D : cdk bootstrap pr√©pare le terrain une seule fois, ce n‚Äôest pas pour les mises √† jour.\nDonc la bonne r√©ponse est A.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:224:8ded40c61832c1d6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 224,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA web application is using Amazon Kinesis Data Streams for clickstream data that may not be consumed for up to 12 hours.How can the developer implement encryption at rest for data within the Kinesis Data Streams?",
      "choices": {
        "A": "Enable SSL connections to Kinesis.",
        "B": "Use Amazon Kinesis Consumer Library.",
        "C": "Encrypt the data once it is at rest with a Lambda function.",
        "D": "Enable server-side encryption in Kinesis Data Streams."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124774-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:42 a.m.",
      "textHash": "8ded40c61832c1d6",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Amazon Kinesis Data Streams est un service AWS qui stocke temporairement des √©v√©nements (ex: clics) dans des ‚Äúshards‚Äù avant qu‚Äôune application ne les lise. Ici, les donn√©es peuvent rester jusqu‚Äô√† 12 heures dans le stream, donc il faut les prot√©ger pendant qu‚Äôelles sont stock√©es (chiffrement ‚Äúau repos‚Äù).\nLe chiffrement au repos signifie que les donn√©es √©crites sur le stockage interne de Kinesis sont chiffr√©es automatiquement, sans changer votre code.\nL‚Äôoption D est correcte : activer le ‚Äúserver-side encryption‚Äù (SSE) dans Kinesis chiffre les enregistrements avec une cl√© g√©r√©e par AWS KMS (service de gestion de cl√©s).\nA (SSL) chiffre seulement les donn√©es en transit (pendant l‚Äôenvoi/r√©ception), pas quand elles sont stock√©es.\nB (KCL) aide √† consommer les donn√©es, ce n‚Äôest pas une fonction de chiffrement.\nC est inutile/risqu√© : une Lambda ne peut pas ‚Äúre-chiffrer‚Äù ce que Kinesis stocke d√©j√†, et ce n‚Äôest pas la m√©thode pr√©vue.\nDonc, pour chiffrer les donn√©es qui attendent d‚Äô√™tre consomm√©es dans Kinesis, on active SSE sur le stream.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un casier au coll√®ge o√π tu ranges des feuilles avec les clics des √©l√®ves pendant 12 heures avant que quelqu‚Äôun vienne les lire.**\n\nConcept : ‚Äúchiffrement au repos‚Äù = quand les donn√©es sont stock√©es dans le casier, elles sont illisibles sans la bonne cl√©, m√™me si quelqu‚Äôun ouvre le casier.\nIci, Kinesis Data Streams est le casier qui garde les donn√©es avant lecture.\nR√©ponse D : activer le chiffrement c√¥t√© serveur dans Kinesis, c‚Äôest comme demander au coll√®ge de mettre automatiquement chaque feuille dans une enveloppe verrouill√©e d√®s qu‚Äôelle est rang√©e.\nDonc, pendant les 12 heures d‚Äôattente, tout reste prot√©g√© sans que toi tu fasses quelque chose √† chaque fois.\nA (SSL) prot√®ge seulement le trajet, comme parler en chuchotant dans le couloir, pas les feuilles une fois dans le casier.\nB (Consumer Library) aide √† lire/traiter, pas √† verrouiller le casier.\nC (Lambda apr√®s) revient √† chiffrer apr√®s avoir rang√©, trop tard : les feuilles ont d√©j√† √©t√© lisibles un moment.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:221:424c8f295fe1f5b4",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 221,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application needs to use the IP address of the client in its processing. The application has been moved into AWS and has been placed behind an Application Load Balancer (ALB). However, all the client IP addresses now appear to be the same. The application must maintain the ability to scale horizontally.Based on this scenario, what is the MOST cost-effective solution to this problem?",
      "choices": {
        "A": "Remove the application from the ALB. Delete the ALB and change Amazon Route 53 to direct traffic to the instance running the application.",
        "B": "Remove the application from the ALCreate a Classic Load Balancer in its place. Direct traffic to the application using the HTTP protocol.",
        "C": "Alter the application code to inspect the X-Forwarded-For header. Ensure that the code can work properly if a list of IP addresses is passed in the header.",
        "D": "Alter the application code to inspect a custom header. Alter the client code to pass the IP address in the custom header."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124773-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:38 a.m.",
      "textHash": "424c8f295fe1f5b4",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Derri√®re un Application Load Balancer (ALB), votre application ne ‚Äúvoit‚Äù plus l‚ÄôIP du client directement : elle voit l‚ÄôIP du load balancer, car c‚Äôest lui qui se connecte √† l‚Äôapplication.\nPour conserver l‚ÄôIP r√©elle, l‚ÄôALB ajoute automatiquement un en-t√™te HTTP standard appel√© X-Forwarded-For, qui contient l‚ÄôIP du client (et parfois une liste d‚ÄôIPs si plusieurs proxys sont pass√©s).\nLa solution la plus √©conomique est donc de modifier le code pour lire X-Forwarded-For et prendre la premi√®re IP (celle du client), tout en g√©rant le cas o√π plusieurs IPs sont pr√©sentes.\nCela ne change pas l‚Äôarchitecture : vous gardez l‚ÄôALB, donc la mise √† l‚Äô√©chelle horizontale (plusieurs instances/containers) reste possible.\nA supprimer l‚ÄôALB casserait la haute dispo et le scaling, et Route 53 ne remplace pas un load balancer pour r√©partir finement le trafic.\nB passer √† un Classic Load Balancer n‚Äôapporte pas d‚Äôavantage ici et n‚Äôest pas n√©cessaire.\nD demander au client d‚Äôenvoyer son IP dans un header custom est peu fiable (facile √† falsifier) et demande de modifier tous les clients.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine l‚Äôentr√©e du lyc√©e : au lieu que chaque √©l√®ve arrive directement en classe, tout le monde passe d‚Äôabord par un surveillant √† l‚Äôaccueil qui r√©partit les √©l√®ves dans diff√©rentes salles.**\n\nConcept : l‚ÄôALB, c‚Äôest comme ce surveillant. Il re√ßoit tout le monde, puis il envoie vers plusieurs salles (serveurs) pour pouvoir accueillir plus d‚Äô√©l√®ves (scaler horizontalement).\nProbl√®me : si le prof demande ‚Äúqui est arriv√© ?‚Äù, il ne voit que le surveillant, donc tous les √©l√®ves semblent avoir la m√™me ‚Äúidentit√©‚Äù (m√™me IP).\nSolution C : le surveillant colle un petit papier sur chaque √©l√®ve avec son vrai nom avant de l‚Äôenvoyer : c‚Äôest l‚Äôen-t√™te X-Forwarded-For, qui contient l‚ÄôIP r√©elle du client.\nL‚Äôappli doit juste lire ce papier. Parfois il y a une liste de noms (plusieurs passages), donc il faut g√©rer une liste.\nPourquoi c‚Äôest le plus √©conomique : on garde l‚Äôaccueil (ALB) et la r√©partition (scaling), on change juste la fa√ßon de lire l‚Äôinfo.\nA enl√®ve l‚Äôaccueil : plus de r√©partition, donc √ßa casse le scaling.\nB change de type d‚Äôaccueil sans besoin : pas le plus rentable.\nD demande aux √©l√®ves d‚Äô√©crire eux-m√™mes leur nom : facile √† tricher, et √ßa oblige √† modifier tous les clients.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:220:7f600590b678dca2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 220,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is receiving HTTP 400: ThrottlingException errors intermittently when calling the Amazon CloudWatch API. When a call fails, no data is retrieved.What best practice should first be applied to address this issue?",
      "choices": {
        "A": "Contact AWS Support for a limit increase.",
        "B": "Use the AWS CLI to get the metrics.",
        "C": "Analyze the applications and remove the API call.",
        "D": "Retry the call with exponential backoff."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124772-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:36 a.m.",
      "textHash": "7f600590b678dca2",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Une erreur HTTP 400 ThrottlingException signifie que l‚ÄôAPI AWS refuse temporairement la requ√™te car trop d‚Äôappels arrivent en m√™me temps (limite de d√©bit).\nAmazon CloudWatch est le service qui stocke et fournit des m√©triques (CPU, erreurs, etc.) via une API.\nComme l‚Äôerreur est intermittente, le service n‚Äôest pas ‚Äúcass√©‚Äù : il vous demande juste de ralentir.\nLa bonne pratique est de r√©essayer automatiquement apr√®s un d√©lai, au lieu d‚Äô√©chouer imm√©diatement.\n‚ÄúExponential backoff‚Äù veut dire : attendre un peu, puis si √ßa √©choue encore, attendre plus longtemps (par ex. 1s, 2s, 4s, 8s), souvent avec un petit al√©a (jitter).\nCela r√©duit la pression sur l‚ÄôAPI et augmente les chances de succ√®s sans surcharger CloudWatch.\nDemander une augmentation de limite (A) peut aider plus tard, mais on applique d‚Äôabord la strat√©gie de retry recommand√©e par AWS.\nChanger d‚Äôoutil (B) ou supprimer l‚Äôappel (C) ne traite pas la cause principale : trop de requ√™tes au m√™me moment.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : si trop d‚Äô√©l√®ves demandent √† √™tre servis en m√™me temps, la cantini√®re dit ‚Äústop, attendez‚Äù pour √©viter le chaos.**\n\nIci, l‚Äôappli demande des infos √† CloudWatch (un ‚Äútableau de scores‚Äù qui garde des mesures). Parfois, elle demande trop vite/trop souvent. AWS r√©pond alors 400 ThrottlingException = ‚Äútu vas trop vite, ralentis‚Äù. La bonne pratique, c‚Äôest D : r√©essayer, mais pas en boucle. ‚ÄúExponential backoff‚Äù = tu attends un peu, puis un peu plus, puis encore plus avant de redemander (comme faire la queue : 1 min, puis 2, puis 4). √áa laisse le temps au service de respirer et tu finis par r√©cup√©rer les donn√©es. Demander une hausse de limite (A) se fait apr√®s, si m√™me en attendant √ßa bloque encore.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:219:86c1dad0c76fb43f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 219,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an existing application that has hardcoded database credentials. A developer needs to modify the existing application. The application is deployed in two AWS Regions with an active-passive failover configuration to meet company‚Äôs disaster recovery strategy.The developer needs a solution to store the credentials outside the code. The solution must comply with the company‚Äôs disaster recovery strategy.Which solution will meet these requirements in the MOST secure way?",
      "choices": {
        "A": "Store the credentials in AWS Secrets Manager in the primary Region. Enable secret replication to the secondary Region. Update the application to use the Amazon Resource Name (ARN) based on the Region.",
        "B": "Store credentials in AWS Systems Manager Parameter Store in the primary Region. Enable parameter replication to the secondary Region. Update the application to use the Amazon Resource Name (ARN) based on the Region.",
        "C": "Store credentials in a config file. Upload the config file to an S3 bucket in the primary Region. Enable Cross-Region Replication (CRR) to an S3 bucket in the secondary region. Update the application to access the config file from the S3 bucket, based on the Region.",
        "D": "Store credentials in a config file. Upload the config file to an Amazon Elastic File System (Amazon EFS) file system. Update the application to use the Amazon EFS file system Regional endpoints to access the config file in the primary and secondary Regions."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124771-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:35 a.m.",
      "textHash": "86c1dad0c76fb43f",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Le probl√®me : des identifiants de base de donn√©es sont ‚Äúen dur‚Äù dans le code, ce qui est risqu√© (fuite, rotation difficile) et doit aussi fonctionner en reprise apr√®s sinistre sur 2 R√©gions.\nAWS Secrets Manager est un service fait pour stocker des secrets (mot de passe, cl√©s) de fa√ßon chiffr√©e, avec contr√¥le d‚Äôacc√®s (IAM) et rotation possible.\nComme l‚Äôapplication est en actif-passif sur deux R√©gions, il faut que le secret existe aussi dans la R√©gion secondaire pour basculer sans panne.\nL‚Äôoption A r√©plique automatiquement le secret de la R√©gion primaire vers la secondaire : en cas de failover, l‚Äôapplication retrouve le m√™me secret localement.\nL‚Äôapplication peut choisir le bon secret via son ARN (identifiant AWS) correspondant √† la R√©gion o√π elle tourne.\nParameter Store peut stocker des valeurs, mais Secrets Manager est g√©n√©ralement plus adapt√© et plus s√©curis√© pour des identifiants (gestion d√©di√©e des secrets, rotation).\nStocker un fichier de config dans S3 ou EFS augmente le risque d‚Äôexposition et n‚Äôoffre pas la m√™me gestion s√©curis√©e/rotation des secrets.\nDonc la solution la plus s√©curis√©e et compatible DR est : Secrets Manager + r√©plication multi-R√©gion (A).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:217:f038071837df2a3e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 217,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has built a market application that stores pricing data in Amazon DynamoDB with Amazon ElastiCache in front. The prices of items in the market change frequently. Sellers have begun complaining that, after they update the price of an item, the price does not actually change in the product listing.What could be causing this issue?",
      "choices": {
        "A": "The cache is not being invalidated when the price of the item is changed.",
        "B": "The price of the item is being retrieved using a write-through ElastiCache cluster.",
        "C": "The DynamoDB table was provisioned with insufficient read capacity.",
        "D": "The DynamoDB table was provisioned with insufficient write capacity."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124781-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:53 a.m.",
      "textHash": "f038071837df2a3e",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_capacity_rcu_wcu",
      "frExplanation": "Ici, les prix sont stock√©s dans DynamoDB (base NoSQL) et on met ElastiCache (cache en m√©moire) devant pour r√©pondre plus vite.\nUn cache garde une copie temporaire des donn√©es. Si on lit le prix depuis le cache, on peut obtenir une ancienne valeur.\nQuand un vendeur modifie un prix dans DynamoDB, il faut aussi mettre √† jour ou supprimer (invalider) l‚Äôentr√©e correspondante dans le cache.\nSi cette invalidation n‚Äôest pas faite, ElastiCache continue de servir l‚Äôancien prix jusqu‚Äô√† expiration (TTL) ou remplacement.\nC‚Äôest exactement le sympt√¥me d√©crit : ‚Äúle prix ne change pas dans la liste‚Äù apr√®s une mise √† jour.\nL‚Äôoption B (write-through) √©crirait dans le cache lors des √©critures, ce qui r√©duit justement ce probl√®me.\nLes options C et D (capacit√© DynamoDB) causeraient plut√¥t des erreurs, des lenteurs ou du throttling, pas un affichage syst√©matique d‚Äôun ancien prix.\nDonc la cause la plus probable est l‚Äôabsence d‚Äôinvalidation/rafra√Æchissement du cache lors des mises √† jour.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine le tableau d‚Äôaffichage du lyc√©e : il y a la feuille officielle au secr√©tariat (la ‚Äúv√©rit√©‚Äù), et une photocopie scotch√©e dans le couloir pour aller plus vite √† lire.**\n\nDynamoDB = la feuille officielle au secr√©tariat (donn√©es √† jour). ElastiCache = la photocopie dans le couloir (lecture rapide). Quand un vendeur change un prix, il met √† jour la feuille officielle. Mais si personne ne remplace la photocopie, les √©l√®ves lisent encore l‚Äôancien prix. C‚Äôest exactement le probl√®me : le cache garde une vieille valeur. ‚ÄúInvalider le cache‚Äù = jeter/mettre √† jour la photocopie d√®s qu‚Äôun prix change. Donc A est la bonne r√©ponse. Les options C/D parleraient d‚Äôun secr√©tariat trop lent (retards/erreurs), pas d‚Äôun prix ancien qui reste affich√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:216:8ae86bf85cb9888a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 216,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on an AWS Lambda function that accesses Amazon DynamoDB. The Lambda function must retrieve an item and update some of its attributes, or create the item if it does not exist. The Lambda function has access to the primary key.Which IAM permissions should the developer request for the Lambda function to achieve this functionality?",
      "choices": {
        "A": "dynamodb:DeleleItemdynamodb:GetItemdynamodb:PutItem",
        "B": "dynamodb:UpdateItemdynamodb:GetItemdynamodb:DescribeTable",
        "C": "dynamodb:GetRecordsdynamodb:PutItemdynamodb:UpdateTable",
        "D": "dynamodb:UpdateItemdynamodb:GetItemdynamodb:PutItem"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124769-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:32 a.m.",
      "textHash": "8ae86bf85cb9888a",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "AWS Lambda est un service qui ex√©cute du code sans g√©rer de serveur. DynamoDB est une base de donn√©es NoSQL o√π chaque √©l√©ment est identifi√© par une cl√© primaire.\nIci, la fonction doit (1) lire un √©l√©ment existant, puis (2) modifier certains attributs, ou (3) cr√©er l‚Äô√©l√©ment s‚Äôil n‚Äôexiste pas.\nPour lire avec la cl√© primaire, il faut l‚Äôaction IAM dynamodb:GetItem.\nPour modifier des attributs d‚Äôun √©l√©ment existant, il faut dynamodb:UpdateItem.\nPour cr√©er un nouvel √©l√©ment, il faut dynamodb:PutItem (√©criture d‚Äôun item complet).\nLes autres actions propos√©es ne correspondent pas au besoin : DescribeTable sert √† lire la structure, UpdateTable modifie la table, GetRecords concerne plut√¥t les streams.\nDonc le trio minimal et logique pour ‚Äúlire + mettre √† jour ou cr√©er‚Äù est GetItem + UpdateItem + PutItem (r√©ponse D).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e : chaque livre a un num√©ro unique (la ‚Äúcl√© primaire‚Äù). Tu es un √©l√®ve (Lambda) qui doit soit retrouver un livre et modifier sa fiche, soit cr√©er la fiche si le livre n‚Äôexiste pas.**\n\nConcept : pour g√©rer un livre, il te faut le droit de le chercher, puis de le modifier ou d‚Äôen ajouter un nouveau.\nGetItem = tu cherches le livre avec son num√©ro.\nUpdateItem = si le livre existe, tu changes des infos sur sa fiche (auteur, √©tat, etc.).\nPutItem = si le livre n‚Äôexiste pas, tu cr√©es une nouvelle fiche compl√®te.\nLa r√©ponse D donne exactement ces 3 droits : chercher, modifier, ou cr√©er.\nA parle de supprimer (inutile ici).\nB demande ‚Äúd√©crire la table‚Äù (comme demander le plan de la biblioth√®que), pas n√©cessaire.\nC contient des actions qui ne correspondent pas √† ‚Äúchercher un livre et le cr√©er/modifier‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:213:537bbb491c0dbc8c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 213,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to troubleshoot an AWS Lambda function in a development environment. The Lambda function is configured in VPC mode and needs to connect to an existing Amazon RDS for SQL Server DB instance. The DB instance is deployed in a private subnet and accepts connections by using port 1433.When the developer tests the function, the function reports an error when it tries to connect to the database.Which combination of steps should the developer take to diagnose this issue? (Choose two.)",
      "choices": {
        "A": "Check that the function‚Äôs security group has outbound access on port 1433 to the DB instance‚Äôs security group. Check that the DB instance‚Äôs security group has inbound access on port 1433 from the function‚Äôs security group.",
        "B": "Check that the function‚Äôs security group has inbound access on port 1433 from the DB instance‚Äôs security group. Check that the DB instance‚Äôs security group has outbound access on port 1433 to the function‚Äôs security group.",
        "C": "Check that the VPC is set up for a NAT gateway. Check that the DB instance has the public access option turned on.",
        "D": "Check that the function‚Äôs execution role permissions include rds:DescribeDBInstances, rds:ModifyDBInstance. and rds:DescribeDBSecurityGroups for the DB instance.",
        "E": "Check that the function‚Äôs execution role permissions include ec2:CreateNetworkInterface, ec2:DescribeNetworkInterfaces, and ec2:DeleteNetworkInterface."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124780-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:48 a.m.",
      "textHash": "537bbb491c0dbc8c",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, la fonction AWS Lambda est ¬´ dans un VPC ¬ª : elle se comporte comme une machine dans votre r√©seau priv√© et doit respecter les r√®gles r√©seau (Security Groups).\nAmazon RDS (SQL Server) √©coute sur le port 1433 : pour se connecter, il faut autoriser le trafic TCP 1433 entre Lambda et la base.\nUn Security Group est un pare-feu : il a des r√®gles ¬´ inbound ¬ª (entr√©es) et ¬´ outbound ¬ª (sorties).\nPour diagnostiquer, v√©rifiez d‚Äôabord que le Security Group de Lambda autorise en sortie (outbound) le port 1433 vers le Security Group de RDS.\nPuis v√©rifiez que le Security Group de RDS autorise en entr√©e (inbound) le port 1433 depuis le Security Group de Lambda.\nC‚Äôest exactement le chemin d‚Äôune connexion : Lambda initie la connexion (sortie), RDS la re√ßoit (entr√©e).\nLes options NAT/public access ne sont pas n√©cessaires pour une connexion priv√©e dans le m√™me VPC, et les permissions IAM ne remplacent pas les r√®gles r√©seau.\nDonc la bonne combinaison de v√©rifications est celle de l‚Äôoption A.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine l‚Äô√©cole : la base de donn√©es (RDS) est une salle de profs ferm√©e (r√©seau priv√©). Ta fonction Lambda est un √©l√®ve qui doit aller parler au prof. Pour entrer, il faut une autorisation de porte, et la porte a un num√©ro : 1433.**\n\nConcept : dans AWS, les ‚Äúsecurity groups‚Äù sont comme des vigiles √† deux portes : entr√©e (inbound) et sortie (outbound). Si l‚Äôun des deux refuse, tu ne passes pas. Ici, Lambda doit INITIER la discussion vers la salle des profs sur la porte 1433. Donc Lambda doit avoir le droit de SORTIR vers 1433, et la salle des profs (DB) doit avoir le droit de LAISSER ENTRER sur 1433 depuis Lambda. C‚Äôest exactement A : sortie 1433 c√¥t√© Lambda + entr√©e 1433 c√¥t√© DB. B inverse les sens (comme si le prof venait chercher l‚Äô√©l√®ve). C parle d‚Äôouvrir l‚Äô√©cole au public, inutile et risqu√©. D/E sont des droits ‚Äúadministratifs‚Äù, pas des autorisations de porte pour se connecter.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:210:8c0e1977b3af8193",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 210,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses Amazon API Gateway to expose a set of APIs to customers. The APIs have caching enabled in API Gateway. Customers need a way to invalidate the cache for each API when they test the API.What should a developer do to give customers the ability to invalidate the API cache?",
      "choices": {
        "A": "Ask the customers to use AWS credentials to call the InvalidateCache API operation.",
        "B": "Attach an InvalidateCache policy to the IAM execution role that the customers use to invoke the API. Ask the customers to send a request that contains the Cache-Control:max-age=0 HTTP header when they make an API call.",
        "C": "Ask the customers to use the AWS SDK API Gateway class to invoke the InvalidateCache API operation.",
        "D": "Attach an InvalidateCache policy to the IAM execution role that the customers use to invoke the API. Ask the customers to add the INVALIDATE_CACHE query string parameter when they make an API call."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122632-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3:14 a.m.",
      "textHash": "8c0e1977b3af8193",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "API Gateway est un service AWS qui re√ßoit des requ√™tes HTTP et les envoie √† votre backend. Le ‚Äúcache‚Äù garde en m√©moire des r√©ponses pour aller plus vite, mais pendant des tests on veut parfois forcer une r√©ponse fra√Æche.\nPour invalider le cache par appel, API Gateway permet au client d‚Äôenvoyer l‚Äôen-t√™te HTTP Cache-Control: max-age=0 : cela indique ‚Äúne pas utiliser la r√©ponse en cache‚Äù.\nCependant, cette action est sensible : il faut aussi autoriser explicitement l‚Äôinvalidation via IAM (le syst√®me de permissions AWS).\nDonc on attache une permission/policy d‚Äôinvalidation de cache au r√¥le IAM utilis√© pour appeler l‚ÄôAPI, puis on demande aux clients d‚Äôajouter l‚Äôen-t√™te Cache-Control: max-age=0.\nLes options A et C demandent aux clients d‚Äôappeler directement une op√©ration AWS (InvalidateCache) avec des identifiants AWS/SDK, ce qui n‚Äôest pas le bon m√©canisme pour un simple appel d‚ÄôAPI et complique l‚Äôacc√®s.\nL‚Äôoption D parle d‚Äôun param√®tre de requ√™te ‚ÄúINVALIDATE_CACHE‚Äù qui n‚Äôest pas la m√©thode standard d‚ÄôAPI Gateway.\nLa r√©ponse B combine la bonne permission IAM + le bon signal HTTP pour forcer l‚Äôinvalidation lors des tests.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le self du lyc√©e : pour aller plus vite, la cantine pr√©pare des plateaux ‚Äúd√©j√† pr√™ts‚Äù (cache). Mais quand tu testes une nouvelle recette, tu veux un plateau fait √† l‚Äôinstant, pas un ancien.**\n\nLe ‚Äúcache‚Äù, c‚Äôest une copie rapide d‚Äôune r√©ponse d√©j√† servie, pour gagner du temps.\nQuand les clients testent l‚ÄôAPI, ils veulent parfois forcer ‚Äúrefais-le maintenant‚Äù, donc ignorer la copie.\nLa r√©ponse B dit : on donne aux clients le droit d‚Äôutiliser cette option (comme un badge autoris√©).\nEt on leur demande d‚Äôajouter un message clair dans la demande : Cache-Control: max-age=0.\nDans l‚Äôanalogie, c‚Äôest comme dire au cuisinier : ‚Äúne me donne pas un plateau pr√©par√©, fais-le frais‚Äù.\nA et C demandent aux clients d‚Äôutiliser des outils/acc√®s AWS compliqu√©s (comme entrer dans la cuisine), pas n√©cessaire.\nD invente un ‚Äúmot magique‚Äù dans l‚ÄôURL, moins standard : ici on utilise l‚Äô√©tiquette officielle (le header).\nDonc B est la bonne r√©ponse : autorisation + demande ‚Äúpas de cache‚Äù au moment du test.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:208:f085019b7ce19313",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 208,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is preparing to begin development of a new version of an application. The previous version of the application is deployed in a production environment. The developer needs to deploy fixes and updates to the current version during the development of the new version of the application. The code for the new version of the application is stored in AWS CodeCommit.Which solution will meet these requirements?",
      "choices": {
        "A": "From the main branch, create a feature branch for production bug fixes. Create a second feature branch from the main branch for development of the new version.",
        "B": "Create a Git tag of the code that is currently deployed in production. Create a Git tag for the development of the new version. Push the two tags to the CodeCommit repository.",
        "C": "From the main branch, create a branch of the code that is currently deployed in production. Apply an IAM policy that ensures no other users can push or merge to the branch.",
        "D": "Create a new CodeCommit repository for development of the new version of the application. Create a Git tag for the development of the new version."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122630-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3:11 a.m.",
      "textHash": "f085019b7ce19313",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "AWS CodeCommit est un service AWS qui h√©berge un d√©p√¥t Git (comme GitHub) pour g√©rer le code avec des branches.\nIci, on doit continuer √† corriger la version en production tout en d√©veloppant une nouvelle version en parall√®le.\nLa bonne pratique Git est d‚Äôutiliser des branches s√©par√©es : une branche d√©di√©e aux correctifs ‚Äúproduction‚Äù et une autre pour la nouvelle version.\nAvec A, on part de la branche principale (main) et on cr√©e deux branches : une pour les bugs en production, une pour le d√©veloppement de la v2.\nAinsi, les correctifs peuvent √™tre livr√©s rapidement sans m√©langer du code incomplet de la nouvelle version.\nLes tags (B) servent surtout √† ‚Äúmarquer‚Äù un √©tat du code (lecture/rep√®re), pas √† d√©velopper et int√©grer des changements continus.\nBloquer une branche avec IAM (C) emp√™che justement de pousser/merger des correctifs, donc ne r√©pond pas au besoin.\nCr√©er un autre d√©p√¥t (D) complique inutilement et ne g√®re pas bien les correctifs sur la version actuelle.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:207:ff16800bb4bf8416",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 207,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a serverless application that connects to an Amazon Aurora PostgreSQL database. The serverless application consists of hundreds of AWS Lambda functions. During every Lambda function scale out, a new database connection is made that increases database resource consumption.The developer needs to decrease the number of connections made to the database. The solution must not impact the scalability of the Lambda functions.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure provisioned concurrency for each Lambda function by setting the ProvisionedConcurrentExecutions parameter to 10.",
        "B": "Enable cluster cache management for Aurora PostgreSQL. Change the connection string of each Lambda function to point to cluster cache management.",
        "C": "Use Amazon RDS Proxy to create a connection pool to manage the database connections. Change the connection string of each Lambda function to reference the proxy.",
        "D": "Configure reserved concurrency for each Lambda function by setting the ReservedConcurrentExecutions parameter to 10."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122629-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3:11 a.m.",
      "textHash": "ff16800bb4bf8416",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:4ab7de87",
      "frExplanation": "Le probl√®me vient du fait que chaque nouvelle instance de Lambda (quand √ßa ‚Äúscale‚Äù) ouvre sa propre connexion √† Aurora PostgreSQL, ce qui surcharge la base (trop de connexions = CPU/RAM consomm√©s, limites atteintes).\nAmazon RDS Proxy est un service g√©r√© qui se place entre Lambda et la base de donn√©es.\nIl maintient un pool (r√©servoir) de connexions d√©j√† ouvertes vers Aurora et r√©utilise ces connexions pour plusieurs invocations Lambda.\nAinsi, m√™me si des centaines de Lambdas d√©marrent, elles se connectent au proxy, et le proxy limite/optimise le nombre de connexions r√©elles vers la base.\nCela r√©duit fortement la consommation c√¥t√© base sans emp√™cher Lambda de monter en charge.\nLes options de ‚Äúconcurrency‚Äù (A et D) limitent ou pr√©-allouent des ex√©cutions Lambda, mais ne r√©solvent pas proprement le probl√®me de connexions et peuvent r√©duire la scalabilit√©.\nL‚Äôoption B (cluster cache management) n‚Äôest pas une solution standard pour g√©rer des connexions PostgreSQL; le besoin est un pool de connexions.\nDonc la bonne r√©ponse est C : utiliser RDS Proxy et pointer la cha√Æne de connexion des Lambdas vers le proxy.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : la base de donn√©es, c‚Äôest le cuisinier. Les fonctions Lambda, c‚Äôest des centaines d‚Äô√©l√®ves qui arrivent d‚Äôun coup pour commander. Chaque ‚Äúconnexion‚Äù, c‚Äôest un √©l√®ve qui ouvre une nouvelle caisse rien que pour lui.**\n\nConcept : si chaque √©l√®ve ouvre sa propre caisse, le cuisinier et la cantine s‚Äô√©puisent vite. Il faut une ‚Äúcaisse centrale‚Äù qui regroupe les commandes et r√©utilise les m√™mes caisses.\nPourquoi C : Amazon RDS Proxy, c‚Äôest ce guichet/caisse centrale. Les √©l√®ves (Lambda) parlent au guichet, pas directement au cuisinier (base). Le guichet garde un petit nombre de caisses ouvertes et les partage entre tous (pool de connexions).\nR√©sultat : beaucoup moins de connexions ouvertes en m√™me temps, donc moins de fatigue c√¥t√© base.\nEt √ßa ne bloque pas l‚Äôarriv√©e de nouveaux √©l√®ves : Lambda peut toujours ‚Äúscaler‚Äù, le guichet organise juste la file.\nA et D limitent/figent le nombre d‚Äô√©l√®ves qui peuvent venir en m√™me temps, donc √ßa peut casser la scalabilit√©.\nB ne r√®gle pas le probl√®me des ‚Äútrop de caisses ouvertes‚Äù comme le fait un guichet de connexions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:202:c252894393cc1fe9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 202,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer must provide an API key to an AWS Lambda function to authenticate with a third-party system. The Lambda function will run on a schedule. The developer needs to ensure that the API key remains encrypted at rest.Which solution will meet these requirements?",
      "choices": {
        "A": "Store the API key as a Lambda environment variable by using an AWS Key Management Service (AWS KMS) customer managed key.",
        "B": "Configure the application to prompt the user to provide the password to the Lambda function on the first run.",
        "C": "Store the API key as a value in the application code.",
        "D": "Use Lambda@Edge and only communicate over the HTTPS protocol."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122624-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3:06 a.m.",
      "textHash": "c252894393cc1fe9",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Une fonction AWS Lambda ex√©cute du code sans serveur. Ici, elle tourne automatiquement selon un planning, donc on ne peut pas compter sur un utilisateur pour saisir une cl√© au d√©marrage.\nOn veut que la cl√© API soit chiffr√©e ¬´ au repos ¬ª (stock√©e de fa√ßon chiffr√©e quand elle est enregistr√©e).\nLes variables d‚Äôenvironnement Lambda peuvent contenir des secrets, et Lambda peut les chiffrer avec AWS KMS.\nAWS KMS (Key Management Service) g√®re des cl√©s de chiffrement; une ¬´ customer managed key ¬ª est une cl√© que vous contr√¥lez (droits, rotation, audit).\nAvec l‚Äôoption A, la valeur est stock√©e chiffr√©e et Lambda la d√©chiffre uniquement au moment de l‚Äôex√©cution si la fonction a les permissions KMS.\nB est impossible car la fonction est planifi√©e (pas d‚Äôutilisateur). C est dangereux: le code peut √™tre lu, versionn√©, ou expos√©. D ne traite que le chiffrement en transit (HTTPS), pas le stockage chiffr√©.\nDonc la bonne r√©ponse est A.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que ton club au lyc√©e a un badge secret (API key) pour entrer dans une salle priv√©e. Le robot du club (Lambda) doit y aller tout seul √† heures fixes, sans personne pour l‚Äôaider. Tu veux que le badge soit rang√© dans un casier ferm√© √† cl√© quand il n‚Äôest pas utilis√©.**\n\nConcept : une ‚Äúcl√© API‚Äù c‚Äôest comme un badge secret. ‚ÄúChiffr√© au repos‚Äù = quand c‚Äôest rang√©, c‚Äôest illisible sans la bonne cl√© du cadenas. Lambda qui tourne ‚Äúsur un planning‚Äù = un robot qui vient automatiquement, donc pas d‚Äôhumain pour taper un mot de passe.\nPourquoi A : mettre la cl√© API dans les ‚Äúvariables d‚Äôenvironnement‚Äù de Lambda, c‚Äôest comme √©crire le badge sur un papier rang√© dans le casier du robot. Et AWS KMS (service de cadenas) avec une cl√© g√©r√©e par toi, c‚Äôest le cadenas qui chiffre/d√©chiffre le papier : rang√© = illisible, utilis√© = lisible par le robot.\nPourquoi pas B : √ßa demande un humain au premier lancement, mais le robot tourne tout seul.\nPourquoi pas C : dans le code, c‚Äôest comme √©crire le badge sur le mur du club, trop facile √† retrouver.\nPourquoi pas D : HTTPS prot√®ge pendant le trajet, pas quand le badge est stock√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:199:d0107519c172c171",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 199,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a new REST API by using Amazon API Gateway and AWS Lambda. The development team tests the API and validates responses for the known use cases before deploying the API to the production environment.The developer wants to make the REST API available for testing by using API Gateway locally.Which AWS Serverless Application Model Command Line Interface (AWS SAM CLI) subcommand will meet these requirements?",
      "choices": {
        "A": "Sam local invoke",
        "B": "Sam local generate-event",
        "C": "Sam local start-lambda",
        "D": "Sam local start-api"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122621-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3 a.m.",
      "textHash": "d0107519c172c171",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:e849e79d",
      "frExplanation": "Ici, on veut tester une API REST d‚ÄôAPI Gateway ¬´ en local ¬ª avant de la d√©ployer en production.\nAmazon API Gateway est le service qui expose des routes HTTP (ex: GET /users) et appelle votre code.\nAWS Lambda ex√©cute votre fonction (le code) quand l‚ÄôAPI re√ßoit une requ√™te.\nAWS SAM CLI permet de simuler ces services sur votre machine.\nLa commande qui d√©marre un serveur HTTP local et √©mule API Gateway est ¬´ sam local start-api ¬ª.\nEnsuite, vous pouvez appeler vos endpoints avec un navigateur ou curl, comme si l‚ÄôAPI √©tait d√©ploy√©e.\n¬´ sam local invoke ¬ª lance une seule fonction Lambda avec un √©v√©nement donn√©, mais ne cr√©e pas d‚ÄôAPI HTTP.\nDonc la bonne r√©ponse est D : sam local start-api.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu cr√©es un nouveau comptoir de commande √† la cantine (l‚ÄôAPI) et qu‚Äôen cuisine un cuisinier pr√©pare les plats √† la demande (Lambda). Avant d‚Äôouvrir au public, tu veux tester le comptoir dans ta salle de classe, sans aller √† la vraie cantine.**\n\nConcept : une API, c‚Äôest comme un guichet o√π tu passes une commande (URL + m√©thode). Lambda, c‚Äôest la personne derri√®re qui ex√©cute la recette et renvoie le plat (r√©ponse). Tester ‚Äúlocalement‚Äù, c‚Äôest monter un faux guichet sur ton ordi pour faire des essais. La commande SAM qui ouvre ce guichet complet sur ton ordi est **sam local start-api** : elle lance un petit serveur qui imite API Gateway et envoie les demandes √† ta Lambda. **invoke** teste une seule recette √† la fois, sans guichet. **generate-event** fabrique juste de faux tickets de commande. **start-lambda** lance seulement la cuisine, pas le comptoir. Donc la bonne r√©ponse est **D**.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:196:5dd1ecff108e0e18",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 196,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company built a new application in the AWS Cloud. The company automated the bootstrapping of new resources with an Auto Scaling group by using AWS CloudFormation templates. The bootstrap scripts contain sensitive data.The company needs a solution that is integrated with CloudFormation to manage the sensitive data in the bootstrap scripts.Which solution will meet these requirements in the MOST secure way?",
      "choices": {
        "A": "Put the sensitive data into a CloudFormation parameter. Encrypt the CloudFormation templates by using an AWS Key Management Service (AWS KMS) key.",
        "B": "Put the sensitive data into an Amazon S3 bucket. Update the CloudFormation templates to download the object from Amazon S3 during bootstrap.",
        "C": "Put the sensitive data into AWS Systems Manager Parameter Store as a secure string parameter. Update the CloudFormation templates to use dynamic references to specify template values.",
        "D": "Put the sensitive data into Amazon Elastic File System (Amazon EFS). Enforce EFS encryption after file system creation. Update the CloudFormation templates to retrieve data from Amazon EFS."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122618-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:56 a.m.",
      "textHash": "5dd1ecff108e0e18",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, le probl√®me est que les scripts de ‚Äúbootstrap‚Äù (ce qui configure une instance au d√©marrage) contiennent des secrets (mots de passe, cl√©s, tokens). Les mettre directement dans CloudFormation (param√®tres ou template) est risqu√© car ils peuvent appara√Ætre en clair dans l‚Äôhistorique, les logs, ou √™tre visibles par des personnes ayant acc√®s √† la stack.\nAWS Systems Manager Parameter Store est un coffre simple pour stocker des valeurs, et le type ‚ÄúSecureString‚Äù chiffre le secret (avec KMS) et contr√¥le qui peut le lire via IAM.\nCloudFormation est ‚Äúint√©gr√©‚Äù √† Parameter Store gr√¢ce aux dynamic references : le template ne contient pas le secret, seulement une r√©f√©rence, et la valeur est r√©cup√©r√©e au moment du d√©ploiement.\nC‚Äôest donc plus s√ªr : secret chiffr√© au repos, acc√®s limit√©, et moins de fuite dans les templates/outputs.\nB (S3) et D (EFS) sont du stockage de fichiers : m√™me chiffr√©, il faut g√©rer permissions, chemins, et le secret peut √™tre t√©l√©charg√©/lu plus facilement ; ce n‚Äôest pas con√ßu comme un gestionnaire de secrets int√©gr√© √† CloudFormation.\nA est le moins s√ªr : chiffrer le template ne prot√®ge pas forc√©ment l‚Äôexposition du secret pendant l‚Äôex√©cution et n‚Äôemp√™che pas qu‚Äôil soit manipul√© comme une valeur de stack.\nDonc la meilleure option, la plus s√©curis√©e et int√©gr√©e √† CloudFormation, est C.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que ta classe pr√©pare une sortie scolaire. Le ‚Äútemplate CloudFormation‚Äù, c‚Äôest la fiche d‚Äôorganisation. Le ‚Äúbootstrap script‚Äù, c‚Äôest la liste d‚Äôinstructions donn√©e aux profs le matin. Et les ‚Äúdonn√©es sensibles‚Äù, c‚Äôest comme le code du cadenas du local o√π sont les affaires.**\n\nConcept : tu ne veux JAMAIS √©crire le code du cadenas directement sur la fiche distribu√©e √† tout le monde. Tu le mets dans un coffre de l‚Äô√©cole, accessible seulement aux profs autoris√©s.\nPourquoi C : Parameter Store en ‚Äúsecure string‚Äù, c‚Äôest ce coffre qui garde le code chiffr√© et contr√¥le qui peut le lire. CloudFormation peut juste mettre une ‚Äúr√©f√©rence‚Äù sur la fiche (dynamic reference) : ‚Äúva chercher le code dans le coffre‚Äù, sans l‚Äôafficher.\nPourquoi pas A : mettre le secret en param√®tre, c‚Äôest comme √©crire le code dans un champ de la fiche : trop facile √† voir/coller.\nPourquoi pas B : S3, c‚Äôest un casier de stockage ; si les droits sont mal r√©gl√©s, quelqu‚Äôun peut lire le fichier.\nPourquoi pas D : EFS, c‚Äôest un disque partag√© ; ce n‚Äôest pas un coffre √† secrets int√©gr√© pour des valeurs dans un template.\nDonc la solution la plus s√ªre et int√©gr√©e √† CloudFormation = C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:193:0e103247bdf85ca1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 193,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a mobile application that will not require users to log in.What is the MOST efficient method to grant users access to AWS resources?",
      "choices": {
        "A": "Use an identity provider to securely authenticate with the application.",
        "B": "Create an AWS Lambda function to create an IAM user when a user accesses the application.",
        "C": "Create credentials using AWS KMS and apply these credentials to users when using the application.",
        "D": "Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122614-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:50 a.m.",
      "textHash": "0e103247bdf85ca1",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, l‚Äôapplication mobile n‚Äôa pas de connexion utilisateur : il faut donc donner un acc√®s AWS ¬´ invit√© ¬ª sans cr√©er de comptes.\nAmazon Cognito est un service qui g√®re des identit√©s pour les applis (utilisateurs connect√©s ou non) et peut fournir des identifiants temporaires.\nAvec Cognito, on peut d√©finir des ¬´ utilisateurs non authentifi√©s ¬ª (guest) et les associer √† un r√¥le IAM.\nIAM (Identity and Access Management) permet de cr√©er des r√¥les avec des permissions pr√©cises (ex: lire un fichier S3, appeler une API).\nLe r√¥le IAM donne un acc√®s limit√© et contr√¥l√©, et les identifiants sont temporaires : plus s√ªr que des cl√©s permanentes.\nA est inutile car il n‚Äôy a pas d‚Äôauthentification demand√©e.\nB est inefficace et dangereux : cr√©er un IAM user par visite ne passe pas √† l‚Äô√©chelle et multiplie les cl√©s √† g√©rer.\nC est incorrect : KMS sert √† chiffrer des donn√©es, pas √† fabriquer/attribuer des identifiants AWS aux utilisateurs.\nDonc la m√©thode la plus efficace et standard est Cognito + r√¥le IAM pour utilisateurs non authentifi√©s (D).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e o√π tu peux entrer sans carte d‚Äô√©l√®ve (pas de connexion), mais tu ne dois pas pouvoir aller partout ni tout emprunter.**\n\nConcept : m√™me sans ‚Äúcompte‚Äù, on peut donner un acc√®s ‚Äúinvit√©‚Äù avec des r√®gles strictes, comme un badge visiteur.\nDans AWS, Amazon Cognito joue le r√¥le du bureau d‚Äôaccueil : il reconna√Æt les visiteurs ‚Äúanonymes‚Äù (sans login) et leur donne un badge temporaire.\nCe badge correspond √† un ‚Äúr√¥le IAM‚Äù : une liste de permissions, comme ‚Äútu peux lire ces 3 BD, mais pas entrer en salle des profs‚Äù.\nPourquoi D : √ßa donne le minimum d‚Äôacc√®s n√©cessaire, automatiquement, sans cr√©er un compte par personne.\nPourquoi pas A : un fournisseur d‚Äôidentit√© sert √† se connecter‚Ä¶ mais ici on ne veut pas de login.\nPourquoi pas B : cr√©er un utilisateur pour chaque visiteur, c‚Äôest comme fabriquer une carte d‚Äô√©l√®ve √† chaque entr√©e : trop lent et ing√©rable.\nPourquoi pas C : KMS sert √† prot√©ger des cl√©s/secrets, pas √† distribuer des badges d‚Äôacc√®s aux gens.\nDonc D = acc√®s invit√© + permissions limit√©es, le plus efficace et s√©curis√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:192:fef4b286cbffcce2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 192,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is migrating its PostgreSQL database into the AWS Cloud. The company wants to use a database that will secure and regularly rotate database credentials. The company wants a solution that does not require additional programming overhead.Which solution will meet these requirements?",
      "choices": {
        "A": "Use Amazon Aurora PostgreSQL for the database. Store the database credentials in AWS Systems Manager Parameter Store. Turn on rotation.",
        "B": "Use Amazon Aurora PostgreSQL for the database. Store the database credentials in AWS Secrets Manager. Turn on rotation.",
        "C": "Use Amazon DynamoDB for the database. Store the database credentials in AWS Systems Manager Parameter Store. Turn on rotation.",
        "D": "Use Amazon DynamoDB for the database. Store the database credentials in AWS Secrets Manager. Turn on rotation."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122615-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:51 a.m.",
      "textHash": "fef4b286cbffcce2",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "On migre une base PostgreSQL : il faut donc une base compatible PostgreSQL. Amazon Aurora PostgreSQL est un service de base de donn√©es g√©r√© (AWS s‚Äôoccupe de beaucoup d‚Äôadministration).\nLe besoin principal est de s√©curiser les identifiants (login/mot de passe) et de les faire tourner r√©guli√®rement (rotation) sans √©crire du code en plus.\nAWS Secrets Manager est con√ßu pour stocker des ‚Äúsecrets‚Äù (mots de passe, cl√©s) et propose une rotation automatique int√©gr√©e pour des bases comme Aurora/RDS, avec tr√®s peu de configuration.\nAWS Systems Manager Parameter Store peut stocker des param√®tres chiffr√©s, mais la rotation automatique n‚Äôest pas aussi ‚Äúcl√© en main‚Äù : elle demande g√©n√©ralement plus de mise en place (ex. Lambda/automation).\nDynamoDB n‚Äôest pas une base PostgreSQL : c‚Äôest une base NoSQL, donc elle ne r√©pond pas au besoin de migrer PostgreSQL.\nDonc la meilleure option est : Aurora PostgreSQL + Secrets Manager + rotation activ√©e (r√©ponse B).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:191:2534626918db065e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 191,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is testing an application that invokes an AWS Lambda function asynchronously. During the testing phase, the Lambda function fails to process after two retries.How can the developer troubleshoot the failure?",
      "choices": {
        "A": "Configure AWS CloudTrail logging to investigate the invocation failures.",
        "B": "Configure Dead Letter Queues by sending events to Amazon SQS for investigation.",
        "C": "Configure Amazon Simple Workflow Service to process any direct unprocessed events.",
        "D": "Configure AWS Config to process any direct unprocessed events."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122613-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:49 a.m.",
      "textHash": "2534626918db065e",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Ici, l‚Äôapplication appelle une fonction AWS Lambda ¬´ en asynchrone ¬ª : l‚Äôappelant envoie l‚Äô√©v√©nement et n‚Äôattend pas la r√©ponse. Si Lambda √©choue, AWS r√©essaie automatiquement (ici 2 fois). Apr√®s ces tentatives, l‚Äô√©v√©nement peut √™tre perdu si on ne pr√©voit rien.\nUne Dead Letter Queue (DLQ) est une ¬´ bo√Æte de secours ¬ª o√π l‚Äôon envoie les √©v√©nements qui n‚Äôont pas pu √™tre trait√©s. En configurant une DLQ vers Amazon SQS (un service de file d‚Äôattente de messages), chaque √©v√©nement en √©chec est stock√© pour analyse.\nEnsuite, le d√©veloppeur peut lire la file SQS, voir le contenu exact de l‚Äô√©v√©nement, l‚Äôerreur associ√©e, et relancer ou corriger le code.\nCloudTrail (A) trace surtout les appels API AWS, pas le d√©tail des √©v√©nements Lambda √©chou√©s. SWF (C) est un ancien service d‚Äôorchestration, pas adapt√© ici. AWS Config (D) sert √† auditer la configuration des ressources, pas √† g√©rer des √©v√©nements non trait√©s.\nDonc la bonne r√©ponse est B : configurer une DLQ vers SQS pour enqu√™ter sur les √©v√©nements en √©chec.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine que tu envoies des devoirs dans une bo√Æte √† devoirs automatique au coll√®ge. Tu ne restes pas devant pour v√©rifier (envoi ‚Äúasynchrone‚Äù). Si la bo√Æte n‚Äôarrive pas √† traiter ton devoir, elle r√©essaie 2 fois. Si √ßa rate encore, il faut un endroit sp√©cial o√π mettre les devoirs ‚Äúrat√©s‚Äù pour que le prof puisse les retrouver et comprendre pourquoi.**\n\nIci, l‚Äôappli envoie un ‚Äúmessage‚Äù √† Lambda (un petit robot qui fait une t√¢che) sans attendre la r√©ponse. Lambda essaie, puis r√©essaie 2 fois. Apr√®s ces √©checs, tu veux enqu√™ter sur ce qui n‚Äôa pas √©t√© trait√©. La meilleure id√©e, c‚Äôest une ‚Äúbo√Æte des devoirs rat√©s‚Äù : une Dead Letter Queue. Avec l‚Äôoption B, les √©v√©nements qui ont √©chou√© sont envoy√©s dans une file (Amazon SQS = une bo√Æte qui garde des messages en attente). Tu peux ensuite lire ces messages, voir ce qui a √©t√© envoy√©, et comprendre l‚Äôerreur. Les autres choix ne stockent pas simplement les √©v√©nements rat√©s pour enqu√™te.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:190:dfaa2d116461e0a3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 190,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company needs to distribute firmware updates to its customers around the world.Which service will allow easy and secure control of the access to the downloads at the lowest cost?",
      "choices": {
        "A": "Use Amazon CloudFront with signed URLs for Amazon S3.",
        "B": "Create a dedicated Amazon CloudFront Distribution for each customer.",
        "C": "Use Amazon CloudFront with AWS Lambda@Edge.",
        "D": "Use Amazon API Gateway and AWS Lambda to control access to an S3 bucket."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122612-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:48 a.m.",
      "textHash": "dfaa2d116461e0a3",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:281a62f4",
      "frExplanation": "Pour distribuer des fichiers (firmware) mondialement, on veut un t√©l√©chargement rapide, peu cher, et avec contr√¥le d‚Äôacc√®s.\nAmazon S3 est un stockage d‚Äôobjets (les fichiers). Amazon CloudFront est un CDN : il met des copies en cache pr√®s des utilisateurs pour acc√©l√©rer et r√©duire les co√ªts de sortie.\nLes ‚Äúsigned URLs‚Äù (URL sign√©es) sont des liens temporaires g√©n√©r√©s avec une cl√© : seuls ceux qui ont le lien valide peuvent t√©l√©charger, et on peut limiter la dur√©e, l‚ÄôIP, etc.\nDonc CloudFront + URL sign√©es permet de garder le bucket S3 priv√©, tout en donnant un acc√®s contr√¥l√© et s√©curis√© aux clients.\nC‚Äôest aussi √©conomique : une seule distribution CloudFront sert tout le monde, et CloudFront est optimis√© pour la diffusion de contenu.\nB est trop co√ªteux et complexe (une distribution par client). C (Lambda@Edge) ajoute du code et des co√ªts/complexit√© inutiles pour un simple contr√¥le d‚Äôacc√®s. D (API Gateway + Lambda) est plus cher et moins adapt√© √† de gros t√©l√©chargements que CloudFront.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que tu dois donner √† des √©l√®ves du monde entier un gros fichier (comme un devoir vid√©o) depuis la biblioth√®que du lyc√©e.**\n\nConcept : la biblioth√®que (S3) garde les fichiers, et des mini-biblioth√®ques partout dans le monde (CloudFront) les distribuent vite, pr√®s des √©l√®ves.\nMais tu veux contr√¥ler qui a le droit de t√©l√©charger : tu donnes un ‚Äúticket‚Äù sp√©cial qui expire (URL sign√©e).\nA : CloudFront + URL sign√©es pour S3 = tu distribues rapidement partout, et seuls ceux qui ont le ticket peuvent entrer.\nC‚Äôest simple : pas besoin d‚Äôun surveillant compliqu√©, juste des tickets.\nC‚Äôest s√©curis√© : le ticket est difficile √† copier et peut expirer.\nC‚Äôest le moins cher : une seule distribution CloudFront pour tous, et pas de calcul en plus.\nB co√ªte cher (une mini-biblioth√®que par √©l√®ve).\nC et D ajoutent des ‚Äúsurveillants‚Äù (logique/traitement) donc plus complexe et souvent plus cher.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:255:70c01212c8d3ce90",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 255,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs a batch processing application by using AWS Lambda functions and Amazon API Gateway APIs with deployment stages for development, user acceptance testing, and production. A development team needs to configure the APIs in the deployment stages to connect to third-party service endpoints.Which solution will meet this requirement?",
      "choices": {
        "A": "Store the third-party service endpoints in Lambda layers that correspond to the stage.",
        "B": "Store the third-party service endpoints in API Gateway stage variables that correspond to the stage.",
        "C": "Encode the third-party service endpoints as query parameters in the API Gateway request URL.",
        "D": "Store the third-party service endpoint for each environment in AWS AppConfig."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134248-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 20, 2024, 4:24 p.m.",
      "textHash": "70c01212c8d3ce90",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:5f6bf7dd",
      "frExplanation": "API Gateway est le service qui expose vos API HTTP. Une ‚Äústage‚Äù (dev, UAT, prod) est une version d√©ploy√©e de l‚ÄôAPI avec sa propre configuration.\nLe besoin est de changer l‚ÄôURL du service tiers selon l‚Äôenvironnement, sans modifier le code ni cr√©er des URLs bizarres.\nLes ‚Äústage variables‚Äù d‚ÄôAPI Gateway sont justement des paires cl√©/valeur propres √† chaque stage (ex: endpoint=https://api-dev‚Ä¶ vs https://api-prod‚Ä¶).\nVotre int√©gration (ou votre Lambda) peut lire cette variable et appeler le bon endpoint automatiquement selon le stage utilis√©.\nA est mauvais: les Lambda Layers servent √† partager des biblioth√®ques/fichiers, pas √† g√©rer des param√®tres par environnement facilement.\nC est mauvais: mettre l‚Äôendpoint en query parameter est fragile, expos√© aux utilisateurs, et facile √† manipuler.\nD est moins adapt√© ici: AppConfig sert √† g√©rer des configurations applicatives, mais la question demande de configurer les APIs par stage; les stage variables sont la solution native et simple.\nDonc la bonne r√©ponse est B: stocker les endpoints dans les variables de stage d‚ÄôAPI Gateway.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une pizzeria qui a 3 cuisines s√©par√©es : entra√Ænement (dev), test avec des amis (UAT), et service officiel (prod). Chaque cuisine doit appeler un livreur diff√©rent (le service tiers) selon l‚Äôendroit.**\n\nConcept : une ‚Äústage‚Äù dans API Gateway, c‚Äôest comme une cuisine diff√©rente pour le m√™me menu. On veut changer une info (l‚Äôadresse/num√©ro du livreur) selon la cuisine, sans r√©√©crire la recette. Les ‚Äústage variables‚Äù, c‚Äôest un petit tableau d‚Äôaffichage dans chaque cuisine : en dev tu √©cris l‚Äôadresse du livreur de dev, en UAT celle de test, en prod celle du vrai. L‚ÄôAPI lit le tableau de SA cuisine et appelle le bon service tiers. Donc B est bon : on stocke les endpoints dans les variables de stage, une valeur par stage. A est moins adapt√© : une ‚ÄúLambda layer‚Äù c‚Äôest plut√¥t une bo√Æte d‚Äôoutils partag√©e, pas un panneau qui change selon la cuisine. C est mauvais : mettre l‚Äôadresse du livreur dans l‚ÄôURL, c‚Äôest comme l‚Äô√©crire sur chaque pizza, c‚Äôest moche et risqu√©. D pourrait marcher mais c‚Äôest une armoire externe plus compliqu√©e ; ici on veut juste r√©gler √ßa directement par stage.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:250:a3caaf8b00a71d27",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 250,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses AWS CloudFormation to deploy an application that uses an Amazon API Gateway REST API with AWS Lambda function integration. The application uses Amazon DynamoDB for data persistence. The application has three stages: development, testing, and production. Each stage uses its own DynamoDB table.The company has encountered unexpected issues when promoting changes to the production stage. The changes were successful in the development and testing stages. A developer needs to route 20% of the traffic to the new production stage API with the next production release. The developer needs to route the remaining 80% of the traffic to the existing production stage. The solution must minimize the number of errors that any single customer experiences.Which approach should the developer take to meet these requirements?",
      "choices": {
        "A": "Update 20% of the planned changes to the production stage. Deploy the new production stage. Monitor the results. Repeat this process five times to test all planned changes.",
        "B": "Update the Amazon Route 53 DNS record entry for the production stage API to use a weighted routing policy. Set the weight to a value of 80. Add a second record for the production domain name. Change the second routing policy to a weighted routing policy. Set the weight of the second policy to a value of 20. Change the alias of the second policy to use the testing stage API.",
        "C": "Deploy an Application Load Balancer (ALB) in front of the REST API. Change the production API Amazon Route 53 record to point traffic to the ALB. Register the production and testing stages as targets of the ALB with weights of 80% and 20%, respectively.",
        "D": "Configure canary settings for the production stage API. Change the percentage of traffic directed to canary deployment to 20%. Make the planned updates to the production stage. Deploy the changes"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124861-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 29, 2023, 2:02 a.m.",
      "textHash": "a3caaf8b00a71d27",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Ici, on veut faire une mise en production ‚Äúprogressive‚Äù : envoyer seulement 20% des utilisateurs vers la nouvelle version, et garder 80% sur l‚Äôancienne, pour limiter l‚Äôimpact si un bug appara√Æt.\nAmazon API Gateway (REST API) g√®re des ‚Äústages‚Äù (dev/test/prod) et peut d√©ployer une nouvelle version de prod en mode canary.\nUn canary deployment dans API Gateway duplique le stage de production : une petite partie du trafic va vers la nouvelle configuration, le reste reste sur l‚Äôancienne.\nC‚Äôest exactement le besoin : 20% vers la nouvelle release, 80% vers l‚Äôexistante, sans changer de DNS ni ajouter d‚Äôinfrastructure.\nCela minimise les erreurs par client : la majorit√© des clients restent sur l‚Äôancienne version, et seuls quelques-uns voient la nouvelle.\nRoute 53 en pond√©r√© (B) ferait du split DNS, mais ce n‚Äôest pas ‚Äúpar requ√™te‚Äù de fa√ßon fine et peut faire basculer un client selon la r√©solution DNS/caching.\nUn ALB devant API Gateway (C) n‚Äôest pas l‚Äôarchitecture normale : API Gateway n‚Äôest pas une cible ALB classique pour faire du weighted target routing.\nDonc la bonne approche est de configurer le canary sur le stage prod et mettre 20% de trafic vers la nouvelle version (D).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : tu veux tester une nouvelle recette de p√¢tes, mais tu ne veux pas que toute l‚Äô√©cole tombe sur une recette rat√©e d‚Äôun coup.**\n\nConcept : au lieu de changer 100% d‚Äôun coup, tu fais un ‚Äútest en douceur‚Äù : 20% des √©l√®ves re√ßoivent la nouvelle recette, 80% gardent l‚Äôancienne.\n√áa s‚Äôappelle un d√©ploiement ‚Äúcanari‚Äù : comme un petit groupe test avant tout le monde.\nPourquoi D : l‚ÄôAPI en production peut envoyer seulement 20% des demandes vers la nouvelle version, et 80% vers l‚Äôancienne.\nSi la nouvelle version a un bug, seuls quelques clients le voient, pas tout le monde (moins d‚Äôerreurs par personne).\nTu surveilles, puis tu augmentes si tout va bien.\nA est risqu√© : tu changes des morceaux au hasard, √ßa ne garantit pas 20/80 proprement.\nB et C d√©tournent le trafic avec d‚Äôautres ‚Äúpanneaux de direction‚Äù, mais D est fait expr√®s pour tester une nouvelle version en production sans casser tout.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:246:4dfbb6e1a7304223",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 246,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn online sales company is developing a serverless application that runs on AWS. The application uses an AWS Lambda function that calculates order success rates and stores the data in an Amazon DynamoDB table. A developer wants an efficient way to invoke the Lambda function every 15 minutes.Which solution will meet this requirement with the LEAST development effort?",
      "choices": {
        "A": "Create an Amazon EventBridge rule that has a rate expression that will run the rule every 15 minutes. Add the Lambda function as the target of the EventBridge rule.",
        "B": "Create an AWS Systems Manager document that has a script that will invoke the Lambda function on Amazon EC2. Use a Systems Manager Run Command task to run the shell script every 15 minutes.",
        "C": "Create an AWS Step Functions state machine. Configure the state machine to invoke the Lambda function execution role at a specified interval by using a Wait state. Set the interval to 15 minutes.",
        "D": "Provision a small Amazon EC2 instance. Set up a cron job that invokes the Lambda function every 15 minutes."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124748-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 27, 2023, 9:42 p.m.",
      "textHash": "4dfbb6e1a7304223",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "On veut ex√©cuter automatiquement une fonction AWS Lambda toutes les 15 minutes, sans g√©rer de serveur. Lambda est un service ¬´ serverless ¬ª : AWS ex√©cute votre code √† la demande. Le besoin est un d√©clenchement planifi√© (comme un minuteur). Amazon EventBridge est un service d‚Äô√©v√©nements qui peut lancer des actions selon un planning (r√®gle avec expression ¬´ rate(15 minutes) ¬ª). En ajoutant la fonction Lambda comme ¬´ cible ¬ª de la r√®gle, EventBridge l‚Äôinvoque directement toutes les 15 minutes, avec tr√®s peu de configuration. Les options avec EC2 (cron) ou Systems Manager demandent de cr√©er/maintenir une machine virtuelle, donc plus d‚Äôeffort. Step Functions sert surtout √† orchestrer plusieurs √©tapes; l‚Äôutiliser juste pour attendre 15 minutes est plus complexe que n√©cessaire. Donc la solution la plus simple et avec le moins de d√©veloppement est EventBridge + r√®gle planifi√©e.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine le lyc√©e : tu veux qu‚Äôune sonnerie automatique rappelle au surveillant toutes les 15 minutes d‚Äôaller noter le nombre d‚Äô√©l√®ves ‚Äúpr√©sents‚Äù vs ‚Äúabsents‚Äù et de ranger le r√©sultat dans un cahier.**\n\nIci, la ‚Äúsonnerie‚Äù = un d√©clencheur automatique, le ‚Äúsurveillant‚Äù = la fonction Lambda (un petit robot qui fait un calcul), et le ‚Äúcahier‚Äù = DynamoDB (un tableau o√π on stocke les r√©sultats).\nLa solution A (EventBridge) c‚Äôest comme installer une sonnerie programm√©e ‚Äútoutes les 15 minutes‚Äù qui appelle directement le surveillant : simple, automatique, et tu n‚Äôas rien √† g√©rer.\nB et D, c‚Äôest comme demander √† un √©l√®ve (un ordinateur EC2) de penser √† regarder l‚Äôheure et de lancer l‚Äôaction : il faut installer, maintenir, et √ßa fait plus de boulot.\nC, c‚Äôest comme cr√©er tout un sc√©nario de vie scolaire juste pour attendre 15 minutes puis agir : trop compliqu√© pour un simple rappel.\nDonc A demande le moins d‚Äôeffort : tu r√®gles la sonnerie et c‚Äôest fini.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:245:363a6673b1f2b59e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 245,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a new application that will be deployed on AWS. The developer has created an AWS CodeCommit repository for the application. The developer has initialized a new project for the application by invoking the AWS Cloud Development Kit (AWS CDK) cdk init command.The developer must write unit tests for the infrastructure as code (IaC) templates that the AWS CDK generates. The developer also must run a validation tool across all constructs in the CDK application to ensure that critical security configurations are activated.Which combination of actions will meet these requirements with the LEAST development overhead? (Choose two.)",
      "choices": {
        "A": "Use a unit testing framework to write custom unit tests against the cdk.out file that the AWS CDK generates. Run the unit tests in a continuous integration and continuous delivery (CI/CD) pipeline that is invoked after any commit to the repository.",
        "B": "Use the CDK assertions module to integrate unit tests with the application. Run the unit tests in a continuous integration and continuous delivery (CI/CD) pipeline that is invoked after any commit to the repository.",
        "C": "Use the CDK runtime context to set key-value pairs that must be present in the cdk.out file that the AWS CDK generates. Fail the stack synthesis if any violations are present.",
        "D": "Write a script that searches the application for specific key configuration strings. Configure the script to produce a report of any security violations.",
        "E": "Use the CDK Aspects class to create custom rules to apply to the CDK application. Fall the stack synthesis if any violations are present."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124831-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:53 p.m.",
      "textHash": "363a6673b1f2b59e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:a75cfa55",
      "frExplanation": "AWS CDK est un outil qui permet d‚Äô√©crire l‚Äôinfrastructure (r√©seaux, bases, fonctions, etc.) en code, puis de g√©n√©rer un mod√®le CloudFormation lors de la ‚Äúsynth√®se‚Äù.\nLe besoin 1 est de faire des tests unitaires sur ce que le CDK va d√©ployer, sans devoir parser des fichiers g√©n√©r√©s √† la main.\nLe module CDK Assertions est fait pour √ßa : il permet de v√©rifier directement, dans des tests, que le template g√©n√©r√© contient bien certaines ressources et propri√©t√©s (ex: chiffrement activ√©, logs, politiques IAM).\nC‚Äôest moins de travail que d‚Äô√©crire des tests ‚Äúcustom‚Äù contre le dossier cdk.out (A), car Assertions fournit des helpers adapt√©s au CDK.\nLe besoin 2 est de valider la s√©curit√© sur tous les constructs : on peut int√©grer ces v√©rifications dans les m√™mes tests (ou en compl√©ment) et les ex√©cuter automatiquement.\nEn pratique, on lance ces tests dans un pipeline CI/CD apr√®s chaque commit CodeCommit : si un test √©choue, le d√©ploiement est bloqu√©.\nLes options bas√©es sur recherche de cha√Ænes (D) ou context (C) sont fragiles et ne garantissent pas une validation fiable du template.\nLes Aspects (E) servent bien √† appliquer des r√®gles globales, mais la r√©ponse attendue ici pour le minimum d‚Äôeffort de tests unitaires est l‚Äôusage direct de CDK Assertions.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:244:72b53b3886adecec",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 244,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn AWS Lambda function is running in a company‚Äôs shared AWS account. The function needs to perform an additional ec2:DescribeInstances action that is directed at the company‚Äôs development accounts. A developer must configure the required permissions across the accounts.How should the developer configure the permissions to adhere to the principle of least privilege?",
      "choices": {
        "A": "Create an IAM role in the shared account. Add the ec2:DescribeInstances permission to the role. Establish a trust relationship between the development accounts for this role. Update the Lambda function IAM role in the shared account by adding the ec2:DescribeInstances permission to the role.",
        "B": "Create an IAM role in the development accounts. Add the ec2:DescribeInstances permission to the role. Establish a trust relationship with the shared account for this role. Update the Lambda function IAM role in the shared account by adding the iam:AssumeRole permissions.",
        "C": "Create an IAM role in the shared account. Add the ec2:DescribeInstances permission to the role. Establish a trust relationship between the development accounts for this role. Update the Lambda function IAM role in the shared account by adding the iam:AssumeRole permissions.",
        "D": "Create an IAM role in the development accounts. Add the ec2:DescribeInstances permission to the role. Establish a trust relationship with the shared account for this role. Update the Lambda function IAM role in the shared account by adding the ec2:DescribeInstances permission to the role."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124787-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 9:40 a.m.",
      "textHash": "72b53b3886adecec",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, la fonction AWS Lambda est dans un compte ¬´ partag√© ¬ª, mais elle doit lire des infos EC2 (DescribeInstances) dans d‚Äôautres comptes ¬´ d√©veloppement ¬ª.\nPrincipe du moindre privil√®ge : chaque compte garde ses propres permissions sur ses propres ressources, et on n‚Äôaccorde que l‚Äôacc√®s n√©cessaire.\nLa bonne pratique multi-comptes est de cr√©er un r√¥le IAM dans le compte cible (les comptes dev) avec uniquement ec2:DescribeInstances.\nEnsuite, ce r√¥le doit faire confiance au compte partag√© (relation de confiance / trust policy) pour autoriser l‚Äôassumption depuis ce compte.\nDans le compte partag√©, le r√¥le d‚Äôex√©cution de Lambda n‚Äôa pas besoin de droits EC2 directs sur les comptes dev ; il a seulement besoin de iam:AssumeRole pour ¬´ endosser ¬ª le r√¥le du compte dev.\nLambda appelle alors STS AssumeRole, obtient des identifiants temporaires, puis ex√©cute DescribeInstances dans le compte dev.\nLes choix qui ajoutent ec2:DescribeInstances au r√¥le de Lambda dans le compte partag√© donnent des droits inutiles ou ne r√©solvent pas l‚Äôacc√®s cross-account.\nDonc B est correct : r√¥le dans les comptes dev + trust vers le compte partag√© + Lambda autoris√©e √† assumer ce r√¥le.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un lyc√©e avec plusieurs salles (comptes AWS). Tu es dans la salle ‚Äúpartag√©e‚Äù et tu veux juste regarder la liste des √©l√®ves d‚Äôune autre salle ‚Äúd√©veloppement‚Äù.**\n\nConcept : pour respecter ‚Äúle minimum de droits‚Äù, chaque salle garde ses propres r√®gles d‚Äôacc√®s. Tu ne donnes pas un pass VIP partout, tu demandes une autorisation pr√©cise √† la salle concern√©e.\nIci, la fonction Lambda (un ‚Äúrobot‚Äù qui fait une t√¢che) est dans le compte partag√©, mais l‚Äôaction ‚ÄúDescribeInstances‚Äù (voir la liste des PC/serveurs) vise les comptes de dev.\nDonc on cr√©e un ‚Äúbadge‚Äù (r√¥le IAM) DANS chaque compte de dev, avec uniquement le droit de ‚Äúvoir la liste‚Äù (ec2:DescribeInstances).\nOn dit ensuite : ‚Äúce badge accepte d‚Äô√™tre utilis√© par la salle partag√©e‚Äù (relation de confiance vers le compte partag√©).\nEnfin, dans le compte partag√©, on donne au robot seulement le droit de ‚Äúprendre ce badge‚Äù (iam:AssumeRole), pas le droit direct de fouiller partout.\nC‚Äôest exactement B : le droit est stock√© l√† o√π sont les ressources (dev), et le robot ne fait que l‚Äôemprunter quand il en a besoin.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:242:8f9b51efd80395e7",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 242,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is using AWS CodePipeline to deliver one of its applications. The delivery pipeline is triggered by changes to the main branch of an AWS CodeCommit repository and uses AWS CodeBuild to implement the test and build stages of the process and AWS CodeDeploy to deploy the application.The pipeline has been operating successfully for several months and there have been no modifications. Following a recent change to the application‚Äôs source code, AWS CodeDeploy has not deployed the updated application as expected.What are the possible causes? (Choose two.)",
      "choices": {
        "A": "The change was not made in the main branch of the AWS CodeCommit repository.",
        "B": "One of the earlier stages in the pipeline failed and the pipeline has terminated.",
        "C": "One of the Amazon EC2 instances in the company‚Äôs AWS CodePipeline cluster is inactive.",
        "D": "The AWS CodePipeline is incorrectly configured and is not invoking AWS CodeDeploy.",
        "E": "AWS CodePipeline does not have permissions to access AWS CodeCommit."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124830-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:45 p.m.",
      "textHash": "8f9b51efd80395e7",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "CodePipeline est une cha√Æne automatique : un changement dans CodeCommit (d√©p√¥t Git) d√©clenche le pipeline, CodeBuild teste/compile, puis CodeDeploy d√©ploie sur les serveurs.\nSi CodeDeploy ne d√©ploie pas la nouvelle version, la cause la plus simple est que le pipeline n‚Äôa jamais √©t√© d√©clench√©.\nCela arrive si le commit n‚Äôa pas √©t√© fait sur la branche surveill√©e (ici ¬´ main ¬ª) : par exemple sur une autre branche ou une pull request non fusionn√©e. (A)\nAutre cause fr√©quente : une √©tape avant CodeDeploy a √©chou√© (tests/compilation dans CodeBuild), donc le pipeline s‚Äôarr√™te et n‚Äôatteint pas le d√©ploiement. (B)\nLes options sur un ‚Äúcluster CodePipeline‚Äù ou une instance EC2 inactive ne correspondent pas : CodePipeline est un service g√©r√©, pas un cluster d‚ÄôEC2. (C)\nSi le pipeline fonctionnait depuis des mois sans changement, une mauvaise configuration ou des permissions manquantes sont moins probables, et elles auraient cass√© plus t√¥t. (D, E)\nDonc les deux causes plausibles sont : mauvais branchement du commit et √©chec d‚Äôune √©tape pr√©c√©dente.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cha√Æne √† l‚Äô√©cole : tu rends ton devoir dans la bo√Æte ‚ÄúMAIN‚Äù, le prof (tests) v√©rifie, puis le surveillant (d√©ploiement) l‚Äôaffiche au tableau.**\n\nConcept : CodePipeline = la cha√Æne automatique. CodeCommit = le casier o√π tu d√©poses ton devoir. CodeBuild = le prof qui corrige. CodeDeploy = celui qui met la nouvelle version en place.\nLa cha√Æne ne d√©marre que si tu d√©poses dans le casier ‚Äúmain‚Äù (la branche main).\nSi tu as modifi√© ton devoir mais dans un autre casier (une autre branche), la cha√Æne ne voit rien.\nDonc personne ne corrige, et rien n‚Äôest affich√© au tableau : pas de d√©ploiement.\nC‚Äôest exactement la cause A : le changement n‚Äôa pas √©t√© fait dans la branche main.\nLes autres choix parlent de pannes ou de mauvaise config, mais la cha√Æne marchait depuis des mois sans changement.\nLa cause la plus logique apr√®s ‚Äúj‚Äôai chang√© le code‚Äù = tu n‚Äôas pas chang√© au bon endroit (main).",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:241:9f513370304a96ea",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 241,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is creating a REST service using an Amazon API Gateway with AWS Lambda integration. The service must run different versions for testing purposes.What would be the BEST way to accomplish this?",
      "choices": {
        "A": "Use an X-Version header to denote which version is being called and pass that header to the Lambda function(s).",
        "B": "Create an API Gateway Lambda authorizer to route API clients to the correct API version.",
        "C": "Create an API Gateway resource policy to isolate versions and provide context to the Lambda function(s).",
        "D": "Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124828-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:43 p.m.",
      "textHash": "9f513370304a96ea",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "API Gateway sert √† exposer une API REST via une URL, et Lambda ex√©cute le code derri√®re chaque appel.\nPour tester plusieurs versions, le plus simple est d‚Äôavoir des URLs diff√©rentes, afin que les testeurs choisissent clairement la version.\nLes ‚Äústages‚Äù d‚ÄôAPI Gateway (ex: dev, test, v1, v2) permettent de d√©ployer la m√™me API en plusieurs environnements, chacun avec son endpoint unique.\nOn peut aussi utiliser des ‚Äústage variables‚Äù (des param√®tres par stage) pour indiquer √† l‚Äôint√©gration quelle version de Lambda appeler (alias/version) ou quel comportement activer.\nAinsi, chaque version est isol√©e, facile √† activer/d√©sactiver, et on √©vite de bricoler la logique de routage dans le code.\nA est moins ‚Äúbest‚Äù car il faut g√©rer le routage dans Lambda et les clients doivent envoyer un header sp√©cial.\nB et C ne sont pas faits pour versionner : un authorizer sert √† l‚Äôauthentification/autorisation, une resource policy sert √† contr√¥ler l‚Äôacc√®s.\nDonc la meilleure approche est de d√©ployer chaque version en stage distinct avec endpoint distinct + variables de stage.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:240:ba68ecc708bd3d25",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 240,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company recently deployed an AWS Lambda function. A developer notices an increase in the function throttle metrics in Amazon CloudWatch.What are the MOST operationally efficient solutions to reduce the function throttling? (Choose two.)",
      "choices": {
        "A": "Migrate the function to Amazon Elastic Kubernetes Service (Amazon EKS).",
        "B": "Increase the maximum age of events in Lambda.",
        "C": "Increase the function‚Äôs reserved concurrency.",
        "D": "Add the lambda:GetFunctionConcurrency action to the execution role.",
        "E": "Request a service quota change for increased concurrency."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124827-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:41 p.m.",
      "textHash": "ba68ecc708bd3d25",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "AWS Lambda ex√©cute du code ‚Äú√† la demande‚Äù. Le throttling signifie que Lambda refuse temporairement des ex√©cutions car la limite de concurrence (nombre d‚Äôex√©cutions en parall√®le) est atteinte.\nAmazon CloudWatch affiche ces m√©triques pour diagnostiquer ce type de saturation.\nLa solution la plus efficace est d‚Äôaugmenter la ‚Äúreserved concurrency‚Äù de la fonction : on r√©serve explicitement plus de places de concurrence pour cette fonction, ce qui r√©duit les refus (r√©ponse C).\nUne autre action op√©rationnelle courante (souvent attendue avec C) est de demander une augmentation de quota de concurrence au niveau du compte si la limite globale est atteinte (r√©ponse E).\nAugmenter l‚Äô√¢ge maximum des √©v√©nements (B) ne r√©duit pas le throttling : cela change seulement combien de temps les √©v√©nements peuvent attendre avant d‚Äôexpirer.\nAjouter une permission IAM (D) ne change pas les limites de concurrence : les r√¥les contr√¥lent l‚Äôacc√®s, pas la capacit√©.\nMigrer vers EKS (A) est lourd et non ‚Äúop√©rationnellement efficient‚Äù pour un simple probl√®me de limite de concurrence Lambda.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : la ‚Äúfonction Lambda‚Äù c‚Äôest un stand qui pr√©pare des sandwichs, et les √©l√®ves arrivent parfois tous en m√™me temps.**\n\nConcept : le ‚Äúthrottling‚Äù, c‚Äôest quand le stand refuse des √©l√®ves parce qu‚Äôil n‚Äôa pas assez de places/serveurs en m√™me temps.\nCloudWatch, c‚Äôest le tableau qui te dit : ‚Äúon a refus√© X √©l√®ves‚Äù.\nPour r√©duire √ßa, il faut augmenter le nombre de sandwichs pr√©parables en parall√®le.\nC (reserved concurrency) = tu r√©serves officiellement plus de ‚Äúplaces‚Äù pour ce stand, donc il peut servir plus d‚Äô√©l√®ves en m√™me temps.\nA (changer vers EKS) = c‚Äôest d√©m√©nager toute la cantine : trop lourd pour juste un probl√®me de file d‚Äôattente.\nB (√¢ge des √©v√©nements) = c‚Äôest garder les √©l√®ves plus longtemps dans la file, √ßa ne cr√©e pas plus de places.\nD (permission) = donner un badge au cuisinier ne rajoute pas de cuisiniers.\nE (demander plus de quota) peut aider, mais la solution directe et simple ici est de r√©server plus de places pour ta fonction (C).",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:236:25e096afe389b0cf",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 236,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an online web application that includes a product catalog. The catalog is stored in an Amazon S3 bucket that is named DOC-EXAMPLE-BUCKET. The application must be able to list the objects in the S3 bucket and must be able to download objects through an IAM policy.Which policy allows MINIMUM access to meet these requirements?",
      "choices": {},
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124825-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:37 p.m.",
      "textHash": "25e096afe389b0cf",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, l‚Äôapplication doit faire 2 actions sur un bucket Amazon S3 (un service de stockage de fichiers) : 1) lister les objets, 2) t√©l√©charger (lire) les objets.\nEn IAM (gestion des droits AWS), on applique le principe du moindre privil√®ge : donner uniquement les permissions n√©cessaires.\nPour lister le contenu d‚Äôun bucket, il faut l‚Äôaction s3:ListBucket, et elle s‚Äôapplique sur la ressource ¬´ bucket ¬ª (ARN du bucket).\nPour t√©l√©charger un objet, il faut s3:GetObject, et elle s‚Äôapplique sur les ressources ¬´ objets ¬ª (ARN du bucket avec /*).\nLa bonne policy minimale contient donc deux statements (ou deux ressources) :\n- Allow s3:ListBucket sur arn:aws:s3:::DOC-EXAMPLE-BUCKET\n- Allow s3:GetObject sur arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\nToute policy qui ajoute Put/Delete, ou qui met ListBucket sur /*, ou qui utilise * partout, donne trop d‚Äôacc√®s et n‚Äôest pas minimale.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e (le bucket S3) qui s‚Äôappelle DOC-EXAMPLE-BUCKET. Les livres sont les fichiers (objets). Un √©l√®ve (l‚Äôapplication) doit voir la liste des livres et en emprunter certains.**\n\nConcept : une ‚Äúpolicy IAM‚Äù, c‚Äôest comme une autorisation √©crite qui dit exactement ce que l‚Äô√©l√®ve a le droit de faire dans la biblioth√®que.\nIci, il faut 2 droits minimum : 1) regarder le catalogue (lister les objets) et 2) prendre un livre (t√©l√©charger un objet).\nLa r√©ponse A est la bonne car elle donne seulement ces deux permissions :\n- ‚ÄúListBucket‚Äù sur la biblioth√®que elle-m√™me (DOC-EXAMPLE-BUCKET) = voir la liste des livres.\n- ‚ÄúGetObject‚Äù sur les livres √† l‚Äôint√©rieur (DOC-EXAMPLE-BUCKET/*) = t√©l√©charger un fichier.\nLes autres choix donnent souvent trop de droits (ex: modifier/supprimer des livres), ce qui n‚Äôest pas demand√©.\nDonc A = le minimum pour lister + t√©l√©charger, sans pouvoir changer quoi que ce soit.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:235:8853b0157711116c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 235,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a three-tier application that is deployed in Amazon Elastic Container Service (Amazon ECS). The application is using an Amazon RDS for MySQL DB instance. The application performs more database reads than writes.During times of peak usage, the application‚Äôs performance degrades. When this performance degradation occurs, the DB instance‚Äôs ReadLatency metric in Amazon CloudWatch increases suddenly.How should a developer modify the application to improve performance?",
      "choices": {
        "A": "Use Amazon ElastiCache to cache query results.",
        "B": "Scale the ECS cluster to contain more ECS instances.",
        "C": "Add read capacity units (RCUs) to the DB instance.",
        "D": "Modify the ECS task definition to increase the task memory."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124822-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:28 p.m.",
      "textHash": "8853b0157711116c",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "L‚Äôapplication tourne sur ECS (des conteneurs) et utilise une base RDS MySQL (base relationnelle g√©r√©e). Elle fait surtout des lectures (reads) et, en pic, la m√©trique CloudWatch ReadLatency augmente : cela veut dire que la base met plus de temps √† r√©pondre aux requ√™tes de lecture.\nQuand le probl√®me est la latence de lecture c√¥t√© base, ajouter plus de conteneurs ECS (B) ou plus de m√©moire aux t√¢ches (D) n‚Äôaide pas directement : cela peut m√™me augmenter le nombre de requ√™tes envoy√©es √† la base.\nL‚Äôoption C (RCUs) concerne DynamoDB, pas RDS MySQL, donc ce n‚Äôest pas applicable.\nLa solution logique est de r√©duire le nombre de lectures r√©p√©titives sur la base : Amazon ElastiCache (Redis/Memcached) est un cache en m√©moire tr√®s rapide.\nOn met en cache les r√©sultats de requ√™tes fr√©quentes (ex: listes, profils, pages populaires) et on sert ces donn√©es depuis ElastiCache au lieu de relire MySQL √† chaque fois.\nMoins de requ√™tes de lecture sur RDS = ReadLatency qui baisse et meilleures performances en p√©riode de pointe.\nDonc la meilleure modification applicative est d‚Äôajouter un cache de r√©sultats de requ√™tes avec ElastiCache (A).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e : tout le monde vient surtout LIRE le m√™me manuel tr√®s demand√©, et il n‚Äôy a qu‚Äôun seul exemplaire au bureau du documentaliste.**\n\nConcept : quand beaucoup de gens demandent la m√™me info, tu peux mettre une copie rapide √† port√©e de main au lieu d‚Äôaller chercher l‚Äôoriginal √† chaque fois.\nIci, la base de donn√©es (le livre original) re√ßoit surtout des lectures.\nEn heure de pointe, la file d‚Äôattente pour lire augmente : c‚Äôest la ‚ÄúReadLatency‚Äù qui monte (attente pour obtenir la r√©ponse).\nSolution A : ElastiCache, c‚Äôest comme une pile de photocopies des pages les plus demand√©es au comptoir.\nL‚Äôappli lit d‚Äôabord la ‚Äúcopie‚Äù (cache) : c‚Äôest beaucoup plus rapide et √ßa √©vite d‚Äôencombrer le documentaliste.\nDonc la base est moins sollicit√©e, l‚Äôattente baisse, et les performances remontent.\nB et D ajoutent des ‚Äú√©l√®ves‚Äù/m√©moire c√¥t√© appli, mais le goulot reste le documentaliste.\nC parle d‚Äôajouter une capacit√© de lecture, mais ce n‚Äôest pas la bonne id√©e ici : le probl√®me vient surtout de trop relire les m√™mes infos, donc on met en cache.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:232:f575fd7def40779b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 232,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer wants to reduce risk when deploying a new version of an existing AWS Lambda function. To test the Lambda function, the developer needs to split the traffic between the existing version and the new version of the Lambda function.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure a weighted routing policy in Amazon Route 53. Associate the versions of the Lambda function with the weighted routing policy.",
        "B": "Create a function alias. Configure the alias to split the traffic between the two versions of the Lambda function.",
        "C": "Create an Application Load Balancer (ALB) that uses the Lambda function as a target. Configure the ALB to split the traffic between the two versions of the Lambda function.",
        "D": "Create the new version of the Lambda function as a Lambda layer on the existing version. Configure the function to split the traffic between the two layers."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124820-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:16 p.m.",
      "textHash": "f575fd7def40779b",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Ici, on veut d√©ployer une nouvelle version d‚Äôune fonction AWS Lambda (un petit code ex√©cut√© ‚Äú√† la demande‚Äù dans le cloud) en limitant le risque.\nLa bonne pratique est de faire un ‚Äúd√©ploiement progressif‚Äù : envoyer une partie du trafic vers la nouvelle version et garder le reste sur l‚Äôancienne.\nLambda g√®re des ‚Äúversions‚Äù num√©rot√©es (immutables) et des ‚Äúalias‚Äù (un nom comme prod ou beta) qui pointent vers une ou plusieurs versions.\nUn alias peut faire du routage pond√©r√© : par exemple 90% vers la version actuelle et 10% vers la nouvelle, puis augmenter si tout va bien.\nC‚Äôest exactement le besoin : tester en conditions r√©elles sans tout basculer d‚Äôun coup.\nRoute 53 sert surtout √† router du trafic DNS vers des endpoints, pas √† g√©rer directement des versions Lambda.\nUn ALB peut appeler Lambda, mais le split entre versions se fait plus simplement et nativement via l‚Äôalias Lambda.\nLes Lambda Layers servent √† partager des biblioth√®ques/d√©pendances, pas √† cr√©er une ‚Äúnouvelle version‚Äù et r√©partir le trafic.\nDonc la solution correcte est de cr√©er un alias et configurer le partage de trafic entre deux versions.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:231:699d927535c86b4e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 231,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Lambda function that will connect to an Amazon RDS for MySQL instance. The developer wants to store the database credentials. The database credentials need to be encrypted and the database password needs to be automatically rotated.Which solution will meet these requirements?",
      "choices": {
        "A": "Store the database credentials as environment variables for the Lambda function. Set the environment variables to rotate automatically.",
        "B": "Store the database credentials in AWS Secrets Manager. Set up managed rotation on the database credentials.",
        "C": "Store the database credentials in AWS Systems Manager Parameter Store as secure string parameters. Set up managed rotation on the parameters.",
        "D": "Store the database credentials in the X-Amz-Security-Token parameter. Set up managed rotation on the parameter."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124819-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:15 p.m.",
      "textHash": "699d927535c86b4e",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Ici, la fonction AWS Lambda (un code ex√©cut√© sans serveur) doit se connecter √† une base Amazon RDS MySQL (base de donn√©es g√©r√©e). Il faut stocker identifiant/mot de passe de fa√ßon chiffr√©e et faire tourner (changer) automatiquement le mot de passe.\nAWS Secrets Manager est un service fait pour stocker des ‚Äúsecrets‚Äù (mots de passe, cl√©s API) chiffr√©s et pour g√©rer la rotation automatique avec RDS via une rotation g√©r√©e (il met √† jour le mot de passe dans RDS et dans le secret).\nLes variables d‚Äôenvironnement Lambda peuvent √™tre chiffr√©es, mais elles ne proposent pas une rotation automatique int√©gr√©e et deviennent difficiles √† g√©rer en s√©curit√©.\nSSM Parameter Store peut stocker des SecureString chiffr√©s, mais la rotation automatique ‚Äúg√©r√©e‚Äù n‚Äôest pas l‚Äôoption standard pour RDS comme dans Secrets Manager (souvent il faut construire soi‚Äëm√™me la rotation).\nX-Amz-Security-Token n‚Äôest pas un endroit pour stocker des mots de passe de base : c‚Äôest li√© √† des jetons temporaires AWS (STS), pas √† des identifiants MySQL.\nDonc la solution logique et pr√©vue par AWS pour chiffrement + rotation automatique des identifiants RDS est Secrets Manager avec rotation g√©r√©e.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:229:d752eb7027571403",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 229,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is planning to use an Amazon API Gateway and AWS Lambda to provide a REST API. The developer will have three distinct environments to manage: development, test, and production.How should the application be deployed while minimizing the number of resources to manage?",
      "choices": {
        "A": "Create a separate API Gateway and separate Lambda function for each environment in the same Region.",
        "B": "Assign a Region for each environment and deploy API Gateway and Lambda to each Region.",
        "C": "Create one API Gateway with multiple stages with one Lambda function with multiple aliases.",
        "D": "Create one API Gateway and one Lambda function, and use a REST parameter to identify the environment."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124815-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:12 p.m.",
      "textHash": "d752eb7027571403",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:c20ff7ab",
      "frExplanation": "Objectif : avoir dev/test/prod sans multiplier les ressources. Amazon API Gateway est le service qui expose l‚ÄôAPI REST, et AWS Lambda ex√©cute le code.\nLa meilleure pratique est de r√©utiliser les m√™mes ‚Äúobjets‚Äù et de s√©parer les environnements via des m√©canismes pr√©vus : les stages et les aliases.\nAvec API Gateway, un ‚Äústage‚Äù (ex: /dev, /test, /prod) est une configuration/d√©ploiement distinct de la m√™me API : URL diff√©rente, variables de stage, throttling, logs, etc.\nAvec Lambda, un ‚Äúalias‚Äù pointe vers une version pr√©cise du code (ex: alias dev -> version 3, prod -> version 10), ce qui √©vite de cr√©er 3 fonctions.\nAinsi, chaque stage d‚ÄôAPI Gateway appelle l‚Äôalias Lambda correspondant : dev appelle alias dev, prod appelle alias prod.\nLes options A et B cr√©ent 3 fois plus de ressources (API + fonctions ou m√™me r√©gions), donc plus de gestion et de risques.\nL‚Äôoption D est fragile : un param√®tre REST pour choisir l‚Äôenvironnement m√©lange les environnements, augmente le risque d‚Äôappeler prod par erreur et complique la s√©curit√©.\nDonc C minimise les ressources tout en gardant une s√©paration claire et s√ªre entre dev/test/prod.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine le site web du lyc√©e comme un jeu vid√©o avec 3 serveurs : entra√Ænement (dev), match amical (test), tournoi (prod). Tu veux g√©rer le moins de ‚Äúsalles‚Äù et de ‚Äúr√®gles‚Äù possible.**\n\nConcept : au lieu de construire 3 b√¢timents s√©par√©s, tu fais un seul b√¢timent avec 3 salles. Chaque salle a ses r√©glages.\nAPI Gateway = la porte d‚Äôentr√©e du b√¢timent (les √©l√®ves arrivent ici). ‚ÄúStages‚Äù = les salles dev/test/prod.\nLambda = le prof robot qui fait le travail derri√®re. ‚ÄúAliases‚Äù = ses 3 badges : dev, test, prod (m√™me prof, badge diff√©rent).\nPourquoi C : 1 seule porte (1 API) + 3 salles (stages) + 1 prof robot + 3 badges (aliases). Peu de choses √† g√©rer.\nA ferait 3 portes + 3 profs : trop de ressources.\nB ajoute 3 campus (r√©gions) : encore plus compliqu√©.\nD m√©lange tout dans la m√™me salle et demande ‚Äútu es en dev ou prod ?‚Äù √† chaque fois : risque d‚Äôerreur.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:226:dcfca4ad19a8f86e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 226,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nGiven the following AWS CloudFormation template:What is the MOST efficient way to reference the new Amazon S3 bucket from another AWS CloudFormation template?",
      "choices": {
        "A": "Add an Export declaration to the Outputs section of the original template and use ImportValue in other templates.",
        "B": "Add Exported: true to the Content.Bucket in the original template and use ImportResource in other templates.",
        "C": "Create a custom AWS CloudFormation resource that gets the bucket name from the ContentBucket resource of the first stack.",
        "D": "Use Fn::Include to include the existing template in other templates and use the ContentBucket resource directly."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124817-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:13 p.m.",
      "textHash": "dcfca4ad19a8f86e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:s3:0d7ecb49",
      "frExplanation": "CloudFormation sert √† d√©crire et cr√©er des ressources AWS (comme un bucket S3) via des ‚Äútemplates‚Äù. Un template d√©ploy√© devient une ‚Äústack‚Äù.\nSi un autre template doit r√©utiliser le bucket cr√©√© par la premi√®re stack, il faut un m√©canisme officiel de partage entre stacks.\nLa m√©thode la plus efficace est de mettre dans la premi√®re stack une valeur en sortie (Outputs) et de l‚Äô‚Äúexporter‚Äù (Export). On y met par exemple le nom du bucket ou son ARN.\nEnsuite, dans l‚Äôautre template, on r√©cup√®re cette valeur avec la fonction ImportValue. C‚Äôest simple, natif, fiable et √©vite du code.\nB est faux : il n‚Äôexiste pas de propri√©t√© standard ‚ÄúExported: true‚Äù ni de fonction ‚ÄúImportResource‚Äù pour √ßa.\nC est inutilement complexe : un ‚Äúcustom resource‚Äù implique du code (souvent Lambda), plus de maintenance et de risques.\nD est inadapt√© : Fn::Include sert √† inclure du contenu de template, pas √† r√©f√©rencer directement une ressource d√©j√† cr√©√©e dans une autre stack.\nDonc la bonne r√©ponse est A : Export dans Outputs + ImportValue dans les autres templates.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine l‚Äô√©cole : une classe (un ‚Äútemplate‚Äù) cr√©e un nouveau casier de rangement (le bucket S3). Une autre classe veut utiliser ce m√™me casier sans se tromper de num√©ro.**\n\nConcept : quand une classe cr√©e un casier, elle doit afficher officiellement son num√©ro sur le panneau d‚Äôaffichage de l‚Äô√©cole, pour que les autres classes puissent le retrouver.\nDans CloudFormation, ce ‚Äúpanneau‚Äù = la section Outputs avec un Export : tu publies le nom/ID du bucket.\nEnsuite, l‚Äôautre template lit ce panneau avec ImportValue : il r√©cup√®re exactement la bonne r√©f√©rence, sans bricolage.\nPourquoi A : c‚Äôest la m√©thode pr√©vue, simple, rapide, et fiable pour partager une info entre deux templates.\nPourquoi pas B : ‚ÄúExported: true‚Äù et ‚ÄúImportResource‚Äù ne sont pas des fa√ßons normales de partager un bucket.\nPourquoi pas C : cr√©er un ‚Äú√©l√®ve messager‚Äù (custom resource) est plus compliqu√© et inutile.\nPourquoi pas D : inclure tout le template, c‚Äôest comme copier toute la classe au lieu de juste noter le num√©ro du casier.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:225:66f661675293ba84",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 225,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application is real-time processing millions of events that are received through an API.What service could be used to allow multiple consumers to process the data concurrently and MOST cost-effectively?",
      "choices": {
        "A": "Amazon SNS with fanout to an SQS queue for each application",
        "B": "Amazon SNS with fanout to an SQS FIFO (first-in, first-out) queue for each application",
        "C": "Amazon Kinesis Firehose",
        "D": "Amazon Kinesis Data Streams"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124775-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:45 a.m.",
      "textHash": "66f661675293ba84",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, l‚Äôapplication re√ßoit des millions d‚Äô√©v√©nements en temps r√©el via une API, et plusieurs applications (consommateurs) doivent lire les m√™mes donn√©es en parall√®le.\nAmazon Kinesis Data Streams est con√ßu pour l‚Äôingestion et le traitement temps r√©el √† tr√®s haut d√©bit : on √©crit une fois dans un ‚Äústream‚Äù, et plusieurs consommateurs peuvent lire simultan√©ment (via plusieurs applications ou consumer groups).\nC‚Äôest aussi √©conomique car on ne duplique pas les messages pour chaque consommateur : chacun lit le m√™me flux.\nAmazon SNS + SQS ‚Äúfanout‚Äù (A/B) copie chaque message dans une file par application : plus il y a de consommateurs, plus on paie en requ√™tes/stockage, donc moins rentable √† grande √©chelle.\nSQS FIFO (B) ajoute des contraintes d‚Äôordre strict et un d√©bit plus limit√©, inutile ici et potentiellement plus co√ªteux.\nKinesis Firehose (C) sert surtout √† livrer des donn√©es vers S3/Redshift/OpenSearch avec peu de contr√¥le de lecture par plusieurs consommateurs ; ce n‚Äôest pas l‚Äôoutil principal pour du multi-consumer temps r√©el.\nDonc la meilleure option, concurrente et la plus cost-effective pour plusieurs consommateurs en temps r√©el, est Kinesis Data Streams (D).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un tournoi de jeu vid√©o o√π des millions d‚Äôactions (tirs, points, objets) arrivent en continu, et plusieurs √©quipes d‚Äôarbitres doivent les analyser en m√™me temps.**\n\nConcept : il faut un ‚Äúflux‚Äù d‚Äô√©v√©nements en direct, et plusieurs groupes peuvent lire le m√™me flux chacun de son c√¥t√©.\nKinesis Data Streams (D) = le fil d‚Äôactions en temps r√©el : tu peux brancher plusieurs √©quipes d‚Äôarbitres dessus, et chacune lit et traite les actions en parall√®le.\nC‚Äôest cost‚Äëeffective car tu partages le m√™me flux au lieu de copier chaque action pour chaque √©quipe.\nSNS+SQS (A/B) ressemble √† faire une photocopie de chaque action pour chaque √©quipe : √ßa marche, mais √ßa multiplie les copies et donc le co√ªt.\nSQS FIFO (B) impose un ordre strict, utile parfois, mais √ßa limite le d√©bit et co√ªte plus, pas id√©al pour ‚Äúmillions d‚Äô√©v√©nements‚Äù.\nKinesis Firehose (C) sert surtout √† ‚Äúd√©verser‚Äù les donn√©es vers un stockage/outil d‚Äôanalyse, pas √† faire du traitement multi‚Äëconsommateurs en direct.\nDonc D est le meilleur choix pour du temps r√©el + plusieurs consommateurs + co√ªt optimis√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:189:ca3d423ac34f2b91",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 189,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an ecommerce application. To track product reviews, the company‚Äôs development team uses an Amazon DynamoDB table.Every record includes the following:‚Ä¢ A Review ID, a 16-digit universally unique identifier (UUID)‚Ä¢ A Product ID and User ID, 16-digit UUIDs that reference other tables‚Ä¢ A Product Rating on a scale of 1-5‚Ä¢ An optional comment from the userThe table partition key is the Review ID. The most performed query against the table is to find the 10 reviews with the highest rating for a given product.Which index will provide the FASTEST response for this query?",
      "choices": {
        "A": "A global secondary index (GSI) with Product ID as the partition key and Product Rating as the sort key",
        "B": "A global secondary index (GSI) with Product ID as the partition key and Review ID as the sort key",
        "C": "A local secondary index (LSI) with Product ID as the partition key and Product Rating as the sort key",
        "D": "A local secondary index (LSI) with Review ID as the partition key and Product ID as the sort key"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122611-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:47 a.m.",
      "textHash": "ca3d423ac34f2b91",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL : pour lire vite, il faut une cl√© de partition qui regroupe les donn√©es √† chercher, et une cl√© de tri pour les ordonner.\nIci, la requ√™te la plus fr√©quente est : ¬´ pour un Product ID donn√©, trouver les 10 avis avec la meilleure note ¬ª.\nLa table principale a comme cl√© de partition Review ID, donc elle ne regroupe pas les avis par produit : chercher par Product ID serait lent (scan ou acc√®s indirect).\nUn index secondaire sert √† cr√©er une autre ‚Äúvue‚Äù optimis√©e pour une autre requ√™te.\nUn GSI (Global Secondary Index) peut avoir une cl√© de partition diff√©rente de la table : on peut donc mettre Product ID en partition key.\nEn mettant Product Rating en sort key, DynamoDB peut trier les avis d‚Äôun produit par note et retourner directement les 10 premiers (avec Limit), tr√®s rapidement.\nLes autres choix ne permettent pas de trier par note pour un produit (B), ou sont impossibles/inefficaces car un LSI doit garder la m√™me partition key que la table (C et D).",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéí Imagine ton casier au lyc√©e** : tu ranges tes affaires diff√©remment selon comment tu les cherches.\n\n**LSI (Local)** = C'est comme **des onglets DANS ton classeur** üìÅ - Cr√©√©s √† la rentr√©e, jamais modifiables, super rapides (m√™me partition).\n\n**GSI (Global)** = C'est comme **un index √† la fin du livre** üìñ - Ajoutable n'importe quand, cherche partout, mais plus lent.\n\n**üß† Mn√©motechnique :** \"**L**SI = **L**ocal, **L**imit√©\" | \"**G**SI = **G**lobal, **G**√©nial\"\n\n**Quand utiliser GSI :** Quand tu veux chercher par un autre crit√®re que la cl√© principale, sur TOUTE la table.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:187:07e10e785472f702",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 187,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA development team maintains a web application by using a single AWS RDS, template. The template defines web servers and an Amazon RDS database. The team uses the CloudFormation template to deploy the CloudFormation stack to different environments.During a recent application deployment, a developer caused the primary development database to be dropped and recreated. The result of this incident was a loss of data. The team needs to avoid accidental database deletion in the future.Which solutions will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Add a CloudFormation DeletionPolicy attribute with the Retain value to the database resource.",
        "B": "Update the CloudFormation stack policy to prevent updates to the database.",
        "C": "Modify the database to use a Multi-AZ deployment.",
        "D": "Create a CloudFormation stack set for the web application and database deployments.",
        "E": "Add a CloudFormation DeletionPolicy attribute with the Retain value to the stack."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122609-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:46 a.m.",
      "textHash": "07e10e785472f702",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, le probl√®me vient de CloudFormation (outil AWS qui cr√©e/modifie des ressources √† partir d‚Äôun ‚Äútemplate‚Äù). Lors d‚Äôun d√©ploiement, la base RDS (service de base de donn√©es g√©r√©) a √©t√© supprim√©e puis recr√©√©e, donc les donn√©es ont disparu.\nLa solution la plus directe est d‚Äôemp√™cher CloudFormation de supprimer la base m√™me si on supprime ou remplace la ressource dans le template.\nAvec DeletionPolicy: Retain sur la ressource RDS, CloudFormation ‚Äúoublie‚Äù la base au moment de la suppression/remplacement, mais la base et ses donn√©es restent.\nC‚Äôest exactement ce qu‚Äôon veut pour √©viter une suppression accidentelle li√©e √† un changement de stack.\nUne stack policy (B) peut limiter certains updates, mais ne garantit pas la protection contre une suppression/remplacement selon les op√©rations; l‚Äôoption attendue pour la suppression est DeletionPolicy.\nMulti-AZ (C) am√©liore la disponibilit√©, pas la protection contre une suppression logique. StackSet (D) sert au d√©ploiement multi-comptes/r√©gions, pas √† la protection. ‚ÄúRetain sur la stack‚Äù (E) n‚Äôexiste pas: DeletionPolicy s‚Äôapplique aux ressources, pas √† la stack enti√®re.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un casier au lyc√©e o√π tu stockes tous tes cours importants. Le ‚Äútemplate‚Äù CloudFormation, c‚Äôest comme une fiche d‚Äôinstallation qui dit : ‚Äúmets un casier (base de donn√©es) et des tables (serveurs web)‚Äù.**\n\nLe probl√®me : un √©l√®ve a refait l‚Äôinstallation et, sans faire expr√®s, a vid√© le casier et l‚Äôa remplac√© par un casier neuf. Tous les cours ont disparu.\nLa solution, c‚Äôest de coller une √©tiquette sur le casier : ‚ÄúNE JAMAIS JETER‚Äù.\nDans AWS, cette √©tiquette s‚Äôappelle DeletionPolicy: Retain sur la ressource base de donn√©es.\n√áa veut dire : m√™me si on supprime ou recr√©e le ‚Äúpack‚Äù (le stack), la base de donn√©es n‚Äôest pas effac√©e.\nDonc les donn√©es restent, m√™me en cas de mauvaise manip.\nB bloque les modifications, mais ne garantit pas le ‚Äúne pas jeter‚Äù lors d‚Äôune suppression/recr√©ation.\nC, D et E ne prot√®gent pas directement la base contre l‚Äôeffacement.\nDonc A est la bonne r√©ponse.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:184:24264a901d650f5b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 184,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is using AWS CloudFormation to deploy a two-tier application. The application will use Amazon RDS as its backend database. The company wants a solution that will randomly generate the database password during deployment. The solution also must automatically rotate the database password without requiring changes to the application.What is the MOST operationally efficient solution that meets these requirements?",
      "choices": {
        "A": "Use an AWS Lambda function as a CloudFormation custom resource to generate and rotate the password.",
        "B": "Use an AWS Systems Manager Parameter Store resource with the SecureString data type to generate and rotate the password.",
        "C": "Use a cron daemon on the application‚Äôs host to generate and rotate the password.",
        "D": "Use an AWS Secrets Manager resource to generate and rotate the password."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122605-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:41 a.m.",
      "textHash": "24264a901d650f5b",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Ici, il faut 1) g√©n√©rer un mot de passe al√©atoire au d√©ploiement CloudFormation et 2) le faire tourner automatiquement, sans modifier l‚Äôapplication.\nAWS Secrets Manager est un service fait pour stocker des ‚Äúsecrets‚Äù (mots de passe, cl√©s) et peut g√©n√©rer un mot de passe automatiquement lors de la cr√©ation.\nIl g√®re aussi la rotation automatique via une rotation planifi√©e (souvent avec une fonction Lambda fournie/mod√®le) et met √† jour le secret.\nL‚Äôapplication n‚Äôa pas besoin de changer si elle lit toujours le mot de passe depuis Secrets Manager (ou via un m√©canisme standard comme un driver/SDK qui r√©cup√®re le secret).\nPour Amazon RDS, Secrets Manager s‚Äôint√®gre bien : il peut mettre √† jour le mot de passe dans la base et dans le secret de fa√ßon coordonn√©e.\nParameter Store SecureString stocke de fa√ßon chiffr√©e, mais la rotation automatique n‚Äôest pas aussi ‚Äúcl√© en main‚Äù que Secrets Manager.\nUn cron sur l‚Äôh√¥te ou une custom resource Lambda ‚Äúmaison‚Äù demande plus d‚Äôop√©rations, de maintenance et de risques d‚Äôerreur.\nDonc la solution la plus efficace op√©rationnellement est d‚Äôutiliser AWS Secrets Manager pour g√©n√©rer et faire tourner le mot de passe.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine le mot de passe de la salle des profs au lyc√©e : il doit √™tre secret, chang√© r√©guli√®rement, et les profs doivent pouvoir entrer sans qu‚Äôon leur redonne le code √† la main √† chaque fois.**\n\nConcept : un ‚Äúgestionnaire de secrets‚Äù, c‚Äôest comme un coffre-fort du lyc√©e qui cr√©e un code au hasard, le garde, et le change automatiquement selon un planning.\nIci, on veut 1) g√©n√©rer un mot de passe au hasard pendant l‚Äôinstallation, et 2) le changer r√©guli√®rement sans casser l‚Äôappli.\nAWS Secrets Manager (D) joue le r√¥le du coffre-fort : il g√©n√®re le mot de passe, le stocke, et le ‚Äúfait tourner‚Äù (rotation) tout seul.\nL‚Äôapplication, au lieu de m√©moriser le code, va toujours le demander au coffre-fort : donc quand le code change, elle continue de fonctionner.\nA (Lambda) et C (cron) = bricolage maison : plus de maintenance, plus de risques d‚Äôoubli.\nB (Parameter Store) peut stocker un secret, mais la rotation automatique n‚Äôest pas aussi ‚Äúcl√© en main‚Äù que Secrets Manager.\nDonc D est le plus simple √† g√©rer et r√©pond exactement aux deux exigences.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:181:35ba2e97979d3765",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 181,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is troubleshooting an Amazon API Gateway API. Clients are receiving HTTP 400 response errors when the clients try to access an endpoint of the API.How can the developer determine the cause of these errors?",
      "choices": {
        "A": "Create an Amazon Kinesis Data Firehose delivery stream to receive API call logs from API Gateway. Configure Amazon CloudWatch Logs as the delivery stream‚Äôs destination.",
        "B": "Turn on AWS CloudTrail Insights and create a trail. Specify the Amazon Resource Name (ARN) of the trail for the stage of the API.",
        "C": "Turn on AWS X-Ray for the API stage. Create an Amazon CloudWatch Logs log group. Specify the Amazon Resource Name (ARN) of the log group for the API stage.",
        "D": "Turn on execution logging and access logging in Amazon CloudWatch Logs for the API stage. Create a CloudWatch Logs log group. Specify the Amazon Resource Name (ARN) of the log group for the API stage."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122602-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:37 a.m.",
      "textHash": "35ba2e97979d3765",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Pour comprendre des erreurs HTTP 400 (souvent ‚Äúrequ√™te invalide‚Äù), il faut voir ce que re√ßoit et fait r√©ellement Amazon API Gateway (service qui expose des API HTTP).\nLa meilleure m√©thode est d‚Äôactiver les logs de l‚Äô√©tape (stage) de l‚ÄôAPI dans Amazon CloudWatch Logs (service de journaux).\nLes ‚Äúaccess logs‚Äù montrent qui appelle l‚ÄôAPI, quelle URL, quels param√®tres, quel code retour (400), et des infos utiles comme l‚ÄôIP et l‚Äôagent client.\nLes ‚Äúexecution logs‚Äù montrent le d√©tail c√¥t√© API Gateway : validation des param√®tres, mapping des requ√™tes/r√©ponses, erreurs de transformation, autorisation, etc.\nEn cr√©ant un log group CloudWatch et en indiquant son ARN dans le stage, API Gateway y √©crit automatiquement ces journaux.\nAvec ces logs, on peut rep√©rer si le client envoie un mauvais JSON, oublie un param√®tre requis, a un mauvais header, ou si un mapping template casse.\nKinesis Firehose (A) est pour livrer des flux de donn√©es, pas n√©cessaire pour diagnostiquer rapidement des 400.\nCloudTrail/Insights (B) trace surtout les appels API AWS de gestion, pas le contenu des requ√™tes des clients.\nX-Ray (C) aide au tra√ßage de latence et des appels entre services, mais pour des 400 API Gateway, les logs d‚Äôex√©cution et d‚Äôacc√®s sont l‚Äôoutil principal.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine le bureau de la vie scolaire au lyc√©e : des √©l√®ves viennent demander un papier. Certains repartent avec ‚Äúrefus√©‚Äù parce qu‚Äôil manque une info sur le formulaire.**\n\nAPI Gateway, c‚Äôest comme ce guichet : il re√ßoit des demandes (requ√™tes) et renvoie une r√©ponse. Un HTTP 400, c‚Äôest ‚Äúta demande est mal form√©e‚Äù (mauvais champ, info manquante). Pour savoir pourquoi, il faut des ‚Äúcarnets de notes‚Äù qui enregistrent ce qui est arriv√©. La r√©ponse D dit d‚Äôactiver deux journaux dans CloudWatch Logs : access logging (qui est venu, quel chemin, quel code) et execution logging (ce que le guichet a fait √©tape par √©tape et o√π √ßa a bloqu√©). En cr√©ant un log group et en le reliant au stage, tu r√©cup√®res les d√©tails du refus, comme la vie scolaire qui note ‚Äúphoto manquante‚Äù ou ‚Äúmauvaise classe‚Äù. Les autres choix ne sont pas faits pour expliquer pr√©cis√©ment des erreurs 400 d‚Äôun endpoint.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:179:fb2607995174ad81",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 179,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on a Python application that runs on Amazon EC2 instances. The developer wants to enable tracing of application requests to debug performance issues in the code.Which combination of actions should the developer take to achieve this goal? (Choose two.)",
      "choices": {
        "A": "Install the Amazon CloudWatch agent on the EC2 instances.",
        "B": "Install the AWS X-Ray daemon on the EC2 instances.",
        "C": "Configure the application to write JSON-formatted logs to /var/log/cloudwatch.",
        "D": "Configure the application to write trace data to /var/log/xray.",
        "E": "Install and configure the AWS X-Ray SDK for Python in the application."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122600-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:35 a.m.",
      "textHash": "fb2607995174ad81",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Pour ¬´ tracer ¬ª des requ√™tes (suivre un appel de bout en bout et voir o√π le temps est pass√©), AWS utilise surtout AWS X-Ray.\nSur une instance EC2, X-Ray a besoin d‚Äôun composant local appel√© le ¬´ daemon ¬ª X-Ray : il re√ßoit les segments de trace envoy√©s par l‚Äôapplication et les transmet au service X-Ray dans AWS.\nDonc il faut installer le daemon X-Ray sur les instances EC2 (r√©ponse B).\nMais le daemon seul ne suffit pas : l‚Äôapplication doit aussi g√©n√©rer des donn√©es de trace.\nPour cela, on installe et configure le SDK AWS X-Ray pour Python dans le code : il instrumente les requ√™tes (HTTP, appels AWS, fonctions) et cr√©e les segments (r√©ponse E).\nCloudWatch et ses logs (A, C) servent surtout √† collecter des m√©triques et des journaux, pas √† faire du tra√ßage distribu√©.\n√âcrire des traces dans un fichier local (D) n‚Äôest pas la m√©thode standard : l‚Äôapp envoie au daemon, puis le daemon envoie √† X-Ray.\nLa combinaison logique pour d√©boguer les performances est donc : daemon X-Ray + SDK X-Ray.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un lyc√©e o√π tu veux comprendre pourquoi les √©l√®ves mettent trop de temps √† aller de la salle de classe √† la cantine.**\n\nLe ‚Äútracing‚Äù, c‚Äôest comme suivre le trajet exact de chaque √©l√®ve, √©tape par √©tape, pour voir o√π √ßa bloque.\nSur une machine EC2 (un ‚Äúordi lou√©‚Äù), il faut un ‚Äúsurveillant‚Äù sp√©cial qui collecte ces trajets et les envoie au tableau de suivi.\nCe surveillant, c‚Äôest le AWS X-Ray daemon : il tourne sur l‚ÄôEC2 et r√©cup√®re les infos de parcours des demandes.\nDonc l‚Äôaction correcte est B : installer le daemon X-Ray sur les EC2.\nCloudWatch (A) c‚Äôest plut√¥t le carnet de notes des √©v√©nements (logs/mesures), pas le suivi d√©taill√© du trajet.\n√âcrire des fichiers dans /var/log (C, D) c‚Äôest juste poser des papiers dans un casier, √ßa ne cr√©e pas le suivi ‚Äútrajet‚Äù tout seul.\nLe SDK Python (E) aide l‚Äôappli √† ‚Äúmarquer les √©tapes‚Äù, mais sans le surveillant (daemon) sur l‚ÄôEC2, personne ne collecte correctement.\nIci, la r√©ponse attendue est donc B.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:178:071a0adbc600ec7c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 178,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building an application for stock trading. The application needs sub-millisecond latency for processing trade requests. The company uses Amazon DynamoDB to store all the trading data that is used to process each trading request.A development team performs load testing on the application and finds that the data retrieval time is higher than expected. The development team needs a solution that reduces the data retrieval time with the least possible effort.Which solution meets these requirements?",
      "choices": {
        "A": "Add local secondary indexes (LSIs) for the trading data.",
        "B": "Store the trading data in Amazon S3, and use S3 Transfer Acceleration.",
        "C": "Add retries with exponential backoff for DynamoDB queries.",
        "D": "Use DynamoDB Accelerator (DAX) to cache the trading data."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122598-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:33 a.m.",
      "textHash": "071a0adbc600ec7c",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:dynamodb:e3eb9788",
      "frExplanation": "Ici, le probl√®me est la latence de lecture dans DynamoDB (base NoSQL g√©r√©e). Pour du trading, on veut des r√©ponses en moins d‚Äôune milliseconde.\nDynamoDB peut √™tre tr√®s rapide, mais sous forte charge certaines lectures restent plus lentes que souhait√©.\nDAX (DynamoDB Accelerator) est un cache en m√©moire g√©r√© par AWS, plac√© ‚Äúdevant‚Äù DynamoDB : l‚Äôapplication lit d‚Äôabord dans DAX, et si la donn√©e est d√©j√† en cache, la r√©ponse est quasi instantan√©e.\nC‚Äôest aussi ‚Äúle moins d‚Äôeffort‚Äù car on change peu le code (souvent juste le client/endpoint) et on ne refait pas le mod√®le de donn√©es.\nA (LSI) aide surtout pour de nouveaux types de requ√™tes/tri, pas pour acc√©l√©rer toutes les lectures existantes, et demande de la conception.\nB (S3 + Transfer Acceleration) concerne le transfert de fichiers via Internet, pas des lectures cl√©/valeur pour du trading.\nC (retries) sert √† g√©rer des erreurs/throttling, mais ajoute du d√©lai et n‚Äôam√©liore pas la latence normale.\nDonc la meilleure solution pour r√©duire rapidement le temps de r√©cup√©ration est DAX.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o en ligne o√π tu dois acheter/vendre des objets ultra vite. La base de donn√©es (DynamoDB) est comme l‚Äôentrep√¥t central du jeu: fiable, mais parfois un peu loin, donc aller chercher un objet prend trop de temps.**\n\nConcept: pour √™tre super rapide, tu mets les objets les plus demand√©s dans une ‚Äúcache‚Äù, comme un petit coffre juste √† c√¥t√© de toi, au lieu de courir √† l‚Äôentrep√¥t √† chaque fois.\nIci, DAX c‚Äôest ce coffre: il garde en m√©moire les donn√©es de trading souvent lues, donc la plupart des demandes sont servies presque instantan√©ment (moins d‚Äô1 milliseconde).\nPourquoi D est le meilleur: c‚Äôest le moins d‚Äôeffort, tu ajoutes DAX devant DynamoDB et tu acc√©l√®res surtout la lecture.\nA (LSI) c‚Äôest r√©organiser les rayons de l‚Äôentrep√¥t: √ßa aide certaines recherches, mais c‚Äôest plus de boulot et pas une ‚Äúturbo‚Äù imm√©diate.\nB (S3) c‚Äôest mettre les objets dans un grand garage de stockage: pas fait pour r√©pondre ultra vite √† des requ√™tes de trading.\nC (retries) c‚Äôest ‚Äúr√©essayer si √ßa rate‚Äù: √ßa peut m√™me rallonger le temps.\nDonc la bonne r√©ponse est D: utiliser DAX pour mettre en cache et r√©duire le temps de r√©cup√©ration.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:176:b2e61afda40d0b7e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 176,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nWhen a developer tries to run an AWS CodeBuild project, it raises an error because the length of all environment variables exceeds the limit for the combined maximum of characters.What is the recommended solution?",
      "choices": {
        "A": "Add the export LC_ALL=\"en_US.utf8\" command to the pre_build section to ensure POSIX localization.",
        "B": "Use Amazon Cognito to store key-value pairs for large numbers of environment variables.",
        "C": "Update the settings for the build project to use an Amazon S3 bucket for large numbers of environment variables.",
        "D": "Use AWS Systems Manager Parameter Store to store large numbers of environment variables."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122596-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:29 a.m.",
      "textHash": "b2e61afda40d0b7e",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "AWS CodeBuild est un service qui ex√©cute des builds/tests dans un conteneur. On peut lui passer des variables d‚Äôenvironnement (ex: API_KEY, URL) pour configurer le build.\nIci, l‚Äôerreur vient du fait que la somme des caract√®res de toutes les variables d√©passe la limite autoris√©e par CodeBuild.\nLa bonne pratique est donc de ne pas ‚Äúgonfler‚Äù les variables directement dans le projet, mais de stocker ces valeurs ailleurs et de les r√©cup√©rer au moment du build.\nAWS Systems Manager Parameter Store est un coffre de param√®tres (paires cl√©/valeur) g√©r√© par AWS, fait pour stocker des configurations et des secrets.\nCodeBuild peut lire ces param√®tres (y compris chiffr√©s) et les injecter comme variables au runtime, sans d√©passer la limite de taille des variables d√©finies dans le projet.\nA ne r√®gle pas un probl√®me de taille (c‚Äôest une option de locale). B (Cognito) sert √† g√©rer des utilisateurs, pas des variables de build. C (S3) peut stocker des fichiers, mais ce n‚Äôest pas la solution recommand√©e pour des variables/param√®tres.\nDonc la solution recommand√©e est d‚Äôutiliser Parameter Store pour stocker et r√©f√©rencer un grand nombre de variables.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un √©l√®ve qui doit passer un contr√¥le avec une seule petite fiche m√©mo autoris√©e. Il a tellement de notes (mots de passe, codes, infos) que tout ne tient pas sur la fiche, donc le prof refuse.**\n\nDans CodeBuild, les ‚Äúvariables d‚Äôenvironnement‚Äù sont comme les infos √©crites sur la fiche m√©mo avant de lancer le devoir. Il y a une limite de taille totale, donc si tu √©cris trop, √ßa plante. La solution, c‚Äôest de ne pas tout mettre sur la fiche : tu ranges les infos dans un casier s√©curis√© de l‚Äô√©cole, et tu ne prends que ce dont tu as besoin au moment du contr√¥le. AWS Systems Manager Parameter Store, c‚Äôest ce ‚Äúcasier‚Äù : un endroit fait pour stocker beaucoup de valeurs (cl√©s, secrets, r√©glages) et les r√©cup√©rer pendant le build. Donc D est recommand√©. A ne r√©duit pas la taille, B (Cognito) sert plut√¥t √† g√©rer des comptes/utilisateurs, et C (S3) est un stockage de fichiers, pas l‚Äôendroit pr√©vu pour des variables.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:174:5f2d306fe96732a8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 174,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer accesses AWS CodeCommit over SSH. The SSH keys configured to access AWS CodeCommit are tied to a user with the following permissions:The developer needs to create/delete branches.Which specific IAM permissions need to be added, based on the principle of least privilege?",
      "choices": {
        "A": "\"codecommit:CreateBranch\"\"codecommit:DeleteBranch\"",
        "B": "\"codecommit:Put*\"",
        "C": "\"codecommit:Update*\"",
        "D": "\"codecommit:*\""
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122594-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:27 a.m.",
      "textHash": "5f2d306fe96732a8",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "AWS CodeCommit est un service AWS qui h√©berge des d√©p√¥ts Git (comme GitHub), avec des acc√®s contr√¥l√©s par IAM (gestion des identit√©s et permissions).\nL‚Äôacc√®s en SSH utilise une paire de cl√©s li√©e √† un utilisateur IAM : ce n‚Äôest pas la cl√© qui donne les droits, ce sont les permissions IAM.\nLe besoin est pr√©cis : cr√©er et supprimer des branches Git dans un d√©p√¥t.\nAvec le principe du moindre privil√®ge, on n‚Äôajoute que les actions strictement n√©cessaires, pas plus.\nLes actions IAM correspondantes sont exactement : codecommit:CreateBranch et codecommit:DeleteBranch.\nLes options \"Put*\" ou \"Update*\" donnent un ensemble large d‚Äôactions (trop permissif) et peuvent autoriser d‚Äôautres modifications non demand√©es.\n\"codecommit:*\" donne tous les droits sur CodeCommit, ce qui viole clairement le moindre privil√®ge.\nDonc la bonne r√©ponse est A, car elle couvre uniquement la cr√©ation/suppression de branches.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que au coll√®ge o√π tu as une carte d‚Äôacc√®s. Les ‚Äúbranches‚Äù sont comme des √©tag√®res temporaires o√π tu pr√©pares un expos√© sans toucher aux livres officiels.**\n\nConcept : en cloud, on donne des droits comme des autorisations sur une carte. ‚ÄúMoins de privil√®ges‚Äù = tu n‚Äôas que les droits n√©cessaires, pas plus. Ici, le d√©veloppeur veut juste cr√©er et supprimer des ‚Äú√©tag√®res‚Äù (branches). Donc on ajoute seulement les deux autorisations pr√©cises : CreateBranch et DeleteBranch (r√©ponse A). B (Put*) c‚Äôest comme te donner le droit de modifier plein de choses ‚Äúau hasard‚Äù dans la biblioth√®que. C (Update*) pareil, trop large. D (*) c‚Äôest les cl√©s du b√¢timent entier, beaucoup trop dangereux. Avec A, tu peux faire exactement ce qu‚Äôil faut, et rien d‚Äôautre.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:173:e1b83300584f2dcf",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 173,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is modifying an existing AWS Lambda function. While checking the code, the developer notices hardcoded parameter values for an Amazon RDS for SQL Server user name, password, database, host, and port. There are also hardcoded parameter values for an Amazon DynamoDB table, an Amazon S3 bucket, and an Amazon Simple Notification Service (Amazon SNS) topic.The developer wants to securely store the parameter values outside the code in an encrypted format and wants to turn on rotation for the credentials. The developer also wants to be able to reuse the parameter values from other applications and to update the parameter values without modifying code.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Create an RDS database secret in AWS Secrets Manager. Set the user name, password, database, host, and port. Turn on secret rotation. Create encrypted Lambda environment variables for the DynamoDB table, S3 bucket, and SNS topic.",
        "B": "Create an RDS database secret in AWS Secrets Manager. Set the user name, password, database, host, and port. Turn on secret rotation. Create SecureString parameters in AWS Systems Manager Parameter Store for the DynamoDB table, S3 bucket, and SNS topic.",
        "C": "Create RDS database parameters in AWS Systems Manager Parameter Store for the user name, password, database, host, and port. Create encrypted Lambda environment variables for the DynamoDB table, S3 bucket, and SNS topic. Create a Lambda function and set the logic for the credentials rotation task. Schedule the credentials rotation task in Amazon EventBridge.",
        "D": "Create RDS database parameters in AWS Systems Manager Parameter Store for the user name, password, database, host, and port. Store the DynamoDB table, S3 bucket, and SNS topic in Amazon S3. Create a Lambda function and set the logic for the credentials rotation. Invoke the Lambda function on a schedule."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122592-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:25 a.m.",
      "textHash": "e1b83300584f2dcf",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Le probl√®me vient des valeurs ¬´ en dur ¬ª dans le code (mot de passe, noms de ressources). Si elles changent, il faut red√©ployer, et c‚Äôest risqu√©.\nAWS Secrets Manager est fait pour stocker des identifiants (ex: RDS SQL Server) de fa√ßon chiffr√©e et surtout activer la rotation automatique (changement r√©gulier du mot de passe) avec peu d‚Äôeffort.\nAWS Systems Manager Parameter Store permet de stocker des param√®tres r√©utilisables par plusieurs applis, hors du code.\nEn mode SecureString, les param√®tres sont chiffr√©s (via KMS) et peuvent √™tre lus par Lambda au d√©marrage ou √† l‚Äôex√©cution.\nMettre DynamoDB/S3/SNS en variables d‚Äôenvironnement Lambda (A) les rend moins r√©utilisables et plus li√©es √† une seule fonction.\nLes options C et D demandent de coder et maintenir soi-m√™me la rotation (Lambda + planification), donc plus d‚Äôoverhead.\nDonc la meilleure solution avec le moins d‚Äôop√©rations est : Secrets Manager pour les identifiants RDS + Parameter Store SecureString pour les autres valeurs (B).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:172:39bd0a85bd5ea330",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 172,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA mobile app stores blog posts in an Amazon DynamoDB table. Millions of posts are added every day, and each post represents a single item in the table. The mobile app requires only recent posts. Any post that is older than 48 hours can be removed.What is the MOST cost-effective way to delete posts that are older than 48 hours?",
      "choices": {
        "A": "For each item, add a new attribute of type String that has a timestamp that is set to the blog post creation time. Create a script to find old posts with a table scan and remove posts that are older than 48 hours by using the BatchWriteItem API operation. Schedule a cron job on an Amazon EC2 instance once an hour to start the script.",
        "B": "For each item, add a new attribute of type String that has a timestamp that is set to the blog post creation time. Create a script to find old posts with a table scan and remove posts that are older than 48 hours by using the BatchWriteItem API operation. Place the script in a container image. Schedule an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate that invokes the container every 5 minutes.",
        "C": "For each item, add a new attribute of type Date that has a timestamp that is set to 48 hours after the blog post creation time. Create a global secondary index (GSI) that uses the new attribute as a sort key. Create an AWS Lambda function that references the GSI and removes expired items by using the BatchWriteItem API operation. Schedule the function with an Amazon CloudWatch event every minute.",
        "D": "For each item, add a new attribute of type Number that has a timestamp that is set to 48 hours after the blog post creation time. Configure the DynamoDB table with a TTL that references the new attribute."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122593-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:26 a.m.",
      "textHash": "39bd0a85bd5ea330",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Le besoin est de supprimer automatiquement des √©l√©ments (posts) apr√®s 48 h, avec des millions d‚Äôinsertions par jour : il faut √©viter tout ‚Äúscan‚Äù de table, car lire toute la table co√ªte cher et ne passe pas √† l‚Äô√©chelle.\nDynamoDB est une base NoSQL g√©r√©e. Elle propose une fonction native appel√©e TTL (Time To Live) qui supprime automatiquement les items expir√©s.\nLa bonne pratique est d‚Äôajouter √† chaque item un attribut num√©rique (Number) contenant un timestamp Unix (en secondes) correspondant √† ‚Äúdate de cr√©ation + 48 h‚Äù.\nEnsuite, on active TTL sur la table en pointant vers cet attribut : DynamoDB se charge de supprimer les items quand ils expirent, sans serveur √† g√©rer.\nC‚Äôest le plus √©conomique car il n‚Äôy a ni EC2, ni conteneur, ni Lambda planifi√©e, ni index suppl√©mentaire, ni scans r√©guliers.\nLes options A/B/C impliquent des scans/traitements p√©riodiques (co√ªts de lecture + compute + orchestration) et sont moins fiables/plus complexes.\nDonc D est correct : TTL + attribut Number d‚Äôexpiration.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le tableau d‚Äôaffichage du lyc√©e o√π on colle des annonces. Chaque annonce a une date de ‚Äúfin d‚Äôaffichage‚Äù √©crite dessus. Quand la date est d√©pass√©e, le surveillant enl√®ve l‚Äôannonce automatiquement, sans que tu aies √† repasser v√©rifier tout le tableau.**\n\nIci, la table DynamoDB = le tableau d‚Äôaffichage, et chaque post = une annonce. On ne veut garder que les posts de moins de 48h. La solution la moins ch√®re, c‚Äôest d‚Äô√©crire sur chaque post une ‚Äúdate d‚Äôexpiration‚Äù (48h apr√®s sa cr√©ation) sous forme de nombre (un timestamp). Puis on active TTL (Time To Live) : c‚Äôest le ‚Äúsurveillant automatique‚Äù de DynamoDB. DynamoDB supprime tout seul les posts expir√©s, sans script, sans machine allum√©e, sans scanner toute la table. Les autres choix, c‚Äôest comme demander √† quelqu‚Äôun de relire tout le tableau r√©guli√®rement pour trouver les vieilles annonces : √ßa prend du temps et co√ªte plus cher. Donc D est la meilleure r√©ponse.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:171:1f25188ec5c7b20d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 171,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has an application that is composed of many different AWS Lambda functions. The Lambda functions all use some of the same dependencies. To avoid security issues, the developer is constantly updating the dependencies of all of the Lambda functions. The result is duplicated effort for each function.How can the developer keep the dependencies of the Lambda functions up to date with the LEAST additional complexity?",
      "choices": {
        "A": "Define a maintenance window for the Lambda functions to ensure that the functions get updated copies of the dependencies.",
        "B": "Upgrade the Lambda functions to the most recent runtime version.",
        "C": "Define a Lambda layer that contains all of the shared dependencies.",
        "D": "Use an AWS CodeCommit repository to host the dependencies in a centralized location."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122591-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:21 a.m.",
      "textHash": "1f25188ec5c7b20d",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:f06eefa5",
      "frExplanation": "Ici, plusieurs fonctions AWS Lambda (du code qui s‚Äôex√©cute sans serveur) partagent les m√™mes biblioth√®ques/d√©pendances. Les mettre √† jour dans chaque fonction s√©par√©ment cr√©e du travail en double et augmente le risque d‚Äôoublis.\nUne ¬´ Lambda Layer ¬ª (couche Lambda) est un paquet s√©par√© qui contient des d√©pendances communes (ex: librairies Python/Node) et que plusieurs fonctions peuvent r√©utiliser.\nEn pla√ßant les d√©pendances partag√©es dans une layer, vous les mettez √† jour une seule fois, puis vous attachez la nouvelle version de la layer aux fonctions.\nCela r√©duit la complexit√©: pas besoin de recopier les fichiers dans chaque d√©ploiement de fonction.\nA ne r√©sout pas le probl√®me (une fen√™tre de maintenance n‚Äôautomatise pas le partage des d√©pendances).\nB ne met pas √† jour vos biblioth√®ques applicatives, seulement la version du runtime.\nD centralise le code source, mais chaque fonction devrait quand m√™me embarquer/installer les d√©pendances; ce n‚Äôest pas le moyen le plus simple c√¥t√© Lambda.\nDonc la meilleure option avec le moins de complexit√© est d‚Äôutiliser une Lambda Layer partag√©e.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:169:fa5d2e455dae9d68",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 169,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is using Amazon RDS as the backend database for its application. After a recent marketing campaign, a surge of read requests to the database increased the latency of data retrieval from the database. The company has decided to implement a caching layer in front of the database. The cached content must be encrypted and must be highly available.Which solution will meet these requirements?",
      "choices": {
        "A": "Amazon CloudFront",
        "B": "Amazon ElastiCache for Memcached",
        "C": "Amazon ElastiCache for Redis in cluster mode",
        "D": "Amazon DynamoDB Accelerator (DAX)"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122589-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:18 a.m.",
      "textHash": "fa5d2e455dae9d68",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:63bfc545",
      "frExplanation": "Le probl√®me vient d‚ÄôAmazon RDS (base de donn√©es relationnelle) qui re√ßoit trop de lectures, ce qui augmente la latence. Une solution classique est d‚Äôajouter un cache devant la base : on garde en m√©moire les r√©sultats souvent demand√©s pour √©viter de relire RDS √† chaque fois.\nAmazon ElastiCache est le service AWS de cache en m√©moire. Deux moteurs existent : Memcached et Redis.\nIci, le cache doit √™tre chiffr√© et hautement disponible. Redis dans ElastiCache supporte le chiffrement (au repos et en transit) et des fonctions de haute disponibilit√©.\nLe ‚Äúcluster mode‚Äù de Redis permet de r√©partir les donn√©es sur plusieurs n≈ìuds (sharding) et d‚Äôajouter de la redondance, ce qui am√©liore la disponibilit√© et la capacit√© lors des pics.\nMemcached ne r√©pond pas aussi bien aux exigences de chiffrement/HA (pas de r√©plication native et options de s√©curit√© plus limit√©es selon les cas).\nCloudFront est un CDN pour du contenu web (fichiers, pages) et ne sert pas de cache de requ√™tes de base de donn√©es.\nDAX est un cache uniquement pour DynamoDB, pas pour RDS.\nDonc la meilleure r√©ponse est ElastiCache for Redis en cluster mode (C).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : la cuisine (la base de donn√©es) pr√©pare les plats, et le comptoir (le cache) garde des portions d√©j√† pr√™tes pour servir vite quand tout le monde arrive d‚Äôun coup.**\n\nConcept : un cache, c‚Äôest un ‚Äúcomptoir‚Äù devant la cuisine. Au lieu de refaire le plat √† chaque √©l√®ve, on sert une portion d√©j√† pr√™te, donc c‚Äôest beaucoup plus rapide.\nIci, apr√®s la pub, il y a surtout des demandes de lecture (des √©l√®ves qui veulent le m√™me plat). Le cache r√©duit l‚Äôattente.\nOn veut aussi que le contenu du comptoir soit chiffr√© (comme des bo√Ætes ferm√©es √† cl√©) : Redis peut chiffrer les donn√©es.\nOn veut qu‚Äôil soit tr√®s disponible : si un comptoir ferme, un autre prend le relais. Le ‚Äúcluster mode‚Äù de Redis, c‚Äôest plusieurs comptoirs qui se partagent le travail et se remplacent.\nPourquoi pas B (Memcached) : c‚Äôest un comptoir rapide mais avec moins d‚Äôoptions de ‚Äúbo√Ætes √† cl√©‚Äù et de haute dispo solide.\nPourquoi pas A (CloudFront) : c‚Äôest plut√¥t pour distribuer des pages/fichiers sur Internet, pas pour acc√©l√©rer une base de donn√©es.\nPourquoi pas D (DAX) : c‚Äôest un cache sp√©cial pour une autre base (DynamoDB), pas pour RDS.\nDonc C : ElastiCache for Redis en cluster mode = cache chiffr√© + tr√®s disponible devant RDS.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:167:f05808ffac171232",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 167,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer at a company needs to create a small application that makes the same API call once each day at a designated time. The company does not have infrastructure in the AWS Cloud yet, but the company wants to implement this functionality on AWS.Which solution meets these requirements in the MOST operationally efficient manner?",
      "choices": {
        "A": "Use a Kubernetes cron job that runs on Amazon Elastic Kubernetes Service (Amazon EKS).",
        "B": "Use an Amazon Linux crontab scheduled job that runs on Amazon EC2.",
        "C": "Use an AWS Lambda function that is invoked by an Amazon EventBridge scheduled event.",
        "D": "Use an AWS Batch job that is submitted to an AWS Batch job queue."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122587-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:17 a.m.",
      "textHash": "f05808ffac171232",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Le besoin est simple : ex√©cuter automatiquement le m√™me appel API une fois par jour √† une heure pr√©cise, sans g√©rer de serveurs.\nAWS Lambda est un service ¬´ serverless ¬ª : vous d√©posez votre code, AWS l‚Äôex√©cute quand il faut, et vous ne g√©rez ni machine, ni mises √† jour, ni capacit√©.\nAmazon EventBridge peut cr√©er une r√®gle planifi√©e (comme un r√©veil/cron) qui d√©clenche Lambda √† l‚Äôheure voulue chaque jour.\nC‚Äôest donc tr√®s efficace op√©rationnellement : pas d‚Äôinfrastructure √† installer, facturation surtout √† l‚Äôex√©cution, et configuration rapide.\nLes options EKS (Kubernetes) et EC2 impliquent de g√©rer des serveurs/cluster, la s√©curit√©, les patchs et la disponibilit√© : trop lourd pour un simple job quotidien.\nAWS Batch est plut√¥t fait pour des traitements batch lourds avec files d‚Äôattente et ressources de calcul, inutilement complexe ici.\nDonc la meilleure solution est : une fonction Lambda d√©clench√©e par un √©v√©nement planifi√© EventBridge (r√©ponse C).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois envoyer chaque jour √† 18h un message automatique sur le groupe de classe, sans rester devant ton t√©l√©phone.**\n\nConcept : tu as besoin d‚Äôun ‚Äúr√©veil‚Äù qui se d√©clenche tout seul √† une heure pr√©cise, fait une action, puis s‚Äôarr√™te.\nAWS Lambda = un mini-robot qui ex√©cute juste la t√¢che (faire l‚Äôappel API) puis dispara√Æt.\nAmazon EventBridge (√©v√©nement planifi√©) = l‚Äôalarme/agenda qui dit √† 18h : ‚Äúvas-y maintenant‚Äù.\nPourquoi C : tu n‚Äôas rien √† installer ni √† garder allum√©, comme un t√©l√©phone qui envoie le message sans que tu le tiennes.\nA (Kubernetes/EKS) et B (EC2 + crontab) = comme acheter et laisser tourner un PC allum√© juste pour envoyer un message 1 fois/jour.\nD (AWS Batch) = comme utiliser une grosse usine de devoirs pour une mini-t√¢che quotidienne.\nDonc le plus simple √† g√©rer et le plus ‚Äúsans prise de t√™te‚Äù : EventBridge d√©clenche Lambda (C).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:162:e4f12b6507b2367a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 162,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has observed an increase in bugs in the AWS Lambda functions that a development team has deployed in its Node.js application. To minimize these bugs, the developer wants to implement automated testing of Lambda functions in an environment that closely simulates the Lambda environment.The developer needs to give other developers the ability to run the tests locally. The developer also needs to integrate the tests into the team‚Äôs continuous integration and continuous delivery (CI/CD) pipeline before the AWS Cloud Development Kit (AWS CDK) deployment.Which solution will meet these requirements?",
      "choices": {
        "A": "Create sample events based on the Lambda documentation. Create automated test scripts that use the cdk local invoke command to invoke the Lambda functions. Check the response. Document the test scripts for the other developers on the team. Update the CI/CD pipeline to run the test scripts.",
        "B": "Install a unit testing framework that reproduces the Lambda execution environment. Create sample events based on the Lambda documentation. Invoke the handler function by using a unit testing framework. Check the response. Document how to run the unit testing framework for the other developers on the team. Update the CI/CD pipeline to run the unit testing framework.",
        "C": "Install the AWS Serverless Application Model (AWS SAM) CLI tool. Use the sam local generate-event command to generate sample events for the automated tests. Create automated test scripts that use the sam local invoke command to invoke the Lambda functions. Check the response. Document the test scripts for the other developers on the team. Update the CI/CD pipeline to run the test scripts.",
        "D": "Create sample events based on the Lambda documentation. Create a Docker container from the Node.js base image to invoke the Lambda functions. Check the response. Document how to run the Docker container for the other developers on the team. Update the CI/CD pipeline to run the Docker container."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122582-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:10 a.m.",
      "textHash": "e4f12b6507b2367a",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:64b2cf34",
      "frExplanation": "Objectif : tester automatiquement des fonctions AWS Lambda (code Node.js) dans un environnement qui ressemble vraiment √† Lambda, et pouvoir lancer ces tests en local + dans la CI/CD avant le d√©ploiement CDK.\nAWS SAM CLI est un outil officiel qui peut √©muler Lambda sur votre machine en utilisant des conteneurs (Docker), donc le comportement est proche du vrai runtime Lambda.\nAvec \"sam local generate-event\", on cr√©e facilement des √©v√©nements r√©alistes (ex: API Gateway, S3) pour alimenter les tests.\nAvec \"sam local invoke\", on ex√©cute la fonction Lambda localement comme si elle tournait dans AWS, puis on v√©rifie la r√©ponse attendue.\nC‚Äôest simple √† partager : chaque d√©veloppeur installe SAM CLI et lance les m√™mes scripts de test.\nC‚Äôest aussi simple √† automatiser : la pipeline CI/CD peut ex√©cuter ces scripts avant \"cdk deploy\" pour bloquer un d√©ploiement si un test √©choue.\nPourquoi pas B : un framework de tests unitaires teste surtout le code, mais ne reproduit pas fid√®lement l‚Äôenvironnement Lambda (runtime, packaging, variables, etc.).\nPourquoi pas A/D : CDK n‚Äôest pas l‚Äôoutil standard pour √©muler Lambda localement, et un Docker ‚Äúmaison‚Äù est plus complexe et moins align√© sur l‚Äô√©mulation officielle SAM.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que ta Lambda est une recette de pizza, et que ‚Äúl‚Äôenvironnement Lambda‚Äù est le vrai four de la pizzeria. Tu veux tester ta recette chez toi avec un four qui ressemble le plus possible √† celui de la pizzeria, et que toute l‚Äô√©quipe puisse refaire les m√™mes tests avant d‚Äôouvrir la pizzeria.**\n\nConcept : une Lambda, c‚Äôest un petit bout de code qui r√©agit √† un ‚Äú√©v√©nement‚Äù (comme une commande de pizza). Pour √©viter les bugs, on doit tester avec des commandes r√©alistes, dans une cuisine qui imite la vraie.\nPourquoi C : AWS SAM CLI, c‚Äôest comme un kit ‚Äúmini-pizzeria √† la maison‚Äù. Il peut fabriquer des fausses commandes r√©alistes (sam local generate-event) et faire tourner la recette dans un four local qui copie Lambda (sam local invoke).\nDonc chaque dev peut lancer les tests sur son PC, pareil pour tout le monde.\nEt on peut aussi brancher ces tests dans la cha√Æne automatique (CI/CD) avant le d√©ploiement CDK, comme un contr√¥le qualit√© obligatoire avant d‚Äôouvrir.\nLes autres choix : A et B testent plus ‚Äú√† la main‚Äù ou trop simplifi√©, D fait un four maison mais moins fid√®le/standard que le kit SAM.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:160:ad3e4f98e6844a6b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 160,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer must analyze performance issues with production-distributed applications written as AWS Lambda functions. These distributed Lambda applications invoke other components that make up the applications.How should the developer identify and troubleshoot the root cause of the performance issues in production?",
      "choices": {
        "A": "Add logging statements to the Lambda functions, then use Amazon CloudWatch to view the logs.",
        "B": "Use AWS CloudTrail and then examine the logs.",
        "C": "Use AWS X-Ray, then examine the segments and errors.",
        "D": "Run Amazon Inspector agents and then analyze performance."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122579-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:03 a.m.",
      "textHash": "ad3e4f98e6844a6b",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Ici, le probl√®me vient d‚Äôune application ¬´ distribu√©e ¬ª : plusieurs fonctions AWS Lambda (ex√©cutions de code sans serveur) s‚Äôappellent entre elles et appellent d‚Äôautres services. Pour trouver la cause d‚Äôune lenteur en production, il faut voir le chemin complet d‚Äôune requ√™te et o√π le temps est d√©pens√©. AWS X-Ray est un service de tra√ßage distribu√© : il suit une requ√™te de bout en bout et cr√©e une ¬´ trace ¬ª d√©coup√©e en segments (chaque appel : Lambda, base de donn√©es, API, etc.). En regardant les segments, on rep√®re rapidement l‚Äô√©tape lente, les erreurs, les timeouts et les d√©pendances probl√©matiques. CloudWatch Logs (A) aide pour lire des messages, mais ne reconstruit pas facilement le parcours complet entre composants. CloudTrail (B) enregistre surtout les appels d‚ÄôAPI AWS (audit/s√©curit√©), pas la performance applicative. Amazon Inspector (D) sert surtout √† d√©tecter des vuln√©rabilit√©s, pas √† analyser des latences de fonctions Lambda. Donc la bonne approche est d‚Äôutiliser X-Ray et d‚Äôexaminer segments et erreurs.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o en √©quipe : pour gagner, ton perso doit passer la balle √† d‚Äôautres joueurs (soigneur, tank, sniper). Si √ßa rame, tu veux savoir quel joueur ou quelle action ralentit tout.**\n\nConcept : une appli ‚Äúdistribu√©e‚Äù en Lambda, c‚Äôest comme une action de jeu d√©coup√©e en plusieurs mini-joueurs qui se passent le relais. Si √ßa devient lent, il faut une ‚Äúcam√©ra de replay‚Äù qui suit le trajet complet. AWS X-Ray (C) fait ce replay : il trace chaque √©tape, montre le temps pass√© chez chaque ‚Äújoueur‚Äù (segment) et o√π √ßa plante (erreurs). Comme √ßa, tu rep√®res la vraie cause : par exemple l‚Äô√©tape B prend 2 secondes au lieu de 0,1. Les logs CloudWatch (A) sont plut√¥t des messages √©crits par chaque joueur, utiles mais pas un replay complet du trajet. CloudTrail (B) c‚Äôest le journal ‚Äúqui a fait quoi‚Äù sur AWS, pas la vitesse de ton action. Inspector (D) cherche surtout des failles de s√©curit√©, pas les ralentissements.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:158:888385da03fbeeea",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 158,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn ecommerce application is running behind an Application Load Balancer. A developer observes some unexpected load on the application during non-peak hours. The developer wants to analyze patterns for the client IP addresses that use the application.Which HTTP header should the developer use for this analysis?",
      "choices": {
        "A": "The X-Forwarded-Proto header",
        "B": "The X-Forwarded-Host header",
        "C": "The X-Forwarded-For header",
        "D": "The X-Forwarded-Port header"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122577-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:02 a.m.",
      "textHash": "888385da03fbeeea",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Quand une appli est derri√®re un Application Load Balancer (ALB), le serveur ne voit souvent plus l‚ÄôIP r√©elle du client : il voit l‚ÄôIP du load balancer.\nPour retrouver l‚ÄôIP d‚Äôorigine, l‚ÄôALB ajoute des en-t√™tes (headers) HTTP √† la requ√™te.\nLe header \"X-Forwarded-For\" contient l‚Äôadresse IP du client (et parfois une liste d‚ÄôIPs si plusieurs proxys sont pass√©s).\nC‚Äôest donc celui √† lire dans les logs applicatifs ou les logs d‚Äôacc√®s pour rep√©rer quels clients g√©n√®rent du trafic, m√™me la nuit.\n\"X-Forwarded-Proto\" indique le protocole (HTTP/HTTPS), pas l‚ÄôIP.\n\"X-Forwarded-Host\" indique le nom de domaine demand√©, pas l‚ÄôIP.\n\"X-Forwarded-Port\" indique le port (80/443), pas l‚ÄôIP.\nDonc pour analyser les patterns par IP client, la bonne r√©ponse est \"X-Forwarded-For\".",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine l‚Äôentr√©e du lyc√©e avec un surveillant qui fait passer tous les √©l√®ves par un seul portail avant d‚Äôaller en classe.**\n\nLe portail, c‚Äôest le ‚Äúr√©partiteur‚Äù (Application Load Balancer) : il re√ßoit tout le monde et envoie vers les bonnes salles (les serveurs). Probl√®me : quand tu regardes la liste des entr√©es, tu vois surtout le nom du surveillant, pas celui de chaque √©l√®ve. Pour savoir quels √©l√®ves viennent en douce √† des heures calmes, le surveillant note sur un papier l‚Äôidentit√© r√©elle de l‚Äô√©l√®ve. Sur le web, ce ‚Äúpapier‚Äù est un en-t√™te HTTP. X-Forwarded-For contient l‚Äôadresse IP du vrai client (l‚Äô√©l√®ve). Les autres en-t√™tes disent autre chose : Proto = si c‚Äôest en ‚Äúhttps‚Äù, Host = le nom du site demand√©, Port = le num√©ro de porte. Donc pour analyser les IP des clients, tu utilises X-Forwarded-For (C).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:157:8d47aa80325972d9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 157,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application runs on multiple EC2 instances behind an ELB.Where is the session data best written so that it can be served reliably across multiple requests?",
      "choices": {
        "A": "Write data to Amazon ElastiCache.",
        "B": "Write data to Amazon Elastic Block Store.",
        "C": "Write data to Amazon EC2 Instance Store.",
        "D": "Write data to the root filesystem."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122576-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:59 a.m.",
      "textHash": "8d47aa80325972d9",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:0dcca819",
      "frExplanation": "Avec plusieurs instances EC2 derri√®re un ELB (load balancer), deux requ√™tes d‚Äôun m√™me utilisateur peuvent arriver sur deux serveurs diff√©rents. Si la ‚Äúsession‚Äù (ex: panier, identifiant, √©tat de connexion) est stock√©e localement sur un serveur, l‚Äôautre serveur ne la verra pas et l‚Äôutilisateur sera d√©connect√© ou perdra ses donn√©es. Il faut donc un stockage partag√©, rapide et accessible par toutes les instances. Amazon ElastiCache est un service de cache en m√©moire (Redis/Memcached) g√©r√© par AWS, con√ßu pour stocker des donn√©es temporaires comme les sessions et les lire tr√®s vite depuis plusieurs serveurs. EBS, l‚ÄôInstance Store et le syst√®me de fichiers racine sont li√©s √† une instance (ou √† un disque attach√©) et ne sont pas un endroit fiable/partag√© pour des sessions derri√®re un load balancer. Donc la meilleure option est d‚Äô√©crire les sessions dans ElastiCache.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine avec plusieurs caisses, et un surveillant (le r√©partiteur) qui envoie chaque √©l√®ve √† une caisse diff√©rente √† chaque passage.**\n\nLe ‚Äúsession data‚Äù, c‚Äôest comme le ticket de cantine avec tes choix (menu, allergies, ce que tu as d√©j√† pay√©). Si tu reviens, tu peux tomber sur une autre caisse, donc il faut que toutes les caisses voient le m√™me ticket. Amazon ElastiCache (A), c‚Äôest comme un tableau partag√© tr√®s rapide derri√®re les caisses : on y note ton ticket, et n‚Äôimporte quelle caisse peut le relire tout de suite. EBS (B) et le disque d‚Äôune caisse (C/D) ressemblent √† un cahier rang√© dans UNE caisse : si tu changes de caisse, l‚Äôautre ne l‚Äôa pas, ou c‚Äôest plus lent/moins fiable. Donc pour que √ßa marche √† chaque requ√™te, m√™me si tu changes de machine, on met la session dans ElastiCache.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:156:319f55a56e3c8eaa",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 156,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is using Amazon OpenSearch Service to implement an audit monitoring system. A developer needs to create an AWS CloudFormation custom resource that is associated with an AWS Lambda function to configure the OpenSearch Service domain. The Lambda function must access the OpenSearch Service domain by using OpenSearch Service internal master user credentials.What is the MOST secure way to pass these credentials to the Lambda function?",
      "choices": {
        "A": "Use a CloudFormation parameter to pass the master user credentials at deployment to the OpenSearch Service domain‚Äôs MasterUserOptions and the Lambda function‚Äôs environment variable. Set the NoEcho attribute to true.",
        "B": "Use a CloudFormation parameter to pass the master user credentials at deployment to the OpenSearch Service domain‚Äôs MasterUserOptions and to create a parameter in AWS Systems Manager Parameter Store. Set the NoEcho attribute to true. Create an IAM role that has the ssm:GetParameter permission. Assign the role to the Lambda function. Store the parameter name as the Lambda function‚Äôs environment variable. Resolve the parameter‚Äôs value at runtime.",
        "C": "Use a CloudFormation parameter to pass the master user credentials at deployment to the OpenSearch Service domain‚Äôs MasterUserOptions and the Lambda function‚Äôs environment variable. Encrypt the parameter‚Äôs value by using the AWS Key Management Service (AWS KMS) encrypt command.",
        "D": "Use CloudFormation to create an AWS Secrets Manager secret. Use a CloudFormation dynamic reference to retrieve the secret‚Äôs value for the OpenSearch Service domain‚Äôs MasterUserOptions. Create an IAM role that has the secretsmanager:GetSecretValue permission. Assign the role to the Lambda function. Store the secret‚Äôs name as the Lambda function‚Äôs environment variable. Resolve the secret‚Äôs value at runtime."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122575-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:59 a.m.",
      "textHash": "319f55a56e3c8eaa",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, on doit transmettre des identifiants (login/mot de passe) du ‚Äúmaster user‚Äù OpenSearch √† une fonction AWS Lambda (code ex√©cut√© √† la demande). Le but est de le faire sans exposer ces secrets dans CloudFormation, dans les logs, ou dans les variables d‚Äôenvironnement.\nAWS Secrets Manager est un coffre-fort g√©r√© pour stocker des secrets (mots de passe, cl√©s) avec chiffrement, contr√¥le d‚Äôacc√®s IAM, et rotation possible. CloudFormation peut cr√©er le secret et utiliser une ‚Äúdynamic reference‚Äù pour injecter la valeur directement dans la configuration du domaine OpenSearch, sans l‚Äô√©crire en clair dans le template.\nEnsuite, Lambda n‚Äôa pas besoin de recevoir le mot de passe en variable d‚Äôenvironnement (risque d‚Äôexposition). Elle re√ßoit seulement le nom du secret, puis lit la valeur au moment de l‚Äôex√©cution via secretsmanager:GetSecretValue, autoris√© par un r√¥le IAM.\nC‚Äôest plus s√ªr que NoEcho (qui n‚Äôemp√™che pas toutes les fuites) et plus adapt√© que Parameter Store pour des identifiants sensibles, car Secrets Manager est con√ßu pour les secrets et leur gestion.\nDonc la meilleure option est D : stocker les identifiants dans Secrets Manager, y acc√©der via IAM, et r√©cup√©rer la valeur √† l‚Äôex√©cution.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:316:6902d7130a0b37e8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 316,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses an AWS Lambda function that reads messages from an Amazon Simple Queue Service (Amazon SQS) standard queue. The Lambda function makes an HTTP call to a third-party API for each message. The company wants to ensure that the Lambda function does not overwhelm the third-party API with more than two concurrent requests.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure a provisioned concurrency of two on the Lambda function.",
        "B": "Configure a batch size of two on the Amazon SQS event source mapping for the Lambda function.",
        "C": "Configure Lambda event filtering to process two messages from Amazon SQS at every invocations.",
        "D": "Configure a maximum concurrency of two on the Amazon SQS event source mapping for the Lambda function."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134135-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 18, 2024, 12:18 p.m.",
      "textHash": "6902d7130a0b37e8",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, une fonction AWS Lambda (code ex√©cut√© √† la demande) lit des messages d‚Äôune file Amazon SQS (service de file d‚Äôattente). Pour chaque message, elle appelle une API externe en HTTP. Le risque: si Lambda traite trop de messages en parall√®le, elle enverra trop d‚Äôappels simultan√©s et saturera l‚ÄôAPI.\nLa bonne approche est de limiter le nombre d‚Äôex√©cutions Lambda en parall√®le d√©clench√©es par SQS. Avec le ‚Äúmaximum concurrency‚Äù sur le mapping SQS‚ÜíLambda, on impose un plafond (ici 2) au nombre d‚Äôinvocations concurrentes provenant de cette file.\nPourquoi pas A: la ‚Äúprovisioned concurrency‚Äù garantit des instances Lambda pr√™tes (moins de latence), mais ne limite pas le nombre total d‚Äôex√©cutions; Lambda peut quand m√™me monter au-del√† de 2.\nPourquoi pas B: un batch size de 2 signifie ‚Äújusqu‚Äô√† 2 messages par invocation‚Äù, mais Lambda peut lancer plusieurs invocations en parall√®le, donc d√©passer 2 appels simultan√©s.\nPourquoi pas C: le filtrage d‚Äô√©v√©nements sert √† choisir quels messages traiter, pas √† limiter la concurrence.\nDonc D est correct: on fixe la concurrence maximale √† 2 pour ne jamais d√©passer 2 requ√™tes HTTP simultan√©es vers l‚ÄôAPI tierce.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une pizzeria qui re√ßoit des tickets de commande dans une bo√Æte (la file SQS). Un livreur (Lambda) prend des tickets et appelle un fournisseur externe (l‚ÄôAPI) pour confirmer chaque pizza. Le fournisseur dit : ‚ÄúPas plus de 2 appels en m√™me temps, sinon je sature.‚Äù**\n\nConcept : ‚Äúconcurrent‚Äù = combien d‚Äôappels sont faits en m√™me temps, comme 2 livreurs qui t√©l√©phonent en m√™me temps au fournisseur.\nSi tu r√®gles ‚Äúmaximum concurrency = 2‚Äù sur le lien entre la bo√Æte de tickets et les livreurs, tu imposes : au maximum 2 livreurs actifs √† la fois, donc max 2 appels simultan√©s vers l‚ÄôAPI.\nB (batch size 2) veut juste dire ‚Äúprendre 2 tickets d‚Äôun coup‚Äù, mais un seul livreur peut quand m√™me encha√Æner et d‚Äôautres livreurs peuvent aussi travailler : √ßa ne garantit pas 2 appels max en m√™me temps.\nA (provisioned concurrency 2) pr√©pare 2 livreurs pr√™ts, mais n‚Äôemp√™che pas d‚Äôen avoir plus si la demande monte.\nC (filtrage) choisit quels tickets traiter, pas combien de livreurs appellent en m√™me temps.\nDonc D est la seule option qui limite vraiment √† 2 requ√™tes simultan√©es vers l‚ÄôAPI.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:150:9c20a132715a000b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 150,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is planning to migrate on-premises company data to Amazon S3. The data must be encrypted, and the encryption keys must support automatic annual rotation. The company must use AWS Key Management Service (AWS KMS) to encrypt the data.Which type of keys should the developer use to meet these requirements?",
      "choices": {
        "A": "Amazon S3 managed keys",
        "B": "Symmetric customer managed keys with key material that is generated by AWS",
        "C": "Asymmetric customer managed keys with key material that is generated by AWS",
        "D": "Symmetric customer managed keys with imported key material"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122571-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:54 a.m.",
      "textHash": "9c20a132715a000b",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Amazon S3 est un service de stockage de fichiers dans le cloud. Pour chiffrer (rendre illisibles) les donn√©es, on utilise des cl√©s g√©r√©es par AWS KMS (service qui cr√©e et prot√®ge des cl√©s de chiffrement).\nExigence 1 : les donn√©es doivent √™tre chiffr√©es dans S3 avec KMS ‚Üí il faut une cl√© KMS.\nExigence 2 : les cl√©s doivent avoir une rotation automatique annuelle ‚Üí cette rotation automatique est disponible pour les cl√©s KMS ¬´ customer managed ¬ª (g√©r√©es par le client) quand le mat√©riel de cl√© est g√©n√©r√© par AWS.\nLes cl√©s S3 g√©r√©es par S3 (A) ne sont pas des cl√©s KMS et ne r√©pondent pas √† ‚Äúdoit utiliser KMS‚Äù.\nLes cl√©s asym√©triques (C) ne sont g√©n√©ralement pas utilis√©es pour le chiffrement c√¥t√© serveur de S3 (SSE-KMS utilise des cl√©s sym√©triques).\nSi on importe son propre mat√©riel de cl√© (D), la rotation automatique n‚Äôest pas prise en charge : il faut r√©importer et g√©rer soi-m√™me.\nDonc il faut une cl√© KMS sym√©trique, g√©r√©e par le client, avec mat√©riel de cl√© g√©n√©r√© par AWS (B).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine le casier de ton √©cole : tu y mets tes affaires (les donn√©es) et tu veux un cadenas (le chiffrement) avec une cl√©. Chaque ann√©e, le lyc√©e change automatiquement les cl√©s pour plus de s√©curit√© (rotation annuelle).**\n\nAmazon S3, c‚Äôest le casier o√π tu stockes les fichiers. Chiffrer, c‚Äôest fermer le casier avec un cadenas pour que personne ne lise sans la cl√©. AWS KMS, c‚Äôest le bureau qui fabrique et garde les cl√©s. On veut une rotation automatique chaque ann√©e : √ßa marche bien quand le bureau (AWS) fabrique la cl√© et g√®re son remplacement. Une cl√© ‚Äúcustomer managed‚Äù = c‚Äôest toi qui choisis et contr√¥les la cl√© (comme demander un cadenas √† ton nom au bureau). ‚ÄúSymmetric‚Äù = une seule cl√© pour fermer et ouvrir, simple comme un cadenas classique. ‚ÄúImported key material‚Äù = tu apportes toi-m√™me le m√©tal/cl√©, et l√† la rotation auto devient compliqu√©e. Donc la bonne r√©ponse est B : cl√© sym√©trique g√©r√©e par le client, mais fabriqu√©e par AWS, avec rotation annuelle automatique.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:149:a9be47011fb2ccae",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 149,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn organization is using Amazon CloudFront to ensure that its users experience low-latency access to its web application. The organization has identified a need to encrypt all traffic between users and CloudFront, and all traffic between CloudFront and the web application.How can these requirements be met? (Choose two.)",
      "choices": {
        "A": "Use AWS KMS to encrypt traffic between CloudFront and the web application.",
        "B": "Set the Origin Protocol Policy to ‚ÄúHTTPS Only‚Äù.",
        "C": "Set the Origin‚Äôs HTTP Port to 443.",
        "D": "Set the Viewer Protocol Policy to ‚ÄúHTTPS Only‚Äù or ‚ÄúRedirect HTTP to HTTPS‚Äù.",
        "E": "Enable the CloudFront option Restrict Viewer Access."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122567-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:42 a.m.",
      "textHash": "a9be47011fb2ccae",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Objectif : chiffrer (HTTPS/TLS) le trafic 1) entre l‚Äôutilisateur et CloudFront, et 2) entre CloudFront et l‚Äôapplication (l‚Äô¬´ origin ¬ª).\nCloudFront est un CDN : il re√ßoit les requ√™tes des utilisateurs (viewer) puis contacte votre serveur d‚Äôorigine (origin).\nPour chiffrer CloudFront ‚Üí origin, il faut forcer CloudFront √† utiliser HTTPS vers l‚Äôorigine : c‚Äôest l‚Äôoption ¬´ Origin Protocol Policy = HTTPS Only ¬ª (r√©ponse B).\nPour chiffrer utilisateur ‚Üí CloudFront, il faut forcer les visiteurs √† utiliser HTTPS : ¬´ Viewer Protocol Policy = HTTPS Only ¬ª ou ¬´ Redirect HTTP to HTTPS ¬ª (r√©ponse D).\nAWS KMS (A) sert √† chiffrer des donn√©es au repos (fichiers, secrets), pas √† chiffrer le trafic r√©seau TLS.\nChanger le port HTTP de l‚Äôorigine √† 443 (C) est incoh√©rent : 443 est le port HTTPS, et ce r√©glage ne force pas TLS.\n¬´ Restrict Viewer Access ¬ª (E) sert √† limiter l‚Äôacc√®s (signed URLs/cookies), pas √† activer le chiffrement.\nDonc les deux actions correctes sont B (origin en HTTPS) et D (viewer en HTTPS).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un service de livraison de pizzas. Le client (utilisateur) parle au livreur CloudFront, puis le livreur parle √† la pizzeria (ton site web). Tu veux que les deux conversations soient en ‚Äúchuchotant cod√©‚Äù pour que personne n‚Äô√©coute.**\n\nConcept : ‚ÄúHTTPS‚Äù = conversation chiffr√©e (comme un message secret). ‚ÄúHTTP‚Äù = conversation normale (tout le monde peut entendre). Il y a 2 trajets : client‚Üílivreur et livreur‚Üípizzeria.\nPour chiffrer client‚ÜíCloudFront, on r√®gle la r√®gle c√¥t√© client : Viewer Protocol Policy en HTTPS (r√©ponse D, mais pas dans la bonne r√©ponse donn√©e).\nPour chiffrer CloudFront‚Üísite, on r√®gle la r√®gle c√¥t√© pizzeria : Origin Protocol Policy.\nLa bonne r√©ponse B (‚ÄúHTTPS Only‚Äù) oblige CloudFront √† parler √† la pizzeria uniquement en HTTPS.\nDonc m√™me si quelqu‚Äôun √©coute entre le livreur et la pizzeria, il ne comprend rien.\nA est faux : KMS sert √† chiffrer des fichiers/donn√©es stock√©es, pas une conversation r√©seau.\nC est faux : 443 est juste un num√©ro de porte, √ßa ne garantit pas le ‚Äúlangage secret‚Äù.\nE est hors-sujet : √ßa contr√¥le qui a le droit d‚Äôacc√©der, pas le chiffrement.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:146:d03361c55fceed0a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 146,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application that uses AWS API Gateway APIs, AWS Lambda functions, and AWS DynamoDB tables. The developer uses the AWS Serverless Application Model (AWS SAM) to build and run serverless applications on AWS. Each time the developer pushes changes for only to the Lambda functions, all the artifacts in the application are rebuilt.The developer wants to implement AWS SAM Accelerate by running a command to only redeploy the Lambda functions that have changed.Which command will meet these requirements?",
      "choices": {
        "A": "sam deploy --force-upload",
        "B": "sam deploy --no-execute-changeset",
        "C": "sam package",
        "D": "sam sync --watch"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122564-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:37 a.m.",
      "textHash": "d03361c55fceed0a",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:07c1ed43",
      "frExplanation": "AWS SAM sert √† construire et d√©ployer des applis ¬´ serverless ¬ª (sans serveurs √† g√©rer) avec API Gateway, Lambda et DynamoDB.\nIci, le probl√®me est que chaque petit changement dans le code Lambda d√©clenche un rebuild/red√©ploiement complet de tous les artefacts, ce qui est lent.\nSAM Accelerate est une fonctionnalit√© faite pour acc√©l√©rer la boucle de dev en ne red√©ployant que ce qui a chang√© (souvent le code des fonctions Lambda).\nLa commande \"sam sync\" synchronise les changements locaux vers AWS sans tout reconstruire, et peut cibler uniquement les ressources modifi√©es.\nL‚Äôoption \"--watch\" surveille les fichiers et relance automatiquement la synchro √† chaque modification : id√©al quand on change souvent le code Lambda.\nA (force-upload) force l‚Äôupload m√™me si rien n‚Äôa chang√©, donc pas l‚Äôobjectif.\nB (no-execute-changeset) pr√©pare un changement mais ne l‚Äôapplique pas.\nC (sam package) ne fait que pr√©parer/packager, pas redeployer de fa√ßon incr√©mentale.\nDonc la bonne commande est \"sam sync --watch\".",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o o√π tu construis un niveau : la porte d‚Äôentr√©e (API) laisse entrer les joueurs, des PNJ (Lambda) font des actions, et un coffre (DynamoDB) garde les objets. Quand tu modifies juste un PNJ, tu n‚Äôas pas envie de reconstruire tout le niveau √† chaque fois.**\n\nConcept : AWS SAM, c‚Äôest l‚Äôoutil qui ‚Äúpr√©pare et envoie‚Äù ton niveau sur le serveur. Sans acc√©l√©ration, il reconstruit tout, m√™me si tu as chang√© un seul PNJ. AWS SAM Accelerate, c‚Äôest comme un mode ‚Äúmise √† jour rapide‚Äù : il n‚Äôenvoie que ce qui a chang√©. La commande sam sync --watch fait exactement √ßa : elle surveille tes changements et red√©ploie seulement les fonctions Lambda modifi√©es, automatiquement. A force-upload, c‚Äôest l‚Äôinverse (√ßa renvoie tout). B ne lance pas vraiment la mise √† jour (il pr√©pare sans appliquer). C emballe le projet mais ne fait pas la mise √† jour rapide. Donc D est la bonne r√©ponse.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:143:b5b8933b48d46b0b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 143,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company‚Äôs website runs on an Amazon EC2 instance and uses Auto Scaling to scale the environment during peak times. Website users across the world are experiencing high latency due to static content on the EC2 instance, even during non-peak hours.Which combination of steps will resolve the latency issue? (Choose two.)",
      "choices": {
        "A": "Double the Auto Scaling group‚Äôs maximum number of servers.",
        "B": "Host the application code on AWS Lambda.",
        "C": "Scale vertically by resizing the EC2 instances.",
        "D": "Create an Amazon CloudFront distribution to cache the static content.",
        "E": "Store the application‚Äôs static content in Amazon S3."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122655-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 12:24 p.m.",
      "textHash": "b5b8933b48d46b0b",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:8e5ec254",
      "frExplanation": "Le probl√®me vient du fait que des utilisateurs partout dans le monde t√©l√©chargent du contenu statique (images, CSS, JS) directement depuis une instance EC2 situ√©e dans une seule r√©gion. Plus la distance est grande, plus la latence augmente, m√™me si le serveur n‚Äôest pas ‚Äúsurcharg√©‚Äù.\nAmazon CloudFront est un CDN : il met en cache ces fichiers dans des points de pr√©sence proches des utilisateurs, ce qui r√©duit fortement le temps de chargement.\nDonc cr√©er une distribution CloudFront (D) est la bonne action pour r√©soudre la latence mondiale.\nAugmenter l‚ÄôAuto Scaling (A) ou redimensionner l‚Äôinstance (C) aide surtout quand le serveur manque de CPU/RAM, mais ne rapproche pas le contenu des utilisateurs.\nPasser sur Lambda (B) ne cible pas le probl√®me du contenu statique et demande une refonte.\nMettre les fichiers statiques dans S3 (E) est souvent fait avec CloudFront, mais la question indique que la r√©ponse correcte attendue est D : CloudFront est l‚Äô√©l√©ment cl√© qui apporte le cache mondial.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine le site web comme une pizzeria dans une seule ville, et les images/vid√©os du site sont des pizzas ‚Äútoujours identiques‚Äù (contenu statique). Des clients du monde entier commandent, donc ceux qui sont loin attendent longtemps.**\n\nConcept : plus tu es loin de la pizzeria, plus la livraison est lente, m√™me s‚Äôil n‚Äôy a pas beaucoup de commandes.\nAuto Scaling (plus de cuisiniers) aide quand il y a foule, mais ne rapproche pas la pizzeria des clients.\nCloudFront, c‚Äôest comme ouvrir plein de petits points-relais partout dans le monde.\nCes relais gardent en avance les pizzas les plus demand√©es (cache du contenu statique).\nDu coup, les clients prennent au relais le plus proche : moins de trajet = moins de latence.\nPourquoi D est bon : il met le contenu statique pr√®s des utilisateurs, tout le temps, m√™me hors ‚Äúheures de pointe‚Äù.\nPourquoi A/C ne suffisent pas : plus de serveurs ou plus gros serveur = toujours la m√™me distance.\nDonc la meilleure action ici est D : cr√©er une distribution CloudFront pour mettre en cache le contenu statique.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:142:f1a5302f9e9e8ff6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 142,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn online food company provides an Amazon API Gateway HTTP API to receive orders for partners. The API is integrated with an AWS Lambda function. The Lambda function stores the orders in an Amazon DynamoDB table.The company expects to onboard additional partners. Some of the partners require additional Lambda functions to receive orders. The company has created an Amazon S3 bucket. The company needs to store all orders and updates in the S3 bucket for future analysis.How can the developer ensure that all orders and updates are stored to Amazon S3 with the LEAST development effort?",
      "choices": {
        "A": "Create a new Lambda function and a new API Gateway API endpoint. Configure the new Lambda function to write to the S3 bucket. Modify the original Lambda function to post updates to the new API endpoint.",
        "B": "Use Amazon Kinesis Data Streams to create a new data stream. Modify the Lambda function to publish orders to the data stream. Configure the data stream to write to the S3 bucket.",
        "C": "Enable DynamoDB Streams on the DynamoDB table. Create a new Lambda function. Associate the stream‚Äôs Amazon Resource Name (ARN) with the Lambda function. Configure the Lambda function to write to the S3 bucket as records appear in the table's stream.",
        "D": "Modify the Lambda function to publish to a new Amazon Simple Notification Service (Amazon SNS) topic as the Lambda function receives orders. Subscribe a new Lambda function to the topic. Configure the new Lambda function to write to the S3 bucket as updates come through the topic."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/117798-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Aug. 10, 2023, 2:15 p.m.",
      "textHash": "f1a5302f9e9e8ff6",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, les commandes sont d√©j√† enregistr√©es dans DynamoDB (base de donn√©es NoSQL). Le besoin est de copier automatiquement chaque cr√©ation/modification (‚Äúorders and updates‚Äù) vers S3 (stockage de fichiers) avec le moins de code possible.\nDynamoDB Streams est fait pour √ßa : il enregistre un flux d‚Äô√©v√©nements √† chaque changement d‚Äôun item (INSERT/MODIFY/REMOVE) dans la table.\nEn activant DynamoDB Streams, vous n‚Äôavez pas besoin de modifier l‚ÄôAPI Gateway ni les Lambdas existantes : tout changement dans la table d√©clenche des √©v√©nements.\nVous cr√©ez une nouvelle fonction Lambda ‚Äúconsommatrice‚Äù du stream : elle re√ßoit les changements et √©crit les donn√©es dans le bucket S3.\nAinsi, peu importe quel partenaire ou quelle Lambda √©crit dans DynamoDB, tous les changements passent par le stream et sont captur√©s.\nC‚Äôest l‚Äôeffort de d√©veloppement le plus faible car on ajoute un m√©canisme standard de capture de changements, sans refactoriser le flux de commande.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : chaque commande de repas est not√©e dans un cahier officiel (la table DynamoDB). √Ä c√¥t√©, il y a une ‚Äúcam√©ra‚Äù qui filme automatiquement chaque fois qu‚Äôon √©crit ou qu‚Äôon modifie une ligne dans le cahier (DynamoDB Streams). Et tu veux aussi archiver tout √ßa dans un classeur de stockage (le bucket S3) pour analyser plus tard.**\n\nConcept : au lieu de demander √† chaque √©l√®ve (chaque fonction Lambda) de penser √† faire une copie, tu mets un syst√®me qui observe le cahier et copie tout automatiquement.\nPourquoi C : DynamoDB Streams enregistre chaque ajout/modif de commande dans le cahier, sans changer les partenaires ni les fonctions qui √©crivent d√©j√†.\nTu ajoutes juste une nouvelle ‚Äúsecr√©taire‚Äù (une nouvelle Lambda) qui lit le flux de changements et range une copie dans le classeur S3.\nR√©sultat : peu de code √† modifier, et √ßa marche m√™me si demain tu ajoutes d‚Äôautres partenaires et d‚Äôautres Lambdas.\nA et D demandent de rerouter/ajouter des √©tapes c√¥t√© commandes (plus de boulot).\nB ajoute un nouveau ‚Äútuyau‚Äù √† mettre partout avant d‚Äô√©crire (plus de changements).\nDonc C = copier automatiquement tout ce qui change dans le cahier vers S3, avec le moins d‚Äôeffort.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:141:35a8edeaa9da171e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 141,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer wants to add request validation to a production environment Amazon API Gateway API. The developer needs to test the changes before the API is deployed to the production environment. For the test, the developer will send test requests to the API through a testing tool.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Export the existing API to an OpenAPI file. Create a new API. Import the OpenAPI file. Modify the new API to add request validation. Perform the tests. Modify the existing API to add request validation. Deploy the existing API to production.",
        "B": "Modify the existing API to add request validation. Deploy the updated API to a new API Gateway stage. Perform the tests. Deploy the updated API to the API Gateway production stage.",
        "C": "Create a new API. Add the necessary resources and methods, including new request validation. Perform the tests. Modify the existing API to add request validation. Deploy the existing API to production",
        "D": "Clone the existing API. Modify the new API to add request validation. Perform the tests. Modify the existing API to add request validation. Deploy the existing API to production."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/117797-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Aug. 10, 2023, 2:13 p.m.",
      "textHash": "35a8edeaa9da171e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:a92aa40b",
      "frExplanation": "Amazon API Gateway sert √† publier une API (des URL) et √† g√©rer des ¬´ stages ¬ª (environnements) comme test et production.\nLa ¬´ request validation ¬ª v√©rifie que la requ√™te re√ßue respecte un format attendu (param√®tres, corps JSON, en-t√™tes) avant d‚Äôappeler le backend.\nPour tester sans impacter la production, le plus simple est d‚Äôutiliser un nouveau stage : m√™me API, mais une URL de stage diff√©rente.\nOption B : on ajoute la validation sur l‚ÄôAPI existante, puis on d√©ploie d‚Äôabord vers un stage de test (ex: /dev ou /test).\nLe d√©veloppeur envoie ses requ√™tes via l‚Äôoutil de test sur l‚ÄôURL du stage de test, donc aucun risque pour les utilisateurs de production.\nQuand tout est valid√©, on red√©ploie exactement la m√™me configuration vers le stage production.\nC‚Äôest le moins d‚Äôoverhead car on ne duplique pas l‚ÄôAPI (pas de nouvelle API, pas d‚Äôimport/export, pas de clonage), juste un d√©ploiement vers un autre stage.\nLes autres options cr√©ent ou dupliquent une API, ce qui ajoute de la gestion (ressources, permissions, synchronisation des changements).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine l‚ÄôAPI comme une cantine du lyc√©e. Les ‚Äúrequ√™tes‚Äù sont les √©l√®ves qui passent au self. La ‚Äúvalidation‚Äù c‚Äôest le surveillant qui v√©rifie: carte de cantine, bon jour, bon menu, sinon refus. Un ‚Äústage‚Äù c‚Äôest une ligne de self s√©par√©e: une pour tester, une pour le vrai self.**\n\nConcept: tu veux ajouter un nouveau contr√¥le √† l‚Äôentr√©e, mais sans bloquer la cantine en plein service. Donc tu testes d‚Äôabord sur une ligne de self de test.\nPourquoi B: tu modifies la m√™me cantine (la m√™me API), puis tu ouvres une ‚Äúligne de test‚Äù (un stage de test) avec les nouveaux contr√¥les. Ton outil de test envoie des √©l√®ves fictifs sur cette ligne, sans toucher la ligne ‚Äúproduction‚Äù o√π mangent les vrais √©l√®ves.\nQuand tout est OK, tu actives exactement les m√™mes changements sur la ligne ‚Äúproduction‚Äù.\nC‚Äôest le moins de travail: pas besoin de reconstruire une nouvelle cantine, ni de copier/exporter/importer des plans. Juste deux lignes (stages) pour le m√™me self.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:139:3ed3cf8b0b3eef07",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 139,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an application for a company. The application needs to read the file doc.txt that is placed in the root folder of an Amazon S3 bucket that is named DOC-EXAMPLE-BUCKET. The company‚Äôs security team requires the principle of least privilege to be applied to the application‚Äôs IAM policy.Which IAM policy statement will meet these security requirements?",
      "choices": {},
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/117476-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Aug. 6, 2023, 3:04 p.m.",
      "textHash": "3ed3cf8b0b3eef07",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Amazon S3 est un service de stockage de fichiers (objets) dans des ‚Äúbuckets‚Äù. Ici, l‚Äôapplication doit seulement lire un fichier pr√©cis : doc.txt, situ√© √† la racine du bucket DOC-EXAMPLE-BUCKET.\nLe principe du moindre privil√®ge signifie : donner uniquement l‚Äôautorisation minimale n√©cessaire, rien de plus (pas d‚Äôacc√®s √† tout le bucket, pas d‚Äô√©criture, pas de suppression).\nPour lire un objet S3, l‚Äôaction IAM attendue est g√©n√©ralement \"s3:GetObject\".\nLa ressource doit pointer vers l‚Äôobjet exact, pas vers \"*\" ni vers tout le bucket : \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/doc.txt\".\nAttention : \"s3:ListBucket\" sert √† lister le contenu du bucket, ce n‚Äôest pas requis si on conna√Æt d√©j√† le nom exact du fichier.\nDonc la bonne d√©claration IAM est celle qui autorise uniquement s3:GetObject sur l‚ÄôARN de doc.txt dans ce bucket, et rien d‚Äôautre.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:134:18659d5357793d51",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 134,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is developing a serverless application that consists of various AWS Lambda functions behind Amazon API Gateway APIs. A developer needs to automate the deployment of Lambda function code. The developer will deploy updated Lambda functions with AWS CodeDeploy. The deployment must minimize the exposure of potential errors to end users. When the application is in production, the application cannot experience downtime outside the specified maintenance window.Which deployment configuration will meet these requirements with the LEAST deployment time?",
      "choices": {
        "A": "Use the AWS CodeDeploy in-place deployment configuration for the Lambda functions. Shift all traffic immediately after deployment.",
        "B": "Use the AWS CodeDeploy linear deployment configuration to shift 10% of the traffic every minute.",
        "C": "Use the AWS CodeDeploy all-at-once deployment configuration to shift all traffic to the updated versions immediately.",
        "D": "Use the AWS CodeDeploy predefined canary deployment configuration to shift 10% of the traffic immediately and shift the remaining traffic after 5 minutes."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/117334-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Aug. 4, 2023, 1:45 p.m.",
      "textHash": "18659d5357793d51",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici on d√©ploie du code AWS Lambda (fonctions ‚Äúserverless‚Äù) derri√®re API Gateway (porte d‚Äôentr√©e HTTP). Avec AWS CodeDeploy, on peut envoyer le trafic vers une nouvelle version progressivement pour √©viter d‚Äôimpacter tous les utilisateurs si un bug existe.\nObjectif 1 : minimiser l‚Äôexposition aux erreurs ‚Üí on √©vite ‚Äútout d‚Äôun coup‚Äù, on teste d‚Äôabord sur une petite partie du trafic.\nObjectif 2 : pas de coupure hors fen√™tre de maintenance ‚Üí on doit garder l‚Äôancienne version active pendant le d√©ploiement (d√©ploiement progressif), donc pas de downtime.\nObjectif 3 : le moins de temps de d√©ploiement ‚Üí on veut une strat√©gie rapide, mais avec un petit test avant bascule totale.\nLe canary 10% puis 100% apr√®s 5 minutes fait exactement √ßa : 10% des utilisateurs valident la nouvelle version, puis bascule compl√®te rapidement.\nLe lin√©aire 10% par minute est plus lent (il faut plusieurs minutes pour atteindre 100%).\n‚ÄúAll-at-once‚Äù bascule tout imm√©diatement : rapide, mais risque maximal si bug (expose tous les utilisateurs).\n‚ÄúIn-place‚Äù n‚Äôest pas le mod√®le adapt√© √† Lambda (on utilise plut√¥t des bascules de trafic entre versions), et ‚Äútout de suite‚Äù ne r√©duit pas le risque.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine la cantine de ton lyc√©e qui change une recette de pizza. Tu veux tester la nouvelle pizza sans que tout le monde tombe sur une pizza rat√©e, et tu n‚Äôas pas le droit de fermer la cantine en dehors du cr√©neau de maintenance.**\n\nIci, les ‚Äúclients‚Äù = les utilisateurs, et le ‚Äútrafic‚Äù = les commandes qui arrivent. D√©ployer une nouvelle version d‚Äôune fonction, c‚Äôest comme changer la recette. Pour limiter les erreurs visibles, tu fais go√ªter √† un petit groupe d‚Äôabord (canary). L‚Äôoption D fait exactement √ßa : 10% des √©l√®ves re√ßoivent la nouvelle pizza tout de suite, puis tu attends 5 minutes pour voir si √ßa se passe bien. Si tout va bien, tu passes le reste √† la nouvelle recette, sans fermer la cantine (pas de coupure). C‚Äôest aussi rapide : seulement 2 √©tapes et une courte attente, donc moins long que 10% chaque minute (B). Tout basculer d‚Äôun coup (C) risque de toucher tout le monde si c‚Äôest rat√©. Donc D minimise le risque, √©vite la panne, et reste le plus rapide.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:132:bc49b2f3e822d1c1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 132,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer creates an AWS Lambda function that retrieves and groups data from several public API endpoints. The Lambda function has been updated and configured to connect to the private subnet of a VPC. An internet gateway is attached to the VPC. The VPC uses the default network ACL and security group configurations.The developer finds that the Lambda function can no longer access the public API. The developer has ensured that the public API is accessible, but the Lambda function cannot connect to the APIHow should the developer fix the connection issue?",
      "choices": {
        "A": "Ensure that the network ACL allows outbound traffic to the public internet.",
        "B": "Ensure that the security group allows outbound traffic to the public internet.",
        "C": "Ensure that outbound traffic from the private subnet is routed to a public NAT gateway.",
        "D": "Ensure that outbound traffic from the private subnet is routed to a new internet gateway."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/117336-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Aug. 4, 2023, 1:51 p.m.",
      "textHash": "bc49b2f3e822d1c1",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:960f354f",
      "frExplanation": "Quand une fonction AWS Lambda est connect√©e √† un VPC, elle utilise le r√©seau de ce VPC pour sortir sur Internet. Ici, Lambda est dans un sous-r√©seau priv√© : par d√©finition, il n‚Äôa pas de route directe vers Internet.\nUn Internet Gateway (IGW) permet l‚Äôacc√®s Internet seulement pour les sous-r√©seaux publics (ceux qui routent vers l‚ÄôIGW et ont des IP publiques). Un sous-r√©seau priv√© n‚Äôa pas d‚ÄôIP publique, donc il ne peut pas appeler une API publique directement.\nLes ACL r√©seau et les Security Groups par d√©faut autorisent g√©n√©ralement le trafic sortant, donc le blocage ne vient pas d‚Äôune r√®gle ‚Äúdeny‚Äù, mais du chemin r√©seau manquant.\nLa solution standard pour donner un acc√®s Internet sortant √† des ressources en sous-r√©seau priv√© est un NAT Gateway (ou NAT instance) plac√© dans un sous-r√©seau public.\nOn ajoute ensuite une route dans la table de routage du sous-r√©seau priv√© : 0.0.0.0/0 -> NAT Gateway.\nAinsi, Lambda peut appeler les API publiques en sortant via le NAT, tout en restant dans un sous-r√©seau priv√©.\nDonc la bonne r√©ponse est C.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:407:d26a33b9bd5ee01a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 407,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an ecommerce web application that uses an on-premises MySQL database as a data store. The company migrates the on-premises MySQL database to Amazon RDS for MySQL.A developer needs to configure the application's access to the RDS for MySQL database. The developer's solution must not use long term credentials.Which solution will meet these requirements?",
      "choices": {
        "A": "Enable IAM database authentication on the RDS for MySQL DB instance. Create an IAM role that has the minimum required permissions. Assign the role to the application.",
        "B": "Store the MySQL credentials as secrets in AWS Secrets Manager. Create an IAM role that has the minimum required permissions to retrieve the secrets. Assign the role to the application.",
        "C": "Configure the MySQL credentials as environment variables that are available at runtime for the application.",
        "D": "Store the MySQL credentials as SecureString parameters in AWS Systems Manager Parameter Store. Create an IAM role that has the minimum required permissions to retrieve the parameters. Assign the role to the application."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144295-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 21, 2024, 3:11 a.m.",
      "textHash": "d26a33b9bd5ee01a",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Objectif : se connecter √† Amazon RDS for MySQL sans ¬´ identifiants longue dur√©e ¬ª (mot de passe stock√© et valable longtemps).\nRDS for MySQL est une base MySQL g√©r√©e par AWS. Une appli doit s‚Äôauthentifier pour y acc√©der.\nAvec l‚Äôauthentification IAM pour la base (IAM Database Authentication), l‚Äôappli n‚Äôutilise pas un mot de passe MySQL fixe.\n√Ä la place, elle obtient un jeton de connexion temporaire sign√© par AWS (valable quelques minutes), donc pas de secret durable √† g√©rer.\nOn cr√©e un r√¥le IAM (une identit√© AWS) avec le minimum de droits, puis l‚Äôappli assume ce r√¥le pour g√©n√©rer le jeton.\nC‚Äôest exactement ce que demande l‚Äô√©nonc√© : acc√®s √† la base sans identifiants permanents.\nB et D stockent quand m√™me un mot de passe MySQL (m√™me s‚Äôil est chiffr√©), donc ce sont toujours des identifiants longue dur√©e.\nC met le mot de passe en variables d‚Äôenvironnement : c‚Äôest aussi un secret durable et souvent moins s√ªr.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:307:74da4c4861cb4c5f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 307,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to deploy an application in three AWS Regions by using AWS CloudFormation. Each Region will use an AWS Elastic Beanstalk environment with an Application Load Balancer (ALB). The developer wants to use AWS Certificate Manager (ACM) to deploy SSL certificates to each ALB.Which solution will meet these requirements?",
      "choices": {
        "A": "Create a certificate in ACM in any one of the Regions. Import the certificate into the ALB that is in each Region.",
        "B": "Create a global certificate in ACM. Update the CloudFormation template to deploy the global certificate to each ALB.",
        "C": "Create a certificate in ACM in each Region. Import the certificate into the ALB for each Region.",
        "D": "Create a certificate in ACM in the us-east-1 Region. Update the CloudFormation template to deploy the certificate to each ALB."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134126-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 18, 2024, 10:45 a.m.",
      "textHash": "74da4c4861cb4c5f",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Un ALB (Application Load Balancer) est un r√©partiteur de trafic ¬´ r√©gional ¬ª : il existe dans une R√©gion AWS pr√©cise.\nACM (AWS Certificate Manager) g√®re les certificats SSL/TLS, mais ses certificats sont aussi ¬´ r√©gionaux ¬ª pour les ALB.\nDonc, un certificat cr√©√© dans une R√©gion (ex: eu-west-1) ne peut pas √™tre directement attach√© √† un ALB dans une autre R√©gion (ex: us-west-2).\nPour avoir HTTPS dans 3 R√©gions, chaque ALB doit avoir un certificat disponible dans sa propre R√©gion.\nLa solution logique est donc de cr√©er (ou demander) un certificat ACM dans chaque R√©gion o√π vous d√©ployez l‚Äôenvironnement Elastic Beanstalk.\nEnsuite, dans chaque R√©gion, vous associez le certificat ACM local √† l‚ÄôALB de cette R√©gion via CloudFormation/Beanstalk.\nLes options parlant de ¬´ certificat global ¬ª ou d‚Äôun seul certificat partag√© entre R√©gions ne fonctionnent pas pour des ALB.\nAinsi, C est correct : un certificat ACM par R√©gion, attach√© √† l‚ÄôALB de la m√™me R√©gion.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu as 3 lyc√©es dans 3 villes diff√©rentes, et que chaque lyc√©e a son propre portail d‚Äôentr√©e (le ‚Äúportail‚Äù = l‚ÄôALB). Tu veux mettre un cadenas officiel sur chaque portail pour que les visiteurs soient s√ªrs que c‚Äôest bien le bon lyc√©e (le cadenas = certificat SSL via ACM).**\n\nConcept : un certificat SSL, c‚Äôest comme un cadenas avec un papier officiel, et il doit √™tre disponible dans la m√™me ville que le portail o√π tu l‚Äôaccroches. ACM ne partage pas automatiquement le m√™me cadenas entre villes.\nPourquoi C : si tu cr√©es un certificat dans CHAQUE ville (chaque Region), alors chaque portail (ALB) peut recevoir son propre cadenas local, sans probl√®me.\nPourquoi pas A/D : prendre un cadenas d‚Äôune seule ville et essayer de l‚Äôaccrocher sur les portails des autres villes ne marche pas, car ils n‚Äôont pas acc√®s √† ce cadenas ‚Äú√† distance‚Äù.\nPourquoi pas B : il n‚Äôexiste pas de ‚Äúcadenas mondial‚Äù ACM que tu poses partout comme √ßa.\nDonc : 1 certificat ACM par Region, puis tu l‚Äôattaches √† l‚ÄôALB de cette Region = C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:290:20fbe1d1f976695b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 290,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs an application on AWS. The application uses an AWS Lambda function that is configured with an Amazon Simple Queue Service (Amazon SQS) queue called high priority queue as the event source. A developer is updating the Lambda function with another SQS queue called low priority queue as the event source. The Lambda function must always read up to 10 simultaneous messages from the high priority queue before processing messages from low priority queue. The Lambda function must be limited to 100 simultaneous invocations.Which solution will meet these requirements?",
      "choices": {
        "A": "Set the event source mapping batch size to 10 for the high priority queue and to 90 for the low priority queue.",
        "B": "Set the delivery delay to 0 seconds for the high priority queue and to 10 seconds for the low priority queue.",
        "C": "Set the event source mapping maximum concurrency to 10 for the high priority queue and to 90 for the low priority queue.",
        "D": "Set the event source mapping batch window to 10 for the high priority queue and to 90 for the low priority queue."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134289-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:07 a.m.",
      "textHash": "20fbe1d1f976695b",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "AWS Lambda ex√©cute du code √† la demande. Avec Amazon SQS (une file de messages), Lambda lit des messages via un ‚Äúevent source mapping‚Äù qui peut limiter combien d‚Äôex√©cutions simultan√©es sont d√©clench√©es par chaque file.\nIci, on veut une priorit√© : toujours traiter jusqu‚Äô√† 10 messages en parall√®le depuis la file ‚Äúhigh priority‚Äù avant de prendre ceux de ‚Äúlow priority‚Äù.\nLa bonne approche est de r√©server une part de la concurrence √† chaque file : 10 pour high priority, et le reste (90) pour low priority.\nLe param√®tre ‚Äúmaximum concurrency‚Äù sur l‚Äôevent source mapping fait exactement cela : il plafonne le nombre d‚Äôinvocations Lambda simultan√©es d√©clench√©es par cette file.\nEn mettant 10 pour high priority, on garantit qu‚Äôelle peut toujours consommer jusqu‚Äô√† 10 messages en parall√®le.\nEn mettant 90 pour low priority, on emp√™che low priority de prendre toute la capacit√© et on respecte la limite totale de 100 invocations.\nLes autres options (batch size, delay, batch window) jouent sur la taille/temps de regroupement ou le d√©lai des messages, pas sur la concurrence r√©serv√©e par file, donc ne garantissent pas la priorit√©.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine avec 2 files: une file \"prioritaire\" (high) et une file \"normale\" (low). Les cuisiniers (Lambda) prennent des commandes (messages) dans ces files.**\n\nConcept: une file SQS = une liste de commandes √† traiter. Lambda = des cuisiniers qui se d√©clenchent pour cuisiner. \"Invocations\" = combien de cuisiniers travaillent en m√™me temps.\nOn veut 2 r√®gles: (1) toujours avoir jusqu‚Äô√† 10 cuisiniers occup√©s sur la file prioritaire avant de servir la normale, (2) au total, max 100 cuisiniers.\nLa bonne solution est C: on fixe une \"concurrence max\" par file: 10 pour la file prioritaire et 90 pour la normale.\nComme √ßa, la file prioritaire a toujours 10 places r√©serv√©es: d√®s qu‚Äôil y a des commandes urgentes, elles prennent ces 10 cuisiniers.\nEt la file normale ne peut jamais d√©passer 90 cuisiniers, donc 10 + 90 = 100 au total.\nA est faux: \"batch size\" = combien de commandes un cuisinier prend d‚Äôun coup, pas combien de cuisiniers travaillent.\nB est faux: retarder la file normale ne garantit pas une priorit√© constante.\nD est faux: \"batch window\" = combien de temps on attend pour remplir un lot, pas une priorit√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:288:9f7af34062be0a48",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 288,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an Amazon DynamoDB table by using the AWS CLI. The DynamoDB table must use server-side encryption with an AWS owned encryption key.How should the developer create the DynamoDB table to meet these requirements?",
      "choices": {
        "A": "Create an AWS Key Management Service (AWS KMS) customer managed key. Provide the key's Amazon Resource Name (ARN) in the KMSMasterKeyId parameter during creation of the DynamoDB table.",
        "B": "Create an AWS Key Management Service (AWS KMS) AWS managed key. Provide the key's Amazon Resource Name (ARN) in the KMSMasterKeyId parameter during creation of the DynamoDB table.",
        "C": "Create an AWS owned key. Provide the key's Amazon Resource Name (ARN) in the KMSMasterKeyId parameter during creation of the DynamoDB table.",
        "D": "Create the DynamoDB table with the default encryption options."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134287-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:02 a.m.",
      "textHash": "9f7af34062be0a48",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "DynamoDB est une base de donn√©es NoSQL g√©r√©e par AWS. Par d√©faut, AWS chiffre (encrypt) automatiquement les donn√©es ‚Äúau repos‚Äù c√¥t√© serveur.\nUne ‚Äúcl√© AWS owned‚Äù (cl√© d√©tenue par AWS) signifie que c‚Äôest AWS qui g√®re enti√®rement la cl√© : vous ne la cr√©ez pas, vous ne voyez pas son ARN, et vous ne la fournissez pas.\nDans DynamoDB, si vous ne sp√©cifiez aucune cl√© KMS, le service utilise l‚Äôoption de chiffrement par d√©faut, qui correspond justement √† une cl√© d√©tenue par AWS.\nLes options A et B impliquent de choisir une cl√© KMS (customer managed ou AWS managed) et de fournir un ARN via KMSMasterKeyId : ce n‚Äôest pas une cl√© ‚ÄúAWS owned‚Äù.\nL‚Äôoption C est impossible en pratique : on ne peut pas ‚Äúcr√©er‚Äù une cl√© AWS owned ni r√©cup√©rer son ARN pour la passer en param√®tre.\nDonc, pour r√©pondre au besoin ‚Äúserver-side encryption avec une cl√© AWS owned‚Äù, il faut simplement cr√©er la table avec les options de chiffrement par d√©faut.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine le casier de ton coll√®ge : tes affaires sont rang√©es dedans et le coll√®ge met automatiquement un cadenas standard pour prot√©ger tout le monde.**\n\nConcept : le ‚Äúchiffrement c√¥t√© serveur‚Äù c‚Äôest comme mettre un cadenas sur le casier, mais c‚Äôest l‚Äô√©cole (AWS) qui le fait pour toi, sans que tu t‚Äôen occupes.\nIci on te demande un cadenas ‚Äúappartenant √† l‚Äô√©cole‚Äù (AWS owned key) : c‚Äôest le cadenas par d√©faut, g√©r√© enti√®rement par AWS.\nDonc tu n‚Äôas pas √† cr√©er ta propre cl√© (comme fabriquer ton cadenas) ni √† donner un num√©ro de cadenas (ARN) √† la cr√©ation.\nA et B : tu cr√©es/choisis un cadenas sp√©cial (cl√© KMS) et tu dois fournir son identifiant : ce n‚Äôest pas ‚ÄúAWS owned‚Äù, c‚Äôest ‚Äúg√©r√© via KMS‚Äù.\nC : ‚Äúcr√©er une cl√© AWS owned‚Äù n‚Äôa pas de sens : tu ne peux pas la cr√©er ni r√©cup√©rer son identifiant, c‚Äôest AWS qui la g√®re en coulisses.\nDonc la bonne action est : cr√©er la table avec les options de chiffrement par d√©faut.\nR√©ponse : D.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:252:e118797ad98450a2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 252,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company‚Äôs developer is creating an application that uses Amazon API Gateway. The company wants to ensure that only users in the Sales department can use the application. The users authenticate to the application by using federated credentials from a third-party identity provider (IdP) through Amazon Cognito. The developer has set up an attribute mapping to map an attribute that is named Department and to pass the attribute to a custom AWS Lambda authorizer.To test the access limitation, the developer sets their department to Engineering in the IdP and attempts to log in to the application. The developer is denied access. The developer then updates their department to Sales in the IdP and attempts to log in. Again, the developer is denied access. The developer checks the logs and discovers that access is being denied because the developer‚Äôs access token has a department value of Engineering.Which of the following is a possible reason that the developer‚Äôs department is still being reported as Engineering instead of Sales?",
      "choices": {
        "A": "Authorization caching is enabled in the custom Lambda authorizer.",
        "B": "Authorization caching is enabled on the Amazon Cognito user pool.",
        "C": "The IAM role for the custom Lambda authorizer does not have a Department tag.",
        "D": "The IAM role for the Amazon Cognito user pool does not have a Department tag."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124863-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 29, 2023, 2:09 a.m.",
      "textHash": "e118797ad98450a2",
      "rawFormat": "discussion-md",
      "conceptKey": "api_gw_cognito_authorizer",
      "frExplanation": "API Gateway prot√®ge une API et peut appeler un ¬´ Lambda authorizer ¬ª (une fonction AWS Lambda) pour d√©cider si une requ√™te est autoris√©e.\nIci, l‚Äôattribut Department vient de l‚ÄôIdP (fournisseur d‚Äôidentit√©) via Amazon Cognito, puis est envoy√© √† l‚Äôauthorizer.\nLe d√©veloppeur change Department dans l‚ÄôIdP, mais l‚ÄôAPI refuse encore car le token/autorisation vu par l‚Äôauthorizer contient toujours ¬´ Engineering ¬ª.\nUne cause fr√©quente est le cache d‚Äôautorisation de l‚Äôauthorizer : API Gateway peut m√©moriser la d√©cision (Allow/Deny) et les valeurs associ√©es pendant un temps (TTL).\nSi le cache est activ√©, API Gateway peut r√©utiliser l‚Äôancien r√©sultat bas√© sur l‚Äôancien token/claims, m√™me apr√®s modification dans l‚ÄôIdP.\nDonc l‚Äôauthorizer continue √† ‚Äúvoir‚Äù Engineering jusqu‚Äô√† expiration du cache ou changement de cl√© de cache.\nLes tags IAM (choix C/D) n‚Äôont rien √† voir avec les attributs Department dans un token Cognito.\nLe cache du user pool Cognito (choix B) n‚Äôest pas le m√©canisme typique qui fige la d√©cision d‚Äôacc√®s c√¥t√© API Gateway.\nLa raison possible est donc : le caching est activ√© dans le Lambda authorizer (A).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:378:76a421e1409f59e0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 378,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a website that displays a daily newsletter. When a user visits the website, an AWS Lambda function processes the browser's request and queries the company's on-premises database to obtain the current newsletter. The newsletters are stored in English. The Lambda function uses the Amazon Translate TranslateText API operation to translate the newsletters, and the translation is displayed to the user.Due to an increase in popularity, the website's response time has slowed. The database is overloaded. The company cannot change the database and needs a solution that improves the response time of the Lambda function.Which solution meets these requirements?",
      "choices": {
        "A": "Change to asynchronous Lambda function invocation.",
        "B": "Cache the translated newsletters in the Lambda/tmp directory.",
        "C": "Enable TranslateText API caching.",
        "D": "Change the Lambda function to use parallel processing."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143767-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 6:52 a.m.",
      "textHash": "76a421e1409f59e0",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:13f309c6",
      "frExplanation": "Le probl√®me vient surtout de la base de donn√©es sur site : chaque visite d√©clenche une requ√™te vers la base, qui finit surcharg√©e et ralentit tout.\nAWS Lambda est un code qui s‚Äôex√©cute √† la demande. Il peut r√©utiliser le m√™me ‚Äúconteneur‚Äù pendant un certain temps entre plusieurs appels.\nLe dossier /tmp dans Lambda est un stockage local temporaire (jusqu‚Äô√† 512 Mo) qui peut persister tant que le m√™me conteneur est r√©utilis√©.\nEn mettant en cache dans /tmp la newsletter d√©j√† traduite (par exemple par date + langue), les appels suivants peuvent r√©pondre sans recontacter la base ni retraduire.\nR√©sultat : moins de requ√™tes vers la base (elle respire) et une r√©ponse plus rapide pour l‚Äôutilisateur.\nA (asynchrone) ne convient pas : un site web attend une r√©ponse imm√©diate.\nC n‚Äôexiste pas comme option simple ‚Äúcaching‚Äù c√¥t√© API TranslateText.\nD (parall√©liser) risque d‚Äôaugmenter encore la charge sur la base au lieu de la r√©duire.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le CDI de ton lyc√©e : chaque √©l√®ve vient demander ¬´ le journal du jour ¬ª, et le documentaliste doit aller le chercher dans une r√©serve lente √† chaque fois. Si tu fais une photocopie du journal et tu la poses sur le comptoir, les prochains √©l√®ves la prennent tout de suite sans retourner √† la r√©serve.**\n\nIci, la base de donn√©es ¬´ sur place ¬ª (on‚Äëpremises) est la r√©serve lente et elle est surcharg√©e. La fonction Lambda, c‚Äôest l‚Äô√©l√®ve qui vient chercher le texte et le fait traduire. Pour acc√©l√©rer, il faut √©viter de redemander et retraduire la m√™me newsletter √† chaque visite. La r√©ponse B dit : garder en cache la newsletter d√©j√† traduite dans le dossier /tmp de Lambda (comme la photocopie sur le comptoir). Du coup, si plusieurs visiteurs demandent la m√™me langue, Lambda relit la version d√©j√† pr√™te et n‚Äôappelle presque plus la base de donn√©es. A n‚Äôaide pas l‚Äôutilisateur √† voir plus vite (asynchrone = tu attends ailleurs). C n‚Äôexiste pas vraiment comme bouton magique de cache. D ferait encore plus de demandes en m√™me temps et peut empirer la surcharge.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:365:894106dfbcf0b77f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 365,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Lambda function that is invoked by messages to an Amazon Simple Notification Service (Amazon SNS) topic. The messages represent customer data updates from a customer relationship management (CRM) systemThe developer wants the Lambda function to process only the messages that pertain to email address changes. Additional subscribers to the SNS topic will process any other messages.Which solution will meet these requirements in the LEAST development effort?",
      "choices": {
        "A": "Use Lambda event filtering to allow only messages that are related to email address changes to invoke the Lambda function.",
        "B": "Use an SNS filter policy on the Lambda function subscription to allow only messages that are related to email address changes to invoke the Lambda function.",
        "C": "Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the SQS queue with a filter policy to allow only messages that are related to email address changes.Connect the SQS queue to the Lambda function.",
        "D": "Configure the Lambda code to check the received message. If the message is not related to an email address change, configure the Lambda function to publish the message back to the SNS topic for the other subscribers to process."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143748-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:25 a.m.",
      "textHash": "894106dfbcf0b77f",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, Amazon SNS est un service qui ‚Äúdiffuse‚Äù des messages √† plusieurs abonn√©s (subscribers). AWS Lambda est une fonction qui s‚Äôex√©cute automatiquement quand elle re√ßoit un √©v√©nement (ici, un message SNS).\nLe besoin: cette Lambda ne doit traiter que les messages qui concernent un changement d‚Äôadresse e‚Äëmail, tandis que les autres abonn√©s doivent recevoir le reste.\nLa solution la plus simple est de filtrer au niveau de l‚Äôabonnement SNS vers Lambda, sans √©crire de code suppl√©mentaire.\nAvec une ‚ÄúSNS filter policy‚Äù, SNS regarde des attributs du message (ex: type=EmailChanged) et n‚Äôenvoie √† cette Lambda que ceux qui correspondent.\nAinsi, la Lambda n‚Äôest m√™me pas invoqu√©e pour les autres messages: moins de co√ªt, moins de complexit√©, et aucun traitement inutile.\nLes autres abonn√©s peuvent avoir leurs propres filtres ou recevoir tous les messages.\nLes options avec SQS ajoutent un composant en plus (plus de configuration). V√©rifier dans le code ou republier (option D) augmente le d√©veloppement et peut cr√©er des boucles.\nDonc la meilleure r√©ponse avec le moins d‚Äôeffort est d‚Äôutiliser une filter policy sur l‚Äôabonnement SNS de la Lambda.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un tableau d‚Äôaffichage au lyc√©e (SNS) o√π le CRM colle des annonces sur les √©l√®ves. Plusieurs clubs (abonn√©s) lisent ces annonces. Ton club ‚ÄúEmail‚Äù (Lambda) ne veut lire QUE les annonces qui parlent d‚Äôun changement d‚Äôadresse email.**\n\nConcept : SNS = tableau d‚Äôaffichage qui envoie la m√™me annonce √† tous les abonn√©s. Lambda = un √©l√®ve/club qui r√©agit quand il re√ßoit une annonce.\nPour √©viter de ‚Äúlire puis jeter‚Äù des annonces, le plus simple est de mettre un filtre √† l‚Äôentr√©e.\nAvec B, on met une r√®gle sur l‚Äôabonnement du club ‚ÄúEmail‚Äù : ‚Äúne me donne que les annonces tagu√©es ‚Äòemail change‚Äô ‚Äù.\nDu coup, Lambda n‚Äôest d√©clench√©e que pour ces messages, et les autres clubs re√ßoivent le reste.\nC ajoute une bo√Æte aux lettres interm√©diaire (SQS) inutile ici, donc plus de boulot.\nD oblige √† coder du tri et √† renvoyer des annonces, c‚Äôest plus compliqu√© et risqu√©.\nA parle de filtrage c√¥t√© Lambda, mais le plus direct/standard ici est le filtre sur l‚Äôabonnement SNS.\nDonc la bonne r√©ponse avec le moins d‚Äôeffort est B.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:452:432673ad6cdfc219",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 452,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to fix an AWS CodeDeploy deployment that failed. During the failed deployment, the developer received the following error message:‚ÄúThe overall deployment failed because too many individual instances failed deployment, too few healthy instances are available for deployment, or some instances in your deployment group are experiencing problems. (Error code: HEALTH-CONSTRAINTS)‚ÄùWhat are the possible causes of the failed deployment? (Choose two.)",
      "choices": {
        "A": "The CodeDeploy agent was not running on the instances that CodeDeploy was trying to deploy to.",
        "B": "The unified Amazon CloudWatch agent was not running on the instances that CodeDeploy was trying to deploy to.",
        "C": "The developer‚Äôs IAM role did not have the necessary permissions to perform code deployment to the instances.",
        "D": "CodeDeploy was trying to deploy to instances that were attached to an IAM instance profile that did not have the required permissions.",
        "E": "CodeDeploy was trying to deploy to instances that were not set up with correct CodeDeploy health checks."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/150946-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 7, 2024, 4:03 p.m.",
      "textHash": "432673ad6cdfc219",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "AWS CodeDeploy sert √† d√©ployer automatiquement une application sur des serveurs (instances EC2) ou d‚Äôautres cibles.\nL‚Äôerreur HEALTH-CONSTRAINTS signifie que CodeDeploy n‚Äôa pas assez de cibles ‚Äúsaines‚Äù pour continuer : trop d‚Äôinstances ont √©chou√© ou sont consid√©r√©es comme indisponibles.\nCause possible n¬∞1 (A) : l‚Äôagent CodeDeploy est un petit programme install√© sur chaque instance qui re√ßoit les ordres (t√©l√©charger, copier, ex√©cuter les scripts). S‚Äôil n‚Äôest pas d√©marr√©, l‚Äôinstance ne peut pas d√©ployer et sera marqu√©e en √©chec.\nCause possible n¬∞2 (D) : les instances utilisent un ‚ÄúIAM instance profile‚Äù (un r√¥le attach√© √† la machine) pour acc√©der √† AWS (ex: lire un bundle dans S3). Si ce r√¥le n‚Äôa pas les permissions n√©cessaires, l‚Äôinstance √©choue et devient ‚Äúnon saine‚Äù pour le d√©ploiement.\nPourquoi pas B : l‚Äôagent CloudWatch sert surtout √† envoyer des m√©triques/logs, pas √† ex√©cuter un d√©ploiement CodeDeploy.\nPourquoi pas C : les permissions du r√¥le du d√©veloppeur affectent le lancement du d√©ploiement, mais l‚Äôerreur HEALTH-CONSTRAINTS est typiquement li√©e √† l‚Äô√©tat/√©checs des instances cibles.\nPourquoi pas E : CodeDeploy ne d√©pend pas de ‚Äúhealth checks CodeDeploy‚Äù sp√©cifiques ; la sant√© vient plut√¥t des √©checs de d√©ploiement et/ou des checks du load balancer/Auto Scaling.\nDonc les deux causes plausibles sont : agent CodeDeploy arr√™t√© (A) et permissions insuffisantes du r√¥le attach√© aux instances (D).",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une prof qui veut distribuer un nouveau sujet d‚Äôexamen √† toute une classe. Chaque √©l√®ve doit avoir son ‚Äúcahier de liaison‚Äù ouvert pour recevoir les consignes. Si trop d‚Äô√©l√®ves ne l‚Äôont pas, la prof arr√™te et dit: ‚ÄúJe ne peux pas continuer, pas assez d‚Äô√©l√®ves pr√™ts.‚Äù**\n\nConcept: CodeDeploy, c‚Äôest la prof qui envoie une mise √† jour. Les ‚Äúinstances‚Äù, ce sont les √©l√®ves. L‚Äô‚Äúagent CodeDeploy‚Äù, c‚Äôest le cahier de liaison qui re√ßoit et ex√©cute les consignes.\nErreur HEALTH-CONSTRAINTS: √ßa veut dire ‚Äútrop de monde n‚Äôest pas en √©tat de recevoir la mise √† jour, donc on stoppe pour √©viter le bazar‚Äù.\nPourquoi A: si l‚Äôagent CodeDeploy ne tourne pas, l‚Äô√©l√®ve ne peut pas recevoir/faire la mise √† jour. Donc plusieurs √©l√®ves √©chouent, et il ne reste ‚Äúpas assez d‚Äô√©l√®ves OK‚Äù pour continuer.\nPourquoi pas B: l‚Äôagent CloudWatch, c‚Äôest plut√¥t pour surveiller (comme un surveillant), pas pour distribuer le sujet.\nPourquoi pas C/D/E ici: ce sont des probl√®mes de droits ou de r√©glages, mais l‚Äôerreur parle surtout d‚Äôinstances ‚Äúpas pr√™tes/qui √©chouent‚Äù, typique quand l‚Äôagent CodeDeploy n‚Äôest pas lanc√©.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:424:cdddfa047e9478f1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 424,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application integrating an Amazon API Gateway with an AWS Lambda function. When calling the API, the developer receives the following error:Wed Nov 08 01:13:00 UTC 2017 : Method completed with status: 502What should the developer do to resolve the error?",
      "choices": {
        "A": "Change the HTTP endpoint of the API to an HTTPS endpoint.",
        "B": "Change the format of the payload sent to the API Gateway.",
        "C": "Change the format of the Lambda function response to the API call.",
        "D": "Change the authorization header in the API call to access the Lambda function."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148937-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 9, 2024, 3:32 p.m.",
      "textHash": "cdddfa047e9478f1",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:a7a2fa09",
      "frExplanation": "Une erreur 502 dans API Gateway signifie souvent ¬´ mauvaise r√©ponse du backend ¬ª (ici, la fonction AWS Lambda).\nAPI Gateway est la ‚Äúporte d‚Äôentr√©e‚Äù HTTP; Lambda ex√©cute votre code et doit renvoyer une r√©ponse dans un format pr√©cis.\nSi Lambda plante, renvoie un objet mal form√©, ou ne renvoie pas les champs attendus, API Gateway ne sait pas construire la r√©ponse HTTP et renvoie 502.\nPour une int√©gration Lambda proxy, la r√©ponse doit typiquement contenir: statusCode (nombre), headers (optionnel), et body (texte JSON sous forme de cha√Æne).\nExemple: {\"statusCode\":200,\"body\":\"{\\\"message\\\":\\\"ok\\\"}\"}.\nDonc il faut corriger le format de la r√©ponse de la Lambda (et v√©rifier les logs CloudWatch pour voir l‚Äôerreur exacte).\nPasser de HTTP √† HTTPS (A) ne corrige pas une r√©ponse Lambda invalide.\nChanger le payload envoy√© (B) ou l‚Äôen-t√™te d‚Äôautorisation (D) donnerait plut√¥t des erreurs 400/401/403, pas typiquement un 502.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine le self du lyc√©e : tu commandes un menu au guichet (API) et la cuisine (Lambda) pr√©pare ton plateau.**\n\nConcept : le guichet attend un plateau pr√©sent√© d‚Äôune fa√ßon pr√©cise (assiette + ticket + couverts). Si la cuisine renvoie un truc bizarre (plat sans assiette, ou sans ticket), le guichet ne sait pas le donner et affiche ‚Äúerreur‚Äù.\nLe code 502, ici, c‚Äôest souvent ‚Äúle guichet a re√ßu une r√©ponse inutilisable de la cuisine‚Äù.\nDonc la bonne action est de changer la fa√ßon dont la cuisine renvoie le plateau : la r√©ponse de Lambda doit √™tre au bon format (ex : un code de statut, des en-t√™tes, et un corps de message).\nA (HTTP/HTTPS) c‚Äôest juste changer la route, pas la pr√©sentation du plateau.\nB (payload envoy√©) concerne la commande, mais l‚Äôerreur arrive quand le guichet finit et re√ßoit la r√©ponse.\nD (autorisation) donnerait plut√¥t un refus d‚Äôacc√®s, pas une r√©ponse ‚Äúcass√©e‚Äù.\nDonc C : corriger le format de r√©ponse de Lambda pour que l‚ÄôAPI puisse la comprendre.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:426:c00d9f068f6630dc",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 426,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a microservice that uses AWS Lambda to process messages from an Amazon Simple Queue Service (Amazon SQS) standard queue. The Lambda function calls external APIs to enrich the SQS message data before loading the data into an Amazon Redshift data warehouse. The SQS queue must handle a maximum of 1,000 messages per second.During initial testing, the Lambda function repeatedly inserted duplicate data into the Amazon Redshift table. The duplicate data led to a problem with data analysis. All duplicate messages were submitted to the queue within 1 minute of each other.How should the developer resolve this issue?",
      "choices": {
        "A": "Create an SQS FIFO queue. Enable message deduplication on the SQS FIFO queue.",
        "B": "Reduce the maximum Lambda concurrency that the SQS queue can invoke.",
        "C": "Use Lambda's temporary storage to keep track of processed message identifiers",
        "D": "Configure a message group ID for every sent message. Enable message deduplication on the SQS standard queue."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148955-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 10, 2024, 7:23 a.m.",
      "textHash": "c00d9f068f6630dc",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, une fonction AWS Lambda (code ex√©cut√© √† la demande) lit des messages d‚Äôune file Amazon SQS (service de file d‚Äôattente). Avec une file SQS ¬´ standard ¬ª, SQS peut livrer un m√™me message plus d‚Äôune fois (at-least-once), surtout si le traitement prend du temps ou si un retry arrive : cela cr√©e des doublons dans Redshift (entrep√¥t de donn√©es).\nLe fait que les doublons aient √©t√© envoy√©s dans la m√™me minute indique un sc√©nario typique de re-livraison/duplication c√¥t√© SQS.\nLa solution la plus simple est d‚Äôutiliser une file SQS FIFO, qui garantit l‚Äôordre et permet la d√©duplication.\nEn activant la d√©duplication FIFO, SQS ignore automatiquement les messages identiques (m√™me ID de d√©duplication ou contenu, selon le mode) pendant une fen√™tre de 5 minutes.\nAinsi, m√™me si le producteur envoie deux fois le m√™me message, Lambda ne le recevra qu‚Äôune fois, √©vitant les insertions en double dans Redshift.\nR√©duire la concurrence Lambda ne supprime pas les doublons, et stocker des IDs dans /tmp n‚Äôest pas fiable (contenu √©ph√©m√®re, pas partag√© entre instances).\nOn ne peut pas activer la d√©duplication sur une file SQS standard, et les ‚Äúmessage group IDs‚Äù sont une notion FIFO.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : des √©l√®ves d√©posent des tickets de commande dans une bo√Æte, et la cuisine les traite.**\n\nConcept : une ‚Äúfile‚Äù SQS standard, c‚Äôest comme une bo√Æte o√π des tickets peuvent parfois √™tre livr√©s deux fois (la cantine pr√©f√®re ‚Äúau moins une fois‚Äù plut√¥t que d‚Äôen perdre). Donc tu peux recevoir des doublons.\nIci, en 1 minute, des tickets identiques sont arriv√©s et la cuisine (Lambda) a cuisin√© deux fois, puis a mis deux fois la m√™me info dans le cahier de comptes (Redshift) ‚Üí analyses fausses.\nSolution : passer √† une file SQS FIFO, c‚Äôest une bo√Æte ‚Äúun par un, dans l‚Äôordre‚Äù, avec un videur anti-doublon.\nLa d√©duplication FIFO, c‚Äôest comme un tampon ‚Äúd√©j√† servi‚Äù sur un ticket : si le m√™me ticket revient dans la fen√™tre de temps, il est ignor√©.\nDonc A est la bonne r√©ponse : FIFO + d√©duplication emp√™che les messages identiques d‚Äô√™tre trait√©s deux fois.\nB ralentit juste la cuisine, mais n‚Äôemp√™che pas les doublons.\nC est fragile : la m√©moire temporaire peut dispara√Ætre, donc tu oublies les tickets d√©j√† vus.\nD est faux : la d√©duplication n‚Äôexiste pas sur une file standard.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:430:b52b1cb5caaae8b2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 430,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that is deployed on AWS Elastic Beanstalk. The application generates user-specific PDFs and stores the PDFs in an Amazon S3 bucket. The application then uses Amazon Simple Email Service (Amazon SES) to send the PDFs by email to subscribers.Users no longer access the PDFs 90 days after the PDFs are generated. The S3 bucket is not versioned and contains many obsolete PDFs.A developer must reduce the number of files in the S3 bucket by removing PDFs that are older than 90 days.Which solution will meet this requirement with the LEAST development effort?",
      "choices": {
        "A": "Update the application code. In the code, add a rule to scan all the objects in the S3 bucket every day and to delete objects after 90 days.",
        "B": "Create an AWS Lambda function. Program the Lambda function to scan all the objects in the S3 bucket every day and to delete objects after 90 days.",
        "C": "Create an S3 Lifecycle rule for the S3 bucket to expire objects after 90 days.",
        "D": "Partition the S3 objects with a // key prefix. Create an AWS Lambda function to remove objects that have prefixes that have reached the expiration date."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/146863-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Sept. 3, 2024, 6:20 p.m.",
      "textHash": "b52b1cb5caaae8b2",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:s3:ad6d0dfa",
      "frExplanation": "Ici, les PDF sont stock√©s dans Amazon S3 (un service de stockage de fichiers). On veut supprimer automatiquement ceux qui ont plus de 90 jours, avec le moins de code possible.\nLa meilleure option est une r√®gle S3 Lifecycle : c‚Äôest une configuration native de S3 qui peut ‚Äúexpirer‚Äù (supprimer) les objets apr√®s un certain √¢ge.\nAvec une r√®gle Lifecycle, vous n‚Äô√©crivez aucun programme, vous d√©finissez juste ‚Äúsupprimer apr√®s 90 jours‚Äù et S3 s‚Äôoccupe du nettoyage en continu.\nLes options A et B demandent de d√©velopper et maintenir du code qui liste tous les objets chaque jour, ce qui est plus long, plus co√ªteux et plus risqu√© (permissions, erreurs, temps d‚Äôex√©cution).\nL‚Äôoption D ajoute encore de la complexit√© (pr√©fixes + Lambda) sans n√©cessit√©.\nComme le bucket n‚Äôest pas versionn√©, l‚Äôexpiration supprime directement les objets, ce qui correspond exactement au besoin.\nDonc la solution la plus simple et avec le moins d‚Äôeffort est de cr√©er une r√®gle S3 Lifecycle pour expirer les objets apr√®s 90 jours.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:431:c9f8d606b3fc2e0a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 431,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is troubleshooting an application. The application includes several AWS Lambda functions that invoke an Amazon API Gateway API. The API Gateway's method request is set up to use an Amazon Cognito authorizer for authentication.All the Lambda functions pass the user ID as part of the Authorization header to the API Gateway API. The API Gateway API returns a 403 status code for all GET requests.How should the developer resolve this issue?",
      "choices": {
        "A": "Modify the client GET request to include a valid API key in the Authorization header.",
        "B": "Modify the client GET request to include a valid token in the Authorization header.",
        "C": "Update the resource policy for the API Gateway API to allow the execute-api:Invoke action.",
        "D": "Modify the client to send an OPTIONS preflight request before the GET request."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148103-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Sept. 26, 2024, 5:18 a.m.",
      "textHash": "c9f8d606b3fc2e0a",
      "rawFormat": "discussion-md",
      "conceptKey": "api_gw_cognito_authorizer",
      "frExplanation": "API Gateway est la ‚Äúporte d‚Äôentr√©e‚Äù HTTP de votre API, et un authorizer Amazon Cognito sert √† v√©rifier l‚Äôidentit√© via un jeton (token) JWT.\nIci, la m√©thode est prot√©g√©e par Cognito : API Gateway attend un vrai token d‚Äôauthentification dans l‚Äôen-t√™te HTTP Authorization (souvent au format \"Bearer <token>\").\nLes fonctions Lambda envoient seulement un user ID dans Authorization : ce n‚Äôest pas une preuve d‚Äôidentit√©, donc Cognito ne peut pas valider la requ√™te.\nQuand l‚Äôauthorizer √©choue (token manquant/invalide/expir√©), API Gateway renvoie typiquement 403 (Forbidden) pour les appels comme GET.\nLa solution est donc de modifier le client pour envoyer un token Cognito valide (ID token ou access token selon la config) dans Authorization.\nUn API key ne remplace pas Cognito (c‚Äôest pour l‚Äôusage/quotas), une resource policy concerne l‚Äôacc√®s r√©seau/compte, et OPTIONS pr√©flight concerne surtout CORS, pas l‚Äôauthentification.\nDonc la bonne r√©ponse est B.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine l‚Äôentr√©e du lyc√©e avec un vigile. Pour entrer, tu dois montrer ton badge officiel du lyc√©e (avec photo et date). Dire juste ton pr√©nom ou ton num√©ro d‚Äô√©l√®ve ne suffit pas.**\n\nConcept : Amazon API Gateway, c‚Äôest la porte d‚Äôentr√©e. Amazon Cognito, c‚Äôest le bureau qui fabrique des badges (des ‚Äútokens‚Äù) pour prouver qui tu es. Les fonctions Lambda, ce sont des √©l√®ves messagers qui veulent passer la porte.\nIci, les Lambda mettent juste l‚ÄôID utilisateur dans ‚ÄúAuthorization‚Äù. C‚Äôest comme montrer un papier avec ton num√©ro d‚Äô√©l√®ve : le vigile refuse ‚Üí 403 (interdit).\nLa bonne solution est B : envoyer un token valide dans le header Authorization, comme un vrai badge sign√© par le lyc√©e.\nA est faux : une ‚ÄúAPI key‚Äù c‚Äôest plut√¥t un ticket d‚Äôacc√®s, pas une preuve d‚Äôidentit√© pour Cognito.\nC ne r√®gle pas l‚Äôidentit√© : c‚Äôest une r√®gle du b√¢timent, pas ton badge.\nD (OPTIONS) sert surtout aux navigateurs pour demander la permission avant, pas √† prouver qui tu es.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:438:80d8f82cd91f6321",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 438,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a virtual reality (VR) game. The game has a serverless backend that consists of Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. Recently, the company noticed a sudden increase of new users globally. The company also noticed delays in the retrieval of user data.Which AWS service or feature can the company use to reduce the database response time to microseconds?",
      "choices": {
        "A": "Amazon ElastiCache",
        "B": "DynamoDB Accelerator (DAX)",
        "C": "DynamoDB auto scaling",
        "D": "Amazon CloudFront"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/150938-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 7, 2024, 12:33 p.m.",
      "textHash": "80d8f82cd91f6321",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:22fba9cb",
      "frExplanation": "Le probl√®me vient des lectures dans la base : les utilisateurs sont plus nombreux et r√©partis dans le monde, donc r√©cup√©rer les donn√©es depuis DynamoDB peut prendre plus de temps.\nAmazon DynamoDB est une base NoSQL g√©r√©e. Pour acc√©l√©rer les lectures, on peut ajouter un cache (m√©moire) devant la base.\nDynamoDB Accelerator (DAX) est un cache en m√©moire sp√©cialement con√ßu pour DynamoDB : il garde les donn√©es les plus demand√©es et r√©pond tr√®s vite.\nDAX peut r√©duire le temps de r√©ponse des lectures √† l‚Äô√©chelle de la microseconde, sans changer beaucoup le code (m√™me API que DynamoDB via un client DAX).\nElastiCache est aussi un cache, mais il faut le g√©rer et l‚Äôint√©grer soi‚Äëm√™me ; il n‚Äôest pas ‚Äúplug-and-play‚Äù pour DynamoDB comme DAX.\nL‚Äôauto scaling DynamoDB augmente la capacit√© (d√©bit) mais ne garantit pas des r√©ponses en microsecondes.\nCloudFront acc√©l√®re la distribution de contenu web (fichiers, r√©ponses HTTP) mais ne sert pas √† acc√©l√©rer directement les requ√™tes DynamoDB.\nDonc la meilleure option pour des lectures en microsecondes est DAX (r√©ponse B).",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:448:54ccce1cf8898f53",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 448,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application on AWS. The application has an Amazon API Gateway API that sends requests to an AWS Lambda function. The API is experiencing increased latency because the Lambda function has limited available CPU to fulfill the requests.Before the developer deploys the API into production, the developer must configure the Lambda function to have more CPU.Which solution will meet this requirement?",
      "choices": {
        "A": "Increase the virtual CPU (vCPU) cores quota of the Lambda function.",
        "B": "Increase the amount of memory that is allocated to the Lambda function.",
        "C": "Increase the ephemeral storage size of the Lambda function.",
        "D": "Increase the timeout value of the Lambda function."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/150940-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 7, 2024, 1:32 p.m.",
      "textHash": "54ccce1cf8898f53",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:8ff8cb2f",
      "frExplanation": "API Gateway re√ßoit les appels HTTP et d√©clenche une fonction AWS Lambda (du code qui s‚Äôex√©cute sans serveur √† g√©rer).\nDans Lambda, la puissance CPU n‚Äôest pas r√©gl√©e directement : elle augmente automatiquement quand on augmente la m√©moire allou√©e.\nDonc si la fonction manque de CPU et devient lente, la solution pratique est d‚Äôaugmenter la m√©moire (ex. 512 Mo -> 1024 Mo), ce qui donne aussi plus de CPU.\nA est faux : on ne choisit pas un nombre de vCPU pour une fonction Lambda comme sur une VM.\nC est hors sujet : le stockage √©ph√©m√®re sert surtout aux fichiers temporaires (/tmp), pas √† acc√©l√©rer le CPU.\nD est hors sujet : augmenter le timeout √©vite un √©chec par d√©passement de temps, mais ne rend pas l‚Äôex√©cution plus rapide.\nAinsi, augmenter la m√©moire est la bonne fa√ßon d‚Äôobtenir plus de CPU et r√©duire la latence.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un stand de cr√™pes √† la kermesse du lyc√©e. Les √©l√®ves (les requ√™tes) arrivent par la file d‚Äôattente (l‚ÄôAPI). Le cuisinier (Lambda) pr√©pare chaque cr√™pe. S‚Äôil est lent, la file s‚Äôallonge (latence).**\n\nConcept : dans Lambda, ‚Äúm√©moire‚Äù = taille du stand + puissance du cuisinier. Plus tu donnes de m√©moire, plus AWS donne aussi de ‚Äúmuscles‚Äù (CPU) pour travailler vite. Ici, le probl√®me dit clairement : pas assez de CPU, donc il faut donner plus de muscles. La seule option qui augmente CPU dans Lambda, c‚Äôest d‚Äôaugmenter la m√©moire (B). A est faux : tu ne choisis pas des ‚Äúc≈ìurs CPU‚Äù s√©par√©ment comme √ßa. C (stockage temporaire) = plus de place pour poser des ingr√©dients, pas plus rapide. D (timeout) = laisser plus de temps avant d‚Äôabandonner, mais √ßa ne rend pas le cuisinier plus rapide.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:415:7b26a4624dee75a4",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 415,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that uses an AWS Lambda function to consume messages from an Amazon Simple Queue Service (Amazon SQS) queue. The SQS queue is configured with a dead-letter queue. Due to a defect in the application, AWS Lambda failed to process some messages. A developer fixed the bug and wants to process the failed messages again.How should the developer resolve this issue?",
      "choices": {
        "A": "Use the SendMessageBatch API to send messages from the dead-letter queue to the original SQS queue.",
        "B": "Use the ChangeMessageVisibility API to configure messages in the dead-letter queue to be visible in the original SQS queue.",
        "C": "Use the StartMessageMoveTask API to move messages from the dead-letter queue to the original SQS queue.",
        "D": "Use the PurgeQueue API to remove messages from the dead-letter queue and return the messages to the original SQS queue."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148595-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 3, 2024, 10:46 a.m.",
      "textHash": "7b26a4624dee75a4",
      "rawFormat": "discussion-md",
      "conceptKey": "sqs_dlq_redrive",
      "frExplanation": "Ici, Lambda lit des messages dans une file SQS. Quand Lambda √©choue plusieurs fois sur un message, SQS l‚Äôenvoie dans une ¬´ dead-letter queue ¬ª (DLQ), une file de secours pour isoler les messages en erreur.\nApr√®s correction du bug, il faut remettre ces messages de la DLQ vers la file d‚Äôorigine pour qu‚Äôils soient retrait√©s.\nL‚ÄôAPI StartMessageMoveTask est faite exactement pour √ßa : elle d√©place (redrive) les messages d‚Äôune DLQ vers la file source, sans devoir r√©√©crire un script de copie.\nA (SendMessageBatch) pourrait fonctionner mais n√©cessite de lire/supprimer les messages et de les renvoyer soi-m√™me : plus complexe et risqu√© (doublons, gestion d‚Äôerreurs).\nB (ChangeMessageVisibility) ne s‚Äôapplique qu‚Äôaux messages dans une m√™me file pour les rendre visibles plus t√¥t/tard, pas pour les d√©placer vers une autre file.\nD (PurgeQueue) supprime d√©finitivement les messages de la DLQ ; cela ne les renvoie pas vers la file d‚Äôorigine.\nDonc la bonne r√©ponse est C.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : il y a une file principale (la file normale) et, si un √©l√®ve n‚Äôarrive pas √† payer 3 fois, on l‚Äôenvoie dans une file ‚Äúprobl√®mes‚Äù √† c√¥t√© (la file des cas bloqu√©s).**\n\nLa file principale = la queue SQS o√π arrivent les ‚Äútickets‚Äù.\nLe robot de caisse = AWS Lambda qui traite chaque ticket.\nQuand le robot bug, certains tickets finissent dans la file ‚Äúprobl√®mes‚Äù (dead-letter queue).\nUne fois le bug r√©par√©, il faut remettre ces tickets dans la file principale pour que le robot les retente.\nL‚Äôaction faite expr√®s pour ‚Äúd√©placer‚Äù des tickets de la file probl√®mes vers la file principale, c‚Äôest StartMessageMoveTask.\nA, c‚Äôest comme recopier les tickets √† la main : possible mais pas l‚Äôoutil pr√©vu.\nB, c‚Äôest juste rendre un ticket visible dans la m√™me file, pas le transf√©rer.\nD, c‚Äôest jeter les tickets √† la poubelle, pas les renvoyer.\nDonc la bonne r√©ponse est C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:416:4437b9171676e184",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 416,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on an application that will be deployed on AWS. The developer needs to test and debug the code locally. The code is packaged and stored in an Amazon S3 bucket.How can the developer test and debug the code locally with the LEAST amount of configuration?",
      "choices": {
        "A": "Create an application and a deployment group in AWS CodeDeploy. For the compute platform, specify the local machine as the individual instance for the deployment. For the repository type, specify that the application is stored in Amazon S3. Start the deployment to test on the local machine.",
        "B": "Create a repository in AWS CodeArtifact. Publish the application code package to the repository. Before deployment, create an upstream repository to test and validate the code.",
        "C": "Create a build project in AWS CodeBuild. In AWS CodePipeline, add a CodeBuild test action by adding a stage and an action. For the action provider, specify a CodeBuild test and the build project. View the build log to see the test results.",
        "D": "Install the AWS CodeDeploy agent locally to validate the deployment package. Run the codedeploy-local command. Specify the S3 bucket where the code package is located by using the --bundle-location option."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/150175-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 24, 2024, 8:36 p.m.",
      "textHash": "4437b9171676e184",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Objectif : tester et d√©boguer sur sa machine locale avec le minimum de configuration, alors que le code est d√©j√† empaquet√© dans un bucket Amazon S3 (stockage de fichiers dans le cloud).\nAWS CodeDeploy sert √† d√©ployer un paquet sur des machines (EC2, on-premises) et peut aussi ex√©cuter un d√©ploiement en local via un outil d√©di√©.\nLa r√©ponse D est la plus simple : on installe l‚Äôagent CodeDeploy sur son PC, puis on lance la commande codedeploy-local.\nAvec --bundle-location, on pointe directement vers le paquet dans S3 : pas besoin de cr√©er de pipeline, de projet de build, ni de ressources AWS suppl√©mentaires.\nCela permet de valider rapidement la structure du bundle (ex: appspec.yml) et d‚Äôex√©cuter les scripts de d√©ploiement localement pour d√©boguer.\nA demande de configurer une application/groupe de d√©ploiement et d‚Äôenregistrer la machine : plus lourd.\nB (CodeArtifact) sert √† g√©rer des d√©pendances/packages, pas √† tester un bundle de d√©ploiement.\nC (CodeBuild/CodePipeline) ex√©cute des tests dans AWS, pas sur la machine locale, et n√©cessite plus de configuration.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que ton devoir est dans un casier du lyc√©e (S3), et tu veux le tester sur ton propre ordinateur √† la maison avant de le rendre.**\n\nConcept : S3 = un casier/stockage en ligne. Ton PC = ta chambre. Tu veux juste r√©cup√©rer le devoir et le v√©rifier chez toi, sans organiser tout le lyc√©e.\nPourquoi D : installer l‚Äôagent CodeDeploy local, c‚Äôest comme installer une appli ‚Äúprof‚Äù sur ton PC qui sait lire le devoir dans le casier et simuler la remise. La commande codedeploy-local te permet de tester et d√©boguer directement sur ta machine, en pointant vers le casier S3 (--bundle-location).\nPourquoi pas A : √ßa te fait cr√©er une ‚Äúclasse + proc√©dure de distribution‚Äù compl√®te, trop de mise en place.\nPourquoi pas B : CodeArtifact, c‚Äôest plut√¥t une ‚Äúbiblioth√®que de pi√®ces‚Äù pour partager du code, pas pour tester local vite.\nPourquoi pas C : CodeBuild/CodePipeline, c‚Äôest une ‚Äúusine √† tests‚Äù dans le cloud, pas sur ton PC, donc plus de config.\nDonc D = le plus simple pour tester localement avec le minimum de configuration.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:161:5249114d42526213",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 161,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer wants to deploy a new version of an AWS Elastic Beanstalk application. During deployment, the application must maintain full capacity and avoid service interruption. Additionally, the developer must minimize the cost of additional resources that support the deployment.Which deployment method should the developer use to meet these requirements?",
      "choices": {
        "A": "All at once",
        "B": "Rolling with additional batch",
        "C": "Blue/green",
        "D": "Immutable"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122580-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:04 a.m.",
      "textHash": "5249114d42526213",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:4d57c9da",
      "frExplanation": "Objectif : d√©ployer une nouvelle version sur Elastic Beanstalk (service qui g√®re serveurs + d√©ploiements) sans coupure et en gardant 100% de capacit√©, tout en limitant le co√ªt.\nLa m√©thode ¬´ All at once ¬ª remplace tout d‚Äôun coup : il y a une interruption ou une baisse de capacit√©, donc non.\n¬´ Blue/green ¬ª cr√©e un environnement complet en double (blue et green) : tr√®s s√ªr, mais co√ªteux car on paie deux environnements entiers.\n¬´ Immutable ¬ª cr√©e aussi un nouveau groupe d‚Äôinstances complet puis bascule : tr√®s fiable, mais ajoute beaucoup de ressources temporaires, donc plus cher.\n¬´ Rolling with additional batch ¬ª ajoute un petit lot (batch) d‚Äôinstances en plus, puis met √† jour par √©tapes.\nGr√¢ce au batch suppl√©mentaire, la capacit√© totale reste √† 100% pendant que certaines instances sont mises √† jour.\nEt comme on n‚Äôajoute qu‚Äôun petit nombre d‚Äôinstances temporaires (pas un environnement complet), le co√ªt suppl√©mentaire est minimis√©.\nDonc la meilleure option est B.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:383:80ce76dde7235b88",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 383,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company maintains a REST service using Amazon API Gateway and the API Gateway native API key validation. The company recently launched a new registration page, which allows users to sign up for the service. The registration page creates a new API key using CreateApiKey and sends the new key to the user. When the user attempts to call the API using this key, the user receives a 403 Forbidden error. Existing users are unaffected and can still call the API.What code updates will grant these new users access to the API?",
      "choices": {
        "A": "The createDeployment method must be called so the API can be redeployed to include the newly created API key.",
        "B": "The updateAuthorizer method must be called to update the API's authorizer to include the newly created API key.",
        "C": "The importApiKeys method must be called to import all newly created API keys into the current stage of the API.",
        "D": "The createUsagePlanKey method must be called to associate the newly created API key with the correct usage plan."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143762-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 6:01 a.m.",
      "textHash": "80ce76dde7235b88",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:5b02a75f",
      "frExplanation": "API Gateway peut prot√©ger une API avec des ¬´ API keys ¬ª (des cl√©s) et un ¬´ usage plan ¬ª (plan d‚Äôutilisation) qui d√©finit quelles cl√©s ont le droit d‚Äôappeler l‚ÄôAPI et avec quels quotas/limites.\nCr√©er une cl√© avec CreateApiKey ne suffit pas : la cl√© existe, mais elle n‚Äôest pas encore autoris√©e pour un plan.\nSi une m√©thode de l‚ÄôAPI exige une API key, API Gateway v√©rifie que la cl√© fournie est valide ET qu‚Äôelle est associ√©e √† un usage plan li√© au stage (environnement) de l‚ÄôAPI.\nLes anciens utilisateurs fonctionnent car leurs cl√©s sont d√©j√† rattach√©es au bon usage plan.\nLes nouveaux utilisateurs re√ßoivent 403 Forbidden car leur cl√© n‚Äôest pas associ√©e √† un usage plan, donc API Gateway la refuse.\nLa bonne mise √† jour de code est d‚Äôappeler createUsagePlanKey pour lier la nouvelle API key au usage plan correct.\nPas besoin de red√©ployer l‚ÄôAPI (A) : les cl√©s ne n√©cessitent pas un nouveau d√©ploiement.\nPas d‚Äôauthorizer √† mettre √† jour (B) : la validation native des API keys n‚Äôutilise pas un authorizer.\nimportApiKeys (C) sert √† importer en masse, pas √† autoriser une cl√© sur un plan/stage.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un club au lyc√©e avec une porte et un vigile. Pour entrer, il faut une carte (API key). Mais en plus, il faut que ton nom soit sur une liste du club (usage plan) qui dit qui a le droit d‚Äôentrer et combien de fois par jour.**\n\nConcept : une API key, c‚Äôest juste une ‚Äúcarte‚Äù. Le usage plan, c‚Äôest la ‚Äúliste + r√®gles‚Äù (ex: 100 entr√©es/jour). Si tu as une carte mais que personne ne t‚Äôa ajout√© √† la liste, le vigile refuse : 403 Forbidden.\nIci, la page d‚Äôinscription fabrique bien une nouvelle carte (CreateApiKey) et la donne √† l‚Äô√©l√®ve. Mais elle oublie de l‚Äôattacher √† la bonne liste du club.\nLes anciens √©l√®ves marchent car leurs cartes sont d√©j√† li√©es √† une liste.\nDonc il faut faire l‚Äôaction qui ‚Äúassocie la carte √† la liste‚Äù : createUsagePlanKey.\nA est faux : pas besoin de ‚Äúr√©installer la porte‚Äù pour une nouvelle carte.\nB est faux : ce n‚Äôest pas un prof/contr√¥le sp√©cial, c‚Äôest juste la liste d‚Äôacc√®s.\nC est faux : on n‚Äôa pas besoin de r√©importer toutes les cartes, juste lier la nouvelle.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:358:3796a1a21c1deb72",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 358,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on an app for a company that uses an Amazon DynamoDB table named Orders to store customer orders. The table uses OrderID as the partition key and there is no sort key. The table contains more than 100,000 records. The developer needs to add a functionality that will retrieve all Orders records that contain an OrderSource attribute with the MobileApp value.Which solution will improve the user experience in the MOST efficient way?",
      "choices": {
        "A": "Perform a Scan operation on the Orders table. Provide a QueryFilter condition to filter to only the items where the OrderSource attribute is equal to the MobileApp value.",
        "B": "Create a local secondary index (LSI) with OrderSource as the partition key. Perform a Query operation by using MobileApp as the key.",
        "C": "Create a global secondary index (GSI) with OrderSource as the sort key. Perform a Query operation by using MobileApp as the key.",
        "D": "Create a global secondary index (GSI) with OrderSource as the partition key. Perform a Query operation by using MobileApp as the key."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143129-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 1, 2024, 1:07 p.m.",
      "textHash": "3796a1a21c1deb72",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL : pour lire vite, il faut interroger par une cl√© (Query), pas parcourir toute la table (Scan).\nIci, la table Orders a seulement OrderID comme cl√© de partition. On veut retrouver toutes les commandes o√π l‚Äôattribut OrderSource = \"MobileApp\".\nComme OrderSource n‚Äôest pas une cl√©, un Scan (A) lirait potentiellement plus de 100 000 √©l√©ments puis filtrerait apr√®s coup : lent et co√ªteux, mauvaise exp√©rience utilisateur.\nUn LSI (B) est impossible ici : il faut l‚Äôavoir d√©fini √† la cr√©ation de la table et il utilise la m√™me cl√© de partition que la table (OrderID), donc ne permet pas de chercher par OrderSource.\nUn GSI permet d‚Äôajouter une nouvelle ‚Äúcl√© d‚Äôacc√®s‚Äù sur un autre attribut, sans changer la cl√© principale.\nPour faire Query avec \"MobileApp\", cet attribut doit √™tre la cl√© de partition du GSI (pas une cl√© de tri) : c‚Äôest l‚Äôoption D.\nAvec le GSI, DynamoDB peut aller directement aux √©l√©ments \"MobileApp\" via Query : plus rapide et plus efficace.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la vie scolaire : tu as une √©norme pile de 100 000 fiches de commandes dans une salle. Chaque fiche a un num√©ro (OrderID) et une √©tiquette ‚Äúsource‚Äù : MobileApp, SiteWeb, etc.**\n\nConcept : chercher ‚Äútoutes les fiches MobileApp‚Äù peut se faire soit en lisant TOUTES les fiches, soit en cr√©ant un tiroir sp√©cial ‚ÄúMobileApp‚Äù.\nA = Scan : c‚Äôest comme feuilleter les 100 000 fiches une par une puis garder celles ‚ÄúMobileApp‚Äù. Tr√®s lent, surtout si beaucoup d‚Äô√©l√®ves demandent en m√™me temps.\nB = LSI : c‚Äôest un rangement ‚Äúbonus‚Äù mais qui d√©pend du m√™me classement principal (OrderID) et ici √ßa ne colle pas pour regrouper par source.\nC = GSI avec OrderSource en sort key : c‚Äôest comme trier √† l‚Äôint√©rieur d‚Äôun tiroir, mais il faut d‚Äôabord un tiroir principal (une cl√©) pour trouver ‚ÄúMobileApp‚Äù.\nD = GSI avec OrderSource en partition key : tu cr√©es un nouveau meuble de rangement o√π les fiches sont regroup√©es directement par ‚Äúsource‚Äù.\nDonc pour ‚Äúdonne-moi toutes les MobileApp‚Äù, tu ouvres le tiroir MobileApp et tu r√©cup√®res vite : meilleure exp√©rience utilisateur et le plus efficace.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:355:03dad003c01a69c8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 355,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing a web application that is deployed on Amazon EC2 instances behind an internet-facing Application Load Balancer (ALB). The developer must add an Amazon CloudFront distribution in front of the ALB. The developer also must ensure that customer data from outside the VPC is encrypted in transit.Which combination of CloudFront configuration settings should the developer use to meet these requirements? (Choose two.)",
      "choices": {
        "A": "Restrict viewer access by using signed URLs.",
        "B": "Set the Origin Protocol Policy setting to Match Viewer.",
        "C": "Enable field-level encryption.",
        "D": "Enable automatic object compression.",
        "E": "Set the Viewer Protocol Policy setting to Redirect HTTP to HTTPS."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136974-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 5:02 a.m.",
      "textHash": "03dad003c01a69c8",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "CloudFront est un r√©seau de diffusion (CDN) plac√© devant votre application pour acc√©l√©rer et s√©curiser l‚Äôacc√®s. Ici, l‚Äôorigine de CloudFront est l‚ÄôALB (Application Load Balancer) qui envoie ensuite le trafic vers vos instances EC2.\n¬´ Chiffrer en transit depuis l‚Äôext√©rieur du VPC ¬ª signifie : quand un client sur Internet parle √† CloudFront, et quand CloudFront parle √† l‚ÄôALB, on veut utiliser HTTPS (TLS).\nLe r√©glage cl√© est l‚ÄôOrigin Protocol Policy : il d√©finit comment CloudFront se connecte √† l‚Äôorigine (l‚ÄôALB).\nAvec ¬´ Match Viewer ¬ª, si le client utilise HTTPS, CloudFront utilisera aussi HTTPS vers l‚ÄôALB : le chiffrement est conserv√© de bout en bout (client ‚Üí CloudFront ‚Üí ALB).\nLes autres options ne garantissent pas ce chiffrement : les signed URLs contr√¥lent l‚Äôacc√®s, la field-level encryption chiffre des champs sp√©cifiques, la compression n‚Äôest pas de la s√©curit√©.\nDonc le bon choix est B, car il aligne le protocole entre le client et l‚Äôorigine pour maintenir HTTPS quand le client est en HTTPS.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un self au lyc√©e : les √©l√®ves (clients) arrivent par l‚Äôentr√©e (CloudFront), puis vont au comptoir (ALB) qui les r√©partit vers les cuisines (serveurs EC2). On veut que le trajet soit ‚Äúen tunnel s√©curis√©‚Äù (chiffr√©) quand ils viennent de l‚Äôext√©rieur.**\n\nConcept : CloudFront est comme un grand hall d‚Äôentr√©e qui re√ßoit tout le monde avant le self. Le ‚Äúchiffrement en transit‚Äù, c‚Äôest comme parler dans un couloir insonoris√© : personne ne peut √©couter.\nPour √™tre s√ªr que c‚Äôest s√©curis√© de bout en bout, il faut que CloudFront parle au comptoir (ALB) avec le m√™me niveau de s√©curit√© que l‚Äô√©l√®ve utilise.\nLe r√©glage B (Origin Protocol Policy = Match Viewer) veut dire : si l‚Äô√©l√®ve arrive en HTTPS (tunnel s√©curis√©), CloudFront continue en HTTPS jusqu‚Äô√† l‚ÄôALB.\nDonc les donn√©es des clients venant de l‚Äôext√©rieur restent chiffr√©es pendant le trajet.\nLes autres choix : A c‚Äôest pour limiter l‚Äôacc√®s (tickets sp√©ciaux), pas pour chiffrer. C chiffre seulement certains champs, pas tout le trajet. D compresse, pas de s√©curit√©. E force HTTPS c√¥t√© √©l√®ve, mais ne garantit pas automatiquement le tunnel jusqu‚Äô√† l‚ÄôALB.\nDonc B est le bon r√©glage pour assurer le chiffrement jusqu‚Äô√† l‚ÄôALB.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:351:b872efb717ee5fab",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 351,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an internal website that contains sensitive data. The company wants to make the website public. The company must ensure that only employees who authenticate through the company's OpenID Connect (OIDC) identity provider (IdP) can access the website. A developer needs to implement authentication without editing the website.Which combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Create a public Network Load Balancer.",
        "B": "Create a public Application Load Balancer.",
        "C": "Configure a listener for the load balancer that listens on HTTPS port 443. Add a default authenticate action providing the OIDC IdP configuration.",
        "D": "Configure a listener for the load balancer that listens on HTTP port 80. Add a default authenticate action providing the OIDC IdP configuration.",
        "E": "Configure a listener for the load balancer that listens on HTTPS port 443. Add a default AWS Lambda action providing an Amazon Resource Name (ARN) to a Lambda authentication function."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136972-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 4:52 a.m.",
      "textHash": "b872efb717ee5fab",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Pour rendre un site ¬´ public ¬ª tout en le prot√©geant, on place un composant devant le site qui contr√¥le l‚Äôacc√®s.\nUn Application Load Balancer (ALB) est un r√©partiteur HTTP/HTTPS (couche 7) qui peut faire de l‚Äôauthentification avant d‚Äôenvoyer la requ√™te au site.\nUn Network Load Balancer (NLB) travaille au niveau r√©seau (TCP/UDP) et ne sait pas g√©rer l‚Äôauthentification OIDC.\nAvec un ALB, on peut ajouter une action ¬´ authenticate ¬ª directement sur le listener : l‚ÄôALB redirige l‚Äôutilisateur vers l‚ÄôIdP OIDC de l‚Äôentreprise, puis ne laisse passer que les utilisateurs connect√©s.\nPour des donn√©es sensibles, on utilise HTTPS (port 443) afin de chiffrer les √©changes.\nCela r√©pond √† la contrainte ¬´ sans modifier le site ¬ª : l‚Äôauthentification est g√©r√©e par l‚ÄôALB, pas par l‚Äôapplication.\nDonc il faut choisir l‚ÄôALB (B) et configurer un listener HTTPS 443 avec l‚Äôaction d‚Äôauthentification OIDC (C).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine le site web comme une salle au lyc√©e avec des documents secrets. Tu veux ouvrir le lyc√©e au public, mais cette salle doit rester accessible seulement aux √©l√®ves qui montrent leur carte d‚Äô√©l√®ve via le ‚Äúbureau des cartes‚Äù officiel (OIDC). Tu ne veux pas changer l‚Äôint√©rieur de la salle (le site).**\n\nLe ‚Äúvideur‚Äù id√©al est plac√© √† l‚Äôentr√©e du b√¢timent, avant d‚Äôarriver √† la salle. Un Application Load Balancer, c‚Äôest comme un videur intelligent qui comprend les ‚Äúportes‚Äù d‚Äôun site (les pages/URL) et peut demander une connexion via ton bureau des cartes (OIDC) sans toucher au site. Un Network Load Balancer, lui, est plut√¥t un videur basique: il laisse passer le trafic sans comprendre les pages, donc il ne g√®re pas bien l‚Äôauthentification OIDC. Donc l‚Äô√©tape indispensable est de cr√©er un Application Load Balancer public (B), pour rendre le site accessible au public tout en gardant le contr√¥le d‚Äôacc√®s √† l‚Äôentr√©e.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:243:cad8fa20f5df9c9e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 243,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a serverless application by using AWS Serverless Application Model (AWS SAM) on multiple AWS Lambda functions. When the application is deployed, the developer wants to shift 10% of the traffic to the new deployment of the application for the first 10 minutes after deployment. If there are no issues, all traffic must switch over to the new version.Which change to the AWS SAM template will meet these requirements?",
      "choices": {
        "A": "Set the Deployment Preference Type to Canary10Percent10Minutes. Set the AutoPublishAlias property to the Lambda alias.",
        "B": "Set the Deployment Preference Type to Linear10PercentEvery10Minutes. Set AutoPublishAlias property to the Lambda alias.",
        "C": "Set the Deployment Preference Type to Canary10Percent10Minutes. Set the PreTraffic and PostTraffic properties to the Lambda alias.",
        "D": "Set the Deployment Preference Type to Linear10PercentEvery10Minutes. Set PreTraffic and PostTraffic properties to the Lambda alias."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124747-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 27, 2023, 9:39 p.m.",
      "textHash": "cad8fa20f5df9c9e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:f79b9a9f",
      "frExplanation": "Ici on parle de d√©ploiement progressif (deployment) d‚Äôune fonction AWS Lambda (ex√©cuter du code sans serveur). Avec AWS SAM, on d√©crit l‚Äôinfra dans un template et SAM peut g√©rer les versions de Lambda.\nPour envoyer seulement une partie du trafic vers une nouvelle version, on utilise un alias Lambda (un ‚Äúnom‚Äù stable comme prod) qui pointe vers une version pr√©cise. SAM peut publier automatiquement une nouvelle version et d√©placer l‚Äôalias.\nLa r√®gle demand√©e est : 10% du trafic pendant 10 minutes, puis 100% si tout va bien. C‚Äôest exactement une strat√©gie ‚ÄúCanary 10% 10 minutes‚Äù.\nDans SAM, cela se configure via DeploymentPreference: Type: Canary10Percent10Minutes.\nMais pour que SAM/CodeDeploy puisse router le trafic entre ancienne et nouvelle version, il faut aussi AutoPublishAlias (ex: prod) afin de cr√©er/mettre √† jour l‚Äôalias et g√©rer le basculement.\nLes options ‚ÄúLinear10PercentEvery10Minutes‚Äù feraient monter le trafic par paliers (10%, 20%, 30%...) et ne correspondent pas.\nPreTraffic/PostTraffic servent √† ex√©cuter des hooks de validation (tests) avant/apr√®s le basculement, pas √† d√©finir le routage lui-m√™me.\nDonc la bonne modification est : Canary10Percent10Minutes + AutoPublishAlias sur l‚Äôalias Lambda (r√©ponse A).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:398:66e6512c255eef32",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 398,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company that has large online business uses an Amazon DynamoDB table to store sales data. The company enabled Amazon DynamoDB Streams on the table. The transaction status of each sale is stored in a TransactionStatus attribute in the table. The value of the TransactionStatus attribute must be either failed, pending, or completed.The company wants to be notified of failed sales where the Price attribute is above a specific threshold. A developer needs to set up notification for the failed sales.Which solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Create an event source mapping between DynamoDB Streams and an AWS Lambda function. Use Lambda event filtering to trigger the Lambda function only if sales fail when the price is above the specified threshold. Configure the Lambda function to publish the data to an Amazon Simple Notification Service (Amazon SNS) topic.",
        "B": "Create an event source mapping between DynamoDB Streams and an AWS Lambda function. Configure the Lambda function handler code to publish to an Amazon Simple Notification Service (Amazon SNS) topic if sales fail when price is above the specified threshold.",
        "C": "Create an event source mapping between DynamoDB Streams and an Amazon Simple Notification Service (Amazon SNS) topic. Use event filtering to publish to the SNS topic if sales fail when the price is above the specified threshold.",
        "D": "Create an Amazon CloudWatch alarm to monitor the DynamoDB Streams sales data. Configure the alarm to publish to an Amazon Simple Notification Service (Amazon SNS) topic if sales fail due when price is above the specified threshold."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143357-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 5, 2024, 2:10 p.m.",
      "textHash": "66e6512c255eef32",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "DynamoDB est une base NoSQL. DynamoDB Streams enregistre chaque modification d‚Äôun item (insert/update/delete) et peut d√©clencher un traitement.\nPour √™tre notifi√©, on peut utiliser SNS (un service qui envoie des notifications √† des emails, SMS, files, etc.).\nLe besoin: d√©tecter uniquement les ventes dont TransactionStatus = \"failed\" ET Price > seuil, puis envoyer une notification.\nLa fa√ßon la plus simple avec le moins de code est de connecter le Stream √† AWS Lambda (fonction ex√©cut√©e automatiquement) via un ‚Äúevent source mapping‚Äù.\nAvec le ‚ÄúLambda event filtering‚Äù, AWS filtre les √©v√©nements avant d‚Äôappeler la fonction: la Lambda ne se d√©clenche que pour les enregistrements qui correspondent aux conditions.\nDonc on √©vite d‚Äô√©crire du code pour parcourir et filtrer tous les √©v√©nements, ce qui r√©duit l‚Äôeffort de d√©veloppement et les co√ªts.\nEnsuite, la Lambda publie simplement le message vers un topic SNS.\nLes autres choix: B n√©cessite du code de filtrage; C n‚Äôest pas une int√©gration directe Streams‚ÜíSNS; D CloudWatch ne lit pas le contenu des √©v√©nements Streams pour filtrer par attributs.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : chaque ticket de repas (vente) est not√© dans un cahier. √Ä c√¥t√©, il y a un ‚Äújournal des changements‚Äù qui √©crit chaque fois qu‚Äôun ticket est ajout√© ou modifi√© (comme un fil d‚Äôactualit√©).**\n\nConcept : DynamoDB = le cahier des ventes. DynamoDB Streams = le journal qui liste chaque changement. Lambda = un surveillant qui lit ce journal. SNS = le haut-parleur qui envoie une alerte.\nOn veut √™tre pr√©venu seulement si : TransactionStatus = failed ET Price > seuil.\nR√©ponse A : on met le surveillant (Lambda) branch√© sur le journal, MAIS on lui donne un filtre √† l‚Äôentr√©e : il ne se r√©veille que pour les tickets ‚Äúfailed‚Äù et chers. Donc presque rien √† coder.\nPuis Lambda envoie l‚Äôannonce au haut-parleur (SNS).\nB marche, mais le surveillant doit lire tous les tickets et trier lui-m√™me (plus de code).\nC ne marche pas : le haut-parleur (SNS) ne lit pas directement le journal comme √ßa.\nD ne convient pas : une alarme regarde des compteurs, pas le d√©tail de chaque ticket (statut + prix).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:147:cc59e46af1820aa5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 147,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an application that gives users the ability to view bank accounts from multiple sources in a single dashboard. The developer has automated the process to retrieve API credentials for these sources. The process invokes an AWS Lambda function that is associated with an AWS CloudFormation custom resource.The developer wants a solution that will store the API credentials with minimal operational overhead.Which solution will meet these requirements in the MOST secure way?",
      "choices": {
        "A": "Add an AWS Secrets Manager GenerateSecretString resource to the CloudFormation template. Set the value to reference new credentials for the CloudFormation resource.",
        "B": "Use the AWS SDK ssm:PutParameter operation in the Lambda function from the existing custom resource to store the credentials as a parameter. Set the parameter value to reference the new credentials. Set the parameter type to SecureString.",
        "C": "Add an AWS Systems Manager Parameter Store resource to the CloudFormation template. Set the CloudFormation resource value to reference the new credentials. Set the resource NoEcho attribute to true.",
        "D": "Use the AWS SDK ssm:PutParameter operation in the Lambda function from the existing custom resource to store the credentials as a parameter. Set the parameter value to reference the new credentials. Set the parameter NoEcho attribute to true."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122565-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:37 a.m.",
      "textHash": "cc59e46af1820aa5",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Ici on veut stocker des identifiants d‚ÄôAPI (des secrets) de fa√ßon s√ªre et avec peu d‚Äôadministration.\nAWS Systems Manager Parameter Store peut stocker des valeurs, et le type SecureString chiffre la valeur (avec une cl√© KMS) et contr√¥le l‚Äôacc√®s via IAM : c‚Äôest adapt√© aux secrets.\nLa r√©ponse B fait exactement cela : la fonction AWS Lambda (code ex√©cut√© √† la demande) appelle l‚ÄôAPI ssm:PutParameter pour enregistrer les nouveaux identifiants en SecureString.\nC‚Äôest ‚Äúminimal overhead‚Äù car on n‚Äôa pas besoin de g√©rer des serveurs, juste un appel API lors de la cr√©ation/mise √† jour via CloudFormation.\nLes options C et D parlent de NoEcho : NoEcho sert surtout √† √©viter d‚Äôafficher une valeur en clair dans les sorties/logs CloudFormation, mais ne garantit pas le chiffrement du secret dans le stockage.\nDe plus, NoEcho n‚Äôest pas un attribut d‚Äôun param√®tre SSM (donc D est incorrect techniquement).\nA utilise Secrets Manager, qui est tr√®s s√©curis√©, mais l‚Äôoption d√©crit GenerateSecretString (g√©n√©rer un secret) alors qu‚Äôici on r√©cup√®re des identifiants existants depuis des sources externes ; B correspond mieux au flux ‚Äúje stocke ce que je viens d‚Äôobtenir‚Äù.\nDonc B est le choix le plus s√ªr et le plus simple : stockage chiffr√© (SecureString) + acc√®s contr√¥l√©, sans gestion op√©rationnelle lourde.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:332:ec38bfd31c158ca4",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 332,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company recently deployed a new serverless user portal. Users have reported that part of the portal is slow. The initial analysis found a single Amazon API Gateway endpoint that is responsible for the performance issues. The endpoint integrates with an AWS Lambda function. However, the Lambda function interacts with other APIs and AWS services.How can a developer find the source of the increased response time by using operational best practices?",
      "choices": {
        "A": "Update the Lambda function by adding logging statements with high-precision timestamps before and after each external request. Deploy the updated Lambda function. After accumulating enough usage data, examine the Amazon CloudWatch logs for the Lambda function to determine the likely sources for the increased response time.",
        "B": "Instrument the Lambda function with the AWS X-Ray SDK. Add HTTP and HTTPS interceptors and SDK client handlers. Deploy the updated Lambda function. Turn on X-Ray tracing. After accumulating enough usage data, use the X-Ray service map to examine the average response times to determine the likely sources.",
        "C": "Review the Lambda function's Amazon CloudWatch metrics by using the metrics explorer. Apply anomaly detection to the Duration metric and the Throttles metric. Review the anomalies to determine the likely sources.",
        "D": "Use Amazon CloudWatch Synthetics to create a new canary. Turn on AWS X-Ray tracing on the canary. Configure the canary to scan the user portal. After accumulating enough usage data, use the CloudWatch Synthetics canary dashboard to view the metrics from the canary."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136634-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 19, 2024, 2:46 p.m.",
      "textHash": "ec38bfd31c158ca4",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Ici, on cherche d‚Äôo√π vient la lenteur sur un chemin pr√©cis : API Gateway (porte d‚Äôentr√©e HTTP) appelle une fonction AWS Lambda (code ex√©cut√© √† la demande), qui elle-m√™me appelle d‚Äôautres API/services.\nLa bonne pratique ‚Äúop√©rationnelle‚Äù pour trouver la cause est le tra√ßage distribu√© : suivre une requ√™te de bout en bout et mesurer chaque √©tape.\nAWS X-Ray sert exactement √† √ßa : il cr√©e des ‚Äútraces‚Äù avec des segments/sous-segments montrant le temps pass√© dans Lambda et dans chaque appel externe.\nEn instrumentant Lambda avec le SDK X-Ray + interceptors HTTP/HTTPS et handlers AWS SDK, on capture automatiquement les latences des appels sortants.\nEnsuite, la carte de service (service map) et les traces X-Ray affichent les temps moyens et o√π √ßa ralentit (ex: appel √† une API tierce, DynamoDB, S3, etc.).\nA (logs manuels) peut aider mais c‚Äôest moins fiable, plus long √† maintenir, et ne donne pas une vue bout-en-bout aussi claire.\nC (m√©triques Duration/Throttles) dit ‚Äúc‚Äôest lent‚Äù mais pas ‚Äúo√π‚Äù dans la cha√Æne.\nD (Synthetics) teste l‚Äôexp√©rience utilisateur, mais ne diagnostique pas finement les appels internes de Lambda comme X-Ray.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un resto de pizzas : les clients commandent au comptoir, et en cuisine un cuisinier pr√©pare la pizza mais doit aussi appeler d‚Äôautres stands (boissons, desserts, livraison). Une commande est lente, mais tu ne sais pas si c‚Äôest la cuisine, un stand, ou le livreur.**\n\nConcept : pour trouver ce qui ralentit, il faut une ‚Äúcam√©ra de suivi‚Äù qui chronom√®tre chaque √©tape de la commande, pas juste un temps total. A (ajouter des logs) c‚Äôest comme √©crire l‚Äôheure √† la main avant/apr√®s chaque appel : √ßa marche, mais c‚Äôest lourd et tu peux rater des d√©tails. B (AWS X-Ray) c‚Äôest la cam√©ra automatique : tu ‚Äú√©tiquettes‚Äù la commande et X-Ray mesure chaque appel externe (HTTP/HTTPS et services AWS) et te fait une carte avec les temps moyens. Tu vois tout de suite quel stand (API/service) fait perdre du temps. C ne montre que des sympt√¥mes (dur√©e globale, blocages), pas l‚Äôendroit pr√©cis. D teste le site comme un robot, mais ne dit pas clairement quel appel interne est le coupable. Donc B est la meilleure pratique pour localiser la source du d√©lai.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:326:f73f7cd07521ba1c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 326,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs a payment application on Amazon EC2 instances behind an Application Load Balance. The EC2 instances run in an Auto Scaling group across multiple Availability Zones. The application needs to retrieve application secrets during the application startup and export the secrets as environment variables. These secrets must be encrypted at rest and need to be rotated every month.Which solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Save the secrets in a text file and store the text file in Amazon S3. Provision a customer managed key. Use the key for secret encryption in Amazon S3. Read the contents of the text file and read the export as environment variables. Configure S3 Object Lambda to rotate the text file every month.",
        "B": "Save the secrets as strings in AWS Systems Manager Parameter Store and use the default AWS Key Management Service (AWS KMS) key. Configure an Amazon EC2 user data script to retrieve the secrets during the startup and export as environment variables. Configure an AWS Lambda function to rotate the secrets in Parameter Store every month.",
        "C": "Save the secrets as base64 encoded environment variables in the application properties. Retrieve the secrets during the application startup. Reference the secrets in the application code. Write a script to rotate the secrets saved as environment variables.",
        "D": "Store the secrets in AWS Secrets Manager. Provision a new customer master key. Use the key to encrypt the secrets. Enable automatic rotation. Configure an Amazon EC2 user data script to programmatically retrieve the secrets during the startup and export as environment variables."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133636-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 12, 2024, 6:27 p.m.",
      "textHash": "f73f7cd07521ba1c",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "On veut stocker des ‚Äúsecrets‚Äù (mots de passe, cl√©s API) de fa√ßon s√ªre, les chiffrer sur disque (au repos) et les changer automatiquement chaque mois, sans beaucoup de code.\nAWS Secrets Manager est justement un service fait pour g√©rer des secrets : stockage s√©curis√©, contr√¥le d‚Äôacc√®s, et rotation automatique int√©gr√©e.\nAvec une cl√© KMS (AWS Key Management Service), on chiffre les secrets au repos avec une cl√© g√©r√©e par vous (customer managed key), ce qui r√©pond √† l‚Äôexigence de chiffrement.\nLa rotation automatique de Secrets Manager permet de renouveler les secrets tous les mois sans √©crire une solution compl√®te soi‚Äëm√™me (moins d‚Äôeffort de d√©veloppement).\nAu d√©marrage des instances EC2, un script ‚Äúuser data‚Äù peut appeler l‚ÄôAPI Secrets Manager pour r√©cup√©rer le secret et le mettre en variables d‚Äôenvironnement.\nLes autres choix demandent plus de bricolage : S3 + rotation via Lambda/Object Lambda n‚Äôest pas pr√©vu pour des secrets, Parameter Store n√©cessite de construire la rotation, et base64 n‚Äôest pas du chiffrement.\nDonc la solution la plus simple et la plus adapt√©e est : Secrets Manager + KMS + rotation automatique + r√©cup√©ration au d√©marrage (D).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine ton lyc√©e : tu as un casier avec des codes (secrets) pour ouvrir des salles importantes. Ces codes doivent √™tre gard√©s dans un coffre ferm√©, et le proviseur veut changer les codes tous les mois automatiquement.**\n\nConcept : les ‚Äúsecrets‚Äù, c‚Äôest comme des codes d‚Äôacc√®s (mot de passe, cl√©) dont l‚Äôappli a besoin au d√©marrage. Il faut un coffre (stockage), un cadenas (chiffrement), et un changement de code mensuel (rotation).\nPourquoi D : AWS Secrets Manager = le coffre-fort fait expr√®s pour stocker des codes. Il garde les secrets chiffr√©s ‚Äúau repos‚Äù (dans le coffre) avec une cl√© (le cadenas). Il sait aussi faire la rotation automatique chaque mois (le proviseur change les codes sans que tu √©crives beaucoup de r√®gles).\nLe script de d√©marrage de la machine (user data) = l‚Äô√©l√®ve qui, en arrivant le matin, va au coffre, r√©cup√®re le code, puis le met dans ses notes du jour (variables d‚Äôenvironnement) pour que l‚Äôappli fonctionne.\nLes autres : S3/texte (A) c‚Äôest comme laisser un papier dans la biblioth√®que, pas fait pour g√©rer des codes + rotation compliqu√©e. Parameter Store (B) peut stocker, mais la rotation demande plus de bricolage. Mettre les secrets dans le code (C) c‚Äôest comme √©crire le code du casier sur ton agenda : mauvaise id√©e et rotation p√©nible.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:293:f191ba9fb33818f9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 293,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is deploying a company's application to Amazon EC2 instances. The application generates gigabytes of data files each day. The files are rarely accessed, but the files must be available to the application's users within minutes of a request during the first year of storage. The company must retain the files for 7 years.How can the developer implement the application to meet these requirements MOST cost-effectively?",
      "choices": {
        "A": "Store the files in an Amazon S3 bucket. Use the S3 Glacier Instant Retrieval storage class. Create an S3 Lifecycle policy to transition the files to the S3 Glacier Deep Archive storage class after 1 year.",
        "B": "Store the files in an Amazon S3 bucket. Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition the files to the S3 Glacier Flexible Retrieval storage class after 1 year.",
        "C": "Store the files on an Amazon Elastic Block Store (Amazon EBS) volume. Use Amazon Data Lifecycle Manager (Amazon DLM) to create snapshots of the EBS volumes and to store those snapshots in Amazon S3.",
        "D": "Store the files on an Amazon Elastic File System (Amazon EFS) mount. Configure EFS lifecycle management to transition the files to the EFS Standard- Infrequent Access (Standard-IA) storage class after 1 year."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134292-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:11 a.m.",
      "textHash": "f191ba9fb33818f9",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "On cherche un stockage tr√®s peu cher pour des fichiers √©normes, rarement lus, mais r√©cup√©rables en quelques minutes pendant la 1re ann√©e, puis conserv√©s 7 ans.\nAmazon S3 est un service de stockage d‚Äôobjets (fichiers) fait pour ce type d‚Äôarchives, avec des ‚Äúclasses‚Äù de stockage selon le co√ªt et la vitesse d‚Äôacc√®s.\nS3 Glacier Instant Retrieval est con√ßu pour des donn√©es rarement consult√©es, mais avec une r√©cup√©ration tr√®s rapide (quasi imm√©diate), donc compatible avec ‚Äúdisponible en minutes‚Äù durant la 1re ann√©e.\nEnsuite, apr√®s 1 an, on peut r√©duire encore le co√ªt avec une r√®gle S3 Lifecycle (automatisation) qui d√©place les objets vers S3 Glacier Deep Archive, la classe la moins ch√®re pour l‚Äôarchivage long (7 ans).\nL‚Äôoption B commence en S3 Standard (plus cher inutilement) et Glacier Flexible Retrieval peut prendre plus longtemps √† restaurer, donc moins adapt√© √† l‚Äôexigence ‚Äúen minutes‚Äù.\nLes options EBS/EFS (C/D) sont des stockages de type disque/syst√®me de fichiers pour serveurs, g√©n√©ralement plus co√ªteux pour des archives massives et ne correspondent pas au besoin d‚Äôarchivage √† long terme.\nDonc la solution la plus √©conomique tout en respectant l‚Äôacc√®s rapide la 1re ann√©e est S3 + Glacier Instant Retrieval puis transition vers Deep Archive.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:283:24d960edd24e2afc",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 283,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer must use multi-factor authentication (MFA) to access data in an Amazon S3 bucket that is in another AWS account.Which AWS Security Token Service (AWS STS) API operation should the developer use with the MFA information to meet this requirement?",
      "choices": {
        "A": "AssumeRoleWithWebIdentity",
        "B": "GetFederationToken",
        "C": "AssumeRoleWithSAML",
        "D": "AssumeRole"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134282-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:57 a.m.",
      "textHash": "24d960edd24e2afc",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, le d√©veloppeur doit acc√©der √† un bucket Amazon S3 (stockage de fichiers) qui se trouve dans un autre compte AWS. Pour acc√©der √† des ressources d‚Äôun autre compte, on utilise g√©n√©ralement un ¬´ r√¥le IAM ¬ª dans le compte cible, et on demande √† AWS STS (service qui d√©livre des identifiants temporaires) de l‚Äôendosser.\nL‚Äôexigence MFA signifie : fournir un code √† usage unique (ex: appli Authenticator) au moment de demander ces identifiants temporaires.\nL‚ÄôAPI STS adapt√©e est AssumeRole : elle permet d‚Äôassumer un r√¥le dans un autre compte et peut inclure les infos MFA (SerialNumber + TokenCode) pour forcer la v√©rification.\nGetFederationToken sert plut√¥t √† donner des identifiants temporaires √† un utilisateur dans le m√™me compte (f√©d√©ration), pas √† assumer un r√¥le cross-account.\nAssumeRoleWithSAML et AssumeRoleWithWebIdentity sont pour des connexions via un fournisseur d‚Äôidentit√© (SAML, OIDC/Cognito), pas pour un sc√©nario MFA classique avec r√¥le cross-account.\nDonc la bonne r√©ponse est D: AssumeRole, car c‚Äôest l‚Äôop√©ration standard pour obtenir des identifiants temporaires cross-account en fournissant le code MFA.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:282:fea8cea9f34b3378",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 282,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is testing a RESTful application that is deployed by using Amazon API Gateway and AWS Lambda. When the developer tests the user login by using credentials that are not valid, the developer receives an HTTP 405: METHOD_NOT_ALLOWED error. The developer has verified that the test is sending the correct request for the resource.Which HTTP error should the application return in response to the request?",
      "choices": {
        "A": "HTTP 401",
        "B": "HTTP 404",
        "C": "HTTP 503",
        "D": "HTTP 505"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134281-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:56 a.m.",
      "textHash": "fea8cea9f34b3378",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:4ba55516",
      "frExplanation": "Dans une API REST, chaque code HTTP a un sens pr√©cis. Ici, l‚Äôutilisateur envoie bien une requ√™te vers la bonne ressource (API Gateway re√ßoit la bonne URL et la bonne m√©thode), mais les identifiants sont faux.\nAmazon API Gateway sert de ‚Äúporte d‚Äôentr√©e‚Äù HTTP, et AWS Lambda ex√©cute le code qui valide le login.\nSi les identifiants sont invalides, le probl√®me est l‚Äôauthentification : le client n‚Äôest pas autoris√© car il n‚Äôa pas fourni des informations valides.\nLe code attendu est donc HTTP 401 (Unauthorized) : ‚Äúvous devez vous authentifier correctement‚Äù.\nHTTP 405 (Method Not Allowed) signifie plut√¥t ‚Äúla m√©thode HTTP (GET/POST/‚Ä¶) n‚Äôest pas autoris√©e sur cette route‚Äù, ce qui ne correspond pas √† un mauvais mot de passe.\nHTTP 404 serait ‚Äúressource introuvable‚Äù, 503 ‚Äúservice indisponible‚Äù, 505 ‚Äúversion HTTP non support√©e‚Äù.\nDonc l‚Äôapplication doit renvoyer 401 quand le login √©choue √† cause de credentials invalides.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:280:dc78a94059ece249",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 280,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has built an AWS Lambda function to convert large image files into output files that can be used in a third-party viewer application. The company recently added a new module to the function to improve the output of the generated files. However, the new module has increased the bundle size and has increased the time that is needed to deploy changes to the function code.How can a developer increase the speed of the Lambda function deployment?",
      "choices": {
        "A": "Use AWS CodeDeploy to deploy the function code.",
        "B": "Use Lambda layers to package and load dependencies.",
        "C": "Increase the memory size of the function.",
        "D": "Use Amazon S3 to host the function dependencies."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134279-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:44 a.m.",
      "textHash": "dc78a94059ece249",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici, le probl√®me est le temps de d√©ploiement qui augmente parce que le ‚Äúbundle‚Äù (le paquet de code) de la fonction AWS Lambda devient trop gros.\nAWS Lambda est un service qui ex√©cute du code sans g√©rer de serveur, et √† chaque d√©ploiement AWS doit t√©l√©verser et pr√©parer votre paquet.\nLes ‚Äúd√©pendances‚Äù (biblioth√®ques/modules) changent souvent moins que votre code principal.\nLes Lambda Layers (couches) permettent de mettre ces d√©pendances dans un paquet s√©par√©, r√©utilisable par une ou plusieurs fonctions.\nAinsi, quand vous modifiez seulement le code de la fonction, vous d√©ployez un paquet plus petit et plus rapide, sans re-t√©l√©verser toutes les biblioth√®ques.\nA (CodeDeploy) g√®re des strat√©gies de d√©ploiement, mais ne r√©duit pas la taille du paquet.\nC (plus de m√©moire) peut acc√©l√©rer l‚Äôex√©cution, pas la vitesse de d√©ploiement.\nD (S3 pour d√©pendances) n‚Äôest pas le m√©canisme standard pour charger des d√©pendances au runtime dans Lambda; les Layers sont faits pour √ßa.\nDonc la meilleure solution pour acc√©l√©rer le d√©ploiement est d‚Äôexternaliser les d√©pendances dans des Lambda Layers.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un expos√© √† rendre au lyc√©e : tu as ton texte principal, et √† c√¥t√© tu as un gros classeur de documents (images, sources, annexes) que tu r√©utilises pour plusieurs expos√©s.**\n\nUne fonction Lambda, c‚Äôest comme ton expos√© : √† chaque modification, tu dois ‚Äúrendre‚Äù le fichier. Si tu mets tout (texte + √©norme classeur) dans un seul fichier, l‚Äôenvoi est long. Les Lambda layers, c‚Äôest comme laisser le gros classeur dans un casier partag√© : tu n‚Äôenvoies plus que le texte principal quand tu changes une phrase. Donc le d√©ploiement (l‚Äôenvoi de la nouvelle version) est beaucoup plus rapide, car les grosses d√©pendances (biblioth√®ques/modules) ne bougent pas. A ne change pas la taille du paquet, √ßa change juste la fa√ßon de livrer. C rend l‚Äôex√©cution potentiellement plus rapide, pas l‚Äôenvoi. D revient √† mettre des documents ailleurs, mais Lambda layers est la m√©thode pr√©vue pour s√©parer et r√©utiliser proprement ces ‚Äúannexes‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:249:9ae3b23638688cd2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 249,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to use Amazon DynamoDB to store customer orders. The developer‚Äôs company requires all customer data to be encrypted at rest with a key that the company generates.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Create the DynamoDB table with encryption set to None. Code the application to use the key to decrypt the data when the application reads from the table. Code the application to use the key to encrypt the data when the application writes to the table.",
        "B": "Store the key by using AWS Key Management Service (AWS KMS). Choose an AWS KMS customer managed key during creation of the DynamoDB table. Provide the Amazon Resource Name (ARN) of the AWS KMS key.",
        "C": "Store the key by using AWS Key Management Service (AWS KMS). Create the DynamoDB table with default encryption. Include the kms:Encrypt parameter with the Amazon Resource Name (ARN) of the AWS KMS key when using the DynamoDB software development kit (SDK).",
        "D": "Store the key by using AWS Key Management Service (AWS KMS). Choose an AWS KMS AWS managed key during creation of the DynamoDB table. Provide the Amazon Resource Name (ARN) of the AWS KMS key."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124860-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 29, 2023, 1:59 a.m.",
      "textHash": "9ae3b23638688cd2",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, on parle de chiffrement ¬´ au repos ¬ª : les donn√©es doivent √™tre chiffr√©es quand elles sont stock√©es sur disque dans DynamoDB.\nL‚Äôentreprise veut une cl√© qu‚Äôelle g√©n√®re et contr√¥le : cela correspond √† une cl√© KMS ¬´ customer managed key ¬ª (cl√© g√©r√©e par le client) dans AWS KMS.\nAWS KMS est le service qui stocke et prot√®ge les cl√©s de chiffrement et permet aux services AWS (comme DynamoDB) de chiffrer/d√©chiffrer automatiquement.\nLa bonne approche est donc de cr√©er/avoir une cl√© KMS g√©r√©e par le client, puis de choisir cette cl√© lors de la cr√©ation de la table DynamoDB (en fournissant son ARN).\nAinsi, DynamoDB chiffre automatiquement toutes les donn√©es au repos avec cette cl√©, sans que l‚Äôapplication n‚Äôait √† chiffrer elle‚Äëm√™me.\nA est faux car chiffrer dans l‚Äôapplication ne garantit pas le chiffrement ¬´ au repos ¬ª g√©r√© par DynamoDB et complique inutilement.\nC est faux car on ne passe pas un param√®tre kms:Encrypt dans les appels DynamoDB SDK pour le chiffrement au repos : c‚Äôest un r√©glage de la table.\nD est faux car une cl√© ¬´ AWS managed ¬ª est g√©r√©e par AWS, pas par l‚Äôentreprise, donc ne respecte pas l‚Äôexigence.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine le casier de l‚Äô√©cole o√π tu ranges les commandes des clients (comme des feuilles). Tu dois fermer le casier avec un cadenas, et le cadenas doit √™tre fabriqu√© par ton √©cole (pas un cadenas ‚Äúfourni par d√©faut‚Äù).**\n\nConcept : ‚Äúchiffrer au repos‚Äù = m√™me quand les feuilles sont juste rang√©es dans le casier, elles restent illisibles sans la cl√© du cadenas.\nLa bo√Æte √† cl√©s (AWS KMS) = l‚Äôendroit s√©curis√© o√π l‚Äô√©cole garde et g√®re les cl√©s/cadenas.\nL‚Äôentreprise veut une cl√© qu‚Äôelle g√©n√®re : donc un cadenas ‚Äúg√©r√© par l‚Äôentreprise‚Äù, pas un cadenas standard.\nR√©ponse B : tu mets la cl√© dans la bo√Æte √† cl√©s (KMS) et, en cr√©ant le casier (table DynamoDB), tu choisis ce cadenas-l√† (customer managed key) en donnant son ‚Äúnom officiel‚Äù (ARN).\nPourquoi pas A : √ßa te force √† chiffrer/d√©chiffrer toi-m√™me √† chaque lecture/√©criture, et le casier n‚Äôest pas prot√©g√© ‚Äúau repos‚Äù par d√©faut.\nPourquoi pas D : ‚ÄúAWS managed key‚Äù = cadenas fourni et g√©r√© par AWS, pas par l‚Äôentreprise.\nPourquoi pas C : tu n‚Äôas pas besoin d‚Äôajouter un bouton sp√©cial √† chaque fois que tu ranges une feuille ; le casier doit √™tre verrouill√© automatiquement avec la bonne cl√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:320:ee386b779fe7ff1e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 320,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer built an application that calls an external API to obtain data, processes the data, and saves the result to Amazon S3. The developer built a container image with all of the necessary dependencies to run the application as a container.The application runs locally and requires minimal CPU and RAM resources. The developer has created an Amazon ECS cluster. The developer needs to run the application hourly in Amazon Elastic Container Service (Amazon ECS).Which solution will meet these requirements with the LEAST amount of infrastructure management overhead?",
      "choices": {
        "A": "Add a capacity provider to manage instances.",
        "B": "Add an Amazon EC2 instance that runs the application.",
        "C": "Define a task definition with an AWS Fargate launch type.",
        "D": "Create an Amazon ECS cluster and add the managed node groups feature to run the application."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134139-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 18, 2024, 12:29 p.m.",
      "textHash": "ee386b779fe7ff1e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:s3:d8628cb7",
      "frExplanation": "On veut ex√©cuter un conteneur (image Docker) dans AWS toutes les heures, avec peu de CPU/RAM, et avec le moins d‚Äôadministration possible.\nAmazon ECS est le service qui orchestre des conteneurs. Pour les faire tourner, il faut soit g√©rer des serveurs (EC2), soit utiliser un mode ‚Äúsans serveur‚Äù.\nAWS Fargate est le mode ‚Äúsans serveur‚Äù pour ECS : AWS fournit et g√®re automatiquement les machines, le r√©seau et la capacit√©.\nAvec une ‚Äútask definition‚Äù en Fargate, vous d√©crivez juste le conteneur, la m√©moire/CPU, et la commande √† lancer ; ECS ex√©cute la t√¢che quand vous la d√©clenchez (par ex. via un planning horaire).\nLes options avec EC2 (capacity provider, ajouter une instance, node groups) impliquent de g√©rer des instances : patching, scaling, capacit√©, co√ªts d‚Äôinstances m√™me quand rien ne tourne.\nComme l‚Äôapplication est l√©g√®re et p√©riodique, Fargate √©vite de maintenir des serveurs allum√©s et r√©duit l‚Äôoverhead.\nDonc la meilleure solution avec le moins de gestion d‚Äôinfrastructure est : d√©finir une task definition avec le launch type AWS Fargate (C).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un club au lyc√©e qui doit faire une petite t√¢che chaque heure (ex: aller chercher une info au CDI, la r√©sumer, puis la ranger dans un classeur). Tu peux soit g√©rer toi-m√™me des √©l√®ves ‚Äúde permanence‚Äù toute la journ√©e, soit appeler un surveillant ‚Äú√† la demande‚Äù juste quand il faut.**\n\nConcept : Amazon ECS, c‚Äôest l‚Äôorganisation du club. Le ‚Äúcontainer‚Äù, c‚Äôest une bo√Æte avec tout le mat√©riel pour faire la t√¢che. Tu veux juste que √ßa tourne 1 fois par heure, sans g√©rer des machines.\nPourquoi C : AWS Fargate, c‚Äôest le surveillant ‚Äú√† la demande‚Äù : tu dis quoi faire (task definition), et AWS s‚Äôoccupe de fournir le PC, l‚Äôallumer, l‚Äô√©teindre, et g√©rer la place et l‚Äô√©nergie.\nA, B, D reviennent √† garder des ‚Äú√©l√®ves/PC‚Äù en permanence (des serveurs) et √† les g√©rer (mises √† jour, capacit√©, pannes), m√™me quand l‚Äôappli ne travaille pas.\nDonc C = le moins de gestion d‚Äôinfrastructure, parfait pour une t√¢che l√©g√®re et planifi√©e chaque heure.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:319:3e75129715f80c3f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 319,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a publicly accessible enterprise website consisting of only static assets. The developer is hosting the website in Amazon S3 and serving the website to users through an Amazon CloudFront distribution. The users of this application must not be able to access the application content directly from an S3 bucket. All content must be served through the Amazon CloudFront distribution.Which solution will meet these requirements?",
      "choices": {
        "A": "Create a new origin access control (OAC) in CloudFront. Configure the CloudFront distribution's origin to use the new OAC. Update the S3 bucket policy to allow CloudFront OAC with read and write access to access Amazon S3 as the origin.",
        "B": "Update the S3 bucket settings. Enable the block all public access setting in Amazon S3. Configure the CloudFront distribution's with Amazon S3 as the origin. Update the S3 bucket policy to allow CloudFront write access.",
        "C": "Update the S3 bucket's static website settings. Enable static website hosting and specifying index and error documents. Update the CloudFront origin to use the S3 bucket's website endpoint.",
        "D": "Update the CloudFront distribution's origin to send a custom header. Update the S3 bucket policy with a condition by using the aws:RequestTag/tag-key key. Configure the tag-key as the custom header name, and the value being matched is the header's value."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134344-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 22, 2024, 7:54 a.m.",
      "textHash": "3e75129715f80c3f",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Objectif : emp√™cher l‚Äôacc√®s direct aux fichiers dans S3, et forcer le passage par CloudFront.\nAmazon S3 stocke les fichiers (images, HTML, CSS). Par d√©faut, si le bucket est public, n‚Äôimporte qui peut lire via l‚ÄôURL S3.\nAmazon CloudFront est un CDN : il sert le contenu plus vite et peut √™tre le seul ‚Äúportail‚Äù d‚Äôacc√®s.\nLa bonne pratique est de rendre le bucket S3 non public et d‚Äôautoriser uniquement CloudFront √† lire les objets.\nL‚ÄôOAC (Origin Access Control) est le m√©canisme moderne pour que CloudFront s‚Äôauthentifie aupr√®s de S3 (acc√®s priv√©).\nOn configure l‚Äôorigine CloudFront avec l‚ÄôOAC, puis on met une policy sur le bucket qui autorise ce CloudFront (principal CloudFront) √† faire des lectures (GetObject).\nAinsi, un utilisateur qui tente d‚Äôouvrir l‚ÄôURL S3 directement est bloqu√©, mais CloudFront peut toujours r√©cup√©rer et servir les fichiers.\nLes autres choix √©chouent : B parle d‚Äô√©criture inutile, C utilise l‚Äôendpoint ‚Äúwebsite‚Äù qui n√©cessite souvent un acc√®s public, D m√©lange des tags (RequestTag) qui ne correspondent pas √† un simple header.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que (S3) qui stocke des photocopies de cours (images, fichiers). Les √©l√®ves ne doivent PAS entrer dans la r√©serve. Ils doivent passer par le comptoir (CloudFront) qui distribue les copies.**\n\nConcept : S3 = la r√©serve o√π tout est stock√©. CloudFront = le comptoir qui sert vite et contr√¥le l‚Äôacc√®s. On veut que personne ne puisse prendre les fichiers directement dans la r√©serve.\nPourquoi A : l‚ÄôOAC, c‚Äôest comme un badge officiel donn√© au comptoir. La r√©serve accepte de donner les photocopies seulement √† quelqu‚Äôun qui a ce badge (CloudFront).\nDonc m√™me si un √©l√®ve conna√Æt l‚Äôadresse de la r√©serve, la porte reste ferm√©e.\nLes autres choix : B parle surtout de bloquer le public mais ne met pas correctement le ‚Äúbadge‚Äù de lecture via CloudFront. C ouvre un acc√®s ‚Äúsite web‚Äù direct √† la r√©serve. D est un faux ‚Äúmot de passe‚Äù (en-t√™te) plus fragile.\nAvec A, seul CloudFront peut lire les fichiers, et tout le monde passe par lui.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:318:d49240391d2e741a",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 318,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is releasing a new feature. Users can request early access to the new feature by using an application form. The company expects a surge of requests when the application form becomes available. Each request will be stored as an item in an Amazon DynamoDB table.Each item will contain the user's username, the submission date, and a validation status of UNVALIDATED. VALID, or NOT VALID. Each item also will contain the user's rating of the process on a scale of 1 to 5.Each user can submit one request. For the DynamoDB table, the developer must choose a partition key that will give the workload well-distributed records across partitions.Which DynamoDB attribute will meet these requirements?",
      "choices": {
        "A": "Username",
        "B": "Submission date",
        "C": "Validation status",
        "D": "Rating of the process on a scale of 1 to 5"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134343-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 22, 2024, 7:53 a.m.",
      "textHash": "d49240391d2e741a",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "Dans Amazon DynamoDB (base NoSQL), la cl√© de partition sert √† r√©partir les donn√©es sur plusieurs ‚Äúpartitions‚Äù pour √©viter qu‚Äôune seule partition soit surcharg√©e.\nPour bien r√©partir, il faut une valeur avec beaucoup de possibilit√©s (haute ‚Äúcardinalit√©‚Äù) et qui varie beaucoup entre les √©l√©ments.\nIci, chaque utilisateur ne peut soumettre qu‚Äôune seule demande : le champ Username est donc unique par item et tr√®s vari√©.\nCela r√©partit naturellement les √©critures lors du pic de demandes, car les items iront sur diff√©rentes partitions.\nLa Submission date risque d‚Äô√™tre tr√®s concentr√©e (beaucoup de demandes au m√™me moment), donc beaucoup d‚Äôitems avec des valeurs proches/identiques.\nValidation status n‚Äôa que 3 valeurs (UNVALIDATED/VALID/NOT VALID) : trop peu, donc ‚Äúhot partition‚Äù probable.\nRating n‚Äôa que 5 valeurs : encore pire pour la r√©partition.\nDonc la meilleure cl√© de partition est Username.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:444:7ae562d881010132",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 444,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a database of products. Queries for frequently accessed products must have retrieval times of microseconds. To ensure data consistency, the application cache must be updated whenever products are added, changed, or deleted.Which solution will meet these requirements?",
      "choices": {
        "A": "Set up an Amazon DynamoDB database and a DynamoDB Accelerator (DAX) cluster.",
        "B": "Set up an Amazon RDS database and an Amazon ElastiCache for Redis cluster. Implement a lazy loading caching strategy with ElastiCache.",
        "C": "Setup an Amazon DynamoDB database that has an in-memory cache. Implement a lazy loading caching strategy in the application.",
        "D": "Set up an Amazon RDS database and an Amazon DynamoDB Accelerator (DAX) cluster. Specify a TTL setting for the DAX cluster."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/149825-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 20, 2024, 7:23 a.m.",
      "textHash": "7ae562d881010132",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:0534320e",
      "frExplanation": "On veut des lectures en microsecondes : c‚Äôest typiquement un besoin de cache en m√©moire tr√®s rapide.\nAmazon DynamoDB est une base NoSQL g√©r√©e. DAX (DynamoDB Accelerator) est un cache en m√©moire sp√©cialement con√ßu pour DynamoDB, avec des temps de lecture en microsecondes.\nLa coh√©rence demand√©e signifie : quand un produit est ajout√©/modifi√©/supprim√©, le cache ne doit pas servir une ancienne valeur.\nDAX s‚Äôint√®gre nativement avec DynamoDB et g√®re automatiquement l‚Äôinvalidation/rafra√Æchissement du cache lors des √©critures, ce qui √©vite de coder une logique complexe.\nLes strat√©gies ‚Äúlazy loading‚Äù (charger dans le cache seulement √† la premi√®re lecture) peuvent laisser des donn√©es p√©rim√©es apr√®s une mise √† jour, sauf si on ajoute une gestion d‚Äôinvalidation.\nRDS + Redis peut √™tre rapide, mais il faut g√©rer soi‚Äëm√™me la mise √† jour/invalidation du cache √† chaque changement pour garantir la coh√©rence.\nL‚Äôoption C est vague (‚ÄúDynamoDB avec cache en m√©moire‚Äù) et repose sur du code applicatif pour la coh√©rence.\nL‚Äôoption D est incoh√©rente : DAX ne s‚Äôutilise pas avec RDS, et le TTL ne garantit pas la coh√©rence imm√©diate.\nDonc DynamoDB + DAX (A) r√©pond √† la fois √† la latence microsecondes et √† la mise √† jour automatique du cache.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le catalogue de produits comme la liste des livres d‚Äôune biblioth√®que. Tu veux retrouver certains livres ‚Äútr√®s demand√©s‚Äù instantan√©ment, et d√®s qu‚Äôun livre est ajout√©/modifi√©/supprim√©, la petite liste rapide au comptoir doit √™tre mise √† jour tout de suite.**\n\nConcept : la base de donn√©es, c‚Äôest l‚Äôentrep√¥t principal des infos (la biblioth√®que). Le cache, c‚Äôest une mini-liste ultra rapide des livres les plus demand√©s (au comptoir) pour r√©pondre en microsecondes.\nPourquoi A : DynamoDB = biblioth√®que tr√®s rapide et bien organis√©e. DAX = le comptoir ‚Äúturbo‚Äù coll√© √† DynamoDB, fait pour r√©pondre en microsecondes.\nEt surtout, DAX garde automatiquement ce comptoir coh√©rent avec la biblioth√®que quand un produit change (ajout/modif/suppression), donc le cache se met √† jour sans bricolage.\nPourquoi pas B/C : ‚Äúlazy loading‚Äù = on remplit le comptoir seulement quand quelqu‚Äôun demande, donc apr√®s une modification tu peux servir une vieille info.\nPourquoi pas D : DAX est fait pour DynamoDB, pas pour une autre base comme RDS, et le TTL c‚Äôest juste une date d‚Äôexpiration, pas une mise √† jour imm√©diate.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:151:518797b74adcb2b9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 151,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA team of developers is using an AWS CodePipeline pipeline as a continuous integration and continuous delivery (CI/CD) mechanism for a web application. A developer has written unit tests to programmatically test the functionality of the application code. The unit tests produce a test report that shows the results of each individual check. The developer now wants to run these tests automatically during the CI/CD process.Which solution will meet this requirement with the LEAST operational effort?",
      "choices": {
        "A": "Write a Git pre-commit hook that runs the tests before every commit. Ensure that each developer who is working on the project has the pre-commit hook installed locally. Review the test report and resolve any issues before pushing changes to AWS CodeCommit.",
        "B": "Add a new stage to the pipeline. Use AWS CodeBuild as the provider. Add the new stage after the stage that deploys code revisions to the test environment. Write a buildspec that fails the CodeBuild stage if any test does not pass. Use the test reports feature of CodeBuild to integrate the report with the CodeBuild console. View the test results in CodeBuild. Resolve any issues.",
        "C": "Add a new stage to the pipeline. Use AWS CodeBuild as the provider. Add the new stage before the stage that deploys code revisions to the test environment. Write a buildspec that fails the CodeBuild stage if any test does not pass. Use the test reports feature of CodeBuild to integrate the report with the CodeBuild console. View the test results in CodeBuild. Resolve any issues.",
        "D": "Add a new stage to the pipeline. Use Jenkins as the provider. Configure CodePipeline to use Jenkins to run the unit tests. Write a Jenkinsfile that fails the stage if any test does not pass. Use the test report plugin for Jenkins to integrate the report with the Jenkins dashboard. View the test results in Jenkins. Resolve any issues."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122569-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:46 a.m.",
      "textHash": "518797b74adcb2b9",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Objectif : ex√©cuter automatiquement des tests unitaires dans un pipeline CI/CD avec le moins d‚Äôadministration.\nCodePipeline orchestre les √©tapes (source, build, tests, d√©ploiement). CodeBuild est un service g√©r√© qui ex√©cute des commandes (compiler, tester) sans g√©rer de serveurs.\nLa meilleure pratique est de lancer les tests AVANT de d√©ployer en environnement de test : si les tests √©chouent, on bloque t√¥t et on √©vite un d√©ploiement inutile.\nAvec l‚Äôoption C, on ajoute une √©tape CodeBuild avant le d√©ploiement et on √©crit un buildspec qui lance les tests et fait √©chouer l‚Äô√©tape si un test √©choue.\nCodeBuild peut aussi publier des ‚Äútest reports‚Äù visibles dans la console, ce qui centralise les r√©sultats sans outil externe.\nA demande une installation manuelle sur chaque poste (pas fiable, effort √©lev√©). D impose g√©rer Jenkins (serveurs, plugins), donc plus d‚Äôop√©rations.\nB ex√©cute les tests apr√®s le d√©ploiement, ce qui est moins logique et plus co√ªteux en temps/ressources.\nDonc C r√©pond au besoin d‚Äôautomatisation avec le minimum d‚Äôeffort op√©rationnel.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cha√Æne de production de pizzas au self : on pr√©pare la p√¢te, on go√ªte (contr√¥le qualit√©), puis seulement apr√®s on met au four et on sert aux √©l√®ves.**\n\nDans un CI/CD, la ‚Äúcha√Æne‚Äù (CodePipeline) fait avancer le code √©tape par √©tape.\nLes tests unitaires, c‚Äôest le ‚Äúgo√ªt‚Äù : on v√©rifie chaque ingr√©dient avant de servir.\nLa solution C ajoute une √©tape automatique de test avec CodeBuild (un cuisinier automatique) AVANT d‚Äôenvoyer la nouvelle pizza dans l‚Äôenvironnement de test (avant de la ‚Äúservir pour essai‚Äù).\nSi un test rate, CodeBuild fait √©chouer l‚Äô√©tape : la pizza ne sort pas de la cuisine.\nLe rapport de tests, c‚Äôest la fiche de d√©gustation affich√©e au tableau (console CodeBuild), sans travail manuel.\nA d√©pend de chaque √©l√®ve (hook local) : facile √† oublier, donc plus d‚Äôeffort et moins fiable.\nB teste apr√®s avoir d√©j√† ‚Äúservi‚Äù en test : on perd du temps √† d√©ployer quelque chose de potentiellement mauvais.\nD ajoute Jenkins (un autre chef √† installer et g√©rer) : plus de maintenance, donc pas ‚Äúleast operational effort‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:237:0a633a7bc4ae9357",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 237,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing an application to encrypt files outside of AWS before uploading the files to an Amazon S3 bucket. The encryption must be symmetric and must be performed inside the application.How can the developer implement the encryption in the application to meet these requirements?",
      "choices": {
        "A": "Create a data key in AWS Key Management Service (AWS KMS). Use the AWS Encryption SDK to encrypt the files.",
        "B": "Create a Hash-Based Message Authentication Code (HMAC) key in AWS Key Management Service (AWS KMS). Use the AWS Encryption SDK to encrypt the files.",
        "C": "Create a data key pair in AWS Key Management Service (AWS KMS). Use the AWS CLI to encrypt the files.",
        "D": "Create a data key in AWS Key Management Service (AWS KMS). Use the AWS CLI to encrypt the files."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124824-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:36 p.m.",
      "textHash": "0a633a7bc4ae9357",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "On veut chiffrer des fichiers avant de les envoyer sur S3, donc le chiffrement doit se faire c√¥t√© application (client-side), pas par S3.\nLe chiffrement demand√© est sym√©trique : la m√™me cl√© sert √† chiffrer et d√©chiffrer.\nAWS KMS est un service qui prot√®ge des cl√©s ma√Ætres et peut g√©n√©rer des ¬´ data keys ¬ª (cl√©s de donn√©es) pour chiffrer localement.\nLe bon mod√®le est : l‚Äôapplication demande √† KMS une data key, utilise la cl√© en clair pour chiffrer le fichier, puis conserve avec le fichier la version chiffr√©e de la data key.\nL‚ÄôAWS Encryption SDK est une biblioth√®que faite pour ce sc√©nario : chiffrement dans le code, gestion correcte des data keys et des m√©tadonn√©es.\nB est faux car une cl√© HMAC sert √† signer/v√©rifier l‚Äôint√©grit√©, pas √† chiffrer.\nC est faux car une ¬´ paire de cl√©s ¬ª implique de l‚Äôasym√©trique (public/priv√©), pas du sym√©trique.\nD est moins adapt√© : la CLI n‚Äôest pas une impl√©mentation ‚Äúdans l‚Äôapplication‚Äù et ne g√®re pas aussi proprement le sch√©ma complet de chiffrement c√¥t√© client.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que tu veux envoyer un devoir secret dans le casier du prof (S3), mais tu veux le mettre dans une enveloppe ferm√©e avant de sortir de chez toi (hors AWS).**\n\nConcept : le chiffrement sym√©trique, c‚Äôest une seule cl√© qui sert √† fermer ET √† ouvrir l‚Äôenveloppe (m√™me cl√© des deux c√¥t√©s).\nAWS KMS, c‚Äôest comme le bureau du proviseur qui fabrique des cl√©s de fa√ßon s√ªre.\nUne ‚Äúdata key‚Äù (cl√© de donn√©es), c‚Äôest la petite cl√© que tu utilises vraiment pour fermer l‚Äôenveloppe de ton devoir.\nLe ‚ÄúAWS Encryption SDK‚Äù, c‚Äôest la trousse d‚Äôoutils dans ton appli qui sait utiliser cette cl√© pour chiffrer le fichier directement dans l‚Äôapplication.\nDonc A est bon : tu demandes une data key √† KMS, puis ton appli chiffre le fichier avec l‚ÄôEncryption SDK avant l‚Äôenvoi.\nB est faux : une cl√© HMAC sert plut√¥t √† prouver ‚Äúc‚Äôest bien moi‚Äù (signature), pas √† chiffrer un fichier.\nC est faux : une ‚Äúpaire‚Äù de cl√©s, c‚Äôest comme deux cl√©s diff√©rentes (plut√¥t asym√©trique), pas demand√© ici.\nD est moins bon : la CLI, c‚Äôest une commande √† taper, pas ‚Äúdans l‚Äôapplication‚Äù comme demand√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:215:f4c33f8a5ad81a23",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 215,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to manage AWS infrastructure as code and must be able to deploy multiple identical copies of the infrastructure, stage changes, and revert to previous versions.Which approach addresses these requirements?",
      "choices": {
        "A": "Use cost allocation reports and AWS OpsWorks to deploy and manage the infrastructure.",
        "B": "Use Amazon CloudWatch metrics and alerts along with resource tagging to deploy and manage the infrastructure.",
        "C": "Use AWS Elastic Beanstalk and AWS CodeCommit to deploy and manage the infrastructure.",
        "D": "Use AWS CloudFormation and AWS CodeCommit to deploy and manage the infrastructure."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124776-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:45 a.m.",
      "textHash": "f4c33f8a5ad81a23",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Ici, on veut g√©rer l‚Äôinfrastructure ¬´ comme du code ¬ª (Infrastructure as Code) : serveurs, r√©seaux, bases, etc., d√©crits dans des fichiers.\nAWS CloudFormation sert exactement √† √ßa : on √©crit des mod√®les (templates) qui d√©crivent les ressources AWS, puis CloudFormation cr√©e/mettre √† jour/supprime tout de fa√ßon automatique.\nD√©ployer plusieurs copies identiques devient simple : on relance le m√™me template pour cr√©er plusieurs ‚Äústacks‚Äù (ex : dev, test, prod) avec les m√™mes composants.\n¬´ Stager ¬ª des changements = modifier le template dans un environnement de test, puis appliquer la mise √† jour √† la stack quand c‚Äôest valid√©.\nRevenir en arri√®re est possible car CloudFormation g√®re les mises √† jour et peut faire un rollback si une mise √† jour √©choue, et on peut red√©ployer une version pr√©c√©dente du template.\nAWS CodeCommit est un d√©p√¥t Git g√©r√© par AWS : il versionne les fichiers CloudFormation (historique, branches, tags), ce qui permet de retrouver une ancienne version et la red√©ployer.\nLes autres choix ne sont pas adapt√©s : CloudWatch sert √† surveiller, OpsWorks/Beanstalk ciblent surtout le d√©ploiement d‚Äôapplications, pas la gestion compl√®te et versionn√©e de toute l‚Äôinfrastructure.\nDonc CloudFormation + CodeCommit r√©pond aux besoins : copies identiques, changements contr√¥l√©s, et retour √† une version pr√©c√©dente.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:212:145bf0f48e5b5d13",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 212,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses Amazon DynamoDB as a data store for its order management system. The company frontend application stores orders in a DynamoDB table. The DynamoDB table is configured to send change events to a DynamoDB stream. The company uses an AWS Lambda function to log and process the incoming orders based on data from the DynamoDB stream.An operational review reveals that the order quantity of incoming orders is sometimes set to 0. A developer needs to create a dashboard that will show how many unique customers this problem affects each day.What should the developer do to implement the dashboard?",
      "choices": {
        "A": "Grant the Lambda function‚Äôs execution role permissions to upload logs to Amazon CloudWatch Logs. Implement a CloudWatch Logs Insights query that selects the number of unique customers for orders with order quantity equal to 0 and groups the results in 1-day periods. Add the CloudWatch Logs Insights query to a CloudWatch dashboard.",
        "B": "Use Amazon Athena to query AWS CloudTrail API logs for API calls. Implement an Athena query that selects the number of unique customers for orders with order quantity equal to 0 and groups the results in 1-day periods. Add the Athena query to an Amazon CloudWatch dashboard.",
        "C": "Configure the Lambda function to send events to Amazon EventBridge. Create an EventBridge rule that groups the number of unique customers for orders with order quantity equal to 0 in 1-day periods. Add a CloudWatch dashboard as the target of the rule.",
        "D": "Turn on custom Amazon CloudWatch metrics for the DynamoDB stream of the DynamoDB table. Create a CloudWatch alarm that groups the number of unique customers for orders with order quantity equal to 0 in 1-day periods. Add the CloudWatch alarm to a CloudWatch dashboard."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124765-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 3:55 a.m.",
      "textHash": "145bf0f48e5b5d13",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Ici, le probl√®me (quantit√© = 0) est d√©tect√© au moment o√π la fonction AWS Lambda traite les √©v√©nements du flux DynamoDB (DynamoDB Streams). Le moyen le plus simple est donc de faire √©crire par Lambda des logs contenant l‚Äôidentifiant client et la quantit√© de commande dans Amazon CloudWatch Logs (service de stockage/consultation de journaux). Ensuite, CloudWatch Logs Insights permet d‚Äôinterroger ces logs comme une base de donn√©es : on filtre les lignes o√π quantity = 0, on compte les clients uniques (distinct customerId) et on regroupe par jour (bin(1d)). Enfin, on ajoute cette requ√™te Logs Insights dans un tableau de bord CloudWatch pour visualiser le nombre de clients impact√©s chaque jour. Les autres choix ne conviennent pas : CloudTrail/Athena analyse des appels API (pas le contenu des commandes), EventBridge ne ‚Äúgroupe‚Äù pas des m√©triques par jour tout seul, et DynamoDB Streams/CloudWatch metrics ne fournissent pas directement un comptage de clients uniques.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : chaque commande d‚Äô√©l√®ve passe au guichet, et un surveillant note tout dans un cahier (un journal) : qui a command√© quoi, et combien.**\n\nIci, DynamoDB = le cahier des commandes, et le ‚Äústream‚Äù = la copie des changements envoy√©e au surveillant. La fonction Lambda = le surveillant qui lit chaque nouvelle commande et √©crit un compte-rendu dans un journal (les logs). Probl√®me : parfois la quantit√© = 0, comme si un √©l√®ve ‚Äúcommandait 0 pizza‚Äù. Pour un tableau de bord, le plus simple est de compter dans le journal : ‚Äúcombien d‚Äô√©l√®ves diff√©rents ont eu une quantit√© 0 chaque jour‚Äù. CloudWatch Logs Insights, c‚Äôest comme une recherche intelligente dans le cahier du surveillant : tu filtres quantit√©=0, tu comptes les clients uniques, tu regroupes par jour. Puis tu affiches ce r√©sultat sur un tableau d‚Äôaffichage (dashboard CloudWatch). Les autres choix ne lisent pas directement ces logs utiles ou ne savent pas facilement compter des clients uniques par jour.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:165:b10372a477f3d283",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 165,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application that runs on AWS Lambda requires access to specific highly confidential objects in an Amazon S3 bucket. In accordance with the principle of least privilege, a company grants access to the S3 bucket by using only temporary credentials.How can a developer configure access to the S3 bucket in the MOST secure way?",
      "choices": {
        "A": "Hardcode the credentials that are required to access the S3 objects in the application code. Use the credentials to access the required S3 objects.",
        "B": "Create a secret access key and access key ID with permission to access the S3 bucket. Store the key and key ID in AWS Secrets Manager. Configure the application to retrieve the Secrets Manager secret and use the credentials to access the S3 objects.",
        "C": "Create a Lambda function execution role. Attach a policy to the role that grants access to specific objects in the S3 bucket.",
        "D": "Create a secret access key and access key ID with permission to access the S3 bucket. Store the key and key ID as environment variables in Lambda. Use the environment variables to access the required S3 objects."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122585-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:15 a.m.",
      "textHash": "b10372a477f3d283",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "AWS Lambda ex√©cute du code sans serveur. Pour acc√©der √† Amazon S3 (stockage de fichiers/objets), il faut des autorisations.\nLe moyen le plus s√ªr est d‚Äô√©viter toute ‚Äúcl√©‚Äù stock√©e (access key/secret key) dans le code, des variables d‚Äôenvironnement ou un coffre : ces cl√©s sont des identifiants long terme et peuvent fuiter.\nUn ‚Äúr√¥le d‚Äôex√©cution Lambda‚Äù (IAM Role) est une identit√© attach√©e √† la fonction : AWS fournit automatiquement des identifiants temporaires (courte dur√©e) √† la fonction.\nOn applique le ‚Äúmoindre privil√®ge‚Äù en attachant au r√¥le une policy IAM qui autorise uniquement les actions n√©cessaires (ex: s3:GetObject) et seulement sur les objets pr√©cis (ARN des objets) du bucket.\nAinsi, il n‚Äôy a rien √† g√©rer ni √† faire tourner (pas de rotation de cl√©s), et l‚Äôacc√®s est limit√© et temporaire par conception.\nA est dangereux (cl√©s en dur). B et D stockent des cl√©s long terme (m√™me si cach√©es), donc moins s√©curis√© que l‚Äôacc√®s via r√¥le.\nDonc la configuration la plus s√©curis√©e est : cr√©er un r√¥le d‚Äôex√©cution Lambda et lui donner une policy limit√©e aux objets S3 requis (C).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé™ Imagine des gardiens dans un festival** : certains v√©rifient vite, d'autres fouillent en profondeur.\n\n**CloudFront Functions** = **Le vigile √† l'entr√©e** üëÆ - V√©rifie vite le billet (URL, headers), ultra rapide (1ms), mais simple.\n\n**Lambda@Edge** = **Le contr√¥le s√©curit√© complet** üõÇ - Peut fouiller le sac, appeler une base (30s max), plus lent mais puissant.\n\n**üß† Mn√©motechnique :** \"**F**unctions = **F**aibles mais **F**r√©n√©tiques (rapides)\" | \"**L**ambda = **L**ourd mais **L**imit√©\"\n\n**Pourquoi Lambda@Edge ici :** Car on doit appeler AWS STS (service externe), impossible avec Functions.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:154:1cc5dfce1fe07fbd",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 154,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer maintains applications that store several secrets in AWS Secrets Manager. The applications use secrets that have changed over time. The developer needs to identify required secrets that are still in use. The developer does not want to cause any application downtime.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Configure an AWS CloudTrail log file delivery to an Amazon S3 bucket. Create an Amazon CloudWatch alarm for the GetSecretValue Secrets Manager API operation requests.",
        "B": "Create a secretsmanager-secret-unused AWS Config managed rule. Create an Amazon EventBridge rule to initiate notifications when the AWS Config managed rule is met.",
        "C": "Deactivate the applications secrets and monitor the applications error logs temporarily.",
        "D": "Configure AWS X-Ray for the applications. Create a sampling rule to match the GetSecretValue Secrets Manager API operation requests."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122573-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:56 a.m.",
      "textHash": "1cc5dfce1fe07fbd",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Objectif : savoir quels secrets AWS Secrets Manager sont encore utilis√©s, sans arr√™ter les applications.\nAWS Secrets Manager stocke des ‚Äúsecrets‚Äù (mots de passe, cl√©s API) et les applis les lisent via l‚ÄôAPI GetSecretValue.\nLa meilleure approche est d‚Äôobserver l‚Äôusage sans rien casser : AWS Config est un service qui v√©rifie en continu l‚Äô√©tat et la conformit√© des ressources.\nLa r√®gle g√©r√©e ‚Äúsecretsmanager-secret-unused‚Äù d√©tecte les secrets qui n‚Äôont pas √©t√© utilis√©s depuis une p√©riode donn√©e (donc probablement inutiles).\nEnsuite, Amazon EventBridge peut d√©clencher une notification (email, ticket, SNS, etc.) quand Config signale qu‚Äôun secret est ‚Äúunused‚Äù.\nCela r√©pond au besoin : identifier les secrets encore utilis√©s (ceux qui ne sont PAS marqu√©s unused) et √©viter toute indisponibilit√©.\nA est moins adapt√© : CloudTrail + alarme sur GetSecretValue peut g√©n√©rer beaucoup de bruit et ne donne pas directement une liste de secrets inutilis√©s.\nC est risqu√© : d√©sactiver des secrets peut casser l‚Äôapplication et provoquer une panne.\nD (X-Ray) sert surtout au tra√ßage de performances applicatives, pas √† l‚Äôinventaire fiable des secrets inutilis√©s.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine le casier des objets trouv√©s au coll√®ge, avec plein de cl√©s √©tiquet√©es (les ‚Äúsecrets‚Äù). Des √©l√®ves (les applis) empruntent certaines cl√©s pour ouvrir des salles. Certaines cl√©s ont √©t√© remplac√©es avec le temps, et tu veux savoir lesquelles sont encore utilis√©es, sans fermer le coll√®ge.**\n\nConcept : un ‚Äúsecret‚Äù = une cl√© confidentielle. AWS Secrets Manager = le casier s√©curis√© qui garde ces cl√©s. Tu veux rep√©rer les cl√©s qui ne servent plus, sans tester en cassant des portes.\nPourquoi B : AWS Config, c‚Äôest comme un surveillant qui fait des contr√¥les r√©guliers et note les r√®gles. La r√®gle ‚Äúsecret-unused‚Äù dit : ¬´ cette cl√© n‚Äôa pas √©t√© utilis√©e depuis un moment ¬ª. EventBridge, c‚Äôest comme un haut-parleur qui t‚Äôenvoie une alerte d√®s qu‚Äôune cl√© est rep√©r√©e comme inutilis√©e.\nDonc tu identifies les secrets encore utilis√©s, sans d√©sactiver quoi que ce soit, et sans provoquer de panne.\nLes autres : C casse volontairement des acc√®s (risque de panne). A et D regardent surtout des ‚Äújournaux/tra√ßage‚Äù d‚Äôactions, mais B te donne directement la liste des secrets inutilis√©s + alerte.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:328:16ab3d1f0c33a853",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 328,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on an ecommerce platform that communicates with several third-party payment processing APIs. The third-party payment services do not provide a test environment.The developer needs to validate the ecommerce platform's integration with the third-party payment processing APIs. The developer must test the API integration code without invoking the third-party payment processing APIs.Which solution will meet these requirements?",
      "choices": {
        "A": "Set up an Amazon API Gateway REST API with a gateway response configured for status code 200. Add response templates that contain sample responses captured from the real third-party API.",
        "B": "Set up an AWS AppSync GraphQL API with a data source configured for each third-party API. Specify an integration type of Mock. Configure integration responses by using sample responses captured from the real third-party API.",
        "C": "Create an AWS Lambda function for each third-party API. Embed responses captured from the real third-party API. Configure Amazon Route 53 Resolver with an inbound endpoint for each Lambda function's Amazon Resource Name (ARN).",
        "D": "Set up an Amazon API Gateway REST API for each third-party API. Specify an integration request type of Mock. Configure integration responses by using sample responses captured from the real third-party API."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134345-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 22, 2024, 8:08 a.m.",
      "textHash": "16ab3d1f0c33a853",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:34097db7",
      "frExplanation": "Objectif : tester votre code d‚Äôint√©gration API sans appeler les vrais services de paiement (pas d‚Äôenvironnement de test).\nAmazon API Gateway sert √† cr√©er des endpoints HTTP/REST que votre application peut appeler comme si c‚Äô√©tait un service externe.\nAvec une int√©gration de type ¬´ Mock ¬ª dans API Gateway, l‚ÄôAPI ne contacte aucun backend : elle renvoie directement une r√©ponse pr√©d√©finie.\nOn configure ensuite des ¬´ integration responses ¬ª (et √©ventuellement des mod√®les) pour renvoyer des exemples de r√©ponses r√©alistes captur√©es auparavant.\nAinsi, votre plateforme e-commerce peut ex√©cuter ses appels, g√©rer les codes HTTP, le JSON, les erreurs, etc., sans risque de d√©clencher de vrais paiements.\nPourquoi D : c‚Äôest exactement la fonctionnalit√© Mock d‚ÄôAPI Gateway, pens√©e pour simuler un service.\nPourquoi pas A : un ‚Äúgateway response‚Äù concerne surtout les r√©ponses g√©n√©r√©es par API Gateway en cas d‚Äôerreur/conditions sp√©cifiques, pas une simulation compl√®te par endpoint.\nPourquoi pas B : AppSync est pour GraphQL, pas adapt√© si vous testez des APIs REST tierces ; et le ‚ÄúMock‚Äù attendu ici est celui d‚ÄôAPI Gateway.\nPourquoi pas C : Route 53 Resolver ne sert pas √† router des appels HTTP vers des fonctions Lambda via ARN ; c‚Äôest pour DNS dans des r√©seaux.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu dois t‚Äôentra√Æner √† commander des pizzas pour la soir√©e, mais les vraies pizzerias ne prennent pas les commandes ‚Äúpour tester‚Äù. Tu veux quand m√™me v√©rifier que ton script d‚Äôappel, ton adresse et ton paiement sont bien envoy√©s.**\n\nConcept : tu cr√©es une ‚Äúfausse pizzeria‚Äù qui r√©pond comme une vraie, sans appeler la vraie pizzeria. Comme √ßa, tu testes ton appli sans d√©clencher de vrais paiements.\nAvec Amazon API Gateway (un ‚Äústandard t√©l√©phonique‚Äù pour APIs), tu peux cr√©er une fausse version de chaque service de paiement.\nL‚Äôoption D dit : pour chaque API externe, tu configures une int√©gration ‚ÄúMock‚Äù (r√©ponse invent√©e) et tu mets des exemples de r√©ponses r√©elles enregistr√©es.\nDonc ton site envoie ses demandes comme d‚Äôhabitude, et la fausse pizzeria r√©pond ‚Äúpaiement OK‚Äù avec le bon format.\nTu valides que ton code comprend bien les r√©ponses, sans contacter les vrais services.\nA ne fait qu‚Äôune r√©ponse globale 200 (trop simpliste, pas une vraie imitation par API).\nB et C ajoutent des outils inutiles/moins adapt√©s ici.\nDonc D est la bonne solution.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:443:8fa0beffac2dad39",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 443,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA gaming application stores scores for players in an Amazon DynamoDB table that has four attributes: user_id, user_name, user_score, and user_rank. The users are allowed to update their names only. A user is authenticated by web identity federation.Which set of conditions should be added in the policy attached to the role for the dynamodb:PutItem API call?",
      "choices": {},
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148960-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 10, 2024, 8:11 a.m.",
      "textHash": "8fa0beffac2dad39",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, on veut autoriser un utilisateur connect√© (via f√©d√©ration d‚Äôidentit√© web, ex: Google/Cognito) √† modifier uniquement son propre enregistrement DynamoDB, et seulement le champ du nom.\nDynamoDB est une base NoSQL : PutItem peut √©crire/remplacer un item complet, donc il faut des conditions IAM pour √©viter qu‚Äôun joueur change le score ou le rang.\nLa bonne approche est d‚Äôutiliser des conditions qui lient l‚Äôaction √† l‚Äôidentit√© de l‚Äôutilisateur (son identifiant dans le jeton) et qui limitent les attributs modifiables.\nOn ajoute une condition pour que la cl√© de partition (user_id) de l‚Äôitem corresponde √† l‚Äôutilisateur authentifi√© (ex: valeur issue des ‚Äúclaims‚Äù du jeton web).\nOn ajoute aussi une condition pour n‚Äôautoriser que l‚Äôattribut user_name dans les attributs √©crits (interdire user_score et user_rank).\nAinsi, m√™me si le client envoie un PutItem avec un score modifi√©, IAM le bloque car les attributs ne sont pas autoris√©s.\nC‚Äôest pour cela que l‚Äôensemble de conditions de la r√©ponse A (contr√¥le sur l‚Äôidentit√© + contr√¥le sur les attributs) est le bon.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine le tableau des scores d‚Äôun jeu vid√©o affich√© au CDI : chaque ligne = un joueur, avec son ID, son pseudo, son score et son rang. Les √©l√®ves ont le droit de changer seulement leur pseudo, pas leur score ni leur rang.**\n\nLe ‚Äúr√¥le + policy‚Äù c‚Äôest comme un badge qui dit ce que tu as le droit de modifier sur le tableau. Avec ‚Äúweb identity federation‚Äù, le badge contient ton identit√© (ton user_id) prouv√©e par un site de connexion. La bonne r√©ponse (A) ajoute des conditions qui disent : 1) tu ne peux modifier que la ligne qui correspond √† TON user_id (comme ‚Äútu ne touches qu‚Äô√† ta propre ligne‚Äù). 2) tu ne peux changer que le champ user_name (le pseudo), et pas user_score ni user_rank (comme ‚Äútu peux corriger ton surnom, pas tes points‚Äù). Donc A est la bonne r√©ponse car elle bloque toute triche : impossible d‚Äô√©crire sur la ligne d‚Äôun autre, et impossible d‚Äôaugmenter son score en douce.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:227:8d170642aa7ba972",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 227,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has built an application that inserts data into an Amazon DynamoDB table. The table is configured to use provisioned capacity. The application is deployed on a burstable nano Amazon EC2 instance. The application logs show that the application has been failing because of a ProvisionedThroughputExceededException error.Which actions should the developer take to resolve this issue? (Choose two.)",
      "choices": {
        "A": "Move the application to a larger EC2 instance.",
        "B": "Increase the number of read capacity units (RCUs) that are provisioned for the DynamoDB table.",
        "C": "Reduce the frequency of requests to DynamoDB by implementing exponential backoff.",
        "D": "Increase the frequency of requests to DynamoDB by decreasing the retry delay.",
        "E": "Change the capacity mode of the DynamoDB table from provisioned to on-demand."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124816-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:12 p.m.",
      "textHash": "8d170642aa7ba972",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_capacity_rcu_wcu",
      "frExplanation": "L‚Äôerreur ProvisionedThroughputExceededException vient de DynamoDB : la table a une capacit√© ‚Äúprovisionn√©e‚Äù (un quota fixe de lectures/√©critures par seconde).\nSi l‚Äôapplication envoie trop de requ√™tes (ou en rafales), DynamoDB refuse temporairement et renvoie cette erreur.\nLa bonne pratique c√¥t√© application est de ralentir et d‚Äô√©taler les tentatives : ‚Äúexponential backoff‚Äù = attendre un peu, puis attendre de plus en plus longtemps avant de r√©essayer.\nCela r√©duit la pression sur la table et laisse le temps √† DynamoDB d‚Äôaccepter les requ√™tes suivantes.\n√Ä l‚Äôinverse, diminuer le d√©lai de retry (augmenter la fr√©quence) aggrave le probl√®me.\nChanger d‚Äôinstance EC2 ne change pas la limite de capacit√© de la table DynamoDB.\nAugmenter les RCUs aide seulement pour les lectures, mais ici l‚Äôapp ‚Äúins√®re‚Äù (√©crit) : le probl√®me est surtout li√© au d√©bit et aux retries.\nDonc l‚Äôaction cl√© est d‚Äôimpl√©menter un backoff exponentiel pour r√©duire les requ√™tes lors des throttles.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine au coll√®ge : elle peut servir seulement un certain nombre de plateaux par minute (capacit√©).**\n\nConcept : DynamoDB en ‚Äúcapacit√© provisionn√©e‚Äù, c‚Äôest comme une cantine avec un nombre de serveurs fix√© √† l‚Äôavance. Si trop d‚Äô√©l√®ves arrivent d‚Äôun coup, la cantine refuse des gens. L‚Äôerreur ProvisionedThroughputExceededException = ‚Äútrop de demandes, on ne peut pas servir tout le monde maintenant‚Äù. Ton appli est sur un petit PC (nano) mais le vrai blocage ici, c‚Äôest surtout la cantine qui sature. La bonne action C (exponential backoff) = quand on se fait refuser, on n‚Äôinsiste pas toutes les 0,1 seconde : on attend un peu, puis un peu plus longtemps √† chaque nouvel √©chec. √áa √©vite de cr√©er un embouteillage et √ßa laisse le temps √† la cantine de rattraper le retard. D est l‚Äôinverse (pire). B/E pourraient aider, mais la r√©ponse attendue ici est de ralentir intelligemment les retries : C.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:418:f235bbcb006d7620",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 418,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has built an application running on AWS Lambda using AWS Serverless Application Model (AWS SAM).What is the correct sequence of steps to successfully deploy the application?",
      "choices": {
        "A": "1. Build the SAM template in Amazon EC2.2. Package the SAM template to Amazon EBS storage.3. Deploy the SAM template from Amazon EBS.",
        "B": "1. Build the SAM template locally.2. Package the SAM template onto Amazon S3.3. Deploy the SAM template from Amazon S3.",
        "C": "1. Build the SAM template locally.2. Deploy the SAM template from Amazon S3.3. Package the SAM template for use.",
        "D": "1. Build the SAM template locally.2. Package the SAM template from AWS CodeCommit.3. Deploy the SAM template to CodeCommit."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148935-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 9, 2024, 3:29 p.m.",
      "textHash": "f235bbcb006d7620",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:57b5999d",
      "frExplanation": "AWS SAM (Serverless Application Model) sert √† d√©crire une appli ¬´ serverless ¬ª (ex: AWS Lambda) avec un fichier template, puis √† la d√©ployer via AWS CloudFormation.\n√âtape 1 : on construit localement (sam build) pour pr√©parer le code et les d√©pendances de la fonction Lambda.\n√âtape 2 : on ¬´ package ¬ª (sam package) : SAM met le code (zip) et autres artefacts dans Amazon S3, un stockage d‚Äôobjets accessible par AWS.\nPourquoi S3 ? CloudFormation/Lambda doivent pouvoir r√©cup√©rer le code depuis une URL/objet S3 lors du d√©ploiement.\n√âtape 3 : on ¬´ deploy ¬ª (sam deploy) : CloudFormation lit le template (r√©f√©rences S3) et cr√©e/mettre √† jour les ressources (Lambda, r√¥les IAM, API, etc.).\nLes autres choix sont faux : EBS/EC2 ne sont pas le flux standard SAM, et CodeCommit est un d√©p√¥t Git (source), pas un emplacement de packaging/d√©ploiement.\nDonc la s√©quence correcte est : Build local ‚Üí Package vers S3 ‚Üí Deploy depuis S3.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois rendre un gros devoir au prof, mais il faut d‚Äôabord le pr√©parer, puis le mettre dans le casier de d√©p√¥t de la classe, puis le prof le r√©cup√®re pour le corriger.**\n\nConcept : AWS SAM, c‚Äôest comme une ‚Äúfiche d‚Äôinstructions‚Äù + tes fichiers, pour dire √† AWS comment lancer ton appli (Lambda = un petit robot qui ex√©cute ton code quand on l‚Äôappelle).\n√âtape 1 ‚ÄúBuild local‚Äù : tu pr√©pares le devoir sur ton ordi (tu v√©rifies et assembles tout).\n√âtape 2 ‚ÄúPackage vers S3‚Äù : tu mets le devoir dans le casier commun (Amazon S3 = un grand placard/stockage en ligne o√π AWS peut aller chercher les fichiers).\n√âtape 3 ‚ÄúDeploy depuis S3‚Äù : le prof prend le devoir depuis le casier et le met en place (AWS d√©ploie l‚Äôappli en utilisant ce qui est dans S3).\nPourquoi B : l‚Äôordre logique c‚Äôest pr√©parer ‚Üí d√©poser dans un endroit accessible ‚Üí installer.\nPourquoi pas A : EC2/EBS, c‚Äôest comme utiliser un PC et un disque dur lou√©s, inutile ici.\nPourquoi pas C : tu ne peux pas ‚Äúinstaller‚Äù avant d‚Äôavoir d√©pos√© le paquet.\nPourquoi pas D : CodeCommit, c‚Äôest un cahier de code (versioning), pas le casier de d√©p√¥t pour le paquet.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:440:0a70e816594e1934",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 440,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses AWS to run its learning management system (LMS) application. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The application's domain name is managed in Amazon Route 53. The application is deployed in a single AWS Region, but the company wants to improve application performance for users all over the world.Which solution will improve global performance with the LEAST operational overhead?",
      "choices": {
        "A": "Set up an Amazon CloudFront distribution that uses the ALB as the origin server. Configure Route 53 to create a DNS alias record that points the application's domain name to the CloudFront distribution URL.",
        "B": "Launch more EC2 instances behind the ALConfigure the ALB to use session affinity (sticky sessions). Create a Route 53 alias record for the ALB by using a geolocation routing policy.",
        "C": "Create an AWS Client VPN endpoint in the VPInstruct users to connect to the VPN to access the application. Create a Route 53 alias record for the VPN endpoint. Configure Route 53 to use a geolocation routing policy.",
        "D": "Deploy the application to multiple Regions across the world. Create a Route 53 alias record for the ALB by using a latency-based routing policy."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/148958-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 10, 2024, 8:04 a.m.",
      "textHash": "0a70e816594e1934",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Objectif : acc√©l√©rer l‚Äôacc√®s pour des utilisateurs partout dans le monde avec le moins de gestion.\nAmazon CloudFront est un CDN : un r√©seau mondial de serveurs qui met en cache et sert le contenu depuis un point proche de l‚Äôutilisateur.\nEn mettant CloudFront devant l‚ÄôALB (Application Load Balancer), les fichiers statiques (images, JS, CSS) et m√™me certaines r√©ponses peuvent √™tre servis plus vite, sans changer l‚Äôapplication ni d√©ployer dans plusieurs r√©gions.\nRoute 53 est le DNS : un enregistrement ‚Äúalias‚Äù peut faire pointer le nom de domaine vers CloudFront facilement.\nC‚Äôest peu d‚Äôoverhead : pas besoin d‚Äôajouter des r√©gions, de g√©rer plusieurs d√©ploiements, ni de complexifier le routage.\nB n‚Äôam√©liore pas vraiment la latence mondiale : plus d‚ÄôEC2 dans une seule r√©gion ne rapproche pas l‚Äôapp des utilisateurs, et les sticky sessions ajoutent des contraintes.\nC (VPN) rend l‚Äôacc√®s plus lent et plus complexe pour les utilisateurs, et ne vise pas la performance web.\nD peut am√©liorer la latence, mais d√©ployer et op√©rer l‚Äôapplication dans plusieurs r√©gions est beaucoup plus lourd que CloudFront.\nDonc la meilleure solution avec le moins d‚Äôeffort op√©rationnel est CloudFront + ALB en origine + alias Route 53 (A).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine le LMS comme une vid√©o de cours que le prof met sur un seul PC au CDI de ton coll√®ge. Les √©l√®ves du monde entier doivent venir la regarder. Pour aller plus vite, tu copies la vid√©o dans plein de petites ‚Äúmini-biblioth√®ques‚Äù proches des √©l√®ves, au lieu de les faire tous venir au CDI.**\n\nConcept : si tout est dans un seul endroit, ceux qui sont loin attendent plus longtemps. Une solution simple est de mettre des ‚Äúcopies‚Äù pr√®s des gens (cache), sans refaire toute l‚Äôorganisation.\nA : CloudFront, c‚Äôest comme ces mini-biblioth√®ques partout dans le monde. Les √©l√®ves prennent la copie la plus proche, donc √ßa charge plus vite. Si la copie n‚Äôexiste pas, CloudFront va la chercher au CDI (l‚ÄôALB + EC2) et la garde pour les prochains.\nRoute 53, c‚Äôest l‚Äôannuaire : au lieu de donner l‚Äôadresse du CDI, il donne l‚Äôadresse de la mini-biblioth√®que la plus proche.\nPourquoi ‚Äúmoins d‚Äôeffort‚Äù : tu ne changes pas ton LMS ni tu n‚Äôouvres plusieurs CDI dans chaque pays. Tu ajoutes juste CloudFront devant.\nB : plus de PC au CDI n‚Äôaide pas ceux qui sont loin, et ‚Äústicky sessions‚Äù complique.\nC : forcer un VPN, c‚Äôest comme obliger tout le monde √† passer par un tunnel : souvent plus lent et p√©nible.\nD : ouvrir des CDI dans plusieurs pays marche, mais c‚Äôest beaucoup plus de gestion.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:392:e46836a82913dfb1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 392,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer received the following error message during an AWS CloudFormation deployment:DELETE_FAILED (The following resource(s) failed to delete: [ASGInstanceRole12345678].)Which action should the developer take to resolve this error?",
      "choices": {
        "A": "Contact AWS Support to report an issue with the Auto Scaling Groups (ASG) service.",
        "B": "Add a DependsOn attribute to the ASGInstanceRole12345678 resource in the CloudFormation template. Then delete the stack.",
        "C": "Modify the CloudFormation template to retain the ASGInstanceRole12345678 resource. Then manually delete the resource after deployment.",
        "D": "Add a force parameter when calling CloudFormation with the role-arn of ASGInstanceRole12345678."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143082-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 29, 2024, 10:11 p.m.",
      "textHash": "e46836a82913dfb1",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "CloudFormation est un service AWS qui cr√©e et supprime des ressources (serveurs, r√¥les, bases) √† partir d‚Äôun ‚Äútemplate‚Äù.\nLors de la suppression d‚Äôun stack, CloudFormation doit supprimer chaque ressource dans le bon ordre.\nIci, l‚Äôerreur DELETE_FAILED indique que la ressource ASGInstanceRole... (un r√¥le IAM) n‚Äôa pas pu √™tre supprim√©e.\nUn r√¥le IAM ne peut pas √™tre supprim√© s‚Äôil est encore ‚Äúutilis√©‚Äù : par exemple attach√© √† un profil d‚Äôinstance, √† des politiques, ou r√©f√©renc√© par un Auto Scaling Group/instances.\nLa solution pratique est de demander √† CloudFormation de conserver ce r√¥le (policy Retain) pour que la suppression du stack puisse se terminer.\nEnsuite, vous supprimez manuellement le r√¥le IAM apr√®s avoir retir√© ce qui l‚Äôutilise (d√©tacher policies, supprimer instance profile, v√©rifier les r√©f√©rences).\nLes autres options ne r√©solvent pas la cause (DependsOn n‚Äôaide pas √† la suppression si le r√¥le est encore attach√©, pas de param√®tre ‚Äúforce‚Äù, et ce n‚Äôest pas forc√©ment un bug AWS).\nDonc la bonne r√©ponse est C.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un club au lyc√©e. Tu veux fermer le club, mais le badge ‚ÄúPr√©sident‚Äù est encore utilis√© par des √©l√®ves pour entrer dans la salle.**\n\nConcept : CloudFormation, c‚Äôest comme une liste qui cr√©e et supprime tout pour toi (le ‚Äúclub‚Äù et ses objets). Parfois, un objet ne peut pas √™tre supprim√© s‚Äôil est encore utilis√©. Ici, ASGInstanceRole, c‚Äôest un ‚Äúbadge/autorisation‚Äù (un r√¥le) utilis√© par des machines du groupe (ASG). Donc quand CloudFormation veut tout effacer, il √©choue sur le badge car quelqu‚Äôun l‚Äôutilise encore. La bonne solution est C : tu dis √† CloudFormation ‚Äúne touche pas √† ce badge‚Äù (retain), tu supprimes le reste du club, puis tu vas enlever le badge √† la main quand plus personne ne l‚Äôutilise. A est faux : ce n‚Äôest pas un bug du service, c‚Äôest une d√©pendance. B ne r√®gle pas : DependsOn aide √† l‚Äôordre de cr√©ation, pas √† forcer une suppression impossible. D n‚Äôexiste pas vraiment : on ne peut pas ‚Äúforcer‚Äù si c‚Äôest encore utilis√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:390:c6976b1765ec87c6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 390,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer manages a website that distributes its content by using Amazon CloudFront. The website's static artifacts are stored in an Amazon S3 bucket.The developer deploys some changes and can see the new artifacts in the S3 bucket. However, the changes do not appear on the webpage that the CloudFront distribution delivers.How should the developer resolve this issue?",
      "choices": {
        "A": "Configure S3 Object Lock to update to the latest version of the files every time an S3 object is updated.",
        "B": "Configure the S3 bucket to clear all old objects from the bucket before new artifacts are uploaded.",
        "C": "Set CloudFront to invalidate the cache after the artifacts have been deployed to Amazon S3.",
        "D": "Set CloudFront to modify the distribution origin after the artifacts have been deployed to Amazon S3."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143800-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 3:29 p.m.",
      "textHash": "c6976b1765ec87c6",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:s3:40147261",
      "frExplanation": "CloudFront est un CDN : il met en cache (garde en m√©moire) des fichiers proches des utilisateurs pour acc√©l√©rer l‚Äôaffichage.\nS3 est le stockage o√π se trouvent vos fichiers statiques (HTML, CSS, JS, images).\nQuand vous remplacez des fichiers dans S3, CloudFront peut continuer √† servir l‚Äôancienne version depuis son cache jusqu‚Äô√† expiration (TTL).\nDonc vous voyez bien les nouveaux fichiers dans S3, mais la page livr√©e par CloudFront reste ancienne.\nLa solution est de demander √† CloudFront d‚Äôoublier ces anciennes copies : une ¬´ invalidation ¬ª de cache.\nApr√®s l‚Äôinvalidation (ex: /index.html, /static/*), CloudFront r√©cup√®re √† nouveau les fichiers depuis S3 et sert la version √† jour.\nObject Lock (A) sert √† emp√™cher la suppression/modification, pas √† rafra√Æchir CloudFront.\nVider le bucket (B) est inutile et risqu√©, et ne force pas CloudFront √† se mettre √† jour.\nChanger l‚Äôorigine (D) ne r√©sout pas le probl√®me si l‚Äôorigine est d√©j√† le bon bucket : c‚Äôest le cache qui bloque.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : la cuisine (S3) pr√©pare les plats, mais le comptoir de service (CloudFront) garde des barquettes d√©j√† pr√™tes pour aller plus vite.**\n\nConcept : CloudFront, c‚Äôest un ‚Äúcomptoir rapide‚Äù qui garde en m√©moire (cache) des copies des fichiers du site pour servir vite, sans retourner √† la ‚Äúcuisine‚Äù √† chaque fois.\nTu changes les plats en cuisine (nouveaux fichiers dans S3), mais le comptoir continue de donner les anciennes barquettes qu‚Äôil a stock√©es.\nDonc tu ne vois pas les changements sur la page, m√™me si S3 est √† jour.\nSolution : demander au comptoir de jeter les anciennes barquettes et de reprendre les nouvelles en cuisine.\nEn AWS, √ßa s‚Äôappelle ‚Äúinvalider le cache‚Äù de CloudFront.\nC‚Äôest exactement le choix C.\nA et B ne forcent pas le comptoir √† arr√™ter de servir l‚Äôancien stock.\nD ne sert √† rien ici : l‚Äôadresse de la cuisine ne change pas, c‚Äôest juste le contenu.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:384:757b12ad0a15465b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 384,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses an AWS CloudFormation template to deploy and manage its AWS infrastructure. The CloudFormation template creates Amazon VPC security groups and Amazon EC2 security groups.A manager finds out that some engineers modified the security groups of a few EC2 instances for testing purposes. A developer needs to determine what modifications occurred.Which solution will meet this requirement?",
      "choices": {
        "A": "Add a Conditions section statement in the source YAML file of the template. Run the CloudFormation stack.",
        "B": "Perform a drift detection operation on the CloudFormation stack.",
        "C": "Execute a change set for the CloudFormation stack.",
        "D": "Use Amazon Detective to detect the modifications."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144606-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 28, 2024, 3:33 a.m.",
      "textHash": "757b12ad0a15465b",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:48fec966",
      "frExplanation": "AWS CloudFormation est un service qui cr√©e et g√®re votre infrastructure √† partir d‚Äôun ‚Äúmod√®le‚Äù (template). Un ‚Äústack‚Äù est le r√©sultat d√©ploy√© de ce mod√®le. Les security groups (dans un VPC/EC2) sont des pare-feu qui contr√¥lent quels ports/adresses peuvent acc√©der aux instances EC2. Si des ing√©nieurs modifient un security group directement dans la console, l‚Äô√©tat r√©el peut ne plus correspondre au template CloudFormation. La ‚Äúdrift detection‚Äù (d√©tection de d√©rive) compare automatiquement ce qui est d√©ploy√© dans AWS avec ce que le template dit, et liste les diff√©rences (modifications, ajouts, suppressions). C‚Äôest exactement ce qu‚Äôil faut pour savoir quelles modifications ont eu lieu. Un change set sert √† pr√©visualiser des changements que CloudFormation va appliquer, pas √† d√©tecter des changements manuels d√©j√† faits. Amazon Detective sert plut√¥t √† enqu√™ter sur des probl√®mes de s√©curit√©/comportement, pas √† comparer un stack √† son template.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:405:2f8fd8b9230959a6",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 405,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer uses an AWS Lambda function in an application to edit users' uploaded photos. The developer needs to update the Lambda function code and needs to test the updates.For testing, the developer must divide the user traffic between the original version of the Lambda function and the new version of the Lambda function.Which combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Publish a version of the original Lambda function. Make the necessary changes to the Lambda code. Publish a new version of the Lambda function.",
        "B": "Use AWS CodeBuild to detect updates to the Lambda function. Configure CodeBuild to incrementally shift traffic from the original version of the Lambda function to the new version of the Lambda function.",
        "C": "Update the original version of the Lambda function to add a function URL. Make the necessary changes to the Lambda code. Publish another function URL for the updated Lambda code.",
        "D": "Create an alias that points to the original version of the Lambda function. Configure the alias to be a weighted alias that also includes the new version of the Lambda function. Divide traffic between the two versions.",
        "E": "Create an alias that points to the original function URL. Configure the alias to be a weighted alias that also includes the additional function URL. Divide traffic between the two function URLs."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144673-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 29, 2024, 8:20 a.m.",
      "textHash": "2f8fd8b9230959a6",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Pour tester une nouvelle version d‚Äôune fonction AWS Lambda (un petit programme ex√©cut√© √† la demande), il faut pouvoir garder l‚Äôancienne version stable tout en d√©ployant une nouvelle version.\nDans Lambda, une ¬´ version ¬ª est un instantan√© immuable du code et de la configuration : une fois publi√©e, elle ne change plus.\n√âtape 1 : publier une version de la fonction actuelle (cela fige l‚Äôoriginal, utile comme r√©f√©rence et pour revenir en arri√®re).\n√âtape 2 : modifier le code, puis publier une nouvelle version (cela cr√©e un second instantan√©, s√©par√© de l‚Äôoriginal).\nAvec ces deux versions, on peut ensuite r√©partir le trafic entre elles (typiquement via un alias pond√©r√©), ce qui r√©pond au besoin de test en conditions r√©elles.\nLes options bas√©es sur CodeBuild ou les Function URLs ne sont pas le m√©canisme standard pour faire du ‚Äútraffic shifting‚Äù entre deux versions de Lambda.\nDonc la bonne combinaison est de publier l‚Äôancienne version, mettre √† jour le code, puis publier une nouvelle version.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:377:88b8b96ab10f42e3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 377,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company launched an online portal to announce a new product that the company will release in 6 months. The portal requests that users enter an email address to receive communications about the product. The company needs to create a REST API that will store the email addresses in Amazon DynamoDB.A developer has created an AWS Lambda function that can store the email addresses. The developer will deploy the Lambda function by using the AWS Serverless Application Model (AWS SAM). The developer must provide access to the Lambda function over HTTP.Which solutions will meet these requirements with the LEAST additional configuration? (Choose two.)",
      "choices": {
        "A": "Expose the Lambda function by using function URLs.",
        "B": "Expose the Lambda function by using a Gateway Load Balancer.",
        "C": "Expose the Lambda function by using a Network Load Balancer.",
        "D": "Expose the Lambda function by using AWS Global Accelerator.",
        "E": "Expose the Lambda function by using Amazon API Gateway."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143759-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:47 a.m.",
      "textHash": "88b8b96ab10f42e3",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:d4107c19",
      "frExplanation": "Pour rendre une fonction AWS Lambda accessible en HTTP, il faut un ‚Äúpoint d‚Äôentr√©e‚Äù web. Lambda est un service qui ex√©cute du code sans serveur, et DynamoDB est une base NoSQL g√©r√©e o√π stocker les emails.\nLa solution avec le moins de configuration est l‚ÄôURL de fonction (Function URL) : AWS fournit directement une URL HTTPS qui appelle la Lambda, sans cr√©er d‚Äôautres services.\nAvec AWS SAM, on peut d√©clarer cette URL simplement dans le template et d√©ployer.\nAmazon API Gateway peut aussi exposer une Lambda en REST API, mais cela demande plus de configuration (routes, m√©thodes, d√©ploiement, √©ventuellement auth, quotas).\nLes Load Balancers (Gateway/NLB) ne sont pas faits pour exposer directement une Lambda comme une API REST simple, et n√©cessitent plus d‚Äôarchitecture r√©seau.\nAWS Global Accelerator sert surtout √† acc√©l√©rer l‚Äôacc√®s mondial vers des endpoints existants, pas √† cr√©er une API HTTP pour Lambda.\nDonc, l‚Äôoption la plus ‚Äúsimple et directe‚Äù pour HTTP est l‚ÄôURL de fonction (A).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu tiens une bo√Æte √† id√©es au lyc√©e : les √©l√®ves d√©posent leur email pour √™tre pr√©venus d‚Äôun √©v√©nement. Tu as d√©j√† un surveillant (la fonction Lambda) qui prend chaque papier et le range dans un classeur (DynamoDB). Il te manque juste une ‚Äúfente‚Äù accessible dans le couloir pour d√©poser les papiers.**\n\nConcept : Lambda = le surveillant qui fait l‚Äôaction, DynamoDB = le classeur qui stocke, et il faut une porte HTTP = la fente o√π les gens d√©posent leur email depuis Internet.\nLa solution la plus simple, c‚Äôest A (function URLs) : AWS donne directement une URL web √† ta fonction, comme coller une fente sur la porte du bureau du surveillant.\nTu envoies un email via cette URL, le surveillant le re√ßoit et le range dans le classeur.\n√áa demande tr√®s peu de r√©glages en plus.\nAPI Gateway (E), c‚Äôest plut√¥t comme construire un vrai guichet avec r√®gles, tickets, contr√¥les : utile, mais plus de configuration.\nLes autres (B, C, D) sont des ‚Äúroutes/acc√©l√©rateurs‚Äù pour du trafic r√©seau, pas la fa√ßon la plus directe d‚Äôouvrir une simple porte HTTP vers Lambda.\nDonc la bonne r√©ponse avec le moins de config : A.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:376:708d5ae48770db79",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 376,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company used AWS to develop an application for customers. The application includes an Amazon API Gateway API that invokes AWS Lambda functions. The Lambda functions process data and store the data in Amazon DynamoDB tables.The company must monitor the entire application to identify potential bottlenecks in the architecture that can negatively affect customers.Which solution will meet this requirement with the LEAST development effort?",
      "choices": {
        "A": "Instrument the application with AWS X-Ray. Inspect the service map to identify errors and issues.",
        "B": "Configure Lambda exceptions and additional logging to Amazon CloudWatch. Use CloudWatch Logs Insights to query the logs.",
        "C": "Configure API Gateway to log responses to Amazon CloudWatch. Create a metric filter for the TooManyRequestsException error message.",
        "D": "Use Amazon CloudWatch metrics for the DynamoDB tables to identify all the ProvisionedThroughputExceededException error messages."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143758-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:46 a.m.",
      "textHash": "708d5ae48770db79",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_capacity_rcu_wcu",
      "frExplanation": "Pour rep√©rer des goulots d‚Äô√©tranglement, il faut voir le ‚Äúparcours‚Äù complet d‚Äôune requ√™te : API Gateway (porte d‚Äôentr√©e), Lambda (code), puis DynamoDB (base NoSQL). AWS X-Ray est un service de tra√ßage distribu√© qui suit une requ√™te de bout en bout et mesure les temps pass√©s dans chaque √©tape. En instrumentant l‚Äôapplication avec X-Ray, on obtient une ‚Äúservice map‚Äù qui montre les d√©pendances, les latences, les erreurs et o√π √ßa ralentit. C‚Äôest le moins d‚Äôeffort car X-Ray est con√ßu pour ce besoin global, sans devoir √©crire beaucoup de requ√™tes de logs ou cr√©er de multiples m√©triques. Les options B, C et D se basent surtout sur des logs/erreurs d‚Äôun seul composant √† la fois et demandent plus de configuration/analyse pour reconstituer le chemin complet. Donc A est la meilleure solution pour surveiller toute l‚Äôarchitecture et identifier rapidement les bottlenecks.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une cantine au coll√®ge : tu commandes au guichet, la cuisine pr√©pare, puis on note ton repas dans un carnet. Si √ßa rame, tu veux savoir o√π √ßa bloque exactement.**\n\nIci, l‚Äôappli c‚Äôest la cantine enti√®re. API Gateway = le guichet qui re√ßoit les commandes. Lambda = les cuisiniers qui pr√©parent. DynamoDB = le carnet o√π on √©crit les infos.\nPour trouver les ‚Äúbouchons‚Äù, il faut suivre une commande de bout en bout, pas juste regarder un seul poste.\nAWS X-Ray (A), c‚Äôest comme mettre un ticket sur chaque commande et voir son trajet et le temps pass√© √† chaque √©tape (guichet, cuisine, carnet).\nLa ‚Äúcarte des services‚Äù te montre tout le parcours et o√π √ßa ralentit ou plante, avec tr√®s peu de travail √† ajouter.\nB, C et D, c‚Äôest comme lire des cahiers de notes s√©par√©s (guichet seulement, cuisine seulement, carnet seulement) : utile, mais tu dois recoller les morceaux toi-m√™me.\nDonc A est le meilleur choix pour surveiller toute l‚Äôappli et rep√©rer vite les goulots d‚Äô√©tranglement, avec le moins d‚Äôeffort.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:370:03d1bf0eac935999",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 370,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to modify an application architecture to meet new functional requirements. Application data is stored in Amazon DynamoDB and processed for analysis in a nightly batch. The system analysts do not want to wait until the next day to view the processed data and have asked to have it available in near-real time.Which application architecture pattern would enable the data to be processed as it is received?",
      "choices": {
        "A": "Event driven",
        "B": "Client-server driven",
        "C": "Fan-out driven",
        "D": "Schedule driven"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143753-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:33 a.m.",
      "textHash": "03d1bf0eac935999",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:dynamodb:ba369195",
      "frExplanation": "Ici, les donn√©es arrivent dans Amazon DynamoDB (base NoSQL g√©r√©e) et sont aujourd‚Äôhui trait√©es la nuit en ‚Äúbatch‚Äù (traitement group√©). Le besoin change : les analystes veulent voir les r√©sultats presque tout de suite, pas le lendemain. Le bon r√©flexe est donc de d√©clencher le traitement d√®s qu‚Äôun nouvel enregistrement arrive, au lieu d‚Äôattendre une heure fixe. Une architecture ‚Äúevent driven‚Äù (pilot√©e par √©v√©nements) fait exactement √ßa : chaque nouvelle donn√©e g√©n√®re un √©v√©nement (ex. via DynamoDB Streams) qui d√©clenche automatiquement un traitement (souvent une fonction AWS Lambda). Ainsi, le calcul se fait en continu, au fil de l‚Äôeau, et les r√©sultats deviennent disponibles en near-real time. ‚ÄúSchedule driven‚Äù correspond au batch planifi√© (comme aujourd‚Äôhui), donc trop lent. ‚ÄúClient-server‚Äù d√©crit un style g√©n√©ral de communication, pas un m√©canisme de traitement en temps r√©el. ‚ÄúFan-out‚Äù sert √† envoyer un m√™me √©v√©nement √† plusieurs consommateurs, mais il faut d‚Äôabord √™tre en mode √©v√©nement : le pattern principal recherch√© est donc event driven.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : au lieu d‚Äôattendre la fin de la journ√©e pour compter les repas, chaque fois qu‚Äôun √©l√®ve passe √† la caisse, √ßa d√©clenche tout de suite une mise √† jour du compteur et des stats.**\n\nIci, les nouvelles donn√©es arrivent dans une ‚Äúbo√Æte‚Äù (DynamoDB). Avant, on faisait une ‚Äúgrosse correction‚Äù la nuit : c‚Äôest le mode schedule driven (√† heure fixe), donc trop lent. Les analystes veulent voir presque tout de suite : il faut traiter d√®s qu‚Äôune donn√©e arrive. Le mod√®le event driven, c‚Äôest exactement √ßa : chaque nouvelle entr√©e est un ‚Äú√©v√©nement‚Äù qui d√©clenche automatiquement le traitement, comme la caisse qui bip √† chaque √©l√®ve. Client-server, c‚Äôest juste ‚Äúun client demande √† un serveur‚Äù, √ßa ne garantit pas le traitement imm√©diat √† l‚Äôarriv√©e. Fan-out, c‚Äôest ‚Äúun √©v√©nement envoy√© √† plusieurs personnes‚Äù, utile si plusieurs traitements, mais la question demande surtout le d√©clenchement imm√©diat. Donc A (Event driven) est la bonne r√©ponse.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:368:dd7f997699651908",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 368,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has written a distributed application that uses microservices. The microservices are running on Amazon EC2 instances. Because of message volume, the developer is unable to match log output from each microservice to a specific transaction. The developer needs to analyze the message flow to debug the application.Which combination of steps should the developer take to meet this requirement? (Choose two.)",
      "choices": {
        "A": "Download the AWS X-Ray daemon. Install the daemon on an EC2 instance. Ensure that the EC2 instance allows UDP traffic on port 2000.",
        "B": "Configure an interface VPC endpoint to allow traffic to reach the global AWS X-Ray daemon on TCP port 2000.",
        "C": "Enable AWS X-Ray. Configure Amazon CloudWatch to push logs to X-Ray.",
        "D": "Add the AWS X-Ray software development kit (SDK) to the microservices. Use X-Ray to trace requests that each microservice makes.",
        "E": "Set up Amazon CloudWatch metric streams to collect streaming data from the microservices."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143751-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:31 a.m.",
      "textHash": "dd7f997699651908",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Ici, le probl√®me est de suivre un m√™me ‚Äúparcours‚Äù (une transaction) √† travers plusieurs microservices sur EC2. Pour √ßa, AWS X-Ray sert √† faire du tra√ßage distribu√© : il relie les appels entre services et montre le chemin complet d‚Äôune requ√™te. Pour que X-Ray fonctionne sur EC2, il faut un ‚Äúagent‚Äù local appel√© X-Ray daemon : il re√ßoit les segments de trace envoy√©s par les applications et les transmet √† AWS X-Ray. Donc il faut installer le daemon sur une instance EC2 (ou sur chaque h√¥te) et autoriser le trafic r√©seau n√©cessaire. Par d√©faut, le daemon √©coute en UDP sur le port 2000, donc le pare-feu / security group doit permettre l‚ÄôUDP 2000. Les options CloudWatch (logs/metric streams) ne reconstruisent pas automatiquement un flux de transaction entre microservices. Un endpoint VPC vers un ‚Äúdaemon global‚Äù n‚Äôest pas le mod√®le : on utilise le daemon local + l‚Äôenvoi vers le service X-Ray.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un tournoi de sport au lyc√©e : plusieurs √©quipes (microservices) se passent le ballon (messages) tr√®s vite. Tu veux suivre UNE action pr√©cise (une transaction) du d√©but √† la fin, mais tout le monde crie en m√™me temps (logs), donc tu ne sais plus qui a fait quoi.**\n\nConcept : pour suivre une action, tu mets un ‚Äútraceur‚Äù officiel qui colle un num√©ro sur le ballon et note chaque passe. Sur AWS, ce traceur s‚Äôappelle X-Ray : il suit le trajet d‚Äôune demande entre services. Pour que √ßa marche sur des ordinateurs lou√©s (EC2), il faut un petit ‚Äúcollecteur‚Äù sur place (le X-Ray daemon) qui r√©cup√®re les infos et les envoie √† X-Ray. La r√©ponse A est la bonne car elle dit exactement √ßa : t√©l√©charger et installer ce collecteur sur une instance EC2, et ouvrir la porte de communication (UDP port 2000) pour qu‚Äôil re√ßoive les traces. Les autres choix parlent de logs ou de m√©triques (comme compter les points), mais √ßa ne relie pas clairement chaque passe √† une action compl√®te.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:353:dc504704ecc44a3e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 353,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer at a company writes an AWS CloudFormation template. The template refers to subnets that were created by a separate AWS CloudFormation template that the company's network team wrote. When the developer attempts to launch the stack for the first time, the launch fails.Which template coding mistakes could have caused this failure? (Choose two.)",
      "choices": {
        "A": "The developer's template does not use the Ref intrinsic function to refer to the subnets.",
        "B": "The developer's template does not use the ImportValue intrinsic function to refer to the subnets.",
        "C": "The Mappings section of the developer's template does not refer to the subnets.",
        "D": "The network team's template does not export the subnets in the Outputs section.",
        "E": "The network team's template does not export the subnets in the Mappings section."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/140014-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 5, 2024, 8:01 a.m.",
      "textHash": "dc504704ecc44a3e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:a656a9ff",
      "frExplanation": "Ici, deux stacks CloudFormation sont s√©par√©es : l‚Äô√©quipe r√©seau cr√©e les subnets, puis le d√©veloppeur veut les r√©utiliser dans une autre stack.\nPour partager une valeur entre stacks, la stack ‚Äúproductrice‚Äù doit la publier dans Outputs avec un champ Export (ex: Export: Name: ...).\nEnsuite, la stack ‚Äúconsommatrice‚Äù doit r√©cup√©rer cette valeur avec la fonction intrins√®que Fn::ImportValue (souvent √©crite !ImportValue).\nSi le template du d√©veloppeur n‚Äôutilise pas ImportValue, il ne peut pas ‚Äúvoir‚Äù les subnets cr√©√©s ailleurs : la cr√©ation √©choue (r√©ponse B).\nSi le template r√©seau n‚Äôexporte pas les IDs de subnets dans Outputs, il n‚Äôy a rien √† importer : la cr√©ation √©choue aussi (r√©ponse D).\nRef sert surtout √† r√©f√©rencer une ressource d√©finie dans le m√™me template, pas √† travers des stacks.\nLa section Mappings est juste un tableau de correspondance statique (ex: par r√©gion), elle ne sert pas √† exporter/importer des subnets.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine l‚Äô√©cole : l‚Äô√©quipe ‚Äúr√©seau‚Äù a construit des salles (les sous-r√©seaux) et a affich√© leurs num√©ros sur un panneau officiel √† l‚Äôaccueil (les ‚Äúexports‚Äù). Toi, tu √©cris un plan pour organiser un tournoi et tu dois indiquer dans quelles salles √ßa se passe.**\n\nConcept : quand deux plans sont s√©par√©s, tu ne peux pas deviner les num√©ros des salles. Tu dois les r√©cup√©rer depuis le panneau officiel.\nDans AWS, ton plan (template) doit ‚Äúimporter‚Äù les valeurs que l‚Äôautre plan a ‚Äúexport√©es‚Äù.\nL‚Äôerreur B : tu n‚Äôutilises pas ImportValue, donc ton plan ne sait pas lire les num√©ros de salles depuis l‚Äôaccueil.\nR√©sultat : au lancement, CloudFormation ne trouve pas les sous-r√©seaux √† utiliser, donc √ßa √©choue.\nRef (A) sert surtout √† pointer vers quelque chose cr√©√© dans TON propre plan, comme une salle que TU as construite.\nMappings (C/E) c‚Äôest plut√¥t une table de correspondance √©crite dans le plan, pas un panneau officiel partag√© entre deux plans.\nDonc la bonne r√©ponse est B : il faut ImportValue pour r√©cup√©rer des sous-r√©seaux cr√©√©s par un autre template.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:348:dd87d68e2832363e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 348,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is developing a serverless application by using AWS Lambda functions. One of the Lambda functions needs to access an Amazon RDS DB instance. The DB instance is in a private subnet inside a VPC.The company creates a role that includes the necessary permissions to access the DB instance. The company then assigns the role to the Lambda function. A developer must take additional action to give the Lambda function access to the DB instance.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Assign a public IP address to the DB instance. Modify the security group of the DB instance to allow inbound traffic from the IP address of the Lambda function.",
        "B": "Set up an AWS Direct Connect connection between the Lambda function and the DB instance.",
        "C": "Configure an Amazon CloudFront distribution to create a secure connection between the Lambda function and the DB instance.",
        "D": "Configure the Lambda function to connect to the private subnets in the VPC. Add security group rules to allow traffic to the DB instance from the Lambda function."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/141193-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 25, 2024, 5:30 a.m.",
      "textHash": "dd87d68e2832363e",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Une fonction AWS Lambda s‚Äôex√©cute par d√©faut en dehors de votre VPC (r√©seau priv√© AWS). M√™me si vous lui donnez un r√¥le IAM (permissions), cela n‚Äôouvre pas l‚Äôacc√®s r√©seau : IAM contr√¥le ‚Äúle droit‚Äù, pas ‚Äúle chemin‚Äù.\nVotre base Amazon RDS est dans un sous-r√©seau priv√© d‚Äôun VPC, donc elle n‚Äôest pas accessible depuis Internet.\nPour que Lambda puisse joindre RDS, il faut attacher la fonction Lambda au VPC et choisir des sous-r√©seaux priv√©s (configuration VPC de Lambda).\nEnsuite, il faut utiliser des Security Groups (pare-feu) : autoriser en entr√©e sur le Security Group de RDS le trafic venant du Security Group de Lambda (port DB, ex. 3306/5432).\nLes autres choix sont inadapt√©s : donner une IP publique √† RDS casserait le principe ‚Äúpriv√©‚Äù, Direct Connect sert √† relier un r√©seau on-premise √† AWS, et CloudFront est pour distribuer du contenu web, pas pour acc√©der √† une base.\nDonc la bonne action est de configurer Lambda dans le VPC + r√®gles de Security Group pour permettre la connexion √† RDS.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:346:be069fb74289a464",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 346,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building a serverless application. The application uses an API key to authenticate with a third-party application. The company wants to store the external API key as a part of an AWS Lambda configuration. The company needs to have full control over the AWS Key Management Service (AWS KMS) keys that will encrypt the API key and should be visible only to authorized entities.Which solution will meet these requirements?",
      "choices": {
        "A": "Store the API key in AWS Systems Manager Parameter Store as a string parameter. Use the default AWS KMS key that AWS provides to encrypt the API key.",
        "B": "Store the API key in AWS Lambda environment variables. Create an AWS KMS customer managed key to encrypt the API key.",
        "C": "Store the API key in the code repository. Use an AWS managed key to encrypt the code repository.",
        "D": "Store the API key as an Amazon DynamoDB table record. Use an AWS managed key to encrypt the API key."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136970-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 4:37 a.m.",
      "textHash": "be069fb74289a464",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, on veut stocker une cl√© API externe dans la configuration d‚ÄôAWS Lambda (un service qui ex√©cute du code sans serveur). La bonne pratique est d‚Äô√©viter de mettre un secret dans le code ou une base de donn√©es ‚Äúpar d√©faut‚Äù si ce n‚Äôest pas n√©cessaire.\nLes variables d‚Äôenvironnement Lambda servent justement √† configurer une fonction (ex: URL, cl√©s, param√®tres) sans modifier le code.\nPour chiffrer ce secret, on utilise AWS KMS (service de gestion de cl√©s de chiffrement).\nLa contrainte ‚Äúplein contr√¥le sur les cl√©s KMS‚Äù signifie qu‚Äôil faut une cl√© g√©r√©e par le client (Customer Managed Key), pas une cl√© g√©r√©e automatiquement par AWS.\nAvec une cl√© KMS g√©r√©e par le client, vous d√©finissez pr√©cis√©ment qui peut utiliser la cl√© (politiques KMS/IAM), donc le secret n‚Äôest lisible que par les entit√©s autoris√©es.\nA est faux car la cl√© KMS par d√©faut est g√©r√©e par AWS (moins de contr√¥le). C est dangereux (secret dans le d√©p√¥t). D ne r√©pond pas au besoin ‚Äúdans la configuration Lambda‚Äù et utilise une cl√© g√©r√©e par AWS.\nDonc: variables d‚Äôenvironnement Lambda + cl√© KMS g√©r√©e par le client = contr√¥le total et acc√®s restreint.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que ton appli est un club au lyc√©e. Pour entrer chez un club partenaire, il faut un badge secret (la cl√© API). Tu veux ranger ce badge dans le casier du club (config Lambda) et le fermer avec TON propre cadenas, dont toi seul contr√¥les les copies (cl√© KMS).**\n\nConcept : une cl√© API, c‚Äôest un mot de passe. On ne la met pas sur un mur public. On la met dans un endroit pr√©vu, et on la chiffre (comme un cadenas) pour que seuls les autoris√©s puissent la lire.\nPourquoi B : les ‚Äúvariables d‚Äôenvironnement Lambda‚Äù, c‚Äôest comme une fiche secr√®te attach√©e au casier du club : l‚Äôappli la lit sans l‚Äô√©crire dans le code. Et ‚Äúcustomer managed key KMS‚Äù, c‚Äôest TON cadenas : tu d√©cides qui a le droit d‚Äôouvrir, tu peux retirer l‚Äôacc√®s, et c‚Äôest visible seulement pour les personnes/roles autoris√©s.\nPourquoi pas A : le cadenas ‚Äúpar d√©faut‚Äù est g√©r√© par AWS, donc tu n‚Äôas pas le contr√¥le total des copies/r√®gles.\nPourquoi pas C : mettre le badge dans le cahier partag√© (repo de code), c‚Äôest risqu√©, m√™me chiffr√©.\nPourquoi pas D : mettre le badge dans un registre (base de donn√©es) n‚Äôest pas ‚Äúdans la config Lambda‚Äù et le cadenas est g√©r√© par AWS, pas toi.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:367:7f79e5e70cb62bec",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 367,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating AWS CloudFormation templates to manage an application's deployment in Amazon Elastic Container Service (Amazon ECS) through AWS CodeDeploy. The developer wants to automatically deploy new versions of the application to a percentage of users before the new version becomes available for all users.How should the developer manage the deployment of the new version?",
      "choices": {
        "A": "Modify the CloudFormation template to include a Transform section and the AWS::CodeDeploy::BlueGreen hook.",
        "B": "Deploy the new version in a new CloudFormation stack. After testing is complete, update the application's DNS records for the new stack.",
        "C": "Run CloudFormation stack updates on the application stack to deploy new application versions when they are available.",
        "D": "Create a nested stack for the new version. Include a Transform section and the AWS::CodeDeploy::BlueGreen hook."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143750-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:29 a.m.",
      "textHash": "7f79e5e70cb62bec",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Objectif : faire un d√©ploiement progressif (canary/traffic shifting) sur ECS via CodeDeploy, pour n‚Äôenvoyer la nouvelle version qu‚Äô√† un pourcentage d‚Äôutilisateurs au d√©but.\nAWS CloudFormation sert √† d√©crire l‚Äôinfrastructure en ‚Äútemplate‚Äù (fichier) et √† la cr√©er/mettre √† jour automatiquement.\nAWS CodeDeploy est le service qui orchestre les d√©ploiements et peut r√©partir le trafic entre une ancienne version (bleu) et une nouvelle (vert).\nPour ECS, CodeDeploy g√®re le ‚Äúblue/green‚Äù : il lance la nouvelle t√¢che, puis d√©place le trafic par pourcentage (ex. 10% puis 100%) via le load balancer.\nDans CloudFormation, on active cette orchestration en ajoutant une section Transform et le hook AWS::CodeDeploy::BlueGreen : cela dit √† CloudFormation d‚Äôutiliser CodeDeploy pendant la mise √† jour.\nAinsi, le d√©ploiement se fait automatiquement avec une mont√©e en charge progressive et possibilit√© de rollback si √ßa casse.\nLes options B et C ne garantissent pas un pourcentage d‚Äôutilisateurs (DNS/stack update = bascule plut√¥t ‚Äútout ou rien‚Äù).\nLa nested stack (D) n‚Äôest pas n√©cessaire : le hook BlueGreen dans le template principal suffit.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:364:e43fa2b9bdac9651",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 364,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer wrote an application that uses an AWS Lambda function to asynchronously generate short videos based on requests from customers. This video generation can take up to 10 minutes. After the video is generated, a URL to download the video is pushed to the customer's web browser. The customer should be able to access these videos for at least 3 hours after generation.Which solution will meet these requirements?",
      "choices": {
        "A": "Store the video in the /tmp folder within the Lambda execution environment. Push a Lambda function URL to the customer.",
        "B": "Store the video in an Amazon Elastic File System (Amazon EFS) file system attached to the function. Generate a pre-signed URL for the video object and push the URL to the customer.",
        "C": "Store the video in Amazon S3. Generate a pre-signed URL for the video object and push the URL to the customer.",
        "D": "Store the video in an Amazon CloudFront distribution. Generate a pre-signed URL for the video object and push the URL to the customer."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143369-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 5, 2024, 4:53 p.m.",
      "textHash": "e43fa2b9bdac9651",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:e1fbfe68",
      "frExplanation": "Ici, la fonction AWS Lambda g√©n√®re une vid√©o et doit ensuite donner au client un lien de t√©l√©chargement valable au moins 3 heures.\nLambda est un service qui ex√©cute du code sans serveur, mais son stockage local (/tmp) est temporaire, limit√©, et peut dispara√Ætre apr√®s l‚Äôex√©cution : ce n‚Äôest pas fiable pour 3 heures (donc A est faux).\nAmazon S3 est un stockage d‚Äôobjets durable (fichiers) con√ßu pour conserver des donn√©es et les servir facilement.\nUne URL pr√©-sign√©e S3 est un lien temporaire qui donne acc√®s √† un objet S3 pendant une dur√©e d√©finie (ex: 3 heures), sans rendre le fichier public.\nDonc on stocke la vid√©o dans S3, puis on g√©n√®re une URL pr√©-sign√©e valable 3 heures et on l‚Äôenvoie au navigateur : cela correspond exactement au besoin.\nEFS (B) est un syst√®me de fichiers pour partager des fichiers entre ex√©cutions, mais les ‚Äúpre-signed URL‚Äù sont un m√©canisme standard de S3, pas d‚ÄôEFS.\nCloudFront (D) est un CDN pour acc√©l√©rer la distribution, mais il ne sert pas de stockage principal et la gestion de liens sign√©s est plus complexe et inutile ici.\nLa solution la plus simple, durable et adapt√©e est donc : S3 + URL pr√©-sign√©e (C).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un club vid√©o au lyc√©e : tu fabriques une vid√©o pour un √©l√®ve, puis tu la poses dans un casier s√©curis√© de la vie scolaire, et tu lui donnes un ticket qui ouvre ce casier seulement pendant 3 heures.**\n\nConcept : Lambda, c‚Äôest comme un √©l√®ve qui fait une t√¢che puis dispara√Æt; il ne faut pas garder la vid√©o ‚Äúdans sa poche‚Äù. Il faut un endroit de stockage fiable (le casier) + un lien temporaire (le ticket). Avec C : Amazon S3 = le casier/stockage o√π la vid√©o reste disponible. Le ‚Äúpre-signed URL‚Äù = un ticket-lien qui marche pendant une dur√©e choisie (au moins 3 heures), puis expire. A est faux : /tmp, c‚Äôest la poche de l‚Äô√©l√®ve, √ßa peut √™tre vid√© et ce n‚Äôest pas fait pour partager longtemps. B est inutile : EFS, c‚Äôest comme un classeur partag√© compliqu√©, pas n√©cessaire juste pour donner un lien de t√©l√©chargement. D est plut√¥t une vitrine de diffusion (CloudFront), mais il faut d‚Äôabord stocker la vid√©o ailleurs; S3 + lien temporaire suffit.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:361:22041a7e46c46c24",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 361,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing an application, which stores data in an Amazon DynamoDB table. The developer wants to query the DynamoDB table by using the partition key and a different sort key value. The developer needs the latest data with all recent write operations.How should the developer write the DynamoDB query?",
      "choices": {
        "A": "Add a local secondary index (LSI) during table creation. Query the LSI by using eventually consistent reads.",
        "B": "Add a local secondary index (LSI) during table creation. Query the LSI by using strongly consistent reads.",
        "C": "Add a global secondary index (GSI) during table creation. Query the GSI by using eventually consistent reads.",
        "D": "Add a global secondary index (GSI) during table creation. Query the GSI by using strongly consistent reads."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143107-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 30, 2024, 6:29 p.m.",
      "textHash": "22041a7e46c46c24",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π chaque item est identifi√© par une cl√© de partition (partition key) et souvent une cl√© de tri (sort key).\nIci, l‚Äôapplication veut garder la m√™me partition key, mais interroger avec une autre valeur de sort key : il faut donc un index qui change la sort key tout en gardant la m√™me partition.\nUn LSI (Local Secondary Index) permet exactement cela : m√™me partition key que la table, mais une sort key diff√©rente, et il doit √™tre cr√©√© au moment de la cr√©ation de la table.\nLe besoin dit ‚Äúlatest data avec toutes les √©critures r√©centes‚Äù : cela signifie lecture fortement coh√©rente (strongly consistent), sinon on peut lire des donn√©es en retard.\nAvec un LSI, DynamoDB peut faire des lectures fortement coh√©rentes.\nUn GSI (Global Secondary Index) change la partition key et/ou la sort key, mais les lectures sur GSI sont toujours √©ventuellement coh√©rentes (eventually consistent), donc pas garanti ‚Äúdernier √©tat‚Äù.\nDonc la bonne approche est : cr√©er un LSI et le requ√™ter en strongly consistent reads.\nR√©ponse : B.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e. Chaque √©l√®ve a un casier (partition key). Dans ton casier, tu ranges des fiches (sort key). Tu veux chercher dans TON casier, mais en triant/rep√©rant les fiches avec un autre crit√®re (un autre sort key). Et tu veux √™tre s√ªr de voir la toute derni√®re fiche ajout√©e, m√™me si elle vient d‚Äô√™tre pos√©e √† l‚Äôinstant.**\n\nConcept : un index, c‚Äôest comme une liste de rep√©rage pour retrouver plus vite des fiches. Un LSI, c‚Äôest une liste alternative MAIS qui reste dans le m√™me casier (m√™me partition key), juste avec un autre tri (autre sort key). Donc pour ‚Äúm√™me partition key + autre sort key‚Äù, il faut un LSI. Ensuite, tu veux les donn√©es les plus r√©centes : c‚Äôest comme exiger que le biblioth√©caire regarde directement dans le casier maintenant, pas une copie qui peut √™tre en retard. √áa s‚Äôappelle une lecture ‚Äústrongly consistent‚Äù. Donc : LSI + strongly consistent reads = B. Les options avec ‚Äúeventually consistent‚Äù risquent d‚Äôoublier la derni√®re fiche pendant quelques secondes. Les GSI, c‚Äôest comme une liste qui m√©lange des fiches de plusieurs casiers, pas n√©cessaire ici.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:331:f5e6a6b5c1375dca",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 331,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses Amazon Simple Queue Service (Amazon SQS) to decouple its microservices architecture. Some messages in an SQS queue contain sensitive information. A developer must implement a solution that encrypts all the data at rest.Which solution will meet this requirement?",
      "choices": {
        "A": "Enable server-side encryption for the SQS queue by using an SQS managed encryption key (SSE-SQS).",
        "B": "Use the aws:SecureTransport condition in the queue policy to ensure that only HTTPS (TLS) is used for all requests to the SQS queue.",
        "C": "Use AWS Certificate Manager (ACM) to generate an SSL/TLS certificate. Reference the certificate when messages are sent to the queue.",
        "D": "Set a message attribute in the SQS SendMessage request for messages that are sent to the queue. Set the Name to ENCRYPT. Set the Value to TRUE."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136633-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 19, 2024, 2:37 p.m.",
      "textHash": "f5e6a6b5c1375dca",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Objectif : chiffrer les messages ¬´ au repos ¬ª, c‚Äôest-√†-dire quand ils sont stock√©s sur les serveurs AWS, pas seulement pendant le transport.\nAmazon SQS est un service de file d‚Äôattente qui stocke temporairement des messages entre microservices.\nPour chiffrer au repos, il faut activer le chiffrement c√¥t√© serveur (Server-Side Encryption) sur la file SQS.\nL‚Äôoption SSE-SQS avec une cl√© g√©r√©e par SQS (SQS managed key) chiffre automatiquement tous les messages stock√©s, sans changer le code de l‚Äôapplication.\nB parle de HTTPS/TLS : cela prot√®ge ¬´ en transit ¬ª (pendant l‚Äôenvoi), mais ne garantit pas le chiffrement une fois stock√©.\nC (ACM) sert √† g√©rer des certificats pour des sites/applications (ex: HTTPS sur un load balancer), pas √† chiffrer le stockage SQS.\nD est juste un attribut de message : SQS ne chiffre pas les donn√©es parce qu‚Äôun attribut dit ENCRYPT=TRUE.\nDonc la bonne r√©ponse est A : activer SSE-SQS pour chiffrer toutes les donn√©es au repos.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une bo√Æte aux lettres au lyc√©e o√π les clubs d√©posent des petits mots pour se coordonner, sans se parler directement. Certains mots contiennent des infos sensibles (ex: codes de casier).**\n\nConcept : ‚Äúdonn√©es au repos‚Äù = les mots quand ils sont stock√©s dans la bo√Æte, pas quand quelqu‚Äôun les transporte. Chiffrer = √©crire les mots en code secret, illisible si quelqu‚Äôun ouvre la bo√Æte. A : activer le chiffrement c√¥t√© service (SSE-SQS) revient √† dire : la bo√Æte aux lettres chiffre automatiquement tous les mots d√®s qu‚Äôils sont d√©pos√©s, et les d√©chiffre seulement pour le bon destinataire. Donc m√™me si quelqu‚Äôun vole la bo√Æte (les donn√©es stock√©es), il ne peut rien lire. B et C parlent surtout du trajet (HTTPS/TLS = transport s√©curis√©), pas du stockage dans la bo√Æte. D est juste une √©tiquette ‚ÄúENCRYPT=TRUE‚Äù sur le mot : √ßa ne chiffre rien tout seul. Donc A est la seule option qui chiffre vraiment les messages ‚Äúau repos‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:330:d7cfdcca72c43960",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 330,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company deploys a new application to AWS. The company is streaming application logs to Amazon CloudWatch Logs. The company's development team must receive notification by email when the word \"ERROR\" appears in any log lines. A developer sets up an Amazon Simple Notification Service (Amazon SNS) topic and subscribes the development team to the topic.What should the developer do next to meet the requirements?",
      "choices": {
        "A": "Select the appropriate log group. Create a CloudWatch metric filter with \"ERROR\" as the search term. Create an alarm on this metric that notifies the SNS topic when the metric is 1 or higher.",
        "B": "In CloudWatch Logs Insights, select the appropriate log group. Create a metric query to search for the term \"ERROR\" in the logs. Create an alarm on this metric that notifies the SNS topic when the metric is 1 or higher.",
        "C": "Select the appropriate log group. Create an SNS subscription filter with \"ERROR\" as the filter pattern. Select the SNS topic as the destination.",
        "D": "Create a CloudWatch alarm that includes \"ERROR\" as a filter pattern, a log group dimension that defines the appropriate log group, and a destination that notifies the SNS topic."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136632-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 19, 2024, 2:32 p.m.",
      "textHash": "d7cfdcca72c43960",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Objectif : envoyer un email d√®s qu‚Äôune ligne de log contient ¬´ ERROR ¬ª.\nCloudWatch Logs stocke les logs, mais pour d√©clencher une alerte il faut d‚Äôabord transformer un motif de log en m√©trique (un compteur).\nUn ¬´ metric filter ¬ª sur un log group permet de chercher un texte (ici ¬´ ERROR ¬ª) et d‚Äôincr√©menter une m√©trique chaque fois qu‚Äôil appara√Æt.\nEnsuite, une alarme CloudWatch surveille cette m√©trique : si la valeur est ‚â• 1 sur une p√©riode, cela signifie qu‚Äôau moins une erreur est arriv√©e.\nL‚Äôalarme peut alors publier une notification vers un topic SNS.\nSNS envoie l‚Äôemail aux abonn√©s (l‚Äô√©quipe dev) via la souscription d√©j√† cr√©√©e.\nPourquoi pas B : Logs Insights sert surtout √† analyser/interroger, pas le chemin standard pour d√©clencher des alarmes en production.\nPourquoi pas C/D : il n‚Äôexiste pas de ‚ÄúSNS subscription filter‚Äù pour filtrer des logs, et une alarme CloudWatch ne lit pas directement les logs sans m√©trique filter.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine le cahier de vie scolaire du lyc√©e : chaque ligne est un message sur ce qui se passe. Tu veux qu‚Äôun surveillant t‚Äôenvoie un mail d√®s qu‚Äôil voit le mot ¬´ ERROR ¬ª √©crit quelque part.**\n\nConcept : les logs = le cahier, SNS = la liste de diffusion mail, et il faut un ¬´ d√©tecteur ¬ª qui compte les fois o√π ¬´ ERROR ¬ª appara√Æt.\nPourquoi A : tu choisis le bon cahier (log group), puis tu cr√©es un filtre qui rep√®re ¬´ ERROR ¬ª et transforme √ßa en compteur (metric) : +1 √† chaque ¬´ ERROR ¬ª.\nEnsuite tu mets une alarme : si le compteur est ‚â• 1, elle d√©clenche un message vers la liste mail (SNS), donc l‚Äô√©quipe re√ßoit l‚Äôemail.\nPourquoi pas B : Logs Insights sert surtout √† chercher/analyser √† la main, pas le meilleur pour une alerte automatique simple.\nPourquoi pas C : SNS ne lit pas directement le cahier pour filtrer des mots.\nPourquoi pas D : une alarme ne lit pas directement les lignes de texte, elle surveille un compteur, donc il faut d‚Äôabord le filtre + metric.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:325:9927ffc29e144bbe",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 325,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs an application on AWS. The application stores data in an Amazon DynamoDB table. Some queries are taking a long time to run. These slow queries involve an attribute that is not the table's partition key or sort key.The amount of data that the application stores in the DynamoDB table is expected to increase significantly. A developer must increase the performance of the queries.Which solution will meet these requirements?",
      "choices": {
        "A": "Increase the page size for each request by setting the Limit parameter to be higher than the default value. Configure the application to retry any request that exceeds the provisioned throughput.",
        "B": "Create a global secondary index (GSI). Set query attribute to be the partition key of the index.",
        "C": "Perform a parallel scan operation by issuing individual scan requests. In the parameters, specify the segment for the scan requests and the total number of segments for the parallel scan.",
        "D": "Turn on read capacity auto scaling for the DynamoDB table. Increase the maximum read capacity units (RCUs)."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134141-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 18, 2024, 12:37 p.m.",
      "textHash": "9927ffc29e144bbe",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π les requ√™tes rapides utilisent une cl√© de partition (et √©ventuellement une cl√© de tri). Si vous filtrez sur un autre attribut, DynamoDB ne peut pas ‚Äúviser‚Äù directement les bons √©l√©ments et peut devoir lire beaucoup de donn√©es, donc c‚Äôest lent et √ßa empirera quand la table grandira.\nUn GSI (Global Secondary Index) est un index suppl√©mentaire qui cr√©e une nouvelle ‚Äúvue‚Äù de la table avec une autre cl√© de partition/tri. En mettant l‚Äôattribut utilis√© dans vos requ√™tes comme cl√© de partition du GSI, vos requ√™tes deviennent des Query efficaces sur l‚Äôindex.\nA n‚Äôacc√©l√®re pas la recherche : augmenter Limit change juste la pagination, et les retries ne r√©solvent pas le probl√®me de conception.\nC (Scan parall√®le) lit toute la table (m√™me si plus vite), co√ªte cher et devient mauvais √† grande √©chelle.\nD augmente la capacit√© de lecture, mais si la requ√™te reste non index√©e, vous continuez √† lire trop de donn√©es : plus de co√ªt, pas une vraie solution.\nDonc la bonne solution est de cr√©er un GSI avec l‚Äôattribut de requ√™te comme cl√© de partition.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la biblioth√®que du lyc√©e : les livres sont rang√©s surtout par ‚Äúmati√®re‚Äù (cl√© principale). Toi, tu veux souvent chercher par ‚Äúnom de l‚Äôauteur‚Äù, mais ce n‚Äôest pas le rangement officiel.**\n\nDans DynamoDB, si tu cherches avec un champ qui n‚Äôest pas la ‚Äúmati√®re‚Äù (partition key) ni le ‚Äúsous-classement‚Äù (sort key), la base doit fouiller plein d‚Äô√©tag√®res, donc c‚Äôest lent.\nComme il y aura beaucoup plus de livres, √ßa deviendra encore plus lent.\nLa solution B, c‚Äôest cr√©er un ‚Äúcatalogue sp√©cial‚Äù (un index) rang√© par auteur : c‚Äôest le Global Secondary Index (GSI).\nTu choisis l‚Äôattribut lent (ex: auteur) comme nouvelle ‚Äúmati√®re‚Äù du catalogue.\nDu coup, quand tu demandes ‚Äútous les livres de cet auteur‚Äù, la biblioth√®que va direct au bon tiroir, sans tout parcourir.\nA ne change pas le rangement, √ßa fait juste des pages plus grosses.\nC c‚Äôest comme demander √† 10 √©l√®ves de fouiller toute la biblioth√®que : √ßa reste une recherche partout.\nD ajoute des biblioth√©caires (plus de capacit√©), mais si tu cherches au mauvais endroit, tu perds quand m√™me du temps.\nDonc B est la bonne r√©ponse.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:309:cdf575dfbb928ca2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 309,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is adopting serverless computing for some of its new services. A development team needs to create a serverless infrastructure by using AWS Serverless Application Model (AWS SAM). All infrastructure must be deployed by using AWS CloudFormation templates.What should the development team do to meet these requirements?",
      "choices": {
        "A": "Add a Resources section to the CloudFormation templates that contains AWS::Lambda::Function resources.",
        "B": "Add a Mappings section to the CloudFormation templates that contains AWS::Serverless::Function and AWS::Serverless::API.",
        "C": "Add a Transform section to the CloudFormation templates. Use the AWS SAM syntax to define the resources.",
        "D": "Add a Parameters section to the CloudFormation templates that specifies the relevant AWS SAM Globals section."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133607-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 12, 2024, 1:28 p.m.",
      "textHash": "cdf575dfbb928ca2",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:642376c6",
      "frExplanation": "AWS SAM (Serverless Application Model) est une extension de CloudFormation qui simplifie la cr√©ation d‚Äôapplications ¬´ serverless ¬ª (sans g√©rer de serveurs).\nCloudFormation d√©ploie l‚Äôinfrastructure √† partir de templates YAML/JSON. Pour que CloudFormation comprenne la syntaxe SAM (ex: AWS::Serverless::Function, AWS::Serverless::Api), il faut activer un ‚Äútransform‚Äù.\nLa section Transform indique √† CloudFormation : ¬´ interpr√®te ce template avec les r√®gles SAM ¬ª, puis CloudFormation le convertit en ressources AWS classiques (Lambda, API Gateway, etc.) et les d√©ploie.\nDonc la bonne action est d‚Äôajouter une section Transform et d‚Äô√©crire les ressources en syntaxe SAM.\nA est possible sans SAM (en √©crivant AWS::Lambda::Function), mais ne r√©pond pas au besoin ‚Äúen utilisant AWS SAM‚Äù.\nB est faux car Mappings sert √† des tableaux de correspondance (valeurs par r√©gion/environnement), pas √† d√©clarer des ressources.\nD est faux car Parameters sert √† passer des valeurs au template ; Globals est une section SAM, pas quelque chose qu‚Äôon ‚Äúsp√©cifie‚Äù via Parameters.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois organiser un tournoi au lyc√©e. Tu as une fiche ‚Äúofficielle‚Äù que l‚Äôadministration comprend (CloudFormation), mais toi tu veux √©crire avec un mod√®le sp√©cial ‚Äútournoi‚Äù super simple (SAM) qui d√©crit matchs, √©quipes, r√®gles en quelques lignes.**\n\nConcept : CloudFormation, c‚Äôest la fiche standard de l‚Äô√©cole pour d√©crire tout ce qu‚Äôil faut installer. SAM, c‚Äôest un ‚Äúmode simplifi√©‚Äù pour d√©crire des services sans serveur (comme des mini-bots qui se lancent seulement quand on en a besoin).\nPour que l‚Äôadministration accepte ta fiche √©crite en mode SAM, tu dois ajouter une ligne ‚Äútraducteur‚Äù en haut : c‚Äôest la section Transform.\nCette section dit : ‚ÄúLis ce document avec les r√®gles SAM, puis convertis-le en fiche CloudFormation normale‚Äù.\nDonc C est correct : on ajoute Transform et on √©crit les ressources en syntaxe SAM.\nA : tu √©cris directement en CloudFormation, pas en SAM.\nB : Mappings sert √† faire des tableaux de correspondance, pas √† d√©finir ces ressources.\nD : Parameters sert √† demander des infos (ex: nombre d‚Äô√©quipes), pas √† activer SAM.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:305:aeb0c5d0904bceca",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 305,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer uses AWS CloudFormation to deploy an Amazon API Gateway API and an AWS Step Functions state machine. The state machine must reference the API Gateway API after the CloudFormation template is deployed. The developer needs a solution that uses the state machine to reference the API Gateway endpoint.Which solution will meet these requirements MOST cost-effectively?",
      "choices": {
        "A": "Configure the CloudFormation template to reference the API endpoint in the DefinitionSubstitutions property for the AWS::StepFunctions::StateMachine resource.",
        "B": "Configure the CloudFormation template to store the API endpoint in an environment variable for the AWS::StepFunctions::StateMachine resource. Configure the state machine to reference the environment variable.",
        "C": "Configure the CloudFormation template to store the API endpoint in a standard AWS::SecretsManager::Secret resource. Configure the state machine to reference the resource.",
        "D": "Configure the CloudFormation template to store the API endpoint in a standard AWS::AppConfig::ConfigurationProfile resource. Configure the state machine to reference the resource."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134341-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 22, 2024, 7:10 a.m.",
      "textHash": "aeb0c5d0904bceca",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:b5ec543c",
      "frExplanation": "CloudFormation sert √† d√©crire et cr√©er des ressources AWS (API Gateway, Step Functions) avec un ‚Äútemplate‚Äù.\nAPI Gateway fournit une URL (endpoint) pour appeler votre API, mais cette URL n‚Äôexiste qu‚Äôapr√®s le d√©ploiement.\nStep Functions ex√©cute un ‚Äústate machine‚Äù dont la d√©finition (Amazon States Language) peut contenir des valeurs √† remplacer.\nLa propri√©t√© DefinitionSubstitutions de AWS::StepFunctions::StateMachine permet d‚Äôinjecter, au moment du d√©ploiement, des valeurs comme l‚ÄôURL de l‚ÄôAPI (obtenue via une r√©f√©rence CloudFormation).\nAinsi, la d√©finition du workflow contient directement le bon endpoint sans service suppl√©mentaire.\nC‚Äôest le plus √©conomique car il n‚Äôy a ni stockage externe ni appels suppl√©mentaires (contrairement √† Secrets Manager ou AppConfig).\nL‚Äôoption ‚Äúvariable d‚Äôenvironnement‚Äù ne convient pas : Step Functions n‚Äôa pas d‚Äôenvironnement comme une fonction Lambda.\nDonc la bonne solution est d‚Äôutiliser DefinitionSubstitutions pour remplacer l‚Äôendpoint dans la d√©finition du state machine.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o o√π tu construis une base avec un plan (CloudFormation). Tu poses d‚Äôabord un portail (API Gateway) avec une adresse, puis tu cr√©es un robot (Step Functions) qui doit aller √† cette adresse apr√®s la construction.**\n\nConcept : le plan doit √©crire l‚Äôadresse du portail directement dans le ‚Äúscript‚Äù du robot au moment o√π tout est construit.\nOption A : DefinitionSubstitutions, c‚Äôest comme un champ ‚Äúremplacer {ADRESSE_PORTAIL} par la vraie adresse‚Äù dans le script du robot. Une fois le plan fini, le robot a l‚Äôadresse exacte et peut y aller.\nC‚Äôest le plus √©conomique : pas besoin d‚Äôun ‚Äúcoffre-fort‚Äù (Secrets Manager) ni d‚Äôun ‚Äúmenu de r√©glages‚Äù s√©par√© (AppConfig), qui ajoutent des √©tapes et parfois des co√ªts.\nOption B ne marche pas bien : un robot Step Functions n‚Äôa pas des ‚Äúvariables d‚Äôenvironnement‚Äù comme un programme classique.\nDonc A est la bonne r√©ponse : simple, direct, et sans services en plus.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:301:40d288d51131fce0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 301,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a serverless application on AWS for a workflow that processes high volumes of data. In the workflow, an AWS Step Functions state machine invokes several AWS Lambda functions.One of the Lambda functions occasionally fails because of timeout errors during periods of high demand. The developer must ensure that the workflow automatically retries the failed function invocation if a timeout error occurs.Which solution will meet this requirement?",
      "choices": {
        "A": "Add a Retry field in the Step Functions state machine definition. Configure the state machine with the maximum number of retry attempts and the timeout error type to retry on.",
        "B": "Add a Timeout field in the Step Functions state machine definition. Configure the state machine with the maximum number of retry attempts.",
        "C": "Add a Fail state to the Step Functions state machine definition. Configure the state machine with the maximum number of retry attempts.",
        "D": "Update the Step Functions state machine to pass the invocation request to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe a Lambda function to the SNS topic. Configure the Lambda function with the maximum number of retry attempts for a timeout error type."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134337-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 22, 2024, 6:27 a.m.",
      "textHash": "40d288d51131fce0",
      "rawFormat": "discussion-md",
      "conceptKey": "lambda_retries_destinations_dlq",
      "frExplanation": "Step Functions est un service qui orchestre un ‚Äúworkflow‚Äù (une suite d‚Äô√©tapes). Chaque √©tape peut appeler une fonction Lambda (du code qui s‚Äôex√©cute sans serveur).\nIci, une Lambda √©choue parfois avec une erreur de d√©lai (‚Äútimeout‚Äù) quand la demande est forte.\nLe besoin est que le workflow relance automatiquement l‚Äôappel quand cette erreur pr√©cise arrive.\nDans Step Functions, on g√®re cela directement dans la d√©finition de l‚Äô√©tat (Task) avec le champ \"Retry\".\n\"Retry\" permet de dire : pour tel type d‚Äôerreur (ex. timeout), r√©essaie jusqu‚Äô√† N fois (et on peut aussi d√©finir un d√©lai entre tentatives).\nC‚Äôest exactement ce que propose la r√©ponse A : configurer le nombre maximum de tentatives et le type d‚Äôerreur √† relancer.\n\"Timeout\" (B) ne fait que fixer une dur√©e limite, il ne d√©clenche pas des retries.\nUn √©tat \"Fail\" (C) arr√™te le workflow, l‚Äôinverse de ce qu‚Äôon veut, et SNS (D) ajoute de la complexit√© inutile : la logique de retry doit √™tre dans Step Functions.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un devoir en groupe √† l‚Äô√©cole : le prof (Step Functions) distribue des mini-t√¢ches √† des √©l√®ves (Lambda). Parfois, un √©l√®ve met trop de temps et la t√¢che ‚Äúexpire‚Äù (timeout).**\n\nConcept : le prof suit une fiche d‚Äôinstructions pour encha√Æner les t√¢ches. Sur cette fiche, il peut √©crire ‚Äúsi un √©l√®ve d√©passe le temps, recommence X fois‚Äù.\nPourquoi A : l‚Äôoption A ajoute une r√®gle ‚ÄúRetry‚Äù sur la fiche du prof : quand l‚Äôerreur est ‚Äútimeout‚Äù, le prof redonne automatiquement la m√™me t√¢che, avec un nombre max d‚Äôessais.\nB est faux : ‚ÄúTimeout‚Äù fixe juste une limite de temps, √ßa ne dit pas de r√©essayer.\nC est faux : ‚ÄúFail‚Äù c‚Äôest comme mettre 0 et arr√™ter le devoir, pas r√©essayer.\nD est trop compliqu√© : passer par un messager (SNS) n‚Äôest pas n√©cessaire, et la r√®gle de retry doit √™tre dans la fiche du prof (Step Functions) pour ce workflow.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:287:40fedc818d72cf6c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 287,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a serverless application that uses an AWS Lambda function. The developer will use AWS CloudFormation to deploy the application. The application will write logs to Amazon CloudWatch Logs. The developer has created a log group in a CloudFormation template for the application to use. The developer needs to modify the CloudFormation template to make the name of the log group available to the application at runtime.Which solution will meet this requirement?",
      "choices": {
        "A": "Use the AWS::Include transform in CloudFormation to provide the log group's name to the application.",
        "B": "Pass the log group's name to the application in the user data section of the CloudFormation template.",
        "C": "Use the CloudFormation template's Mappings section to specify the log group's name for the application.",
        "D": "Pass the log group's Amazon Resource Name (ARN) as an environment variable to the Lambda function."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134286-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 6:58 a.m.",
      "textHash": "40fedc818d72cf6c",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Une fonction AWS Lambda est du code qui s‚Äôex√©cute sans serveur. Pour √©crire des logs, elle utilise Amazon CloudWatch Logs, qui organise les logs dans un ¬´ log group ¬ª (groupe de journaux).\nAvec AWS CloudFormation, on d√©crit l‚Äôinfrastructure (Lambda, log group, etc.) dans un template. Mais pour que le code Lambda connaisse le nom du log group ¬´ au moment o√π il s‚Äôex√©cute ¬ª, il faut lui fournir cette info comme configuration runtime.\nLa mani√®re standard est d‚Äôutiliser des variables d‚Äôenvironnement Lambda : CloudFormation peut y injecter une valeur (par exemple l‚ÄôARN ou le nom du log group) et le code la lit via l‚Äôenvironnement.\nL‚ÄôARN (Amazon Resource Name) identifie de fa√ßon unique la ressource et contient le nom du log group, donc l‚Äôapplication peut l‚Äôutiliser pour cibler le bon groupe.\nLes autres options ne conviennent pas : AWS::Include sert √† inclure des morceaux de template, pas √† passer une valeur au runtime ; ¬´ user data ¬ª concerne des instances EC2, pas Lambda ; Mappings est statique et ne ¬´ transmet ¬ª rien au code.\nDonc il faut passer l‚ÄôARN du log group en variable d‚Äôenvironnement √† la fonction Lambda.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un club au lyc√©e. Tu as une bo√Æte ‚Äúcarnet de bord‚Äù o√π le club note tout ce qu‚Äôil fait (les logs). Tu pr√©pares l‚Äôorganisation du club sur une feuille (CloudFormation) et tu veux que, le jour J, l‚Äô√©l√®ve responsable (Lambda) sache exactement quelle bo√Æte utiliser.**\n\nConcept : CloudFormation, c‚Äôest la feuille d‚Äôorganisation qui cr√©e les objets (comme la bo√Æte de carnet). Lambda, c‚Äôest l‚Äô√©l√®ve qui ex√©cute une t√¢che sans ‚Äúsalle fixe‚Äù (serverless). CloudWatch Logs, c‚Äôest le carnet o√π on √©crit ce qui se passe.\nPourquoi D : pour que l‚Äô√©l√®ve trouve la bonne bo√Æte au moment o√π il travaille, tu lui donnes une √©tiquette claire dans sa poche : une ‚Äúvariable d‚Äôenvironnement‚Äù (un petit m√©mo). L‚ÄôARN, c‚Äôest comme l‚Äôadresse compl√®te et unique de la bo√Æte (impossible de se tromper).\nPourquoi pas A/B/C : Include sert √† coller du contenu dans la feuille, pas √† donner une info au moment de l‚Äôaction. ‚ÄúUser data‚Äù c‚Äôest pour des machines qu‚Äôon configure au d√©marrage, pas pour Lambda. ‚ÄúMappings‚Äù c‚Äôest un tableau de correspondance fixe, pas fait pour transmettre une valeur cr√©√©e/choisie pour l‚Äôex√©cution.\nDonc : mettre l‚ÄôARN du log group en variable d‚Äôenvironnement de Lambda = l‚Äôappli conna√Æt le nom/adresse du carnet pendant qu‚Äôelle tourne.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:247:090d0f02edd854c0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 247,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company deploys a photo-processing application to an Amazon EC2 instance. The application needs to process each photo in less than 5 seconds. If processing takes longer than 5 seconds, the company‚Äôs development team must receive a notification.How can a developer implement the required time measurement and notification with the LEAST operational overhead?",
      "choices": {
        "A": "Create an Amazon CloudWatch custom metric. Each time a photo is processed, publish the processing time as a metric value. Create a CloudWatch alarm that is based on a static threshold of 5 seconds. Notify the development team by using an Amazon Simple Notification Service (Amazon SNS) topic.",
        "B": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Each time a photo is processed, publish the processing time to the queue. Create an application to consume from the queue and to determine whether any values are more than 5 seconds. Notify the development team by using an Amazon Simple Notification Service (Amazon SNS) topic.",
        "C": "Create an Amazon CloudWatch custom metric. Each time a photo is processed, publish the processing time as a metric value. Create a CloudWatch alarm that enters ALARM state if the average of values is greater than 5 seconds. Notify the development team by sending an Amazon Simple Email Service (Amazon SES) message.",
        "D": "Create an Amazon Kinesis data stream. Each time a photo is processed, publish the processing time to the data stream. Create an Amazon CloudWatch alarm that enters ALARM state if any values are more than 5 seconds. Notify the development team by using an Amazon Simple Notification Service (Amazon SNS) topic."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124858-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 29, 2023, 1:56 a.m.",
      "textHash": "090d0f02edd854c0",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "On veut mesurer un temps d‚Äôex√©cution (dur√©e de traitement) et √™tre alert√© si une photo d√©passe 5 secondes, avec le moins d‚Äôadministration possible.\nAmazon CloudWatch sert √† collecter des m√©triques (chiffres dans le temps) et √† d√©clencher des alarmes automatiquement.\nLa solution la plus simple est d‚Äôenvoyer, apr√®s chaque photo, une m√©trique personnalis√©e ‚ÄúprocessingTime‚Äù avec la valeur en secondes.\nEnsuite, une alarme CloudWatch avec un seuil fixe √† 5 secondes peut d√©clencher une alerte d√®s qu‚Äôune valeur d√©passe ce seuil.\nPour pr√©venir l‚Äô√©quipe, Amazon SNS est fait pour envoyer des notifications (email, SMS, webhook) √† plusieurs abonn√©s facilement.\nB ajoute une application √† maintenir pour lire SQS et calculer le d√©passement : plus d‚Äôoverhead.\nC utilise une moyenne : une photo lente peut √™tre ‚Äúcach√©e‚Äù par des photos rapides, et SES n‚Äôest pas l‚Äôoutil standard d‚Äôalerte.\nD (Kinesis) est trop lourd pour ce besoin simple : ingestion/streaming inutile.\nDonc A est le meilleur choix : CloudWatch metric + alarm seuil 5s + SNS.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : chaque √©l√®ve qui passe au self est chronom√©tr√©. Si quelqu‚Äôun met plus de 5 secondes √† √™tre servi, un panneau d‚Äôalerte s‚Äôallume et le CPE re√ßoit un message.**\n\nIci, chaque photo = un √©l√®ve, et le temps de traitement = le temps au self. Une ‚Äúm√©trique‚Äù CloudWatch, c‚Äôest comme le tableau qui enregistre tous les chronos. L‚Äôappli envoie √† chaque photo son temps (comme noter chaque chrono). Une ‚Äúalarme‚Äù CloudWatch, c‚Äôest la r√®gle du panneau : si un chrono d√©passe 5 secondes (seuil fixe), √ßa d√©clenche. SNS, c‚Äôest le syst√®me qui envoie la notif au groupe (message au CPE/√©quipe). C‚Äôest le moins de boulot car AWS surveille et alerte tout seul. B et D ajoutent une file/flux + un programme en plus (comme embaucher quelqu‚Äôun pour relire tous les chronos). C regarde la moyenne : si une photo est tr√®s lente mais les autres rapides, la moyenne peut cacher le probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:223:7b43166db4641ddd",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 223,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nAn application uses AWS X-Ray to generate a large amount of trace data on an hourly basis. A developer wants to use filter expressions to limit the returned results through user-specified custom attributes.How should the developer use filter expressions to filter the results in X-Ray?",
      "choices": {
        "A": "Add custom attributes as annotations in the segment document.",
        "B": "Add custom attributes as metadata in the segment document.",
        "C": "Add custom attributes as new segment fields in the segment document.",
        "D": "Create new sampling rules that are based on custom attributes."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124806-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 12:40 p.m.",
      "textHash": "7b43166db4641ddd",
      "rawFormat": "discussion-md",
      "conceptKey": "xray_tracing",
      "frExplanation": "AWS X-Ray sert √† suivre le parcours d‚Äôune requ√™te dans une application (des ‚Äútraces‚Äù compos√©es de ‚Äúsegments‚Äù).\nQuand on veut rechercher/filtrer des traces, X-Ray ne peut filtrer que sur des champs index√©s.\nLes ‚Äúannotations‚Äù sont des paires cl√©/valeur (string/number/bool) ajout√©es aux segments et elles sont index√©es par X-Ray.\nDonc on peut √©crire des filter expressions qui testent ces annotations (ex: annotation.userId = \"123\").\nLes ‚Äúmetadata‚Äù sont aussi des infos cl√©/valeur, mais elles ne sont pas index√©es : elles servent au diagnostic, pas au filtrage.\nAjouter de ‚Äúnouveaux champs‚Äù arbitraires dans le segment n‚Äôest pas une m√©thode support√©e pour le filtrage.\nLes r√®gles de sampling contr√¥lent combien de traces sont collect√©es, pas comment on filtre les r√©sultats d√©j√† stock√©s.\nDonc il faut mettre les attributs personnalis√©s en annotations dans le document de segment.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le carnet de surveillance d‚Äôun lyc√©e : chaque incident est une ‚Äúfiche‚Äù avec des infos faciles √† trier (classe, heure, type) et aussi des notes libres du prof en bas de page.**\n\nAWS X-Ray, c‚Äôest comme un carnet qui enregistre le trajet d‚Äôune demande dans ton appli (une ‚Äúfiche de suivi‚Äù).\nSi tu veux filtrer vite (ex: ‚Äúmontre-moi seulement les fiches o√π la classe = 2ndeB‚Äù), il faut des √©tiquettes faites pour √™tre tri√©es.\nDans X-Ray, ces √©tiquettes triables s‚Äôappellent des ‚Äúannotations‚Äù : ce sont des champs pr√©vus pour les recherches/filtrages.\nLa ‚Äúmetadata‚Äù, c‚Äôest plut√¥t comme les notes libres du prof : utile pour lire, mais pas faite pour filtrer facilement.\nCr√©er de nouveaux champs au hasard ou changer l‚Äô√©chantillonnage (sampling) ne sert pas √† filtrer les r√©sultats d√©j√† enregistr√©s.\nDonc la bonne m√©thode est A : mettre les attributs personnalis√©s en annotations pour que les filtres les retrouvent.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:214:61b942dac953d1fd",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 214,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to launch a new Amazon EC2 instance by using the AWS CLI.Which AWS CLI command should the developer use to meet this requirement?",
      "choices": {
        "A": "aws ec2 bundle-instance",
        "B": "aws ec2 start-instances",
        "C": "aws ec2 confirm-product-instance",
        "D": "aws ec2 run-instances"
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124777-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:46 a.m.",
      "textHash": "61b942dac953d1fd",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:0dd9b923",
      "frExplanation": "Pour cr√©er (lancer) une nouvelle machine virtuelle sur AWS, on utilise Amazon EC2 (Elastic Compute Cloud).\nAvec l‚ÄôAWS CLI, la commande qui ‚Äúcr√©e et d√©marre‚Äù une nouvelle instance s‚Äôappelle run-instances.\nD. aws ec2 run-instances est donc la bonne r√©ponse : elle lance une nouvelle instance √† partir d‚Äôune AMI (image syst√®me) et de param√®tres (type, r√©seau, cl√©s SSH, etc.).\nB. start-instances sert seulement √† d√©marrer une instance d√©j√† existante qui est arr√™t√©e : elle ne cr√©e rien de nouveau.\nA. bundle-instance est une ancienne commande li√©e √† certains types d‚Äôinstances/AMI (surtout Windows/ancien mod√®le) et ne sert pas √† lancer une instance.\nC. confirm-product-instance sert √† confirmer l‚Äôutilisation d‚Äôun produit Marketplace/logiciel, pas √† cr√©er une instance.\nEn r√©sum√© : ‚Äúrun‚Äù = cr√©er + lancer ; ‚Äústart‚Äù = red√©marrer une instance existante.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine que tu joues √† un jeu vid√©o et que tu veux faire appara√Ætre un nouveau personnage sur la carte.**\n\nConcept : une instance EC2, c‚Äôest comme un ‚Äúpersonnage/ordinateur‚Äù que tu fais appara√Ætre pour qu‚Äôil travaille. La CLI, c‚Äôest comme taper une commande dans la console du jeu.\nPour ‚Äúlancer un nouveau personnage‚Äù, il faut une commande qui le cr√©e et le fait appara√Ætre.\nD) run-instances = ‚Äúcr√©e et d√©marre une nouvelle instance‚Äù : c‚Äôest exactement ‚Äúspawn un nouveau perso‚Äù.\nB) start-instances = ‚Äúred√©marre un perso d√©j√† existant‚Äù : tu ne cr√©es rien de nouveau.\nA) bundle-instance = ‚Äúfaire un paquet/sauvegarde sp√©ciale‚Äù : pas pour lancer.\nC) confirm-product-instance = ‚Äúconfirmer un achat/licence‚Äù : pas pour cr√©er.\nDonc la bonne commande pour lancer une NOUVELLE instance est : aws ec2 run-instances.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:209:8ce97364844cc0d3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 209,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS CloudFormation stack. The stack contains IAM resources with custom names. When the developer tries to deploy the stack, they receive an InsufficientCapabilities error.What should the developer do to resolve this issue?",
      "choices": {
        "A": "Specify the CAPABILITY_AUTO_EXPAND capability in the CloudFormation stack.",
        "B": "Use an administrators role to deploy IAM resources with CloudFormation.",
        "C": "Specify the CAPABILITY_IAM capability in the CloudFormation stack.",
        "D": "Specify the CAPABILITY_NAMED_IAM capability in the CloudFormation stack."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122631-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3:14 a.m.",
      "textHash": "8ce97364844cc0d3",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "CloudFormation est un service AWS qui cr√©e des ressources (serveurs, r√¥les, permissions) √† partir d‚Äôun mod√®le.\nIAM g√®re les identit√©s et les permissions (r√¥les, policies). Cr√©er/modifier IAM est sensible.\nSi votre stack CloudFormation contient des ressources IAM, CloudFormation exige une ‚Äúautorisation explicite‚Äù via un param√®tre de capacit√©.\nL‚Äôerreur InsufficientCapabilities signifie : ‚Äúje vois que tu veux cr√©er des permissions, mais tu n‚Äôas pas confirm√© que tu l‚Äôacceptes‚Äù.\nQuand les ressources IAM ont des noms personnalis√©s (ex: RoleName, PolicyName), il faut CAPABILITY_NAMED_IAM.\nCAPABILITY_IAM suffit seulement si CloudFormation g√©n√®re les noms automatiquement.\nCAPABILITY_AUTO_EXPAND concerne surtout les macros/transformations, pas l‚Äôacceptation des ressources IAM.\nDonc la solution est d‚Äôajouter CAPABILITY_NAMED_IAM lors du d√©ploiement de la stack.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu organises une sortie scolaire. Tu veux aussi distribuer des badges d‚Äôacc√®s (qui peut entrer o√π) et tu veux √©crire des noms pr√©cis sur les badges (ex: ‚ÄúChef de groupe ‚Äì L√©a‚Äù).**\n\nCloudFormation, c‚Äôest comme une liste d‚Äôorganisation qui cr√©e tout automatiquement. IAM, c‚Äôest les badges d‚Äôacc√®s (les droits). Quand tu donnes des noms personnalis√©s aux badges, le lyc√©e veut une autorisation sp√©ciale, sinon il bloque: ‚ÄúInsufficientCapabilities‚Äù = ‚Äútu n‚Äôas pas sign√© le papier‚Äù. La solution, c‚Äôest de cocher la case qui dit: ‚Äúj‚Äôaccepte de cr√©er des badges d‚Äôacc√®s avec des noms pr√©cis‚Äù. Dans AWS, cette case s‚Äôappelle CAPABILITY_NAMED_IAM. CAPABILITY_IAM serait pour cr√©er des badges sans noms impos√©s. Donc la bonne r√©ponse est D.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:206:8840133ac865f6ce",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 206,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is setting up infrastructure by using AWS CloudFormation. If an error occurs when the resources described in the Cloud Formation template are provisioned, successfully provisioned resources must be preserved. The developer must provision and update the CloudFormation stack by using the AWS CLI.Which solution will meet these requirements?",
      "choices": {
        "A": "Add an --enable-termination-protection command line option to the create-stack command and the update-stack command.",
        "B": "Add a --disable-rollback command line option to the create-stack command and the update-stack command.",
        "C": "Add a --parameters ParameterKey=PreserveResources,ParameterValue=True command line option to the create-stack command and the update-stack command.",
        "D": "Add a --tags Key=PreserveResources,Value=True command line option to the create-stack command and the update-stack command."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122628-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3:10 a.m.",
      "textHash": "8840133ac865f6ce",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:da8b07c7",
      "frExplanation": "CloudFormation est un service AWS qui cr√©e des ressources (serveurs, bases de donn√©es, etc.) √† partir d‚Äôun fichier ‚Äútemplate‚Äù. Un ‚Äústack‚Äù est l‚Äôensemble de ces ressources g√©r√©es ensemble.\nPar d√©faut, si la cr√©ation ou la mise √† jour d‚Äôun stack √©choue, CloudFormation fait un ‚Äúrollback‚Äù : il annule tout et supprime aussi les ressources d√©j√† cr√©√©es pour revenir √† l‚Äô√©tat initial.\nIci, on veut l‚Äôinverse : si une erreur arrive, les ressources d√©j√† cr√©√©es avec succ√®s doivent rester en place.\nAvec l‚ÄôAWS CLI, l‚Äôoption --disable-rollback emp√™che CloudFormation de supprimer les ressources d√©j√† provisionn√©es quand une op√©ration √©choue.\nIl faut donc ajouter --disable-rollback lors de create-stack (et aussi lors de update-stack si on veut √©viter le rollback pendant une mise √† jour).\nLes autres options ne r√©pondent pas au besoin : termination protection emp√™che la suppression manuelle du stack, et param√®tres/tags ‚ÄúPreserveResources‚Äù n‚Äôont pas d‚Äôeffet sp√©cial sur le rollback.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu construis une maquette en cours de techno avec une notice (le ‚Äútemplate‚Äù). Le prof te dit : si tu rates une √©tape, tu ne dois pas casser ce qui est d√©j√† bien mont√©.**\n\nCloudFormation, c‚Äôest la ‚Äúnotice‚Äù qui demande √† AWS de construire des √©l√©ments (les ‚Äúressources‚Äù). Par d√©faut, si une √©tape rate, AWS fait du ‚Äúrollback‚Äù : il d√©monte tout ce qu‚Äôil avait d√©j√† mont√©, comme si tu d√©truisais la maquette pour revenir au d√©but. Ici on veut l‚Äôinverse : garder ce qui a r√©ussi m√™me s‚Äôil y a une erreur. L‚Äôoption CLI --disable-rollback (r√©ponse B) dit : ‚Äúne d√©monte pas ce qui est d√©j√† construit‚Äù. A (termination protection) emp√™che surtout de supprimer la pile enti√®re, pas d‚Äô√©viter le d√©montage automatique apr√®s erreur. C et D (param√®tres/tags) sont juste des √©tiquettes/infos, √ßa ne change pas le comportement de d√©montage.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:204:ccc367054a77d71e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 204,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a serverless application by using the AWS Serverless Application Model (AWS SAM). The developer is currently testing the application in a development environment. When the application is nearly finished, the developer will need to set up additional testing and staging environments for a quality assurance team.The developer wants to use a feature of the AWS SAM to set up deployments to multiple environments.Which solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Add a configuration file in TOML format to group configuration entries to every environment. Add a table for each testing and staging environment. Deploy updates to the environments by using the sam deploy command and the --config-env flag that corresponds to each environment.",
        "B": "Create additional AWS SAM templates for each testing and staging environment. Write a custom shell script that uses the sam deploy command and the --template-file flag to deploy updates to the environments.",
        "C": "Create one AWS SAM configuration file that has default parameters. Perform updates to the testing and staging environments by using the --parameter-overrides flag in the AWS SAM CLI and the parameters that the updates will override.",
        "D": "Use the existing AWS SAM template. Add additional parameters to configure specific attributes for the serverless function and database table resources that are in each environment. Deploy updates to the testing and staging environments by using the sam deploy command."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122626-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3:07 a.m.",
      "textHash": "ccc367054a77d71e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:c9859c18",
      "frExplanation": "AWS SAM sert √† d√©crire et d√©ployer une application ¬´ serverless ¬ª (ex. AWS Lambda) via un template et la commande sam deploy.\nQuand on a plusieurs environnements (dev, test, staging), le plus simple est d‚Äô√©viter de dupliquer des templates ou de retaper des param√®tres √† chaque d√©ploiement.\nLa fonctionnalit√© pr√©vue pour √ßa est le fichier de configuration SAM en TOML (samconfig.toml) : on peut y d√©finir des sections par environnement.\nChaque section contient les valeurs propres √† l‚Äôenvironnement (nom de stack CloudFormation, r√©gion, param√®tres, etc.).\nEnsuite, on d√©ploie vers l‚Äôenvironnement voulu avec sam deploy --config-env <env>, sans script custom et sans multiples templates.\nC est plus manuel (il faut passer --parameter-overrides √† chaque fois) et donc plus d‚Äôeffort et plus d‚Äôerreurs possibles.\nB demande de maintenir plusieurs templates + un script : effort √©lev√©.\nD ajoute des param√®tres dans le template mais ne fournit pas un m√©canisme simple de s√©lection d‚Äôenvironnement ; on retombe sur du manuel.\nDonc A est la solution la plus directe et avec le moins d‚Äôeffort de d√©veloppement.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:201:a4389b258d449469",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 201,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that stores data in Amazon RDS instances. The application periodically experiences surges of high traffic that cause performance problems. During periods of peak traffic, a developer notices a reduction in query speed in all database queries.The team‚Äôs technical lead determines that a multi-threaded and scalable caching solution should be used to offload the heavy read traffic. The solution needs to improve performance.Which solution will meet these requirements with the LEAST complexity?",
      "choices": {
        "A": "Use Amazon ElastiCache for Memcached to offload read requests from the main database.",
        "B": "Replicate the data to Amazon DynamoDSet up a DynamoDB Accelerator (DAX) cluster.",
        "C": "Configure the Amazon RDS instances to use Multi-AZ deployment with one standby instance. Offload read requests from the main database to the standby instance.",
        "D": "Use Amazon ElastiCache for Redis to offload read requests from the main database."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122623-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3:03 a.m.",
      "textHash": "a4389b258d449469",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:34eef9fc",
      "frExplanation": "Le probl√®me vient des pics de trafic : beaucoup de lectures (SELECT) ralentissent Amazon RDS (base de donn√©es relationnelle g√©r√©e).\nUne solution simple est d‚Äôajouter un cache : on garde en m√©moire les r√©sultats des requ√™tes fr√©quentes pour √©viter de relire la base √† chaque fois.\nAmazon ElastiCache est le service AWS de cache en m√©moire. Il existe deux moteurs : Memcached et Redis.\nMemcached est tr√®s simple √† mettre en place, multi-thread (utilise plusieurs c≈ìurs CPU) et se scale facilement en ajoutant des n≈ìuds : id√©al pour ‚Äúoffloader‚Äù des lectures.\nCela r√©duit la charge sur RDS et acc√©l√®re les r√©ponses pendant les pics, sans changer le mod√®le de donn√©es.\nRedis offre plus de fonctionnalit√©s (persistance, r√©plication, structures de donn√©es), mais ajoute de la complexit√© inutile ici.\nDynamoDB + DAX demanderait de migrer/dupliquer des donn√©es vers une autre base : plus complexe.\nMulti-AZ sert surtout √† la haute disponibilit√© (failover), pas √† r√©partir les lectures sur le standby.\nDonc ElastiCache for Memcached est le choix le moins complexe qui r√©pond au besoin.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : la cuisine (la base de donn√©es) pr√©pare les plats, et un comptoir avec des plateaux d√©j√† pr√™ts (le cache) sert vite quand tout le monde arrive d‚Äôun coup.**\n\nConcept : un cache, c‚Äôest une ‚Äúm√©moire rapide‚Äù qui garde les r√©ponses les plus demand√©es, pour √©viter de redemander √† la cuisine √† chaque √©l√®ve.\nIci, le probl√®me vient surtout des lectures (beaucoup de gens demandent les m√™mes infos), donc on veut un comptoir rapide et capable de servir plusieurs files en m√™me temps (multi-thread, scalable).\nA (ElastiCache Memcached) = un comptoir simple, tr√®s rapide, fait pour r√©pondre √† plein de demandes de lecture et soulager la cuisine, sans prise de t√™te.\nD (Redis) marche aussi comme comptoir, mais il a plus de fonctions (comme g√©rer des r√®gles et des ‚Äúcasiers‚Äù), donc souvent plus de r√©glages = plus complexe.\nB, c‚Äôest comme changer carr√©ment de cuisine + ajouter un turbo : gros chantier.\nC, c‚Äôest juste une cuisine de secours en cas de panne, pas un deuxi√®me comptoir pour servir les √©l√®ves.\nDonc A am√©liore la vitesse avec le moins de complexit√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:200:09d6518172372f8e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 200,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a serverless application on AWS that uses a fleet of AWS Lambda functions that have aliases. The company regularly publishes new Lambda function by using an in-house deployment solution. The company wants to improve the release process and to use traffic shifting. A newly published function version should initially make available only to a fixed percentage of production users.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure routing on the alias of the new function by using a weighted alias.",
        "B": "Configure a canary deployment type for Lambda.",
        "C": "Configure routing on the new versions by using environment variables.",
        "D": "Configure a linear deployment type for Lambda."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122622-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3:01 a.m.",
      "textHash": "09d6518172372f8e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:403f4f68",
      "frExplanation": "AWS Lambda ex√©cute du code sans serveur. Quand vous publiez une nouvelle version, elle est immuable (elle ne change plus). Un alias est un ‚Äúnom‚Äù (ex: prod) qui pointe vers une version pr√©cise.\nPour faire du traffic shifting, il faut envoyer une partie des utilisateurs vers la nouvelle version tout en gardant le reste sur l‚Äôancienne.\nUn ‚Äúweighted alias‚Äù (alias pond√©r√©) permet exactement cela : l‚Äôalias prod peut pointer √† 95% vers la version actuelle et 5% vers la nouvelle.\nAinsi, seule une fraction fixe des requ√™tes de production teste la nouvelle version, et vous pouvez augmenter ou r√©duire le pourcentage facilement.\nLes options canary/linear sont des strat√©gies souvent g√©r√©es via des outils de d√©ploiement (ex: CodeDeploy), mais la question demande une solution directe r√©pondant au besoin ‚Äúpourcentage fixe au d√©but‚Äù.\nLes variables d‚Äôenvironnement ne routent pas le trafic : elles ne font que configurer le code.\nDonc la bonne r√©ponse est A : configurer le routage sur l‚Äôalias avec un poids.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:194:54a3ab60d0d29067",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 194,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has developed a new serverless application using AWS Lambda functions that will be deployed using the AWS Serverless Application Model (AWS SAM) CLI.Which step should the developer complete prior to deploying the application?",
      "choices": {
        "A": "Compress the application to a .zip file and upload it into AWS Lambda.",
        "B": "Test the new AWS Lambda function by first tracing it in AWS X-Ray.",
        "C": "Bundle the serverless application using a SAM package.",
        "D": "Create the application environment using the eb create my-env command."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122616-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:52 a.m.",
      "textHash": "54a3ab60d0d29067",
      "rawFormat": "discussion-md",
      "conceptKey": "xray_tracing",
      "frExplanation": "Ici, l‚Äôapplication est ¬´ serverless ¬ª avec AWS Lambda (un service qui ex√©cute du code sans g√©rer de serveurs) et elle sera d√©ploy√©e avec AWS SAM CLI (un outil qui d√©ploie des applis serverless d√©crites en template). Avant de d√©ployer, SAM doit pr√©parer les artefacts (code, d√©pendances) et les rendre accessibles √† AWS. La commande/√©tape ¬´ SAM package ¬ª sert √† regrouper (bundle) l‚Äôapplication et √† t√©l√©verser les fichiers n√©cessaires dans un bucket S3, puis √† g√©n√©rer un template pr√™t pour le d√©ploiement. Ensuite seulement, on fait le d√©ploiement (ex: sam deploy) √† partir de ce template packag√©. A est faux car SAM g√®re l‚Äôemballage, on n‚Äôupload pas manuellement un zip dans Lambda pour une appli SAM compl√®te. B est hors sujet : X-Ray sert au tra√ßage/diagnostic, pas une √©tape obligatoire avant d√©ploiement. D est faux car eb create concerne Elastic Beanstalk, pas SAM/Lambda.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois envoyer un devoir √† ton prof via l‚ÄôENT : tu as plein de fichiers (texte, images) et tu dois les mettre dans un seul dossier bien pr√©par√© avant de cliquer sur ‚Äúenvoyer‚Äù.**\n\nIci, l‚Äôappli ‚Äúserverless‚Äù c‚Äôest comme ton devoir compos√© de plusieurs morceaux. AWS SAM CLI, c‚Äôest l‚Äôoutil qui va l‚Äôenvoyer et l‚Äôinstaller au bon endroit. Avant de d√©ployer, tu dois ‚Äúemballer‚Äù tout ce qu‚Äôil faut dans un paquet propre : c‚Äôest l‚Äô√©tape SAM package (r√©ponse C). Comme √ßa, AWS sait exactement quoi prendre et o√π le trouver. A parle d‚Äôun .zip pour UNE fonction, mais l‚Äôappli SAM peut contenir plusieurs √©l√©ments, pas juste un zip. B (X-Ray) c‚Äôest comme v√©rifier le trajet du devoir apr√®s l‚Äôenvoi, pas une √©tape obligatoire avant. D (eb create) c‚Äôest pour un autre type d‚Äôappli (comme une autre plateforme), pas pour SAM/Lambda.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:186:661cd4b2a6c9ee9c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 186,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to deploy an application running on AWS Fargate using Amazon ECS. The application has environment variables that must be passed to a container for the application to initialize.How should the environment variables be passed to the container?",
      "choices": {
        "A": "Define an array that includes the environment variables under the environment parameter within the service definition.",
        "B": "Define an array that includes the environment variables under the environment parameter within the task definition.",
        "C": "Define an array that includes the environment variables under the entryPoint parameter within the task definition.",
        "D": "Define an array that includes the environment variables under the entryPoint parameter within the service definition."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122608-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:44 a.m.",
      "textHash": "661cd4b2a6c9ee9c",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:48e6626e",
      "frExplanation": "Sur ECS avec AWS Fargate, vous ex√©cutez des conteneurs sans g√©rer de serveurs.\nDeux notions cl√©s : la ¬´ task definition ¬ª (mod√®le qui d√©crit comment lancer le conteneur) et le ¬´ service ¬ª (qui maintient un certain nombre de t√¢ches en cours).\nLes variables d‚Äôenvironnement (ex: DB_HOST, API_KEY) font partie de la configuration du conteneur au d√©marrage.\nCette configuration se d√©finit dans la task definition, dans la section containerDefinitions, champ ¬´ environment ¬ª (liste cl√©/valeur).\nLe service ne sert pas √† d√©crire le contenu du conteneur, mais √† orchestrer le d√©ploiement (nombre de t√¢ches, r√©seau, scaling).\nLe champ entryPoint sert √† d√©finir la commande/point d‚Äôentr√©e du conteneur, pas des variables d‚Äôenvironnement.\nDonc il faut d√©clarer un tableau de variables sous ¬´ environment ¬ª dans la task definition.\nR√©ponse correcte : B.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o o√π tu cr√©es un ‚Äúloadout‚Äù (classe) avant une partie : armes, perks, r√©glages. Puis tu lances plusieurs parties avec ce m√™me loadout.**\n\nConcept : les ‚Äúvariables d‚Äôenvironnement‚Äù, c‚Äôest comme des r√©glages secrets (pseudo, langue, mode facile/difficile) que le perso doit avoir AU D√âMARRAGE pour fonctionner.\nDans ECS/Fargate, la ‚Äútask definition‚Äù = la fiche de ton loadout : elle d√©crit le conteneur et tout ce qu‚Äôil re√ßoit au lancement.\nLe ‚Äúservice‚Äù = le bouton qui lance et relance des parties (il g√®re combien de matchs tournent), mais il ne red√©finit pas les r√©glages internes du perso.\nDonc pour donner ces r√©glages au conteneur, tu les mets dans la task definition, dans le champ environment.\nentryPoint, c‚Äôest plut√¥t ‚Äúquelle commande d√©marre le jeu‚Äù, pas ‚Äúquels r√©glages il utilise‚Äù.\nDonc la bonne r√©ponse est B : variables dans environment de la task definition.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:185:6773d3354f494963",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 185,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has been asked to create an AWS Lambda function that is invoked any time updates are made to items in an Amazon DynamoDB table. The function has been created, and appropriate permissions have been added to the Lambda execution role. Amazon DynamoDB streams have been enabled for the table, but the function is still not being invoked.Which option would enable DynamoDB table updates to invoke the Lambda function?",
      "choices": {
        "A": "Change the StreamViewType parameter value to NEW_AND_OLD_IMAGES for the DynamoDB table.",
        "B": "Configure event source mapping for the Lambda function.",
        "C": "Map an Amazon Simple Notification Service (Amazon SNS) topic to the DynamoDB streams.",
        "D": "Increase the maximum runtime (timeout) setting of the Lambda function."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122607-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:43 a.m.",
      "textHash": "6773d3354f494963",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Pour d√©clencher une fonction AWS Lambda √† chaque modification d‚Äôune table DynamoDB, il ne suffit pas d‚Äôactiver DynamoDB Streams.\nDynamoDB Streams enregistre les changements (insert/update/delete) dans un ‚Äúflux‚Äù d‚Äô√©v√©nements, mais Lambda ne lit pas ce flux automatiquement.\nIl faut cr√©er un lien explicite entre le flux DynamoDB et la fonction Lambda : c‚Äôest l‚Äô‚Äúevent source mapping‚Äù.\nCet event source mapping dit √† Lambda : ‚Äúlis ce stream, r√©cup√®re les nouveaux enregistrements, et appelle la fonction avec ces donn√©es‚Äù.\nChanger StreamViewType (A) ne fait que modifier le contenu des √©v√©nements (nouvelles/anciennes valeurs), pas le d√©clenchement.\nSNS (C) n‚Äôest pas n√©cessaire ici : DynamoDB Streams s‚Äôint√®gre directement avec Lambda via event source mapping.\nLe timeout (D) n‚Äôaide pas si la fonction n‚Äôest jamais appel√©e.\nDonc la bonne action est de configurer l‚Äôevent source mapping pour que les mises √† jour DynamoDB invoquent Lambda.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une salle de classe : le cahier d‚Äôappel (la table DynamoDB) note chaque changement d‚Äô√©l√®ve. Un haut-parleur (Lambda) doit annoncer d√®s qu‚Äôun changement arrive. Le ‚Äújournal des changements‚Äù (DynamoDB Streams) est activ√©, mais personne n‚Äôa branch√© le journal au haut-parleur.**\n\nConcept : DynamoDB Streams, c‚Äôest comme un carnet qui enregistre toutes les modifications. Lambda, c‚Äôest un √©l√®ve ‚Äúd√©l√©gu√©‚Äù qui agit quand on l‚Äôappelle.\nProbl√®me : avoir le carnet + le d√©l√©gu√© ne suffit pas. Il faut une r√®gle de branchement : ‚Äúquand une ligne est ajout√©e au carnet, appelle le d√©l√©gu√©‚Äù.\nB est correct : ‚Äúevent source mapping‚Äù = le c√¢ble/la r√®gle qui relie le carnet des changements √† Lambda, pour que chaque update d√©clenche automatiquement la fonction.\nA change juste le contenu not√© dans le carnet (avant/apr√®s), mais ne branche rien.\nC ajoute un autre interm√©diaire (SNS) inutile ici.\nD change le temps autoris√© au d√©l√©gu√© pour parler, mais il n‚Äôest jamais appel√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:170:c9c98e1ec65df9ad",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 170,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer at a company recently created a serverless application to process and show data from business reports. The application‚Äôs user interface (UI) allows users to select and start processing the files. The UI displays a message when the result is available to view. The application uses AWS Step Functions with AWS Lambda functions to process the files. The developer used Amazon API Gateway and Lambda functions to create an API to support the UI.The company‚Äôs UI team reports that the request to process a file is often returning timeout errors because of the size or complexity of the files. The UI team wants the API to provide an immediate response so that the UI can display a message while the files are being processed. The backend process that is invoked by the API needs to send an email message when the report processing is complete.What should the developer do to configure the API to meet these requirements?",
      "choices": {
        "A": "Change the API Gateway route to add an X-Amz-Invocation-Type header with a static value of ‚ÄòEvent‚Äô in the integration request. Deploy the API Gateway stage to apply the changes.",
        "B": "Change the configuration of the Lambda function that implements the request to process a file. Configure the maximum age of the event so that the Lambda function will run asynchronously.",
        "C": "Change the API Gateway timeout value to match the Lambda function timeout value. Deploy the API Gateway stage to apply the changes.",
        "D": "Change the API Gateway route to add an X-Amz-Target header with a static value of ‚ÄòAsync‚Äô in the integration request. Deploy the API Gateway stage to apply the changes."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122590-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:19 a.m.",
      "textHash": "c9c98e1ec65df9ad",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:bbc0f197",
      "frExplanation": "Le probl√®me vient du fait qu‚ÄôAPI Gateway attend la fin de l‚Äôex√©cution de Lambda pour r√©pondre. Si le traitement est long (gros fichiers), API Gateway d√©passe son d√©lai et renvoie un timeout.\nPour que l‚ÄôUI re√ßoive tout de suite une r√©ponse, il faut d√©clencher Lambda en mode asynchrone : API Gateway envoie la demande, puis r√©pond imm√©diatement (par exemple 202 Accepted) sans attendre le r√©sultat.\nDans API Gateway, on force ce mode asynchrone en ajoutant l‚Äôen-t√™te HTTP ¬´ X-Amz-Invocation-Type: Event ¬ª dans la requ√™te d‚Äôint√©gration vers Lambda.\nEnsuite, le traitement continue en arri√®re-plan (Step Functions + Lambdas) et peut envoyer un email √† la fin (via SES/SNS ou une autre Lambda).\nB est faux : ‚Äúmaximum age of the event‚Äù concerne surtout les tentatives/√©v√©nements asynchrones c√¥t√© Lambda, pas le fait qu‚ÄôAPI Gateway r√©ponde imm√©diatement.\nC est faux : on ne peut pas simplement augmenter le timeout d‚ÄôAPI Gateway pour des traitements tr√®s longs, et ce n‚Äôest pas l‚Äôobjectif (r√©ponse imm√©diate).\nD est faux : ¬´ X-Amz-Target ¬ª n‚Äôest pas l‚Äôen-t√™te utilis√© pour rendre l‚Äôinvocation Lambda asynchrone via API Gateway.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine : tu donnes ton ticket, et parfois ton plat est long √† pr√©parer. Au lieu de te faire attendre au comptoir, on te donne tout de suite un bip (ou un num√©ro) et tu vas t‚Äôasseoir. Quand c‚Äôest pr√™t, on t‚Äôappelle.**\n\nIci, l‚ÄôUI (l‚Äô√©cran) fait la commande ‚Äútraite ce gros fichier‚Äù. Mais √ßa prend trop de temps, donc √ßa ‚Äúexpire‚Äù (timeout), comme si la cantine te disait ‚Äútrop long, je ferme le guichet‚Äù.\nLa solution, c‚Äôest de demander au guichet (API Gateway) : ‚Äúne me fais pas attendre, lance juste la pr√©paration et r√©pond tout de suite‚Äù.\nLe header X-Amz-Invocation-Type = 'Event' (r√©ponse A) veut dire : appel ‚Äúen mode je d√©pose la commande et je pars‚Äù.\nDonc l‚ÄôAPI r√©pond imm√©diatement, l‚ÄôUI peut afficher ‚ÄúTraitement en cours‚Ä¶‚Äù.\nEnsuite, en coulisses, Step Functions + Lambda continuent la cuisson.\nQuand c‚Äôest fini, le backend peut envoyer l‚Äôemail, comme la cantine qui t‚Äôappelle quand ton plat est pr√™t.\nB ne r√®gle pas le fait que l‚ÄôAPI attend trop longtemps.\nC ne marche pas : l‚ÄôAPI a une limite de temps, tu ne peux pas l‚Äô√©tirer assez.\nD utilise un mauvais ‚Äúbouton‚Äù : ce n‚Äôest pas celui qui d√©clenche l‚Äôappel asynchrone ici.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:163:c1e641b767ba2a10",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 163,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is troubleshooting an application that uses Amazon DynamoDB in the us-west-2 Region. The application is deployed to an Amazon EC2 instance. The application requires read-only permissions to a table that is named Cars. The EC2 instance has an attached IAM role that contains the following IAM policy:When the application tries to read from the Cars table, an Access Denied error occurs.How can the developer resolve this error?",
      "choices": {
        "A": "Modify the IAM policy resource to be ‚Äúarn:aws:dynamodb:us-west-2:account-id:table/*‚Äù.",
        "B": "Modify the IAM policy to include the dynamodb:* action.",
        "C": "Create a trust policy that specifies the EC2 service principal. Associate the role with the policy.",
        "D": "Create a trust relationship between the role and dynamodb.amazonaws.com."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122581-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:07 a.m.",
      "textHash": "c1e641b767ba2a10",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, l‚Äôerreur ¬´ Access Denied ¬ª vient souvent du fait que l‚Äôapplication n‚Äôutilise pas r√©ellement le r√¥le IAM pr√©vu. IAM est le syst√®me d‚Äôautorisations d‚ÄôAWS, et un ¬´ r√¥le ¬ª doit √™tre autoris√© √† √™tre assum√© par un service.\nSur une instance EC2, pour que les identifiants temporaires du r√¥le soient fournis √† l‚Äôapplication, le r√¥le doit avoir une relation de confiance (trust policy) qui autorise le service EC2 (principal ec2.amazonaws.com) √† assumer ce r√¥le.\nSi cette trust policy manque ou est incorrecte, m√™me avec une policy qui autorise DynamoDB en lecture, l‚Äôapplication n‚Äôobtient pas les bons droits et DynamoDB refuse l‚Äôacc√®s.\nLa solution est donc de cr√©er/ajuster la trust policy pour EC2 et d‚Äôassocier correctement le r√¥le √† l‚Äôinstance (via un instance profile).\nA et B changent les permissions DynamoDB (resource/actions) mais ne corrigent pas le probl√®me si EC2 ne peut pas assumer le r√¥le.\nD est faux car DynamoDB n‚Äôa pas besoin d‚Äôassumer le r√¥le de votre instance : c‚Äôest l‚Äôapplication sur EC2 qui appelle DynamoDB.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:152:d01ea1e8b9e55bff",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 152,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has multiple Amazon VPC endpoints in the same VPC. A developer needs to configure an Amazon S3 bucket policy so users can access an S3 bucket only by using these VPC endpoints.Which solution will meet these requirements?",
      "choices": {
        "A": "Create multiple S3 bucket polices by using each VPC endpoint ID that have the aws:SourceVpce value in the StringNotEquals condition.",
        "B": "Create a single S3 bucket policy that has the aws:SourceVpc value and in the StringNotEquals condition to use VPC ID.",
        "C": "Create a single S3 bucket policy that has the aws:SourceVpce value and in the StringNotEquals condition to use vpce*.",
        "D": "Create a single S3 bucket policy that has multiple aws:sourceVpce value in the StringNotEquals condition. Repeat for all the VPC endpoint IDs."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122572-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:55 a.m.",
      "textHash": "d01ea1e8b9e55bff",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Objectif : autoriser l‚Äôacc√®s au bucket S3 uniquement si la requ√™te passe par des ¬´ VPC endpoints ¬ª (points d‚Äôacc√®s priv√©s depuis votre r√©seau VPC, sans Internet).\nDans une policy de bucket S3, on peut tester d‚Äôo√π vient la requ√™te avec une condition.\nLa cl√© aws:SourceVpce correspond √† l‚ÄôID exact du VPC endpoint utilis√© (ex : vpce-123...).\nComme l‚Äôentreprise a plusieurs endpoints dans le m√™me VPC, il faut autoriser plusieurs IDs vpce-‚Ä¶\nLa bonne pratique est de faire une seule policy et de refuser (Deny) tout ce qui ne vient pas de la liste autoris√©e.\nDonc on met une condition StringNotEquals sur aws:SourceVpce avec plusieurs valeurs (tous les IDs des endpoints).\nA est faux : on ne cr√©e pas plusieurs policies de bucket s√©par√©es (un bucket a une seule bucket policy).\nB est faux : aws:SourceVpc (VPC ID) ne garantit pas le passage par un endpoint S3 pr√©cis.\nC est faux : utiliser un joker vpce* n‚Äôest pas une liste s√ªre/valide pour restreindre √† des endpoints sp√©cifiques.\nD est correct : une seule policy, plusieurs valeurs aws:SourceVpce, et on r√©p√®te tous les IDs d‚Äôendpoints autoris√©s.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:148:8f073244465790d0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 148,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is trying to get data from an Amazon DynamoDB table called demoman-table. The developer configured the AWS CLI to use a specific IAM user‚Äôs credentials and ran the following command:aws dynamodb get-item --table-name demoman-table --key '{\"id\": {\"N\":\"1993\"}}'The command returned errors and no rows were returned.What is the MOST likely cause of these issues?",
      "choices": {
        "A": "The command is incorrect; it should be rewritten to use put-item with a string argument.",
        "B": "The developer needs to log a ticket with AWS Support to enable access to the demoman-table.",
        "C": "Amazon DynamoDB cannot be accessed from the AWS CLI and needs to be called via the REST API.",
        "D": "The IAM user needs an associated policy with read access to demoman-table."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122566-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:41 a.m.",
      "textHash": "8f073244465790d0",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, le d√©veloppeur utilise l‚ÄôAWS CLI (outil en ligne de commande) avec les identifiants d‚Äôun utilisateur IAM (compte/identit√© avec des permissions). DynamoDB est une base de donn√©es NoSQL AWS, et la commande get-item sert √† lire un √©l√©ment pr√©cis via sa cl√© primaire. Si la commande renvoie des erreurs et aucun r√©sultat, la cause la plus fr√©quente est un manque d‚Äôautorisations : par d√©faut, un utilisateur IAM ne peut rien faire sans politique (policy) attach√©e. Pour lire dans une table, il faut une policy autorisant au minimum dynamodb:GetItem (et souvent dynamodb:DescribeTable) sur la ressource demoman-table. Les autres choix ne tiennent pas : on n‚Äôutilise pas put-item pour lire, pas besoin d‚ÄôAWS Support pour ‚Äúactiver‚Äù une table, et DynamoDB est bien accessible via l‚ÄôAWS CLI. Donc l‚Äôutilisateur IAM doit avoir une policy de lecture sur cette table.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:276:333f0460e2dd363d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 276,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to implement a custom machine learning (ML) library in an application. The size of the library is 15 GB. The size of the library is increasing. The application uses AWS Lambda functions. All the Lambda functions must have access to the library.Which solution will meet these requirements?",
      "choices": {
        "A": "Save the library in Lambda layers. Attach the layers to all Lambda functions.",
        "B": "Save the library in Amazon S3. Download the library from Amazon S3 inside the Lambda function.",
        "C": "Save the library as a Lambda container image. Redeploy the Lambda functions with the new image.",
        "D": "Save the library in an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all the Lambda functions."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134276-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:32 a.m.",
      "textHash": "333f0460e2dd363d",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:a0488c52",
      "frExplanation": "AWS Lambda ex√©cute du code sans serveur, mais chaque fonction a des limites de taille pour son package et ses ‚Äúlayers‚Äù (couches partag√©es). Un layer ne peut pas contenir une biblioth√®que de 15 Go : c‚Äôest bien trop gros, et la taille augmente. Mettre la biblioth√®que dans S3 obligerait chaque ex√©cution √† t√©l√©charger des gigaoctets, ce qui serait tr√®s lent, co√ªteux et risquerait des timeouts. Utiliser une image conteneur Lambda peut aller plus loin en taille, mais il faut reconstruire et red√©ployer toutes les fonctions √† chaque mise √† jour, et on duplique la biblioth√®que dans chaque image. Amazon EFS est un syst√®me de fichiers r√©seau partag√© : on y stocke la biblioth√®que une seule fois et toutes les fonctions Lambda peuvent le monter et lire les fichiers. C‚Äôest donc la meilleure solution pour un gros contenu partag√© et √©volutif.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une classe o√π tout le monde doit utiliser le m√™me √©norme manuel de 15 kg, et il grossit chaque mois. Tu peux soit donner une copie √† chaque √©l√®ve, soit mettre le manuel dans une salle commune accessible √† tous.**\n\nConcept : une fonction Lambda, c‚Äôest comme un √©l√®ve qui arrive, fait un exercice vite, puis repart. Elle ne peut pas transporter un √©norme manuel avec elle √† chaque fois.\nA (Layers) = donner une copie du manuel √† chaque √©l√®ve : 15 GB, c‚Äôest trop lourd et √ßa devient pire en grandissant.\nB (S3) = laisser le manuel dans un casier et demander √† chaque √©l√®ve d‚Äôaller le chercher avant de travailler : √ßa fait perdre du temps √† chaque devoir.\nC (Image) = refaire le sac complet de chaque √©l√®ve √† chaque mise √† jour du manuel : long et p√©nible √† red√©ployer.\nD (EFS) = mettre le manuel dans la ‚Äúbiblioth√®que de l‚Äô√©cole‚Äù et chaque √©l√®ve y acc√®de quand il en a besoin.\nDonc D est le meilleur : un seul endroit partag√©, gros volume, et toutes les fonctions Lambda peuvent l‚Äôutiliser sans recopier ni ret√©l√©charger.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:239:71f8999b3480102e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 239,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer created a web API that receives requests by using an internet-facing Application Load Balancer (ALB) with an HTTPS listener. The developer configures an Amazon Cognito user pool and wants to ensure that every request to the API is authenticated through Amazon Cognito.What should the developer do to meet this requirement?",
      "choices": {
        "A": "Add a listener rule to the listener to return a fixed response if the Authorization header is missing. Set the fixed response to 401 Unauthorized.",
        "B": "Create an authentication action for the listener rules of the ALSet the rule action type to authenticate-cognito. Set the OnUnauthenticatedRequest field to ‚Äúdeny.‚Äù",
        "C": "Create an Amazon API Gateway API. Configure all API methods to be forwarded to the ALB endpoint. Create an authorizer of the COGNITO_USER_POOLS type. Configure every API method to use that authorizer.",
        "D": "Create a new target group that includes an AWS Lambda function target that validates the Authorization header by using Amazon Cognito. Associate the target group with the listener."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124826-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:40 p.m.",
      "textHash": "71f8999b3480102e",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Ici, l‚ÄôAPI est derri√®re un Application Load Balancer (ALB) public avec HTTPS. On veut que chaque requ√™te soit obligatoirement authentifi√©e via Amazon Cognito (service AWS qui g√®re des utilisateurs et √©met des jetons JWT).\nLa bonne approche est d‚Äôutiliser la fonction native de l‚ÄôALB : une action d‚Äôauthentification dans les r√®gles du listener.\nAvec l‚Äôaction ¬´ authenticate-cognito ¬ª, l‚ÄôALB redirige l‚Äôutilisateur vers Cognito si besoin, v√©rifie le jeton, puis ne transmet la requ√™te au backend que si l‚Äôutilisateur est authentifi√©.\nEn mettant OnUnauthenticatedRequest √† ¬´ deny ¬ª, toute requ√™te non authentifi√©e est bloqu√©e automatiquement (pas de contournement possible).\nA est insuffisant : v√©rifier seulement l‚Äôabsence du header Authorization ne valide pas le jeton (un header peut √™tre pr√©sent mais faux/expir√©).\nC fonctionnerait mais ajoute API Gateway inutilement alors que l‚ÄôALB sait d√©j√† faire l‚Äôauth Cognito.\nD est plus complexe et redondant : une Lambda pour valider le header n‚Äôest pas n√©cessaire, l‚ÄôALB peut le faire directement.\nDonc il faut configurer une r√®gle du listener ALB avec l‚Äôaction ¬´ authenticate-cognito ¬ª et refuser les requ√™tes non authentifi√©es.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine l‚Äôentr√©e de ton lyc√©e avec un vigile et une liste d‚Äô√©l√®ves (Cognito = la liste officielle). Le portail (ALB) laisse passer vers les salles (ton API) seulement si tu prouves que tu es bien inscrit.**\n\nConcept : on veut que CHAQUE √©l√®ve qui entre montre sa carte, sinon il n‚Äôacc√®de √† rien. Le meilleur endroit pour contr√¥ler, c‚Äôest au portail, avant d‚Äôaller dans les couloirs.\nR√©ponse B : on r√®gle le portail (les r√®gles du listener de l‚ÄôALB) pour faire ‚Äúauthentifier avec Cognito‚Äù. Donc le vigile v√©rifie la carte sur la liste.\nEt ‚ÄúOnUnauthenticatedRequest = deny‚Äù = si tu n‚Äôas pas de carte ou qu‚Äôelle est fausse, tu es refus√© direct.\nA : juste v√©rifier ‚Äúil manque une carte‚Äù ne prouve pas qu‚Äôelle est vraie.\nC : ajouter une autre entr√©e (API Gateway) marche, mais c‚Äôest un d√©tour inutile.\nD : demander √† un prof (Lambda) de v√©rifier √† chaque fois, c‚Äôest plus compliqu√© que le vigile au portail.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:233:febd3f6fa2d91c41",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 233,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has created a large AWS Lambda function. Deployment of the function is failing because of an InvalidParameterValueException error. The error message indicates that the unzipped size of the function exceeds the maximum supported value.Which actions can the developer take to resolve this error? (Choose two.)",
      "choices": {
        "A": "Submit a quota increase request to AWS Support to increase the function to the required size.",
        "B": "Use a compression algorithm that is more efficient than ZIP.",
        "C": "Break up the function into multiple smaller functions.",
        "D": "Zip the .zip file twice to compress the file more.",
        "E": "Move common libraries, function dependencies, and custom runtimes into Lambda layers."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/126181-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Nov. 15, 2023, 4:27 a.m.",
      "textHash": "febd3f6fa2d91c41",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:285d9a30",
      "frExplanation": "AWS Lambda ex√©cute du code sans serveur, mais impose une limite stricte sur la taille du code ¬´ d√©compress√© ¬ª (unzipped) du package.\nL‚Äôerreur dit que, une fois d√©compress√©, votre fonction d√©passe cette limite : le d√©ploiement est donc refus√©.\nLa solution logique est de r√©duire ce qui se trouve dans le package de la fonction.\nOption C : d√©couper la grosse fonction en plusieurs petites fonctions r√©duit la taille de chaque d√©ploiement et rend le code plus modulaire.\nOption E : mettre les biblioth√®ques communes et d√©pendances dans des ¬´ Lambda Layers ¬ª (couches r√©utilisables) enl√®ve ces fichiers du package principal et diminue sa taille.\nOption A est faux : on ne peut pas demander d‚Äôaugmenter cette limite de taille de code pour Lambda comme un quota classique.\nOptions B et D ne r√©solvent pas le probl√®me : Lambda attend un ZIP standard, et ¬´ zipper deux fois ¬ª n‚Äôaide pas la taille une fois d√©compress√©e.\nDonc les actions efficaces sont de fractionner le code (C) et d‚Äôexternaliser les d√©pendances via des layers (E).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois rendre un √©norme dossier de cours dans une bo√Æte de d√©p√¥t au lyc√©e, mais la bo√Æte a une taille maximale.**\n\nConcept : AWS Lambda, c‚Äôest comme une petite ‚Äúbo√Æte‚Äù qui ex√©cute ton devoir automatiquement. Mais la bo√Æte a une limite de taille, m√™me apr√®s avoir d√©compress√© le dossier.\nIci, l‚Äôerreur dit : ‚Äúune fois d√©zipp√©, ton dossier est trop gros‚Äù. Donc ce n‚Äôest pas juste un probl√®me de compression.\nPourquoi C : tu d√©coupes ton gros dossier en plusieurs petits devoirs (plusieurs petites fonctions). Chaque devoir rentre dans la bo√Æte, et ensemble ils font le travail.\nPourquoi pas A : tu ne peux pas demander une bo√Æte plus grande, la limite est fixe.\nPourquoi pas B/D : changer de zip ou zipper deux fois n‚Äôaide pas si, une fois ouvert, le contenu d√©passe la limite.\nAstuce utile (hors r√©ponse donn√©e) : tu peux aussi mettre des ‚Äúannexes‚Äù communes dans un classeur √† part (des layers), pour all√©ger chaque devoir.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:230:5501feede97cbd7d",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 230,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer registered an AWS Lambda function as a target for an Application Load Balancer (ALB) using a CLI command. However, the Lambda function is not being invoked when the client sends requests through the ALB.Why is the Lambda function not being invoked?",
      "choices": {
        "A": "A Lambda function cannot be registered as a target for an ALB.",
        "B": "A Lambda function can be registered with an ALB using AWS Management Console only.",
        "C": "The permissions to invoke the Lambda function are missing.",
        "D": "Cross-zone is not enabled on the ALB."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124818-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:14 p.m.",
      "textHash": "5501feede97cbd7d",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Un Application Load Balancer (ALB) re√ßoit des requ√™tes HTTP/HTTPS et les envoie vers une ¬´ cible ¬ª (target). Une fonction AWS Lambda est un code ex√©cut√© √† la demande, sans serveur √† g√©rer.\nM√™me si vous avez bien enregistr√© la Lambda comme cible via la CLI, l‚ÄôALB n‚Äôa pas automatiquement le droit d‚Äôex√©cuter (invoke) cette fonction.\nAWS bloque par d√©faut tout appel √† une Lambda si la politique de permissions (resource-based policy) n‚Äôautorise pas explicitement le service appelant.\nIl faut donc ajouter une permission du type ¬´ lambda:AddPermission ¬ª pour autoriser le principal elasticloadbalancing.amazonaws.com √† invoquer la fonction, souvent avec une condition sur l‚ÄôARN du target group.\nSans cette autorisation, l‚ÄôALB re√ßoit la requ√™te mais l‚Äôappel √† Lambda est refus√©, donc la fonction ne se d√©clenche pas.\nLes autres choix sont faux : une Lambda peut √™tre cible d‚Äôun ALB, ce n‚Äôest pas limit√© √† la console, et le cross-zone concerne la r√©partition entre zones, pas l‚Äôautorisation d‚Äôinvocation.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine l‚Äôentr√©e du lyc√©e (l‚ÄôALB) qui re√ßoit tous les √©l√®ves et les envoie vers une salle pr√©cise (la fonction Lambda) pour faire un devoir.**\n\nConcept : l‚ÄôALB, c‚Äôest comme un surveillant √† l‚Äôentr√©e qui distribue les √©l√®ves vers la bonne salle selon la demande.\nMais pour entrer dans une salle, il faut une autorisation (un ‚Äúbadge‚Äù).\nSi le surveillant sait quelle salle viser, mais que la salle n‚Äôa pas donn√© le droit au surveillant de faire entrer des √©l√®ves, personne ne rentre.\nDonc la salle (Lambda) ne d√©marre jamais le devoir (n‚Äôest pas ‚Äúinvoqu√©e‚Äù).\nA est faux : on peut bien envoyer vers une Lambda.\nB est faux : on peut le faire en ligne de commande aussi.\nD n‚Äôa rien √† voir : ‚Äúcross-zone‚Äù c‚Äôest juste r√©partir entre b√¢timents, pas donner l‚Äôautorisation.\nLa bonne r√©ponse est C : il manque la permission pour que l‚ÄôALB ait le droit d‚Äôappeler Lambda.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:308:32b324d7ef39891e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 308,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company needs to deploy all its cloud resources by using AWS CloudFormation templates. A developer must create an Amazon Simple Notification Service (Amazon SNS) automatic notification to help enforce this rule. The developer creates an SNS topic and subscribes the email address of the company's security team to the SNS topic.The security team must receive a notification immediately if an IAM role is created without the use of CloudFormation.Which solution will meet this requirement?",
      "choices": {
        "A": "Create an AWS Lambda function to filter events from CloudTrail if a role was created without CloudFormation. Configure the Lambda function to publish to the SNS topic. Create an Amazon EventBridge schedule to invoke the Lambda function every 15 minutes.",
        "B": "Create an AWS Fargate task in Amazon Elastic Container Service (Amazon ECS) to filter events from CloudTrail if a role was created without CloudFormation. Configure the Fargate task to publish to the SNS topic. Create an Amazon EventBridge schedule to run the Fargate task every 15 minutes.",
        "C": "Launch an Amazon EC2 instance that includes a script to filter events from CloudTrail if a role was created without CloudFormation. Configure the script to publish to the SNS topic. Create a cron job to run the script on tile EC2 instance every 15 minutes.",
        "D": "Create an Amazon EventBridge rule to filter events from CloudTrail if a role was created without CloudFormation. Specify the SNS topic as the target of the EventBridge rule."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134342-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 22, 2024, 7:19 a.m.",
      "textHash": "32b324d7ef39891e",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Objectif : alerter tout de suite si quelqu‚Äôun cr√©e un r√¥le IAM (droits d‚Äôacc√®s) autrement que via CloudFormation (outil qui d√©ploie l‚Äôinfra √† partir de mod√®les). \nCloudTrail enregistre automatiquement les actions AWS (ex : CreateRole) sous forme d‚Äô√©v√©nements. \nAmazon EventBridge peut √©couter ces √©v√©nements en temps r√©el et appliquer un filtre (r√®gle) sur le type d‚Äôaction et des indices montrant que ce n‚Äôest pas CloudFormation (ex : l‚Äôappel vient d‚Äôun utilisateur/console plut√¥t que du service CloudFormation). \nEnsuite, EventBridge peut envoyer directement l‚Äô√©v√©nement vers une cible, ici un topic SNS. \nSNS envoie imm√©diatement un email aux abonn√©s (√©quipe s√©curit√©). \nLes options A/B/C utilisent un traitement ‚Äútoutes les 15 minutes‚Äù (schedule/cron), donc pas imm√©diat et plus complexe √† maintenir. \nDonc la bonne solution est une r√®gle EventBridge filtrant les √©v√©nements CloudTrail et ciblant SNS (D).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:421:34db6079c3f4ff06",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 421,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is setting up the deployment of application stacks to new test environments by using the AWS Cloud Development Kit (AWS CDK). The application contains the code for several AWS Lambda functions that will be deployed as assets. Each Lambda function is defined by using the AWS CDK Lambda construct library.The developer has already successfully deployed the application stacks to the alpha environment in the first account by using the AWS CDK CLI's cdk deploy command. The developer is preparing to deploy to the beta environment in a second account for the first time. The developer makes no significant changes to the CDK code between deployments, but the initial deployment in the second account is unsuccessful and returns a NoSuchBucket error.Which command should the developer run before redeployment to resolve this error?",
      "choices": {
        "A": "cdk synth",
        "B": "cdk bootstrap",
        "C": "cdk init",
        "D": "cdk destroy"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/146859-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Sept. 3, 2024, 5:57 p.m.",
      "textHash": "34db6079c3f4ff06",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:ba75bdf3",
      "frExplanation": "Avec AWS CDK, vous d√©crivez votre infrastructure en code, puis CDK d√©ploie via CloudFormation. Pour les fonctions AWS Lambda, le code est souvent envoy√© comme ‚Äúasset‚Äù (un fichier zip) dans un bucket Amazon S3 temporaire g√©r√© par CDK. Dans un nouveau compte/r√©gion, ce bucket (et d‚Äôautres ressources de support comme des r√¥les IAM) n‚Äôexiste pas encore. L‚Äôerreur NoSuchBucket signifie justement que CDK essaie d‚Äôuploader l‚Äôasset dans un bucket S3 qui n‚Äôa pas √©t√© cr√©√©. La commande cdk bootstrap pr√©pare l‚Äôenvironnement cible (compte + r√©gion) en cr√©ant le ‚ÄúCDK Toolkit stack‚Äù avec le bucket S3 et les r√¥les n√©cessaires. Apr√®s bootstrap, cdk deploy peut t√©l√©verser les assets et d√©ployer normalement. cdk synth ne fait que g√©n√©rer le template, cdk init cr√©e un projet, et cdk destroy supprime des stacks.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o o√π tu veux construire une base dans un nouveau serveur (compte AWS). Pour construire, le jeu a besoin d‚Äôun ‚Äúcoffre de stockage‚Äù commun o√π il met les mat√©riaux avant de les poser.**\n\nAvec AWS CDK, tes fonctions Lambda sont comme des ‚Äúobjets‚Äù √† livrer avant d‚Äô√™tre install√©s. CDK les met d‚Äôabord dans un seau de stockage (un bucket) puis les d√©ploie. Dans le 1er compte (alpha), ce coffre existait d√©j√†, donc √ßa marche. Dans le 2e compte (beta), c‚Äôest la premi√®re fois : le coffre n‚Äôexiste pas, donc erreur NoSuchBucket = ‚Äúcoffre introuvable‚Äù. La commande cdk bootstrap sert justement √† pr√©parer le nouveau compte en cr√©ant ce coffre et les ressources de base n√©cessaires. Ensuite, cdk deploy peut relivrer les objets et installer la base. Donc il faut lancer B: cdk bootstrap avant de red√©ployer.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:166:d8682226f3375cc7",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 166,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has code that is stored in an Amazon S3 bucket. The code must be deployed as an AWS Lambda function across multiple accounts in the same AWS Region as the S3 bucket. An AWS CloudFormation template that runs for each account will deploy the Lambda function.What is the MOST secure way to allow CloudFormation to access the Lambda code in the S3 bucket?",
      "choices": {
        "A": "Grant the CloudFormation service role the S3 ListBucket and GetObject permissions. Add a bucket policy to Amazon S3 with the principal of ‚ÄúAWS‚Äù: [account numbers].",
        "B": "Grant the CloudFormation service role the S3 GetObject permission. Add a bucket policy to Amazon S3 with the principal of ‚Äú*‚Äù.",
        "C": "Use a service-based link to grant the Lambda function the S3 ListBucket and GetObject permissions by explicitly adding the S3 bucket‚Äôs account number in the resource.",
        "D": "Use a service-based link to grant the Lambda function the S3 GetObject permission. Add a resource of ‚Äú*‚Äù to allow access to the S3 bucket."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122584-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:13 a.m.",
      "textHash": "d8682226f3375cc7",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, le code (zip) est dans Amazon S3 (stockage de fichiers). CloudFormation (outil qui d√©ploie des ressources via un template) doit lire ce fichier pour cr√©er la fonction AWS Lambda (ex√©cution de code). Comme le d√©ploiement se fait dans plusieurs comptes, il faut autoriser un acc√®s inter-comptes au bucket S3, mais de fa√ßon minimale.\nLa meilleure pratique est : donner uniquement les droits n√©cessaires au r√¥le de service CloudFormation (ListBucket pour voir le contenu du bucket si besoin, et GetObject pour t√©l√©charger le zip).\nEnsuite, on ajoute une bucket policy S3 qui autorise explicitement comme ‚Äúprincipal‚Äù uniquement les comptes AWS autoris√©s (liste des num√©ros de compte). Cela limite l‚Äôacc√®s √† un ensemble pr√©cis d‚Äôidentit√©s.\nLes options B et D sont dangereuses car elles utilisent principal ‚Äú*‚Äù ou ressource ‚Äú*‚Äù, ce qui ouvre l‚Äôacc√®s trop largement (potentiellement √† tout le monde).\nLes options C et D parlent d‚Äôautoriser la fonction Lambda, mais le probl√®me est que CloudFormation doit acc√©der au code au moment du d√©ploiement, avant que la fonction n‚Äôexiste ou n‚Äôait un r√¥le configur√©.\nDonc A est la plus s√©curis√©e : acc√®s minimal + autorisation explicite aux comptes concern√©s.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que (le bucket S3) o√π le prof a mis le sujet d‚Äôun devoir (le code). Plusieurs classes (plusieurs comptes AWS) doivent r√©cup√©rer ce sujet pour faire le m√™me devoir dans la m√™me √©cole (m√™me r√©gion). Le surveillant (CloudFormation) vient chercher le sujet pour chaque classe.**\n\nConcept : pour √™tre s√©curis√©, on donne l‚Äôacc√®s seulement √† la bonne personne, et seulement √† ce dont elle a besoin. Ici, le surveillant doit juste voir la liste du casier (ListBucket) et prendre la feuille (GetObject). R√©ponse A : on donne ces deux droits au surveillant, et on met une r√®gle √† l‚Äôentr√©e de la biblioth√®que qui dit : ‚Äúseules ces classes pr√©cises (num√©ros de comptes) ont le droit‚Äù. C‚Äôest comme une liste d‚Äô√©l√®ves autoris√©s, pas une porte ouverte. B est dangereux car ‚Äú*‚Äù = tout le monde peut entrer. C et D donnent des droits au mauvais √©l√®ve (Lambda) alors que c‚Äôest le surveillant (CloudFormation) qui doit aller chercher la feuille.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:417:7528fd8865556d31",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 417,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an application on Amazon Elastic Container Service (Amazon ECS). The developer needs to configure the application parameters. The developer must configure limits for the application's maximum number of simultaneous connections and maximum number of transactions per second.The maximum number of connections and transactions can change in the future. The developer needs a solution that can automatically deploy these changes to the application, as needed, without causing downtime.Which solution will meet these requirements?",
      "choices": {
        "A": "Make the configuration changes for the application. Use AWS CodeDeploy to create a deployment configuration. Specify an in-place deployment to deploy the changes.",
        "B": "Bootstrap the application to use the AWS Cloud Development Kit (AWS CDK) and make the configuration changes. Specify the ECSCanary10Percent15Minutes launch type in the properties section of the ECS resource. Deploy the application by using the AWS CDK to implement the changes.",
        "C": "Install the AWS AppConfig agent on Amazon ECS. Configure an IAM role with access to AWS AppConfig. Make the deployment changes by using AWS AppConfig. Specify Canary10Percent20Minutes as the deployment strategy.",
        "D": "Create an AWS Lambda function to make the configuration changes. Create an Amazon CloudWatch alarm that monitors the Lambda function every 5 minutes to check if the Lambda function has been updated. When the Lambda function is updated, deploy the changes by using AWS CodeDeploy."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/146743-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Sept. 1, 2024, 9:07 a.m.",
      "textHash": "7528fd8865556d31",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici, on parle de param√®tres d‚Äôapplication (limites de connexions et de transactions) qui peuvent changer souvent. On veut appliquer ces changements automatiquement, sans arr√™ter le service (pas de downtime).\nAWS AppConfig est un service fait pour g√©rer et d√©ployer des configurations d‚Äôapplication (valeurs, limites, ‚Äúfeature flags‚Äù) de fa√ßon contr√¥l√©e, sans red√©ployer l‚Äôimage du conteneur.\nSur ECS, on installe l‚Äôagent AppConfig (ou on int√®gre le SDK) pour que l‚Äôapplication r√©cup√®re la nouvelle configuration pendant qu‚Äôelle tourne.\nAvec un r√¥le IAM, l‚Äôapplication a le droit de lire la configuration AppConfig en toute s√©curit√©.\nLa strat√©gie ‚ÄúCanary10Percent20Minutes‚Äù d√©ploie le changement progressivement (ex: 10% des t√¢ches/instances au d√©but), ce qui r√©duit le risque et √©vite une coupure.\nC‚Äôest exactement le besoin: changements futurs, d√©ploiement automatique des param√®tres, et mise √† jour progressive sans interruption.\nLes options CodeDeploy/CDK concernent surtout le d√©ploiement de code/infrastructure, pas la gestion dynamique de param√®tres d‚Äôex√©cution.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : tu dois afficher des r√®gles du jour comme ‚Äúmax 200 √©l√®ves en m√™me temps‚Äù et ‚Äúmax 50 plateaux par minute‚Äù. Ces r√®gles changent parfois (jour de gr√®ve, menu sp√©cial) et tu veux les mettre √† jour sans fermer la cantine.**\n\nConcept : l‚Äôappli sur ECS, c‚Äôest la cantine qui sert des √©l√®ves (connexions) et des plateaux (transactions). Les ‚Äúparam√®tres‚Äù sont les r√®gles affich√©es. On veut pouvoir changer ces r√®gles souvent, automatiquement, sans arr√™ter le service.\nPourquoi C : AWS AppConfig, c‚Äôest le panneau d‚Äôaffichage officiel des r√®gles, que la cantine lit en continu. L‚Äôagent AppConfig sur ECS, c‚Äôest comme un surveillant qui r√©cup√®re la nouvelle r√®gle et l‚Äôapplique. La strat√©gie ‚ÄúCanary 10% pendant 20 min‚Äù, c‚Äôest tester la nouvelle r√®gle sur 10% des √©l√®ves d‚Äôabord : si √ßa se passe bien, on l‚Äôapplique √† tout le monde, sans fermer.\nPourquoi pas A/B/D : CodeDeploy/CDK, c‚Äôest plut√¥t ‚Äúr√©nover la cantine‚Äù (d√©ployer du code), pas juste changer une r√®gle. D est bricol√© (surveiller une fonction toutes les 5 min) et pas fait pour des param√®tres en direct.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:164:ea0efa53c09cae79",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 164,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nWhen using the AWS Encryption SDK, how does the developer keep track of the data encryption keys used to encrypt data?",
      "choices": {
        "A": "The developer must manually keep track of the data encryption keys used for each data object.",
        "B": "The SDK encrypts the data encryption key and stores it (encrypted) as part of the returned ciphertext.",
        "C": "The SDK stores the data encryption keys automatically in Amazon S3.",
        "D": "The data encryption key is stored in the Userdata for the EC2 instance."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122586-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:16 a.m.",
      "textHash": "ea0efa53c09cae79",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:4f921e13",
      "frExplanation": "Avec l‚ÄôAWS Encryption SDK, on chiffre des donn√©es avec une ¬´ cl√© de chiffrement des donn√©es ¬ª (DEK), une cl√© temporaire g√©n√©r√©e pour ce fichier/message.\nProbl√®me : si on perd cette DEK, on ne pourra plus jamais d√©chiffrer les donn√©es.\nLa bonne pratique est donc de ne pas la stocker s√©par√©ment ni de la g√©rer √† la main.\nL‚ÄôEncryption SDK r√©sout cela en chiffrant la DEK (on parle de ¬´ cl√© de donn√©es chiffr√©e ¬ª) avec une cl√© ma√Ætresse (souvent via AWS KMS).\nEnsuite, il ajoute cette DEK chiffr√©e directement dans le r√©sultat chiffr√© (le ciphertext), comme une ‚Äúenveloppe‚Äù attach√©e aux donn√©es.\nAinsi, quand on veut d√©chiffrer, le SDK lit le ciphertext, r√©cup√®re la DEK chiffr√©e, la fait d√©chiffrer, puis d√©chiffre les donn√©es.\nDonc le d√©veloppeur n‚Äôa pas √† suivre les DEK objet par objet (A faux), ce n‚Äôest pas stock√© automatiquement dans S3 (C faux), ni dans l‚ÄôUserdata EC2 (D faux).\nR√©ponse correcte : B.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que tu envoies un message secret √† un ami au lyc√©e. Tu fermes le message dans une petite bo√Æte avec un cadenas. La ‚Äúcl√©‚Äù du cadenas sert √† ouvrir la bo√Æte.**\n\nConcept : pour chiffrer (rendre secret) un fichier, on utilise une ‚Äúcl√© de chiffrement‚Äù, comme la cl√© d‚Äôun cadenas.\nAvec l‚ÄôAWS Encryption SDK, tu ne veux pas noter la cl√© sur un papier √† part (sinon tu la perds ou on te la vole).\nDonc le SDK fait un truc malin : il met la cl√© du cadenas dans une mini-bo√Æte, puis il ferme cette mini-bo√Æte avec un super cadenas (une autre cl√© plus prot√©g√©e).\nEt il colle cette mini-bo√Æte ferm√©e directement avec la bo√Æte principale (le fichier chiffr√©).\nR√©sultat : quand tu r√©cup√®res le fichier, tout ce qu‚Äôil faut pour retrouver la cl√© est d√©j√† ‚Äúdans le colis‚Äù, mais en version prot√©g√©e.\nDonc B est vrai : la cl√© de chiffrement des donn√©es est chiffr√©e et stock√©e avec le texte chiffr√©.\nA est faux (pas besoin de tout noter √† la main), C et D sont faux (ce n‚Äôest pas stock√© automatiquement dans un endroit comme S3 ou dans une machine).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:274:844e1e0c08c49fb1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 274,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is preparing to migrate an application to the company's first AWS environment. Before this migration, a developer is creating a proof-of-concept application to validate a model for building and deploying container-based applications on AWS.Which combination of steps should the developer take to deploy the containerized proof-of-concept application with the LEAST operational effort? (Choose two.)",
      "choices": {
        "A": "Package the application into a .zip file by using a command line tool. Upload the package to Amazon S3.",
        "B": "Package the application into a container image by using the Docker CLI. Upload the image to Amazon Elastic Container Registry (Amazon ECR).",
        "C": "Deploy the application to an Amazon EC2 instance by using AWS CodeDeploy.",
        "D": "Deploy the application to Amazon Elastic Kubernetes Service (Amazon EKS) on AWS Fargate.",
        "E": "Deploy the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134274-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:29 a.m.",
      "textHash": "844e1e0c08c49fb1",
      "rawFormat": "discussion-md",
      "conceptKey": "ecr_image_scanning",
      "frExplanation": "Pour d√©ployer une application ¬´ conteneuris√©e ¬ª avec le moins d‚Äôeffort, il faut suivre le mod√®le standard des conteneurs : 1) construire une image, 2) la stocker dans un registre, 3) la lancer sur un service manag√©.\nDocker sert √† empaqueter l‚Äôapplication dans une ¬´ image ¬ª (comme un paquet complet avec le code + d√©pendances).\nAmazon ECR (Elastic Container Registry) est le registre AWS qui stocke ces images Docker : c‚Äôest l‚Äôendroit naturel o√π pousser l‚Äôimage pour qu‚ÄôAWS puisse la r√©cup√©rer.\nDonc l‚Äô√©tape B est indispensable : cr√©er l‚Äôimage avec Docker CLI puis l‚Äôenvoyer dans ECR.\nLes options A et C sont plus orient√©es ‚Äúfichiers/serveurs‚Äù (.zip, EC2 + CodeDeploy) et demandent plus d‚Äôadministration (machines, d√©ploiements, mises √† jour).\nPour r√©duire l‚Äôop√©rationnel, on √©vite de g√©rer des serveurs et on utilise un service qui ex√©cute les conteneurs pour nous.\nECS sur Fargate (ou EKS sur Fargate) lance les conteneurs sans g√©rer d‚Äôinstances EC2 ; c‚Äôest plus simple qu‚Äôun cluster √† maintenir.\nEn pratique, la combinaison la plus ‚Äúlow ops‚Äù est : B (image dans ECR) + E (ex√©cution sur ECS Fargate).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine un club de jeux vid√©o au lyc√©e : tu veux faire tester un mini-jeu sans g√©rer des PC compliqu√©s pour tout le monde.**\n\nConcept : une ‚Äúapplication en conteneur‚Äù, c‚Äôest comme une cartouche de jeu qui contient le jeu + tout ce qu‚Äôil faut pour qu‚Äôil marche pareil partout.\n√âtape 1 : tu fabriques cette cartouche (image de conteneur) avec un outil (Docker), comme quand tu compiles/emballes ton jeu.\n√âtape 2 : tu la ranges dans un casier sp√©cial pour cartouches (Amazon ECR), pour la retrouver et la lancer facilement.\nPourquoi B : ECR est le ‚Äúrayon officiel‚Äù pr√©vu pour stocker des images de conteneurs, donc c‚Äôest simple et propre.\nPourquoi pas A : un .zip dans S3, c‚Äôest juste un sac de fichiers dans un grand stockage, pas un endroit fait pour des ‚Äúcartouches‚Äù de conteneurs.\nEt pour ‚Äúle moins d‚Äôeffort‚Äù : tu √©vites de bricoler des machines toi-m√™me, tu pr√©pares juste la cartouche et tu la mets au bon endroit.\nDonc la bonne action √† faire ici : B.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:271:91ec75b32ad87c18",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 271,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that is hosted on Amazon EC2 instances. The application stores objects in an Amazon S3 bucket and allows users to download objects from the S3 bucket. A developer turns on S3 Block Public Access for the S3 bucket. After this change, users report errors when they attempt to download objects. The developer needs to implement a solution so that only users who are signed in to the application can access objects in the S3 bucket.Which combination of steps will meet these requirements in the MOST secure way? (Choose two.)",
      "choices": {
        "A": "Create an EC2 instance profile and role with an appropriate policy. Associate the role with the EC2 instances.",
        "B": "Create an IAM user with an appropriate policy. Store the access key ID and secret access key on the EC2 instances.",
        "C": "Modify the application to use the S3 GeneratePresignedUrl API call.",
        "D": "Modify the application to use the S3 GetObject API call and to return the object handle to the user.",
        "E": "Modify the application to delegate requests to the S3 bucket."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134271-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:13 a.m.",
      "textHash": "91ec75b32ad87c18",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Activer ¬´ S3 Block Public Access ¬ª emp√™che tout acc√®s public au bucket : les liens/objets ne sont plus t√©l√©chargeables directement par n‚Äôimporte qui.\nObjectif : seuls les utilisateurs connect√©s √† l‚Äôapplication doivent pouvoir t√©l√©charger. Donc l‚Äôacc√®s √† S3 doit passer par l‚Äôapplication, pas par un bucket public.\nLa fa√ßon la plus s√ªre est de donner des droits S3 aux serveurs EC2 via un r√¥le IAM attach√© (instance profile) : c‚Äôest l‚Äôoption A.\nUn r√¥le IAM pour EC2 fournit des identifiants temporaires automatiquement, sans stocker de secrets sur le disque.\nL‚Äôoption B est moins s√ªre car elle impose de stocker une cl√© d‚Äôacc√®s et un secret sur les instances (risque de fuite/vol).\nAvec le r√¥le (A), l‚Äôapplication peut appeler S3 (ex: GetObject) en tant que serveur autoris√©, puis contr√¥ler l‚Äôacc√®s selon l‚Äôutilisateur connect√©.\nAinsi, le bucket reste priv√© (Block Public Access), et seuls les utilisateurs authentifi√©s via l‚Äôapplication obtiennent le contenu.\nEn r√©sum√© : utiliser un r√¥le IAM pour EC2 est la m√©thode la plus s√©curis√©e pour permettre √† l‚Äôapplication d‚Äôacc√©der √† S3 sans rendre le bucket public.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:261:45fa0d80d82703cd",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 261,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs an application on Amazon EC2 instances. The EC2 instances open connections to an Amazon RDS for SQL Server database. A developer needs to store and access the credentials and wants to automatically rotate the credentials. The developer does not want to store the credentials for the database in the code.Which solution will meet these requirements in the MOST secure way?",
      "choices": {
        "A": "Create an IAM role that has permissions to access the database. Attach the IAM role to the EC2 instances.",
        "B": "Store the credentials as secrets in AWS Secrets Manager. Create an AWS Lambda function to update the secrets and the database. Retrieve the credentials from Secrets Manager as needed.",
        "C": "Store the credentials in an encrypted text file in an Amazon S3 bucket. Configure the EC2 instance launch template to download the credentials from Amazon S3 as the instance launches. Create an AWS Lambda function to update the secrets and the database.",
        "D": "Store the credentials in an Amazon DynamoDB table. Configure an Amazon CloudWatch Events rule to invoke an AWS Lambda function to periodically update the secrets and database."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133069-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 6, 2024, 5:18 p.m.",
      "textHash": "45fa0d80d82703cd",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Ici, le besoin est de stocker des identifiants (login/mot de passe) de base de donn√©es sans les mettre dans le code, et de les faire tourner automatiquement.\nAWS Secrets Manager est un service con√ßu pour stocker des ‚Äúsecrets‚Äù (mots de passe, cl√©s) de fa√ßon chiffr√©e et contr√¥l√©e par des permissions IAM.\nIl propose nativement la rotation automatique des identifiants via une fonction AWS Lambda (petit code ex√©cut√© √† la demande) qui met √† jour √† la fois le secret et la base RDS.\nL‚Äôapplication sur EC2 r√©cup√®re le mot de passe au moment o√π elle en a besoin (API Secrets Manager), donc rien n‚Äôest ‚Äúen dur‚Äù dans le code ni dans un fichier.\nC‚Äôest plus s√©curis√© que S3/DynamoDB, qui ne sont pas des coffres-forts √† secrets et demandent plus de bricolage et de risques d‚Äôexposition.\nL‚Äôoption A ne convient pas : pour RDS SQL Server, on ne remplace pas simplement un mot de passe par un r√¥le IAM pour se connecter.\nDonc la solution la plus s√ªre et adapt√©e est de stocker dans Secrets Manager et d‚Äôutiliser Lambda pour la rotation (B).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:218:4f228f0491e6fa70",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 218,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company requires that all applications running on Amazon EC2 use IAM roles to gain access to AWS services. A developer is modifying an application that currently relies on IAM user access keys stored in environment variables to access Amazon DynamoDB tables using boto, the AWS SDK for Python.The developer associated a role with the same permissions as the IAM user to the EC2 instance, then deleted the IAM user. When the application was restarted, the AWS AccessDeniedException messages started appearing in the application logs. The developer was able to use their personal account on the server to run DynamoDB API commands using the AWS CLI.What is the MOST likely cause of the exception?",
      "choices": {
        "A": "IAM policies might take a few minutes to propagate to resources.",
        "B": "Disabled environment variable credentials are still being used by the application.",
        "C": "The AWS SDK does not support credentials obtained using an instance role.",
        "D": "The instance‚Äôs security group does not allow access to http://169.254.169.254."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124770-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 8:34 a.m.",
      "textHash": "4f228f0491e6fa70",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Sur EC2, la bonne pratique est d‚Äôutiliser un ¬´ IAM role ¬ª (r√¥le) : l‚Äôinstance r√©cup√®re automatiquement des identifiants temporaires via le service de m√©tadonn√©es, sans stocker de cl√©s.\nIci, l‚Äôapplication utilisait avant des cl√©s d‚Äôun utilisateur IAM stock√©es dans des variables d‚Äôenvironnement (AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY).\nM√™me si vous avez attach√© un r√¥le √† l‚Äôinstance et supprim√© l‚Äôutilisateur IAM, l‚Äôapplication peut continuer √† lire ces variables d‚Äôenvironnement en priorit√©.\nLe SDK AWS (boto/boto3) suit un ordre de recherche : variables d‚Äôenvironnement d‚Äôabord, puis r√¥le d‚Äôinstance.\nDonc l‚Äôapplication tente encore d‚Äôutiliser les anciennes cl√©s (d√©sormais invalides), ce qui provoque AccessDeniedException.\nLe fait que le d√©veloppeur puisse appeler DynamoDB avec l‚ÄôAWS CLI sur le serveur indique que le r√¥le d‚Äôinstance fonctionne, mais pas forc√©ment que l‚Äôapplication l‚Äôutilise.\nSolution : supprimer/vider les variables d‚Äôenvironnement de cl√©s, retirer toute configuration statique, et laisser le SDK utiliser le r√¥le d‚Äôinstance.\nC‚Äôest pourquoi la cause la plus probable est que des identifiants via variables d‚Äôenvironnement sont toujours utilis√©s (B).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:414:5dba22a30de42e1b",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 414,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has AWS Lambda functions that need to access a company's internal data science libraries and reference data. Separate teams manage the libraries and the data. The teams must be able to update and upload new data independently. The Lambda functions are connected to the company's central VPC.Which solution will provide the Lambda functions with access to the libraries and data?",
      "choices": {
        "A": "Attach an Amazon Elastic Block Store (Amazon EBS) volume to the Lambda functions by using EBS Multi-Attach in the central VPC. Update the Lambda function execution roles to give the functions to access the EBS volume. Update the Lambda function code to reference the files in the EBS volume.",
        "B": "Compress the libraries and reference data in a Lambda /tmp folder. Update the Lambda function code to reference the files in the /tmp folder.",
        "C": "Set up an Amazon Elastic File System (Amazon EFS) file system with mount targets in the central VPConfigure the Lambda functions to mount the EFS file system. Update the Lambda function execution roles to give the functions to access the EFS file system.",
        "D": "Set up an Amazon FSx for Windows File Server file system with mount targets in the central VPC. Configure the Lambda functions to mount the Amazon FSx file system. Update the Lambda function execution roles to give the functions to access the Amazon FSx file system."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144610-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 28, 2024, 8:34 a.m.",
      "textHash": "5dba22a30de42e1b",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "AWS Lambda ex√©cute du code sans serveur, mais son disque local est tr√®s limit√© : le dossier /tmp est temporaire et propre √† chaque ex√©cution, donc pas adapt√© pour partager des biblioth√®ques et des donn√©es mises √† jour par plusieurs √©quipes.\nLes fonctions Lambda sont dans un VPC : elles peuvent donc acc√©der √† des ressources r√©seau priv√©es via des points de montage.\nAmazon EFS est un syst√®me de fichiers NFS partag√© (comme un dossier r√©seau) que plusieurs clients peuvent lire/√©crire en m√™me temps.\nEn cr√©ant EFS avec des ‚Äúmount targets‚Äù dans le VPC, Lambda peut monter ce dossier et lire les librairies + donn√©es au moment de l‚Äôex√©cution.\nChaque √©quipe peut d√©poser/mettre √† jour ses fichiers dans EFS ind√©pendamment, sans red√©ployer le code Lambda.\nL‚Äôoption EBS ne convient pas : EBS est un disque bloc attach√© √† des instances EC2, pas √† Lambda (Multi-Attach ne r√©sout pas ce point).\nFSx Windows est surtout pour environnements Windows/SMB et plus lourd ; Lambda supporte nativement EFS pour ce besoin.\nDonc la bonne solution est de monter un EFS dans les fonctions Lambda et d‚Äôautoriser l‚Äôacc√®s via le r√¥le d‚Äôex√©cution (IAM) et la configuration r√©seau.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:197:8bede26b6a665465",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 197,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company needs to set up secure database credentials for all its AWS Cloud resources. The company‚Äôs resources include Amazon RDS DB instances, Amazon DocumentDB clusters, and Amazon Aurora DB instances. The company‚Äôs security policy mandates that database credentials be encrypted at rest and rotated at a regular interval.Which solution will meet these requirements MOST securely?",
      "choices": {
        "A": "Set up IAM database authentication for token-based access. Generate user tokens to provide centralized access to RDS DB instances, Amazon DocumentDB clusters, and Aurora DB instances.",
        "B": "Create parameters for the database credentials in AWS Systems Manager Parameter Store. Set the Type parameter to SecureString. Set up automatic rotation on the parameters.",
        "C": "Store the database access credentials as an encrypted Amazon S3 object in an S3 bucket. Block all public access on the S3 bucket. Use S3 server-side encryption to set up automatic rotation on the encryption key.",
        "D": "Create an AWS Lambda function by using the SecretsManagerRotationTemplate template in the AWS Secrets Manager console. Create secrets for the database credentials in Secrets Manager. Set up secrets rotation on a schedule."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122619-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:56 a.m.",
      "textHash": "8bede26b6a665465",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Le besoin est de stocker des identifiants de base de donn√©es (utilisateur/mot de passe) de fa√ßon chiffr√©e et de les faire changer automatiquement (rotation).\nAWS Secrets Manager est un service con√ßu pr√©cis√©ment pour g√©rer des ‚Äúsecrets‚Äù (mots de passe, cl√©s), les chiffrer au repos avec KMS, contr√¥ler l‚Äôacc√®s via IAM, et automatiser la rotation.\nAvec l‚Äôoption D, on cr√©e le secret dans Secrets Manager puis on active une rotation planifi√©e ; le mod√®le de rotation cr√©e une fonction AWS Lambda qui met √† jour le mot de passe dans la base (RDS/Aurora/DocumentDB) et dans Secrets Manager.\nC‚Äôest ‚Äúle plus s√©curis√©‚Äù car la rotation est native, audit√©e, et int√©gr√©e aux bases support√©es, sans bricolage.\nA ne convient pas : l‚Äôauthentification IAM ne couvre pas DocumentDB et ne g√®re pas un mot de passe stock√©/rot√© pour tous les cas.\nB : Parameter Store peut chiffrer (SecureString) mais la rotation automatique n‚Äôest pas une capacit√© native compl√®te comme Secrets Manager.\nC : S3 peut chiffrer un fichier, mais ne g√®re pas la rotation du mot de passe (changer la cl√© de chiffrement ‚â† changer le mot de passe).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:180:a722d0d6aa6929f5",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 180,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that runs as a series of AWS Lambda functions. Each Lambda function receives data from an Amazon Simple Notification Service (Amazon SNS) topic and writes the data to an Amazon Aurora DB instance.To comply with an information security policy, the company must ensure that the Lambda functions all use a single securely encrypted database connection string to access Aurora.Which solution will meet these requirements?",
      "choices": {
        "A": "Use IAM database authentication for Aurora to enable secure database connections for all the Lambda functions.",
        "B": "Store the credentials and read the credentials from an encrypted Amazon RDS DB instance.",
        "C": "Store the credentials in AWS Systems Manager Parameter Store as a secure string parameter.",
        "D": "Use Lambda environment variables with a shared AWS Key Management Service (AWS KMS) key for encryption."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122601-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:37 a.m.",
      "textHash": "a722d0d6aa6929f5",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "On veut que toutes les fonctions AWS Lambda (du code qui s‚Äôex√©cute sans serveur) utilisent UNE seule cha√Æne de connexion vers Amazon Aurora (base de donn√©es) et qu‚Äôelle soit chiffr√©e.\nIl faut donc un endroit central, s√©curis√©, pour stocker un secret (identifiant/mot de passe/URL) et le lire depuis plusieurs Lambdas.\nAWS Systems Manager Parameter Store permet de stocker des param√®tres; en mode ‚ÄúSecureString‚Äù, la valeur est chiffr√©e avec AWS KMS (service de gestion de cl√©s).\nChaque Lambda peut lire le m√™me param√®tre au d√©marrage (ou en cache) via l‚ÄôAPI, avec des permissions IAM contr√¥l√©es (qui a le droit de lire).\nC‚Äôest ‚Äúun seul‚Äù secret partag√©, chiffr√© au repos, et g√©r√© sans le mettre en dur dans le code.\nA ne r√©pond pas au besoin de ‚Äúcha√Æne de connexion unique‚Äù stock√©e comme secret; c‚Äôest une autre m√©thode d‚Äôauthentification.\nB est incorrect: on ne stocke pas des secrets dans une autre base RDS.\nD chiffre des variables d‚Äôenvironnement, mais ce n‚Äôest pas un stockage central de secrets et la rotation/gestion partag√©e est moins adapt√©e que Parameter Store SecureString.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:391:e6a2eb88b0eee772",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 391,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a development team that uses AWS CodeCommit for version control. The development team has CodeCommit repositories in multiple AWS accounts. The team is expanding to include developers who work in various locations.The company must ensure that the developers have secure access to the repositories.Which solution will meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "A": "Configure IAM roles for each developer and grant access individually.",
        "B": "Configure permission sets in AWS IAM Identity Center to grant access to the accounts.",
        "C": "Share AWS access keys with the development team for direct repository access.",
        "D": "Use public SSH keys for authentication to the CodeCommit repositories."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143081-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 29, 2024, 10:09 p.m.",
      "textHash": "e6a2eb88b0eee772",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Ici, l‚Äô√©quipe utilise AWS CodeCommit (un service AWS pour h√©berger des d√©p√¥ts Git priv√©s) dans plusieurs comptes AWS, et de nouveaux d√©veloppeurs arrivent depuis diff√©rents lieux. Le besoin principal est un acc√®s s√©curis√© et simple √† g√©rer √† grande √©chelle.\nAWS IAM Identity Center (ex-SSO) permet de g√©rer les identit√©s des utilisateurs au m√™me endroit et de leur donner acc√®s √† plusieurs comptes AWS via des ‚Äúpermission sets‚Äù (ensembles de permissions r√©utilisables).\nAvec les permission sets, on attribue un niveau d‚Äôacc√®s une seule fois (ex: lecture/√©criture CodeCommit) puis on l‚Äôapplique √† un ou plusieurs comptes, sans recr√©er des utilisateurs/roles partout.\nC‚Äôest plus ‚Äúop√©rationnellement efficace‚Äù car l‚Äôonboarding/offboarding (ajouter/retirer un d√©veloppeur) se fait centralement, avec moins d‚Äôerreurs et une meilleure tra√ßabilit√©.\nA est moins efficace: g√©rer un r√¥le et des droits par d√©veloppeur et par compte devient vite lourd et r√©p√©titif.\nC est dangereux: partager des cl√©s d‚Äôacc√®s viole les bonnes pratiques (pas de tra√ßabilit√© individuelle, risque de fuite).\nD (cl√©s SSH) peut servir √† s‚Äôauthentifier √† Git, mais ne r√©sout pas la gestion centralis√©e multi-comptes et le contr√¥le fin des acc√®s comme IAM Identity Center.\nDonc la meilleure solution, s√©curis√©e et simple √† administrer, est B.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:371:caad30c753e6beb8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 371,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company hosts its application in the us-west-1 Region. The company wants to add redundancy in the us-east-1 Region.The application secrets are stored in AWS Secrets Manager in us-west-1. A developer needs to replicate the secrets to us-east-1.Which solution will meet this requirement?",
      "choices": {
        "A": "Configure secret replication for each secret. Add us-east-1 as a replication Region. Choose an AWS Key Management Service (AWS KMS) key in us-east-1 to encrypt the replicated secrets.",
        "B": "Create a new secret in us-east-1 for each secret. Configure secret replication in us-east-1. Set the source to be the corresponding secret in us-west-1. Choose an AWS Key Management Service (AWS KMS) key in us-west-1 to encrypt the replicated secrets.",
        "C": "Create a replication rule for each secret. Set us-east-1 as the destination Region. Configure the rule to run during secret rotation. Choose an AWS Key Management Service (AWS KMS) key in us-east-1 to encrypt the replicated secrets.",
        "D": "Create a Secrets Manager lifecycle rule to replicate each secret to a new Amazon S3 bucket in us-west-1. Configure an S3 replication rule to replicate the secrets to us-east-1."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143754-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:36 a.m.",
      "textHash": "caad30c753e6beb8",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "AWS Secrets Manager est un service qui stocke des informations sensibles (mots de passe, cl√©s API) et peut les copier automatiquement vers d‚Äôautres R√©gions AWS.\nIci, l‚Äôapplication est en us-west-1 mais on veut de la redondance en us-east-1 : il faut donc que les m√™mes secrets existent aussi en us-east-1.\nLa bonne approche est d‚Äôactiver la ‚Äúr√©plication‚Äù directement sur chaque secret dans Secrets Manager et d‚Äôajouter us-east-1 comme R√©gion de r√©plication.\nChaque R√©gion chiffre ses donn√©es avec une cl√© AWS KMS (service de gestion de cl√©s de chiffrement) locale : on choisit donc une cl√© KMS en us-east-1 pour chiffrer la copie.\nC‚Äôest exactement ce que d√©crit la r√©ponse A : configuration native, simple, et g√©r√©e par AWS.\nB est incorrect car on ne cr√©e pas d‚Äôabord un secret ‚Äúdestination‚Äù pour ensuite le faire r√©pliquer : la r√©plication se configure depuis le secret source.\nC invente une ‚Äúr√®gle de r√©plication pendant la rotation‚Äù : la r√©plication n‚Äôest pas un job planifi√©, elle est g√©r√©e automatiquement.\nD est inadapt√© : copier des secrets vers S3 est une mauvaise pratique (risque de s√©curit√©) et ce n‚Äôest pas le m√©canisme pr√©vu pour la r√©plication multi-R√©gion.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que ton appli est une √©cole en Californie (us-west-1) et que tu ouvres une √©cole ‚Äúde secours‚Äù √† New York (us-east-1). Les ‚Äúsecrets‚Äù sont comme les codes des casiers et les mots de passe du Wi‚ÄëFi, gard√©s dans un coffre du bureau (Secrets Manager).**\n\nConcept : pour √™tre pr√™t si l‚Äô√©cole de Californie a un souci, l‚Äô√©cole de New York doit avoir les m√™mes codes, copi√©s automatiquement.\nLa bonne m√©thode, c‚Äôest de dire au coffre : ‚Äúcopie ce secret aussi dans l‚Äôautre √©cole‚Äù.\nA fait exactement √ßa : tu actives la r√©plication pour chaque secret et tu ajoutes New York comme destination.\nEt tu choisis un cadenas local √† New York (cl√© KMS) pour fermer le coffre l√†-bas : m√™me si quelqu‚Äôun intercepte, il ne peut pas lire.\nB est bizarre : √ßa dit de cr√©er √† la main puis de ‚Äúr√©pliquer depuis l‚Äôautre c√¥t√©‚Äù, et le cadenas est du mauvais endroit.\nC invente une r√®gle ‚Äúpendant la rotation‚Äù : la r√©plication n‚Äôattend pas un changement de code.\nD met les secrets dans un autre stockage (S3) : ce n‚Äôest plus le coffre √† secrets, donc pas le bon outil.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:366:05f297e51e538499",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 366,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is designing a fault-tolerant environment where client sessions will be saved.How can the developer ensure that no sessions are lost if an Amazon EC2 instance fails?",
      "choices": {
        "A": "Use sticky sessions with an Elastic Load Balancer target group.",
        "B": "Use Amazon SQS to save session data.",
        "C": "Use Amazon DynamoDB to perform scalable session handling.",
        "D": "Use Elastic Load Balancer connection draining to stop sending requests to failing instances."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143749-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:27 a.m.",
      "textHash": "05f297e51e538499",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Une ‚Äúsession‚Äù client (ex: panier, utilisateur connect√©) ne doit pas √™tre stock√©e uniquement dans la m√©moire d‚Äôune instance EC2, car si l‚Äôinstance tombe, tout dispara√Æt.\nPour √©viter la perte, il faut mettre l‚Äô√©tat de session dans un stockage externe, partag√© et durable.\nAmazon DynamoDB est une base de donn√©es NoSQL g√©r√©e par AWS : elle r√©plique les donn√©es automatiquement et reste disponible m√™me si un serveur applicatif (EC2) √©choue.\nChaque requ√™te peut lire/√©crire la session dans DynamoDB via une cl√© (ex: sessionId), donc n‚Äôimporte quelle instance peut reprendre la session.\nLes ‚Äústicky sessions‚Äù (A) gardent un utilisateur sur la m√™me instance, mais si cette instance tombe, la session en m√©moire est perdue.\nAmazon SQS (B) est une file de messages pour traiter des t√¢ches, pas un stockage de session consultable rapidement.\nLe connection draining (D) aide √† retirer proprement une instance, mais ne sauvegarde pas les sessions.\nDonc la bonne r√©ponse est d‚Äôutiliser DynamoDB pour g√©rer les sessions de fa√ßon scalable et tol√©rante aux pannes.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o en √©quipe au lyc√©e : chaque joueur a une ‚Äúsauvegarde‚Äù (inventaire, position, points) pendant la partie. Les PC de la salle peuvent tomber en panne.**\n\nConcept : une ‚Äúsession client‚Äù, c‚Äôest comme ta sauvegarde pendant la partie. Si elle est stock√©e seulement sur le PC o√π tu joues et que ce PC plante, tu perds tout.\nPour √©viter √ßa, il faut mettre la sauvegarde dans un endroit central, solide, accessible par tous les PC.\nAmazon DynamoDB, c‚Äôest comme un grand casier central ultra-rapide o√π on note les sauvegardes de tous les joueurs.\nSi un PC (une instance EC2) tombe, tu te reconnectes sur un autre PC et il relit ta sauvegarde dans le casier : rien n‚Äôest perdu.\nA (sticky sessions) te renvoie toujours vers le m√™me PC : s‚Äôil casse, ta sauvegarde casse avec.\nB (SQS) c‚Äôest une file d‚Äôattente de messages, pas un ‚Äúcasier‚Äù pour relire l‚Äô√©tat complet d‚Äôune session.\nD (connection draining) aide √† finir des actions en cours, mais ne stocke pas ta sauvegarde.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:354:00d43e8ea28488fe",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 354,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is running an application on an Amazon EC2 instance. When the application tries to read an Amazon S3 bucket, the application fails. The developer notices that the associated IAM role is missing the S3 read permission. The developer needs to give the application the ability to read the S3 bucket.Which solution will meet this requirement with the LEAST application disruption?",
      "choices": {
        "A": "Add the permission to the role. Terminate the existing EC2 instance. Launch a new EC2 instance.",
        "B": "Add the permission to the role so that the change will take effect automatically.",
        "C": "Add the permission to the role. Hibernate and restart the existing EC2 instance.",
        "D": "Add the permission to the S3 bucket. Restart the EC2 instance."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/140015-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "May 5, 2024, 8:05 a.m.",
      "textHash": "00d43e8ea28488fe",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Sur EC2, une application peut acc√©der √† d‚Äôautres services AWS (comme S3) via un r√¥le IAM attach√© √† l‚Äôinstance.\nIAM (Identity and Access Management) g√®re les permissions : si le r√¥le n‚Äôa pas ‚Äúlecture S3‚Äù, l‚Äôacc√®s au bucket √©choue.\nLa solution la moins disruptive est simplement d‚Äôajouter la permission S3 Read √† la politique du r√¥le IAM existant.\nLes identifiants temporaires fournis √† l‚Äôinstance via le r√¥le sont mis √† jour automatiquement : l‚Äôapplication r√©cup√©rera les nouveaux droits sans recr√©er l‚Äôinstance.\nDonc pas besoin de terminer/relancer l‚ÄôEC2 (A) ni d‚Äôhiberner/red√©marrer (C) : ce serait une interruption inutile.\nModifier la politique du bucket S3 (D) peut marcher, mais ce n‚Äôest pas n√©cessaire ici et red√©marrer l‚Äôinstance ajoute une disruption.\nConclusion : on met √† jour le r√¥le IAM, et l‚Äôeffet se propage automatiquement (B).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un √©l√®ve qui doit aller lire un livre √† la biblioth√®que de l‚Äô√©cole. Il a une ‚Äúcarte d‚Äôacc√®s‚Äù (le r√¥le IAM) qui dit ce qu‚Äôil a le droit de faire. La biblioth√®que, c‚Äôest le seau de fichiers (S3).**\n\nConcept : si ta carte n‚Äôa pas ‚Äúdroit de lire‚Äù, le biblioth√©caire refuse, m√™me si tu es d√©j√† dans l‚Äô√©cole. Ici, l‚Äôappli sur l‚Äôordi (EC2) essaie de lire S3, mais sa carte (r√¥le IAM) n‚Äôa pas l‚Äôautorisation. Solution B : on ajoute juste le droit ‚Äúlecture‚Äù sur la carte, et l‚Äô√©l√®ve peut imm√©diatement emprunter le livre, sans changer d‚Äôordi ni red√©marrer. Les autres choix font plus de bazar : A = jeter l‚Äôordi et en reprendre un, C = le mettre en pause/hiberner, D = toucher aux r√®gles de la biblioth√®que et red√©marrer. Donc B = le moins de perturbation : on corrige l‚Äôautorisation du r√¥le et √ßa s‚Äôapplique automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:413:8c832546759c5f45",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 413,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer built an application by using multiple AWS Lambda functions. The Lambda functions must access dynamic configuration data at runtime. The data is maintained as a 6 KB JSON document in AWS AppConfig. The configuration data needs to be updated without requiring the redeployment of the application.The developer needs a solution that will give the Lambda functions access to the dynamic configuration data.What should the developer do to meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Migrate the document from AWS AppConfig to a Lambda environment variable. Read the document at the runtime.",
        "B": "Configure the AWS AppConfig Agent Lambda extension. Access the dynamic configuration data by calling the extension on a local host.",
        "C": "Use the AWS X-Ray SDK to call the AWS AppConfig APIs. Retrieve the configuration file at runtime.",
        "D": "Migrate the configuration file to a Lambda deployment package. Read the file from the file system at runtime."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144609-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 28, 2024, 8:31 a.m.",
      "textHash": "8c832546759c5f45",
      "rawFormat": "discussion-md",
      "conceptKey": "xray_tracing",
      "frExplanation": "On veut que plusieurs fonctions AWS Lambda (du code qui s‚Äôex√©cute √† la demande) lisent une configuration qui peut changer pendant l‚Äôex√©cution.\nAWS AppConfig est un service fait pour stocker et d√©ployer des param√®tres de configuration (ici un JSON de 6 KB) sans red√©ployer l‚Äôapplication.\nLa meilleure option avec le moins d‚Äôeffort est d‚Äôutiliser l‚Äôextension ¬´ AWS AppConfig Agent Lambda extension ¬ª.\nUne extension Lambda est un composant pr√™t √† l‚Äôemploi qui tourne √† c√¥t√© de votre fonction et peut r√©cup√©rer/cacher la config.\nVotre code appelle simplement un endpoint local (localhost) fourni par l‚Äôextension pour obtenir la config √† jour, sans g√©rer l‚Äôauthentification ni les appels r√©seau complexes.\nA est faux: les variables d‚Äôenvironnement n√©cessitent une mise √† jour de la configuration Lambda (et souvent un red√©ploiement) et ne sont pas id√©ales pour des changements fr√©quents.\nC est trop de travail: X-Ray sert au tra√ßage/monitoring, pas √† r√©cup√©rer de la config; appeler les APIs AppConfig directement demande plus de code et gestion d‚Äôerreurs.\nD est faux: mettre le fichier dans le package de d√©ploiement impose de red√©ployer √† chaque changement.\nDonc B r√©pond au besoin ‚Äúconfig dynamique √† runtime‚Äù avec le minimum de d√©veloppement.",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:406:3c96aa17d661a6c1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 406,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company had an Amazon RDS for MySQL DB instance that was named mysql-db. The DB instance was deleted within the past 90 days.A developer needs to find which IAM user or role deleted the DB instance in the AWS environment.Which solution will provide this information?",
      "choices": {
        "A": "Retrieve the AWS CloudTrail events for the resource mysql-db where the event name is DeleteDBInstance. Inspect each event.",
        "B": "Retrieve the Amazon CloudWatch log events from the most recent log stream within the rds/mysql-db log group. Inspect the log events.",
        "C": "Retrieve the AWS X-Ray trace summaries. Filter by services with the name mysql-db. Inspect the ErrorRootCauses values within each summary.",
        "D": "Retrieve the AWS Systems Manager deletions inventory. Filter the inventory by deletions that have a TypeName value of RDS. Inspect the deletion details."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144456-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 23, 2024, 8:07 p.m.",
      "textHash": "3c96aa17d661a6c1",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Pour savoir ¬´ qui a supprim√© ¬ª une ressource AWS, il faut consulter le journal d‚Äôaudit des actions : AWS CloudTrail.\nCloudTrail enregistre les appels d‚ÄôAPI (qui, quoi, quand, depuis quelle IP) faits par un utilisateur IAM ou un r√¥le.\nLa suppression d‚Äôune base Amazon RDS se fait via l‚Äôaction API \"DeleteDBInstance\".\nDonc, en recherchant dans CloudTrail les √©v√©nements li√©s √† la ressource mysql-db avec le nom d‚Äô√©v√©nement DeleteDBInstance, on voit l‚Äôidentit√© (userIdentity) de l‚ÄôIAM user/role qui a lanc√© la suppression.\nCloudWatch Logs du groupe rds/mysql-db contient surtout des logs de la base (erreurs SQL, d√©marrage, etc.), pas l‚Äôidentit√© IAM qui supprime l‚Äôinstance.\nAWS X-Ray sert √† tracer des requ√™tes d‚Äôapplications, pas √† auditer des actions d‚Äôadministration AWS.\nSystems Manager Inventory ne tient pas un registre des suppressions RDS avec l‚Äôidentit√© IAM.\nDonc la seule source fiable pour retrouver l‚Äôutilisateur/role ayant supprim√© l‚Äôinstance est CloudTrail (choix A).",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine ton coll√®ge : la base de donn√©es mysql-db, c‚Äôest comme un casier important avec des dossiers. Un jour, le casier dispara√Æt. Tu veux savoir quel √©l√®ve ou quel surveillant l‚Äôa fait enlever.**\n\nConcept : pour savoir ‚Äúqui a fait quoi‚Äù, il faut un journal officiel des actions, comme le cahier o√π l‚Äôadministration note chaque d√©cision (date, action, personne). Sur AWS, ce journal s‚Äôappelle CloudTrail : il enregistre qui a cliqu√© quoi, avec quel compte (utilisateur ou r√¥le). Donc on cherche dans CloudTrail l‚Äôaction ‚ÄúDeleteDBInstance‚Äù sur la ressource ‚Äúmysql-db‚Äù : chaque ligne te dit exactement qui a supprim√© le casier. B, c‚Äôest plut√¥t le contenu/les messages du casier (logs), pas qui l‚Äôa supprim√©. C, c‚Äôest un outil pour suivre des bugs et des lenteurs d‚Äôapplis, pas les suppressions. D n‚Äôest pas le journal officiel des actions de suppression. Donc A est la bonne r√©ponse.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:205:65dccfd7daac4787",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 205,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on an application that processes operating data from IoT devices. Each IoT device uploads a data file once every hour to an Amazon S3 bucket. The developer wants to immediately process each data file when the data file is uploaded to Amazon S3.The developer will use an AWS Lambda function to process the data files from Amazon S3. The Lambda function is configured with the S3 bucket information where the files are uploaded. The developer wants to configure the Lambda function to immediately invoke after each data file is uploaded.Which solution will meet these requirements?",
      "choices": {
        "A": "Add an asynchronous invocation to the Lambda function. Select the S3 bucket as the source.",
        "B": "Add an Amazon EventBridge event to the Lambda function. Select the S3 bucket as the source.",
        "C": "Add a trigger to the Lambda function. Select the S3 bucket as the source.",
        "D": "Add a layer to the Lambda function. Select the S3 bucket as the source."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122627-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 3:09 a.m.",
      "textHash": "65dccfd7daac4787",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Ici, on veut traiter un fichier d√®s qu‚Äôil arrive dans Amazon S3 (un service de stockage de fichiers). AWS Lambda est un service qui ex√©cute du code automatiquement sans g√©rer de serveur. Pour lancer Lambda ‚Äútout de suite‚Äù apr√®s un upload, il faut un √©v√©nement S3 (ObjectCreated) qui d√©clenche la fonction. La mani√®re standard est d‚Äôajouter un ‚Äútrigger‚Äù (d√©clencheur) S3 sur la fonction Lambda, en choisissant le bucket et le type d‚Äô√©v√©nement (cr√©ation d‚Äôobjet). Ainsi, chaque nouveau fichier d√©pos√© dans le bucket appelle automatiquement Lambda. L‚Äôoption A parle d‚Äôinvocation asynchrone mais ne configure pas la source d‚Äô√©v√©nement S3 correctement. EventBridge (B) peut router des √©v√©nements, mais pour ce besoin simple S3‚ÜíLambda, le trigger S3 est la solution directe. Une layer (D) sert √† partager des biblioth√®ques/d√©pendances, pas √† d√©clencher une ex√©cution.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une bo√Æte aux lettres au lyc√©e o√π chaque √©l√®ve (un objet connect√©) d√©pose un devoir (un fichier) toutes les heures. Tu as un surveillant (Lambda) qui doit corriger d√®s qu‚Äôun devoir arrive.**\n\nConcept : il faut un ‚Äúsignal‚Äù automatique qui pr√©vient le surveillant pile au moment o√π un devoir est d√©pos√©.\nDans AWS, le dossier o√π on d√©pose les fichiers s‚Äôappelle S3 (comme la bo√Æte aux lettres).\nLambda, c‚Äôest le surveillant qui ex√©cute une action sans attendre.\nLa bonne solution est C : ajouter un ‚Äútrigger‚Äù (d√©clencheur), c‚Äôest comme installer une sonnette sur la bo√Æte.\nD√®s qu‚Äôun nouveau devoir est d√©pos√©, la sonnette sonne et le surveillant se met au travail.\nA parle juste de ‚Äúfa√ßon d‚Äôappeler‚Äù Lambda, mais sans installer la sonnette sur la bo√Æte.\nB, c‚Äôest comme passer par le bureau de la vie scolaire pour relayer l‚Äôinfo : possible, mais pas le plus direct ici.\nD (layer), c‚Äôest comme donner un manuel au surveillant, √ßa n‚Äôaide pas √† savoir quand un devoir arrive.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:401:8d7d2c30001148fd",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 401,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a monolithic desktop-based application that processes images. A developer is converting the application into an AWS Lambda function by using Python. Currently, the desktop application runs every 5 minutes to process the latest image from an Amazon S3 bucket. The desktop application completes the image processing task within 1 minute.During testing on AWS, the developer notices that the Lambda function runs at the specified 5-minute interval. However, the Lambda function takes more than 2 minutes to complete the image processing task. The developer needs a solution that will improve the Lambda function's performance.Which solution will meet this requirement?",
      "choices": {
        "A": "Update the instance type of the Lambda function to a compute optimized instance with at least eight virtual CPU (vCPU).",
        "B": "Update the configuration of the Lambda function to use the latest Python runtime.",
        "C": "Increase the memory that is allocated to the Lambda function.",
        "D": "Configure a reserved concurrency on the Lambda function."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143029-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 28, 2024, 9:19 p.m.",
      "textHash": "8d7d2c30001148fd",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:a74a7976",
      "frExplanation": "AWS Lambda ex√©cute du code sans serveur : vous choisissez surtout la m√©moire, et AWS fournit automatiquement du CPU proportionnel √† cette m√©moire.\nIci, la fonction Lambda est plus lente que l‚Äôapplication desktop (2+ minutes au lieu de 1), donc il faut plus de puissance de calcul pendant l‚Äôex√©cution.\nDans Lambda, augmenter la m√©moire augmente aussi la quantit√© de CPU et la bande passante r√©seau/disque disponibles, ce qui acc√©l√®re souvent les traitements d‚Äôimages.\nC‚Äôest donc l‚Äôaction la plus directe pour am√©liorer les performances : augmenter la m√©moire allou√©e.\nA est faux : on ne choisit pas un ‚Äútype d‚Äôinstance‚Äù (vCPU/instance) pour Lambda comme pour EC2.\nB peut aider marginalement, mais ne garantit pas un gain important ; le principal levier de performance est m√©moire/CPU.\nD (reserved concurrency) sert √† limiter/garantir le nombre d‚Äôex√©cutions en parall√®le, pas √† rendre une ex√©cution individuelle plus rapide.\nDonc la meilleure solution est C.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un √©l√®ve qui doit corriger une pile de photos/feuilles toutes les 5 minutes au CDI. Il a une petite table et peu de place pour √©taler ses affaires.**\n\nConcept : AWS Lambda, c‚Äôest comme un √©l√®ve ‚Äú√† la demande‚Äù qui fait une t√¢che quand on lui demande, avec un espace de travail et des moyens limit√©s.\nIci, la t√¢che (traiter une image) est plus lente sur Lambda que sur le PC : comme si l‚Äô√©l√®ve avait une table trop petite, il perd du temps.\nSur Lambda, ‚Äúplus de m√©moire‚Äù = souvent aussi ‚Äúplus de puissance‚Äù pour travailler (un peu comme lui donner une plus grande table + plus d‚Äôoutils).\nDonc C est bon : augmenter la m√©moire acc√©l√®re souvent le traitement.\nA est faux : on ne choisit pas un ‚Äútype d‚Äôordinateur‚Äù pr√©cis pour Lambda comme √ßa.\nB aide parfois un peu, mais ce n‚Äôest pas le levier principal pour gagner beaucoup de vitesse.\nD sert √† g√©rer combien d‚Äô√©l√®ves travaillent en m√™me temps, pas √† rendre un √©l√®ve plus rapide.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:314:d0b214d0327b83cc",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 314,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer works for a company that only has a single pre-production AWS account with an AWS CloudFormation AWS Serverless Application Model (AWS SAM) stack. The developer made changes to an existing AWS Lambda function specified in the AWS SAM template and additional Amazon Simple Notification service (Amazon SNS) topics.The developer wants to do a one-time deploy of the changes to test if the changes are working. The developer does not want to impact the existing pre-production application that is currently being used by other team members as part of the release pipeline.Which solution will meet these requirements?",
      "choices": {
        "A": "Use the AWS SAM CLI to package and deploy the SAM application to the pre-production AWS account. Specify the debug parameter.",
        "B": "Use the AWS SAM CLI to package and create a change set against the pre-production AWS account. Execute the change set in a new AWS account designated for a development environment.",
        "C": "Use the AWS SAM CLI to package and deploy the SAM application to a new AWS account designated for a development environment.",
        "D": "Update the CloudFormation stack in the pre-production account. Add a separate stage that points to a new AWS account designated for a development environment."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133612-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 12, 2024, 2:25 p.m.",
      "textHash": "d0b214d0327b83cc",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Le besoin : tester une modification (Lambda + nouveaux topics SNS) une seule fois, sans casser l‚Äôapplication de pr√©-production d√©j√† utilis√©e par l‚Äô√©quipe.\nAWS SAM est un outil qui d√©crit et d√©ploie une appli ‚Äúserverless‚Äù via CloudFormation (un service qui cr√©e/modifie des ressources AWS √† partir d‚Äôun template).\nSi vous d√©ployez dans le m√™me compte et la m√™me stack de pr√©-prod, vous risquez de remplacer la fonction Lambda ou d‚Äôajouter des ressources qui perturbent le pipeline.\nLa solution la plus s√ªre est d‚Äôisoler : utiliser un autre compte AWS (environnement dev) pour que vos changements n‚Äôaffectent pas la pr√©-prod.\nAvec le SAM CLI, ‚Äúpackage‚Äù pr√©pare les artefacts (ex: code Lambda dans S3) et ‚Äúdeploy‚Äù cr√©e une nouvelle stack dans le compte cible.\nDonc C est correct : d√©ployer l‚Äôapplication SAM dans un nouveau compte d√©di√© au d√©veloppement.\nA est faux : un param√®tre debug n‚Äôemp√™che pas les modifications de la stack existante.\nB est incoh√©rent : un change set se cr√©e/ex√©cute dans le m√™me compte/stack, pas ‚Äúcr√©√© ici, ex√©cut√© ailleurs‚Äù.\nD est inutile/risqu√© : modifier la stack de pr√©-prod pour ajouter des stages impacte justement l‚Äôenvironnement partag√©.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que la pr√©-production, c‚Äôest la cuisine d‚Äôessai de la cantine du lyc√©e : tout le monde de l‚Äô√©quipe y go√ªte les plats avant le ‚Äúvrai service‚Äù.**\n\nConcept : si tu changes une recette dans cette cuisine, tu risques de g√¢cher les tests des autres. Donc tu fais tes essais dans une autre cuisine, s√©par√©e.\nIci, la ‚Äúrecette‚Äù = le mod√®le AWS SAM (plan), le ‚Äúplat‚Äù = l‚Äôappli, et la ‚Äúmachine qui fait une t√¢che‚Äù = la fonction Lambda.\nTu as modifi√© la fonction Lambda et ajout√© des ‚Äúpanneaux d‚Äôannonce‚Äù (SNS topics) : √ßa change l‚Äôappli.\nTu veux tester une seule fois sans toucher la cuisine d‚Äôessai utilis√©e par les autres.\nLa bonne solution est C : d√©ployer dans un nouveau compte AWS, comme ouvrir une cuisine d‚Äôessai perso juste pour toi.\nA d√©ploie dans la m√™me cuisine (pr√©-production) ‚Üí √ßa d√©range les autres.\nB m√©lange ‚Äúpr√©parer un plan de changement‚Äù dans une cuisine puis ex√©cuter ailleurs ‚Üí incoh√©rent.\nD demande de modifier l‚Äôorganisation du service (pipeline) ‚Üí trop lourd pour un test unique.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:284:fbc7a32d674a6b66",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 284,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer designed an application on an Amazon EC2 instance. The application makes API requests to objects in an Amazon S3 bucket.Which combination of steps will ensure that the application makes the API requests in the MOST secure manner? (Choose two.)",
      "choices": {
        "A": "Create an IAM user that has permissions to the S3 bucket. Add the user to an IAM group.",
        "B": "Create an IAM role that has permissions to the S3 bucket.",
        "C": "Add the IAM role to an instance profile. Attach the instance profile to the EC2 instance.",
        "D": "Create an IAM role that has permissions to the S3 bucket. Assign the role to an IAM group.",
        "E": "Store the credentials of the IAM user in the environment variables on the EC2 instance."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134283-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 5:58 a.m.",
      "textHash": "fbc7a32d674a6b66",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Pour acc√©der √† un bucket S3, l‚Äôapplication doit avoir des permissions IAM (gestion des acc√®s AWS).\nLa m√©thode la plus s√ªre sur une instance EC2 est d‚Äôutiliser un r√¥le IAM, pas un utilisateur avec des cl√©s.\nUn r√¥le IAM fournit des identifiants temporaires automatiquement, renouvel√©s par AWS, sans stocker de secrets sur le serveur.\nDonc on cr√©e un r√¥le IAM avec les permissions n√©cessaires sur le bucket S3 (lecture/√©criture selon le besoin) : c‚Äôest le choix B.\nEnsuite, pour que l‚Äôinstance EC2 puisse ‚Äúporter‚Äù ce r√¥le, on l‚Äôattache via un instance profile (profil d‚Äôinstance) : c‚Äôest le choix C.\nLes options avec un IAM user (A, E) impliquent des cl√©s d‚Äôacc√®s √† stocker (variables d‚Äôenvironnement, fichiers), ce qui augmente le risque de fuite.\nL‚Äôoption D est incorrecte : on n‚Äôassigne pas un r√¥le √† un groupe IAM (les groupes sont pour les utilisateurs).\nAvec B + C, l‚Äôapplication appelle S3 de fa√ßon s√©curis√©e et sans gestion manuelle de mots de passe/clefs.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine l‚Äô√©cole : la salle des profs (S3) a des documents, et un √©l√®ve (ton appli sur EC2) doit aller les consulter.**\n\nConcept : pour entrer, tu peux soit donner une cl√© copi√©e (mot de passe), soit donner un badge officiel qui ouvre seulement cette salle.\nLe badge, c‚Äôest un ‚Äúr√¥le‚Äù : une autorisation attach√©e √† la machine, sans cl√© √©crite nulle part.\nB est bon : cr√©er un r√¥le avec le droit d‚Äôacc√©der au ‚Äúplacard S3‚Äù, c‚Äôest comme cr√©er un badge ‚Äúacc√®s salle des profs‚Äù.\nC‚Äôest plus s√ªr que A/E : A+E revient √† √©crire une cl√© dans le sac de l‚Äô√©l√®ve (variables d‚Äôenvironnement) : si quelqu‚Äôun la trouve, il peut entrer.\nAvec un r√¥le, pas besoin de stocker de secrets : l‚Äôacc√®s est donn√© automatiquement et peut √™tre limit√© juste au bon placard.\nDonc la mani√®re la plus s√©curis√©e, c‚Äôest d‚Äôutiliser un r√¥le (B) plut√¥t qu‚Äôun utilisateur + mot de passe stock√©.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:344:8301a25995a056c0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 344,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a new application for a pet store. The application will manage customer rewards points. The developer will use Amazon DynamoDB to store the data for the application. The developer needs to optimize query performance and limit partition overload before actual performance analysis.Which option should the developer use for a partition key to meet these requirements?",
      "choices": {
        "A": "A randomly generated universally unique identifier (UUID)",
        "B": "The customer's full name",
        "C": "The date when the customer signed up for the rewards program",
        "D": "The name of the customer's pet"
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136968-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 4:26 a.m.",
      "textHash": "8301a25995a056c0",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base de donn√©es NoSQL o√π les donn√©es sont r√©parties en ¬´ partitions ¬ª selon la cl√© de partition (partition key).\nSi trop d‚Äô√©l√©ments arrivent sur la m√™me valeur de cl√©, une partition peut √™tre surcharg√©e (hot partition) et les requ√™tes deviennent lentes.\nPour de bonnes performances, il faut une cl√© avec beaucoup de valeurs possibles et une r√©partition uniforme des √©critures/lectures.\nUn UUID (identifiant unique al√©atoire) cr√©e des valeurs tr√®s vari√©es, donc les donn√©es se r√©partissent naturellement sur de nombreuses partitions.\nLe nom complet, la date d‚Äôinscription ou le nom de l‚Äôanimal risquent de se r√©p√©ter (beaucoup de clients le m√™me jour, noms courants), ce qui concentre le trafic.\nDonc, choisir un UUID comme cl√© de partition aide √† √©viter la surcharge et am√©liore les performances de requ√™te d√®s le d√©part.\nR√©ponse correcte : A.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine avec plusieurs caisses. Chaque √©l√®ve doit choisir une caisse selon une r√®gle (ex: premi√®re lettre du nom). Si trop d‚Äô√©l√®ves vont √† la m√™me caisse, √ßa bouchonne.**\n\nDans DynamoDB, la ‚Äúpartition key‚Äù est la r√®gle qui d√©cide dans quelle ‚Äúcaisse‚Äù (zone de stockage) va chaque client. Pour que les recherches soient rapides, il faut r√©partir les clients le plus uniform√©ment possible, sinon une caisse est surcharg√©e (partition overload). Un UUID, c‚Äôest comme donner √† chaque √©l√®ve un num√©ro totalement al√©atoire: les √©l√®ves se r√©partissent naturellement sur toutes les caisses. Le nom complet, la date d‚Äôinscription ou le nom de l‚Äôanimal cr√©ent des ‚Äúpaquets‚Äù (beaucoup de gens s‚Äôinscrivent le m√™me jour, beaucoup ont des pr√©noms similaires, beaucoup de chiens s‚Äôappellent pareil). Donc √ßa fait des bouchons. Avec un UUID (A), la charge est bien r√©partie d√®s le d√©but, m√™me avant d‚Äôanalyser les performances.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:312:b64b19886ec53bd0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 312,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company hosts its application on AWS. The application runs on an Amazon Elastic Container Service (Amazon ECS) cluster that uses AWS Fargate. The cluster runs behind an Application Load Balancer. The application stores data in an Amazon Aurora database. A developer encrypts and manages database credentials inside the application.The company wants to use a more secure credential storage method and implement periodic credential rotation.Which solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Migrate the secret credentials to Amazon RDS parameter groups. Encrypt the parameter by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant AWS KMS permissions to access Amazon RDS.",
        "B": "Migrate the credentials to AWS Systems Manager Parameter Store. Encrypt the parameter by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant Amazon ECS Fargate permissions to access to AWS Secrets Manager.",
        "C": "Migrate the credentials to ECS Fargate environment variables. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant Amazon ECS Fargate permissions to access to AWS Secrets Manager.",
        "D": "Migrate the credentials to AWS Secrets Manager. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Turn on secret rotation. Use IAM policies and roles to grant Amazon ECS Fargate permissions to access to AWS Secrets Manager by using keys."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133610-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 12, 2024, 2:11 p.m.",
      "textHash": "b64b19886ec53bd0",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Ici, le besoin est de stocker des identifiants de base de donn√©es de fa√ßon plus s√ªre et de les faire changer automatiquement (rotation), avec le moins d‚Äôadministration possible.\nAWS Secrets Manager est un service con√ßu exactement pour √ßa : il stocke des ‚Äúsecrets‚Äù (mot de passe, token), les chiffre (souvent avec une cl√© AWS KMS) et peut faire une rotation p√©riodique automatiquement.\nAurora/RDS s‚Äôint√®gre nativement avec Secrets Manager pour mettre √† jour le mot de passe dans la base lors de la rotation.\nL‚Äôapplication sur ECS Fargate n‚Äôa plus √† ‚Äúg√©rer‚Äù le mot de passe : elle le lit √† la demande depuis Secrets Manager.\nAvec IAM (r√¥les/politiques), on autorise uniquement la t√¢che Fargate √† lire ce secret, ce qui limite l‚Äôacc√®s.\nLes autres choix ne conviennent pas : les RDS parameter groups ne servent pas √† stocker des mots de passe, les variables d‚Äôenvironnement sont moins s√ªres et compliquent la rotation, et Parameter Store n‚Äôest pas l‚Äôoption la plus simple pour la rotation automatique.\nDonc la solution la plus s√©curis√©e et avec le moins d‚Äôoverhead op√©rationnel est de migrer vers AWS Secrets Manager avec rotation activ√©e.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üé° Imagine un parc d'attractions** : bracelet qui d√©termine o√π tu peux aller.\n\n**Credentials** = **Ton bracelet d'entr√©e** üéüÔ∏è - Prouve qui tu es (cl√© d'acc√®s = identit√©).\n\n**IAM Role** = **Ton pass VIP/Standard/Enfant** üé´ - D√©termine O√ô tu peux aller (permissions).\n\n**üß† Mn√©motechnique :** \"**C**redentials = **C**arte d'identit√© (qui es-tu ?)\" | \"**IAM** = **J**e peux **A**ller **M** o√π ?\"\n\n**Pourquoi un r√¥le :** Pour ne PAS mettre de secrets dans le code. Le r√¥le donne des permissions temporaires, renouvel√©es automatiquement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:268:3fb284bef7782886",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 268,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) standard queue. The developer notices that the Lambda function processes some messages multiple times.How should developer resolve this issue MOST cost-effectively?",
      "choices": {
        "A": "Change the Amazon SQS standard queue to an Amazon SQS FIFO queue by using the Amazon SQS message deduplication ID.",
        "B": "Set up a dead-letter queue.",
        "C": "Set the maximum concurrency limit of the AWS Lambda function to 1.",
        "D": "Change the message processing to use Amazon Kinesis Data Streams instead of Amazon SQS."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134268-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 4:26 a.m.",
      "textHash": "3fb284bef7782886",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Amazon SQS est un service de file d‚Äôattente : une application y d√©pose des messages, et un consommateur (ici AWS Lambda) les lit et les traite.\nUne file SQS ¬´ standard ¬ª garantit surtout la disponibilit√© et la rapidit√©, mais elle peut livrer un m√™me message plus d‚Äôune fois (livraison ¬´ au moins une fois ¬ª). Donc voir des doublons est normal.\nPour √©viter de retraiter des doublons, il faut une file qui g√®re l‚Äôunicit√© : SQS FIFO (¬´ First-In-First-Out ¬ª) garantit l‚Äôordre et la livraison ¬´ exactement une fois ¬ª dans les conditions pr√©vues.\nAvec une FIFO, on utilise un Deduplication ID (ou la d√©duplication bas√©e sur le contenu) pour que SQS ignore les messages identiques envoy√©s plusieurs fois sur une fen√™tre de temps.\nUne dead-letter queue (DLQ) sert √† isoler les messages qui √©chouent, pas √† emp√™cher les doublons.\nLimiter la concurrence Lambda √† 1 r√©duit le parall√©lisme mais n‚Äôemp√™che pas SQS de red√©livrer un message.\nRemplacer par Kinesis change compl√®tement l‚Äôarchitecture et co√ªte/complexifie plus : la solution la plus simple et √©conomique ici est de passer en SQS FIFO avec d√©duplication.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : tu donnes des tickets-repas √† des √©l√®ves pour r√©cup√©rer leur plateau. Parfois, un √©l√®ve repasse et redonne le m√™me ticket, et la cantine lui redonne un plateau en double.**\n\nConcept : une ‚Äúfile de messages‚Äù (SQS) c‚Äôest comme une bo√Æte de tickets √† traiter, et Lambda c‚Äôest le surveillant qui prend un ticket et sert un plateau. Avec une file ‚Äústandard‚Äù, il peut arriver que le m√™me ticket soit donn√© deux fois (doublon), donc Lambda traite le m√™me message plusieurs fois. Solution A : passer en file ‚ÄúFIFO‚Äù (First In, First Out = dans l‚Äôordre) et utiliser un ‚ÄúID anti-doublon‚Äù : c‚Äôest comme √©crire un num√©ro unique sur chaque ticket, et la cantine refuse de servir deux fois le m√™me num√©ro. C‚Äôest le plus rentable car tu gardes le m√™me syst√®me de tickets, mais tu ajoutes juste une r√®gle simple pour bloquer les doublons. Les autres choix ne suppriment pas vraiment les doublons (ou co√ªtent/complexifient plus).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:397:ab65210d0985987e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 397,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has a continuous integration and continuous delivery (CI/CD) pipeline that uses AWS CodeArtifact and AWS CodeBuild. The build artifacts are between 0.5 GB and 1.5 GB in size. The builds happen frequently and retrieve many dependencies from CodeArtifact each time.The builds have been slow because of the time it takes to transfer dependencies. The developer needs to improve build performance by reducing the number of dependencies that are retrieved for each build.Which solution will meet this requirement?",
      "choices": {
        "A": "Specify an Amazon S3 cache in CodeBuild. Add the S3 cache folder path to the buildspec.yaml file for the build project.",
        "B": "Specify a local cache in CodeBuild. Add the CodeArtifact repository name to the buildspec.yaml file for the build project.",
        "C": "Specify a local cache in CodeBuild. Add the cache folder path to the buildspec.yaml file for the build project.",
        "D": "Retrieve the buildspec.yaml file directly from CodeArtifact. Add the CodeArtifact repository name to the buildspec.yaml file for the build project."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143073-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 29, 2024, 7:41 p.m.",
      "textHash": "ab65210d0985987e",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici, le probl√®me vient du fait que chaque build ret√©l√©charge beaucoup de d√©pendances (biblioth√®ques) depuis AWS CodeArtifact (un d√©p√¥t de paquets), ce qui prend du temps.\nAWS CodeBuild ex√©cute les builds dans un environnement temporaire : sans cache, tout est re-t√©l√©charg√© √† chaque ex√©cution.\nLa bonne approche est donc de mettre en cache le dossier o√π les d√©pendances sont stock√©es (ex: dossier Maven/Gradle/npm) pour le r√©utiliser entre builds.\nLe ‚Äúlocal cache‚Äù de CodeBuild conserve ces fichiers sur l‚Äôh√¥te de build et √©vite de re-t√©l√©charger les m√™mes d√©pendances √† chaque fois.\nDans le buildspec.yaml, on doit indiquer le chemin du dossier √† mettre en cache (cache folder path), pas le nom du d√©p√¥t CodeArtifact.\nAinsi, seules les nouvelles d√©pendances (ou versions) seront t√©l√©charg√©es, ce qui acc√©l√®re fortement les builds.\nA (cache S3) peut aider mais ajoute des transferts vers/depuis S3 ; ici on veut surtout r√©duire les t√©l√©chargements r√©p√©t√©s de d√©pendances.\nD ne r√©pond pas au besoin : d√©placer buildspec.yaml dans CodeArtifact n‚Äôacc√©l√®re pas le t√©l√©chargement des d√©pendances.\nDonc la solution correcte est d‚Äôactiver un cache local et de configurer le chemin du dossier de d√©pendances dans buildspec.yaml (C).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un devoir en groupe √† l‚Äô√©cole : √† chaque s√©ance, tu dois aller √† la biblioth√®que chercher les m√™mes livres et feuilles. √áa te fait perdre du temps. Tu d√©cides donc de garder une pile de copies dans ton casier pour les reprendre vite la prochaine fois.**\n\nIci, CodeBuild = la salle o√π tu fais le devoir (la compilation). CodeArtifact = la biblioth√®que o√π sont les ‚Äúlivres‚Äù (les d√©pendances). Les builds sont lents car tu refais trop d‚Äôallers-retours pour reprendre les m√™mes d√©pendances. La solution, c‚Äôest de mettre un ‚Äúcasier sur place‚Äù : un cache local dans CodeBuild. Et tu dois dire exactement quoi ranger dans ce casier : le chemin du dossier de cache dans buildspec.yaml. Comme √ßa, au build suivant, CodeBuild reprend les d√©pendances d√©j√† t√©l√©charg√©es au lieu de tout re-t√©l√©charger. C‚Äôest exactement le choix C : cache local + chemin du dossier de cache.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:145:7d7ab8f5ce3704be",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 145,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Lambda function that searches for items from an Amazon DynamoDB table that contains customer contact information. The DynamoDB table items have the customer‚Äôs email_address as the partition key and additional properties such as customer_type, name and job_title.The Lambda function runs whenever a user types a new character into the customer_type text input. The developer wants the search to return partial matches of all the email_address property of a particular customer_type. The developer does not want to recreate the DynamoDB table.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Add a global secondary index (GSI) to the DynamoDB table with customer_type as the partition key and email_address as the sort key. Perform a query operation on the GSI by using the begins_with key condition expression with the email_address property.",
        "B": "Add a global secondary index (GSI) to the DynamoDB table with email_address as the partition key and customer_type as the sort key. Perform a query operation on the GSI by using the begins_with key condition expression with the email_address property.",
        "C": "Add a local secondary index (LSI) to the DynamoDB table with customer_type as the partition key and email_address as the sort key. Perform a query operation on the LSI by using the begins_with key condition expression with the email_address property.",
        "D": "Add a local secondary index (LSI) to the DynamoDB table with job_title as the partition key and email_address as the sort key. Perform a query operation on the LSI by using the begins_with key condition expression with the email_address property."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122563-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:36 a.m.",
      "textHash": "7d7ab8f5ce3704be",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π une requ√™te efficace doit utiliser une cl√© (partition key) et √©ventuellement une cl√© de tri (sort key). Ici, la table est cl√©-partition = email_address, donc on ne peut pas facilement chercher ‚Äútous les emails d‚Äôun customer_type‚Äù ni faire des recherches par pr√©fixe sans index adapt√©.\nLe besoin est : filtrer d‚Äôabord par customer_type, puis retourner des emails qui commencent par les caract√®res tap√©s (recherche partielle). Pour cela, il faut pouvoir faire un Query avec customer_type = valeur et begins_with(email_address, \"abc\").\nUn GSI (Global Secondary Index) permet d‚Äôajouter un nouvel acc√®s avec une autre partition key, sans recr√©er la table.\nEn mettant customer_type comme partition key du GSI et email_address comme sort key, on peut faire Query sur le GSI : customer_type = X et begins_with sur email_address.\nUn LSI (Local Secondary Index) exige la m√™me partition key que la table (ici email_address), donc il ne peut pas avoir customer_type comme partition key : C et D sont impossibles.\nLe choix B garde email_address en partition key, ce qui n‚Äôaide pas √† regrouper par customer_type et ne r√©pond pas au besoin principal.\nDonc la bonne solution est d‚Äôajouter un GSI (customer_type, email_address) et de requ√™ter avec begins_with sur email_address.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une biblioth√®que au lyc√©e. Les livres sont rang√©s d‚Äôabord par ‚Äúcat√©gorie‚Äù (manga, roman, BD), puis √† l‚Äôint√©rieur par le d√©but du titre. Tu veux trouver vite tous les titres qui commencent par ‚Äúha‚Ä¶‚Äù dans la cat√©gorie ‚Äúmanga‚Äù, sans refaire tout le rangement de la biblioth√®que.**\n\nConcept : DynamoDB, c‚Äôest comme un rangement. La ‚Äúcl√© de partition‚Äù = le rayon (cat√©gorie). La ‚Äúcl√© de tri‚Äù = l‚Äôordre dans le rayon (d√©but du titre). ‚Äúbegins_with‚Äù = chercher les titres qui commencent par des lettres.\nIci, on veut : choisir un customer_type (le rayon), puis chercher des emails qui commencent par les lettres tap√©es (d√©but du titre). Sans recr√©er la table, on ajoute un ‚Äúnouveau plan de rangement‚Äù √† c√¥t√© : un GSI.\nR√©ponse A : GSI avec customer_type en partition (rayon) et email_address en tri (ordre). On peut alors faire une recherche ‚Äúcommence par‚Äù sur email_address dans ce customer_type.\nPourquoi pas B : le rayon resterait email_address, donc impossible de filtrer d‚Äôabord par customer_type efficacement.\nPourquoi pas C/D : un LSI doit garder le m√™me rayon que la table d‚Äôorigine (ici email_address), donc customer_type ou job_title ne peuvent pas devenir le rayon.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:182:fdfceffd92bc024c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 182,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company developed an API application on AWS by using Amazon CloudFront, Amazon API Gateway, and AWS Lambda. The API has a minimum of four requests every second. A developer notices that many API users run the same query by using the POST method. The developer wants to cache the POST request to optimize the API resources.Which solution will meet these requirements?",
      "choices": {
        "A": "Configure the CloudFront cache. Update the application to return cached content based upon the default request headers.",
        "B": "Override the cache method in the selected stage of API Gateway. Select the POST method.",
        "C": "Save the latest request response in Lambda /tmp directory. Update the Lambda function to check the /tmp directory.",
        "D": "Save the latest request in AWS Systems Manager Parameter Store. Modify the Lambda function to take the latest request response from Parameter Store."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122604-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:41 a.m.",
      "textHash": "fdfceffd92bc024c",
      "rawFormat": "discussion-md",
      "conceptKey": "secrets_manager_vs_ssm",
      "frExplanation": "Ici l‚ÄôAPI est derri√®re API Gateway (porte d‚Äôentr√©e HTTP) et Lambda (code ex√©cut√© √† la demande), avec CloudFront (CDN) devant. Beaucoup d‚Äôutilisateurs envoient la m√™me requ√™te en POST, donc on veut √©viter de recalculer la m√™me r√©ponse √† chaque fois.\nAPI Gateway propose un cache int√©gr√© par ‚Äústage‚Äù (environnement) : il peut stocker la r√©ponse d‚Äôune m√©thode et la renvoyer directement pour les requ√™tes identiques, ce qui r√©duit les appels √† Lambda et la charge.\nPar d√©faut, le cache est surtout utilis√© pour GET, mais on peut configurer/autoriser la mise en cache pour une m√©thode comme POST en ajustant les param√®tres de cache au niveau du stage et de la m√©thode.\nC‚Äôest exactement ce que fait la r√©ponse B : activer/adapter le cache d‚ÄôAPI Gateway pour la m√©thode POST.\nA est moins adapt√© car CloudFront ne met pas facilement en cache des POST (et d√©pend de r√®gles sp√©cifiques) et ‚Äúheaders par d√©faut‚Äù ne garantit pas une cl√© de cache correcte.\nC ne marche pas bien car /tmp est local √† une instance Lambda, non partag√© et peut √™tre effac√©; avec plusieurs instances, le cache serait incoh√©rent.\nD d√©tourne Parameter Store (fait pour config/secrets) : ce n‚Äôest pas un cache de r√©ponses, c‚Äôest plus lent et co√ªteux, et ne g√®re pas bien la logique de cache/expiration.\nDonc la solution la plus simple, pr√©vue pour √ßa, est le cache d‚ÄôAPI Gateway sur la m√©thode POST (B).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le bureau de la vie scolaire au lyc√©e : des √©l√®ves viennent poser des questions. Parfois, 50 √©l√®ves demandent exactement la m√™me chose (ex: ‚Äúc‚Äôest quoi l‚Äôhoraire du bus ?‚Äù).**\n\nLe ‚Äúcache‚Äù, c‚Äôest comme afficher la r√©ponse sur un panneau : au lieu de r√©p√©ter 50 fois, le bureau pointe le panneau. Ici, l‚ÄôAPI Gateway est le ‚Äúguichet‚Äù qui re√ßoit les demandes, et Lambda est le ‚Äúsurveillant‚Äù qui va chercher la r√©ponse. Comme beaucoup de gens envoient la m√™me question en POST, on veut que le guichet garde la r√©ponse en m√©moire pour la redonner vite. La r√©ponse B est bonne car elle dit : ‚Äúdans API Gateway, active le cache pour la m√©thode POST √† ce niveau-l√†‚Äù. A est moins adapt√© car CloudFront cache surtout des requ√™tes simples (souvent GET) et se base sur des d√©tails de la demande, donc √ßa ne r√®gle pas proprement le POST ici. C est mauvais car /tmp dans Lambda, c‚Äôest comme un post-it sur un surveillant : √ßa peut dispara√Ætre et ce n‚Äôest pas partag√©. D est mauvais car Parameter Store, c‚Äôest comme ranger la r√©ponse dans un classeur √† chaque fois : trop lent et pas fait pour servir de cache rapide.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:396:2626fb0de84531b1",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 396,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to use a code template to create an automated deployment of an application onto Amazon EC2 instances. The template must be configured to repeat deployment, installation, and updates of resources for the application. The template must be able to create identical environments and roll back to previous versions.Which solution will meet these requirements?",
      "choices": {
        "A": "Use AWS Amplify for automatic deployment templates. Use a traffic-splitting deployment to copy any deployments. Modify any resources created by Amplify, if necessary.",
        "B": "Use AWS CodeBuild for automatic deployment. Upload the required AppSpec file template. Save the appspec.yml file in the root directory folder of the revision. Specify the deployment group that includes the EC2 instances for the deployment.",
        "C": "Use AWS CloudFormation to create an infrastructure template in JSON format to deploy the EC2 instances. Use CloudFormation helper scripts to install the necessary software and to start the application. Call the scripts directly from the template.",
        "D": "Use AWS AppSync to deploy the application. Upload the template as a GraphQL schema. Specify the EC2 instances for deployment of the application. Use resolvers as a version control mechanism and to make any updates to the deployments."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144607-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 28, 2024, 7:28 a.m.",
      "textHash": "2626fb0de84531b1",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici on veut un ¬´ mod√®le de code ¬ª qui d√©crit l‚Äôinfrastructure et le d√©ploiement de fa√ßon r√©p√©table : m√™mes serveurs EC2, m√™mes r√©glages, m√™mes installations, √† chaque fois.\nAWS CloudFormation est justement un service qui permet d‚Äô√©crire un template (JSON/YAML) pour cr√©er et mettre √† jour des ressources AWS de mani√®re automatique et identique entre environnements (dev/test/prod).\nCloudFormation g√®re aussi les mises √† jour : si vous changez le template, il applique les changements de fa√ßon contr√¥l√©e.\nEt surtout, CloudFormation peut faire un rollback : si une mise √† jour √©choue, il revient √† l‚Äô√©tat pr√©c√©dent connu (stack pr√©c√©dente).\nLes ‚Äúhelper scripts‚Äù (ex: cfn-init, user data) permettent d‚Äôinstaller les logiciels et d√©marrer l‚Äôapplication sur les instances EC2 pendant la cr√©ation.\nAmplify vise surtout les applis web/mobile (front-end) et pas le d√©ploiement complet sur EC2 avec rollback d‚Äôinfra.\nCodeBuild sert √† compiler/tester du code (CI), pas √† d√©finir l‚Äôinfrastructure et le rollback d‚Äôenvironnements entiers.\nAppSync est un service GraphQL, sans rapport avec le d√©ploiement d‚Äôapplications sur EC2.\nDonc la solution qui r√©pond √† r√©p√©tabilit√© + environnements identiques + rollback est CloudFormation (C).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:412:edeff3440e09fb10",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 412,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is setting up AWS CodePipeline for a new application. During each build, the developer must generate a test report.Which solution will meet this requirement?",
      "choices": {
        "A": "Create an AWS CodeBuild build project that runs tests. Configure the buildspec file with the test report information.",
        "B": "Create an AWS CodeDeploy deployment that runs tests. Configure the AppSpec file with the test report information.",
        "C": "Run the builds on an Amazon EC2 instance that has AWS Systems Manager Agent (SSM Agent) installed and activated.",
        "D": "Create a repository in AWS CodeArtifact. Select the test report template."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144608-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 28, 2024, 8:29 a.m.",
      "textHash": "edeff3440e09fb10",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Dans CodePipeline, chaque √©tape (source, build, d√©ploiement) utilise un service adapt√©.\nPour g√©n√©rer un rapport de tests pendant le build, on utilise AWS CodeBuild : c‚Äôest le service qui compile le code et ex√©cute des commandes (tests unitaires, lint, etc.).\nCodeBuild lit un fichier buildspec.yml, qui d√©crit quoi faire (install, build, post_build) et comment publier des artefacts.\nLe buildspec permet aussi de d√©clarer des ¬´ reports ¬ª (rapports de tests) pour que CodeBuild collecte les r√©sultats (ex: JUnit, Cucumber) et les affiche dans AWS.\nDonc la solution A est correcte : un projet CodeBuild + buildspec configur√© pour ex√©cuter les tests et produire le rapport.\nB est faux car CodeDeploy sert au d√©ploiement sur des serveurs, pas √† construire et g√©n√©rer des rapports de tests.\nC est inutilement complexe : une instance EC2 + SSM peut ex√©cuter des commandes, mais ce n‚Äôest pas l‚Äôoutil standard int√©gr√© √† CodePipeline pour les rapports.\nD est hors sujet : CodeArtifact est un d√©p√¥t de paquets (biblioth√®ques), pas un g√©n√©rateur de rapports de tests.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un prof qui corrige les contr√¥les de la classe et doit rendre, √† chaque fois, une feuille de r√©sultats (le bulletin de tests).**\n\nConcept : dans une ‚Äúcha√Æne‚Äù automatique (CodePipeline), il faut une √©tape qui fait le travail et produit un compte-rendu.\nCodeBuild, c‚Äôest comme la salle de correction : il ex√©cute les tests pendant la ‚Äúcorrection‚Äù (le build).\nLe buildspec, c‚Äôest la consigne √©crite pour dire quoi faire et quoi rendre, dont le rapport de tests.\nDonc A est bon : on lance les tests dans CodeBuild et on d√©crit le rapport dans le buildspec.\nB est faux : CodeDeploy, c‚Äôest plut√¥t ‚Äúdistribuer les copies‚Äù (d√©ployer l‚Äôappli), pas corriger et faire le bulletin.\nC est faux : utiliser un PC (EC2) √† la main ne garantit pas un rapport automatique √† chaque build.\nD est faux : CodeArtifact, c‚Äôest une ‚Äú√©tag√®re de biblioth√®que‚Äù pour stocker des paquets, pas un g√©n√©rateur de rapports.\nConclusion : pour g√©n√©rer un rapport √† chaque build, il faut CodeBuild + buildspec (A).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:410:4791f0c5097838e8",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 410,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer builds a serverless application on AWS by using Amazon API Gateway, AWS Lambda functions, and Amazon Route 53. During testing, the developer notices errors but cannot immediately locate the root cause.To identify the errors, the developer needs to search all the application's logs.What should the developer do to meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Set up API Gateway health checks to monitor the application's availability. Use the Amazon CloudWatch PutMetricData API operation to publish the logs to CloudWatch. Search and query the logs by using Amazon Athena.",
        "B": "Set up Route 53 health checks to monitor the application's availability. Turn on AWS CloudTrail logs for all the AWS services that the application uses. Send the logs to a specified Amazon S3 bucket. Use Amazon Athena to query the log files directly from Amazon S3.",
        "C": "Configure all the application's AWS services to publish a real-time feed of log events to an Amazon Kinesis Data Firehose delivery stream. Configure the delivery stream to publish all the logs to an Amazon S3 bucket. Use Amazon OpenSearch Service to search and analyze the logs.",
        "D": "Set up Route 53 health checks to monitor the application's availability. Turn on Amazon CloudWatch Logs for the API Gateway stages to log API requests with a JSON log format. Use CloudWatch Logs Insights to search and analyze the logs from the AWS services that the application uses."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143937-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 16, 2024, 3:29 a.m.",
      "textHash": "4791f0c5097838e8",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Pour trouver la cause d‚Äôerreurs, il faut pouvoir chercher rapidement dans les journaux (logs) de toute l‚Äôapplication. Dans une appli serverless, API Gateway et Lambda √©crivent naturellement leurs logs dans Amazon CloudWatch Logs (service de collecte/stockage de logs). La meilleure option avec le moins d‚Äôeffort est d‚Äôactiver les logs d‚ÄôAPI Gateway (au niveau des stages) en format JSON, puis d‚Äôutiliser CloudWatch Logs Insights, qui permet de faire des recherches et requ√™tes directement sur les logs (filtrer, compter, trouver des erreurs). Route 53 health checks ne donnent que l‚Äô√©tat ‚Äúup/down‚Äù, pas le d√©tail des erreurs, mais ils peuvent aider √† surveiller. Les autres choix ajoutent beaucoup d‚Äôinfrastructure (S3 + Athena, Kinesis Firehose + OpenSearch) ou utilisent CloudTrail (plut√¥t pour l‚Äôaudit des appels API AWS, pas pour les erreurs applicatives). Donc D centralise et interroge les logs avec le minimum d‚Äôop√©rations.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine ton appli comme un coll√®ge : Route 53 = le panneau qui dit o√π est l‚Äôentr√©e, API Gateway = le surveillant √† la porte qui laisse entrer, Lambda = les profs qui font l‚Äôaction demand√©e. Les ‚Äúlogs‚Äù, c‚Äôest le cahier d‚Äôincidents o√π on note tout ce qui s‚Äôest pass√©.**\n\nConcept : pour trouver une erreur, tu veux un seul endroit o√π lire le cahier d‚Äôincidents, et pouvoir chercher vite dedans (comme ‚Äúqui est entr√© √† 10h12 ?‚Äù).\nR√©ponse D : tu demandes au ‚Äúsurveillant‚Äù (API Gateway) d‚Äô√©crire chaque passage en d√©tail (format JSON = fiche bien rang√©e), et tu mets tout dans CloudWatch Logs (le grand cahier central).\nPuis tu utilises CloudWatch Logs Insights : c‚Äôest la fonction ‚Äúrecherche‚Äù dans le cahier (filtrer, trouver les erreurs, voir les pics).\nC‚Äôest le moins de boulot au quotidien : pas besoin d‚Äôenvoyer les cahiers dans d‚Äôautres b√¢timents (S3), ni d‚Äôinstaller une grosse salle d‚Äôarchives (OpenSearch/Kinesis).\nLes health checks Route 53, c‚Äôest juste ‚Äúl‚Äô√©cole est ouverte ?‚Äù, mais pour la cause exacte, il faut lire les logs : D te donne √ßa simplement.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:395:a4bcfdf03f651571",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 395,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer creates an Amazon DynamoDB table. The table has OrderID as the partition key and NumberOfItemsPurchased as the sort key. The data type of the partition key and the sort key is Number.When the developer queries the table, the results are sorted by NumberOfItemsPurchased in ascending order. The developer needs the query results to be sorted by NumberOfItemsPurchased in descending order.Which solution will meet this requirement?",
      "choices": {
        "A": "Create a local secondary index (LSI) on the NumberOfItemsPurchased sort key.",
        "B": "Change the sort key from NumberOfItemsPurchased to NumberOfItemsPurchasedDescending.",
        "C": "In the Query operation, set the ScanIndexForward parameter to false.",
        "D": "In the Query operation, set the KeyConditionExpression parameter to false."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143801-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 3:40 p.m.",
      "textHash": "a4bcfdf03f651571",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π une table peut avoir une cl√© de partition (OrderID) et une cl√© de tri (NumberOfItemsPurchased).\nQuand on fait une op√©ration Query (requ√™te) sur une m√™me partition, DynamoDB renvoie les √©l√©ments tri√©s par la cl√© de tri.\nPar d√©faut, ce tri est en ordre croissant (du plus petit au plus grand).\nPour obtenir l‚Äôordre d√©croissant, on ne change pas le sch√©ma ni ne cr√©e un index : on change juste un param√®tre de la requ√™te.\nLe param√®tre ScanIndexForward contr√¥le le sens du tri sur la cl√© de tri : true = croissant, false = d√©croissant.\nDonc il faut mettre ScanIndexForward √† false dans l‚Äôop√©ration Query.\nUn LSI ne sert pas √† inverser l‚Äôordre, mais √† proposer une autre cl√© de tri/attributs pour des requ√™tes diff√©rentes.\nKeyConditionExpression sert √† filtrer sur les cl√©s (conditions), pas √† d√©finir le sens du tri.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le CDI de ton lyc√©e : tu cherches tous les livres d‚Äôun m√™me auteur (OrderID), et ils sont rang√©s sur l‚Äô√©tag√®re par nombre de pages (NumberOfItemsPurchased), du plus petit au plus grand.**\n\nDans DynamoDB, la ‚Äúpartition key‚Äù sert √† choisir le bon bac (tous les √©l√©ments d‚Äôun m√™me OrderID). La ‚Äúsort key‚Äù sert √† classer √† l‚Äôint√©rieur de ce bac (par NumberOfItemsPurchased). Par d√©faut, c‚Äôest comme l‚Äô√©tag√®re du CDI : tri du plus petit au plus grand (ordre croissant). Si tu veux l‚Äôinverse, tu ne changes pas les livres ni l‚Äô√©tag√®re : tu demandes juste au biblioth√©caire de te les donner du plus grand au plus petit. Dans la requ√™te (Query), le bouton qui dit ‚Äúdans quel sens je lis l‚Äô√©tag√®re‚Äù s‚Äôappelle ScanIndexForward. Le mettre √† false = lire √† l‚Äôenvers = ordre d√©croissant. Donc la bonne r√©ponse est C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:393:bdbe82f260fb4e89",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 393,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs a critical application on Amazon Elastic Container Service (Amazon ECS) by using Amazon EC2 instances. The company needs to migrate the application to Amazon ECS on AWS Fargate. A developer is configuring Fargate and the ECS capacity providers to make the change.Which solution will meet these requirements with the LEAST downtime during migration?",
      "choices": {
        "A": "Use the PutClusterCapacityProviders API operation to associate the ECS cluster with the FARGATE and FARGATE_SPOT capacity provider strategies. Use FARGATE as Provider 1 with a base value. Use FARGATE_SPOT as Provider 2 for failover.",
        "B": "Use the CreateCapacityProvider API operation to associate the ECS cluster with the FARGATE and FARGATE_SPOT capacity provider strategies. Use FARGATE as Provider 1 with a base value. Use FARGATE_SPOT as Provider 2 for failover.",
        "C": "Use the PutClusterCapacityProviders API operation to associate the ECS cluster with the FARGATE and FARGATE_SPOT capacity provider strategies. Use FARGATE_SPOT as Provider 1 with a base value. Use FARGATE as Provider 2 for failover.",
        "D": "Use the CreateCapacityProvider API operation to associate the ECS cluster with the FARGATE and FARGATE_SPOT capacity provider strategies. Use FARGATE_SPOT as Provider 1 with a base value. Use FARGATE as Provider 2 for failover."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143072-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 29, 2024, 7:31 p.m.",
      "textHash": "bdbe82f260fb4e89",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:e97a164a",
      "frExplanation": "Ici, l‚Äôobjectif est de passer d‚ÄôECS sur EC2 (vous g√©rez des serveurs) √† ECS sur Fargate (AWS ex√©cute les conteneurs sans g√©rer de serveurs) avec un minimum d‚Äôarr√™t.\nLes ‚Äúcapacity providers‚Äù indiquent √† ECS o√π lancer les t√¢ches : FARGATE (stable) ou FARGATE_SPOT (moins cher mais peut √™tre interrompu).\nPour une migration avec peu de downtime, on veut que les nouvelles t√¢ches d√©marrent d‚Äôabord sur FARGATE (fiable), puis √©ventuellement utiliser FARGATE_SPOT en compl√©ment.\nL‚ÄôAPI correcte pour attacher/mettre √† jour les capacity providers d‚Äôun cluster existant est PutClusterCapacityProviders (et non CreateCapacityProvider, qui sert √† cr√©er un provider personnalis√©).\nMettre FARGATE en Provider 1 avec une valeur ‚Äúbase‚Äù garantit qu‚Äôun nombre minimum de t√¢ches tourne toujours sur Fargate, donc moins de risque d‚Äôinterruption.\nMettre FARGATE_SPOT en Provider 2 ‚Äúpour failover/compl√©ment‚Äù √©vite de d√©pendre du Spot pour la disponibilit√©.\nDonc la solution A est la plus s√ªre et r√©duit le risque de coupure pendant la bascule.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : avant, tu devais amener tes propres tables et chaises (EC2). Maintenant, tu veux une cantine qui fournit tout automatiquement (Fargate). Et tu as aussi une option ‚Äúplaces moins ch√®res mais parfois indisponibles‚Äù (Fargate Spot).**\n\nConcept : un ‚Äúcapacity provider‚Äù, c‚Äôest comme choisir d‚Äôo√π viennent les places pour manger. Tu peux dire : d‚Äôabord les places normales, et si besoin, les places discount.\nPour avoir le moins d‚Äôarr√™t, tu veux que la cantine te garantisse des places stables au d√©but : donc Fargate en premier avec un minimum (base) = toujours des places s√ªres.\nEnsuite, tu peux compl√©ter avec Fargate Spot, mais seulement en bonus, pas en principal, car il peut dispara√Ætre.\nL‚Äôaction PutClusterCapacityProviders, c‚Äôest ‚Äúmettre √† jour le plan de places du r√©fectoire‚Äù sur un cluster d√©j√† existant, sans tout recr√©er.\nCreateCapacityProvider, c‚Äôest plut√¥t ‚Äúcr√©er un nouveau type de fournisseur‚Äù, pas n√©cessaire ici et √ßa complique la migration.\nDonc A : on met √† jour le cluster, Fargate en base (stable), Spot en secours/compl√©ment (moins de risque de coupure).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:389:94b432e6eba59dcb",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 389,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer has an application that uses an Amazon DynamoDB table with a configured local secondary index (LSI). During application testing, the DynamoDB table metrics report a ProvisionedThroughputExceededException error message. The number of requests made by the test suite did not exceed the table's provisioned capacity limits.What is the cause of this issue?",
      "choices": {
        "A": "The data in the table's partition key column is not evenly distributed.",
        "B": "The LSI's capacity is different from the table's capacity.",
        "C": "The application is not implementing exponential backoff retry logic while interacting with the DynamoDB API.",
        "D": "The application has the IAM permission to query the DynamoDB table but not to query the LSI."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143067-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 29, 2024, 2:34 p.m.",
      "textHash": "94b432e6eba59dcb",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL qui r√©partit les donn√©es sur plusieurs ‚Äúpartitions‚Äù selon la cl√© de partition (partition key).\nLa capacit√© provisionn√©e (lectures/√©critures) est aussi r√©partie entre ces partitions.\nSi beaucoup d‚Äô√©l√©ments ont la m√™me valeur de cl√© de partition (ou quelques valeurs tr√®s fr√©quentes), tout le trafic vise une seule partition : c‚Äôest un ‚Äúhot partition‚Äù.\nM√™me si le total des requ√™tes reste sous la capacit√© globale de la table, une partition peut d√©passer sa part locale et d√©clencher ProvisionedThroughputExceededException.\nUn LSI (Local Secondary Index) partage la m√™me cl√© de partition que la table, donc il subit le m√™me probl√®me de r√©partition.\nDonc la cause la plus probable est une distribution non uniforme des valeurs de la cl√© de partition (A).\nB est faux car un LSI n‚Äôa pas une capacit√© s√©par√©e : il consomme la capacit√© de la table.\nC peut aider √† g√©rer les erreurs, mais n‚Äôexplique pas pourquoi elles arrivent sans d√©passer la capacit√© globale.\nD donnerait plut√¥t une erreur d‚Äôautorisation (AccessDenied), pas un d√©passement de d√©bit.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**üéí Imagine ton casier au lyc√©e** : tu ranges tes affaires diff√©remment selon comment tu les cherches.\n\n**LSI (Local)** = C'est comme **des onglets DANS ton classeur** üìÅ - Cr√©√©s √† la rentr√©e, jamais modifiables, super rapides (m√™me partition).\n\n**GSI (Global)** = C'est comme **un index √† la fin du livre** üìñ - Ajoutable n'importe quand, cherche partout, mais plus lent.\n\n**üß† Mn√©motechnique :** \"**L**SI = **L**ocal, **L**imit√©\" | \"**G**SI = **G**lobal, **G**√©nial\"\n\n**Quand utiliser GSI :** Quand tu veux chercher par un autre crit√®re que la cl√© principale, sur TOUTE la table.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:380:30102009d8de8864",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 380,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a web application that contains an Amazon API Gateway REST API. A developer has created an AWS CloudFormation template for the initial deployment of the application. The developer has deployed the application successfully as part of an AWS CodePipeline continuous integration and continuous delivery (CI/CD) process. All resources and methods are available through the deployed stage endpoint.The CloudFormation template contains the following resource types:‚Ä¢ AWS::ApiGateway::RestApi‚Ä¢ AWS::ApiGateway::Resource‚Ä¢ AWS::ApiGateway::Method‚Ä¢ AWS::ApiGateway::Stage‚Ä¢ AWS::ApiGateway::DeploymentThe developer adds a new resource to the REST API with additional methods and redeploys the template. CloudFormation reports that the deployment is successful and that the stack is in the UPDATE_COMPLETE state. However, calls to all new methods are returning 404 (Not Found) errors.What should the developer do to make the new methods available?",
      "choices": {
        "A": "Specify the disable-rollback option during the update-stack operation.",
        "B": "Unset the CloudFormation stack failure options.",
        "C": "Add an AWS CodeBuild stage to CodePipeline to run the aws apigateway create-deployment AWS CLI command.",
        "D": "Add an action to CodePipeline to run the aws cloudfront create-invalidation AWS CLI command."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143761-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:55 a.m.",
      "textHash": "30102009d8de8864",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "API Gateway (REST API) expose des ‚Äúressources‚Äù et des ‚Äúm√©thodes‚Äù (GET/POST‚Ä¶). Pour que ces changements soient visibles, il faut cr√©er un ‚ÄúDeployment‚Äù et l‚Äôassocier √† un ‚ÄúStage‚Äù (ex: prod). Dans CloudFormation, si le Deployment ne change pas (m√™me logical ID, pas de nouvelle valeur), CloudFormation peut ne pas recr√©er le d√©ploiement, m√™me si la stack est UPDATE_COMPLETE. R√©sultat: les nouvelles routes existent dans la config, mais le stage pointe encore vers l‚Äôancien deployment, donc les nouvelles m√©thodes r√©pondent 404. La solution est de forcer un nouveau d√©ploiement apr√®s la mise √† jour, par exemple en ajoutant dans CodePipeline une √©tape (via CodeBuild) qui ex√©cute `aws apigateway create-deployment` pour le stage. Les options de rollback (A/B) ne publient pas de nouvelles routes, et CloudFront invalidation (D) concerne le cache CDN, pas la publication des m√©thodes API Gateway.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine le site web comme un self au lyc√©e. L‚ÄôAPI Gateway, c‚Äôest le comptoir o√π tu commandes. Les ‚Äúm√©thodes‚Äù sont les plats possibles. Le ‚Äústage endpoint‚Äù, c‚Äôest la porte du self ouverte aux √©l√®ves. Le ‚Äúdeployment‚Äù, c‚Äôest le menu imprim√© et affich√© au mur.**\n\nConcept : ajouter un nouveau plat en cuisine ne suffit pas, il faut aussi r√©imprimer et r√©afficher le menu pour que tout le monde puisse le commander. Ici, CloudFormation a bien cr√©√© les nouveaux ‚Äúplats‚Äù (ressources + m√©thodes), donc il dit UPDATE_COMPLETE. Mais le ‚Äúmenu affich√©‚Äù (le deployment de l‚ÄôAPI) n‚Äôa pas √©t√© refait, donc √† la porte, on te dit ‚Äú404 = ce plat n‚Äôexiste pas sur le menu‚Äù. La solution C revient √† dire : dans la cha√Æne automatique (CodePipeline), ajoute une √©tape qui force la r√©impression du menu en lan√ßant la commande ‚Äúcreate-deployment‚Äù. A et B ne changent pas le menu affich√©. D (CloudFront) c‚Äôest plut√¥t vider un cache, comme rafra√Æchir une affiche, mais si le menu n‚Äôa pas √©t√© r√©imprim√©, √ßa ne cr√©e pas les nouveaux plats.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:373:469e4759ccba1eef",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 373,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a serverless application that uses Amazon API Gateway backed by AWS Lambda proxy integration. The company is developing several backend APIs. The company needs a landing page to provide an overview of navigation to the APIs.A developer creates a new/LandingPage resource and a new GET method that uses mock integration.What should the developer do next to meet these requirements?",
      "choices": {
        "A": "Configure the integration request mapping template with Content-Type of text/html and statusCode of 200. Configure the integration response mapping template with Content-Type of application/json. In the integration response mapping template, include the LandingPage HTML code that references the APIs.",
        "B": "Configure the integration request mapping template with Content-Type of application/json. In the integration request mapping template, include the LandingPage HMTL code that references the APIs. Configure the integration response mapping template with Content-Type of text/html and statusCode of 200.",
        "C": "Configure the integration request mapping template with Content-Type of application/json and statusCode of 200. Configure the integration response mapping template with Content-Type of text/html. In the integration response mapping template, include the LandingPage HTML code that references the APIs.",
        "D": "Configure the integration request mapping template with Content-Type of text/html. In the integration request mapping template, include the LandingPage HTML code that references the APIs. Configure the integration response mapping template with Content-Type of application/json and statusCode of 200."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143069-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 29, 2024, 2:48 p.m.",
      "textHash": "469e4759ccba1eef",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:7e5e537d",
      "frExplanation": "API Gateway est la ‚Äúporte d‚Äôentr√©e‚Äù HTTP de votre appli. Une int√©gration ‚Äúmock‚Äù signifie qu‚ÄôAPI Gateway r√©pond tout seul, sans appeler Lambda ni un backend.\nPour une page d‚Äôaccueil, le navigateur attend du HTML : la r√©ponse doit donc avoir le bon type de contenu (Content-Type: text/html) et un code succ√®s (200).\nAvec une int√©gration mock, ce que vous voulez renvoyer (le code HTML) se met dans la partie ‚Äúintegration response‚Äù via un mapping template : c‚Äôest l√† qu‚Äôon fabrique le corps de la r√©ponse.\nLa partie ‚Äúintegration request‚Äù sert surtout √† construire la requ√™te envoy√©e au backend. Ici il n‚Äôy a pas de backend, donc on peut juste d√©finir un mod√®le simple (souvent application/json) pour satisfaire la configuration.\nLe choix C fait exactement cela : statusCode 200 c√¥t√© request (pour le mock) et HTML + text/html c√¥t√© response.\nLes autres choix mettent le HTML au mauvais endroit (request) ou renvoient application/json au lieu de text/html, ce qui ne convient pas pour afficher une page web.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le site comme le hall d‚Äôentr√©e de ton lyc√©e. Tu veux un panneau d‚Äôaccueil (page HTML) qui liste toutes les salles (les APIs) et o√π aller. Le ‚Äúmock‚Äù c‚Äôest comme un panneau d√©j√† pr√™t, sans prof derri√®re pour r√©pondre : il affiche juste un message fixe.**\n\nConcept : une requ√™te GET, c‚Äôest un √©l√®ve qui demande ‚Äúje veux voir la page d‚Äôaccueil‚Äù. La r√©ponse doit √™tre une page web, donc du HTML. Dans API Gateway, la partie ‚Äúintegration request‚Äù c‚Äôest ce que tu envoies au panneau, et ‚Äúintegration response‚Äù c‚Äôest ce que le panneau affiche √† l‚Äô√©l√®ve. Avec un mock, tu n‚Äôas rien √† envoyer de sp√©cial : tu dis juste ‚ÄúOK‚Äù (statusCode 200 = tout va bien). Ensuite tu mets le contenu de la page dans la r√©ponse, et tu pr√©cises que c‚Äôest du text/html (comme dire ‚Äúc‚Äôest une affiche papier‚Äù, pas un fichier de donn√©es). Donc C est bon : statusCode 200 c√¥t√© request, et HTML + Content-Type text/html c√¥t√© response, avec le code de la LandingPage.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:360:14c077b12a2745a9",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 360,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building an ecommerce application. When there is a sale event, the application needs to concurrently call three third-party systems to record the sale. The developer wrote three AWS Lambda functions. There is one Lambda function for each third-party system, which contains complex integration logic.These Lambda functions are all independent. The developer needs to design the application so each Lambda function will run regardless of others' success or failure.Which solution will meet these requirements?",
      "choices": {
        "A": "Publish the sale event from the application to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the three Lambda functions to poll the queue.",
        "B": "Publish the sale event from the application to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the three Lambda functions to be triggered by the SNS topic.",
        "C": "Publish the sale event from the application to an Application Load Balancer (ALB). Add the three Lambda functions as ALB targets.",
        "D": "Publish the sale event from the application to an AWS Step Functions state machine. Move the logic from the three Lambda functions into the Step Functions state machine."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143366-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 5, 2024, 4:46 p.m.",
      "textHash": "14c077b12a2745a9",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Ici, on veut d√©clencher 3 fonctions AWS Lambda (du code qui s‚Äôex√©cute √† la demande) en parall√®le quand une vente arrive, et surtout que chacune s‚Äôex√©cute m√™me si une autre √©choue.\nAmazon SNS est un service de ‚Äúpublication/abonnement‚Äù : l‚Äôapplication publie un message (l‚Äô√©v√©nement de vente) dans un topic, puis SNS envoie une copie du message √† chaque abonn√©.\nEn abonnant les 3 Lambda au m√™me topic SNS, chaque Lambda re√ßoit son propre d√©clenchement ind√©pendamment des autres.\nSi une Lambda √©choue, cela n‚Äôemp√™che pas SNS de d√©clencher les deux autres : elles ont d√©j√† re√ßu leur message.\n√Ä l‚Äôinverse, avec SQS (file), un message est g√©n√©ralement consomm√© par un seul lecteur : les 3 Lambda se ‚Äúpartageraient‚Äù les messages au lieu de toutes les ex√©cuter.\nUn ALB sert √† router des requ√™tes HTTP vers des cibles, pas √† diffuser un √©v√©nement √† plusieurs traitements ind√©pendants.\nStep Functions orchestre des √©tapes et peut g√©rer des erreurs, mais la question demande surtout un d√©clenchement fan-out simple sans d√©pendance entre fonctions.\nDonc la bonne solution est SNS + 3 abonnements Lambda (r√©ponse B).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une annonce au micro du lyc√©e : ¬´ Promo √† la cantine ! ¬ª. Tu veux que 3 clubs diff√©rents (sport, th√©√¢tre, journal) fassent chacun une action en m√™me temps, sans d√©pendre des autres.**\n\nConcept : un ‚Äútopic‚Äù SNS, c‚Äôest comme le micro du lyc√©e. Tu annonces une info une seule fois, et tous ceux qui sont abonn√©s l‚Äôentendent.\nIci, l‚Äôappli ‚Äúcrie‚Äù l‚Äô√©v√©nement de vente sur SNS. Les 3 fonctions Lambda (3 clubs) sont abonn√©es.\nR√©sultat : chacune d√©marre de son c√¥t√©, en parall√®le. Si le club sport oublie, th√©√¢tre et journal agissent quand m√™me.\nPourquoi pas SQS (A) : c‚Äôest plut√¥t une file d‚Äôattente ‚Äúun ticket = un seul √©l√®ve le prend‚Äù. Les 3 Lambdas se partageraient les messages, pas 1 message pour 3.\nPourquoi pas ALB (C) : c‚Äôest comme un vigile qui envoie les gens vers une seule porte √† la fois, pas vers 3 clubs en m√™me temps.\nPourquoi pas Step Functions (D) : c‚Äôest un chef d‚Äôorchestre qui impose un sc√©nario; ici on veut juste diffuser l‚Äôinfo √† 3 ind√©pendants.\nDonc B : SNS + 3 abonnements = les 3 Lambdas se lancent, m√™me si l‚Äôune √©choue.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:357:2be2adfc9b0dda6f",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 357,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is publishing critical log data to a log group in Amazon CloudWatch Logs. The log group was created 2 months ago. The developer must encrypt the log data by using an AWS Key Management Service (AWS KMS) key so that future data can be encrypted to comply with the company's security policy.Which solution will meet this requirement with the LEAST effort?",
      "choices": {
        "A": "Use the AWS Encryption SDK for encryption and decryption of the data before writing to the log group.",
        "B": "Use the AWS KMS console to associate the KMS key with the log group.",
        "C": "Use the AWS CLI aws logs create-log-group command, and specify the key Amazon Resource Name (ARN).",
        "D": "Use the AWS CLI aws logs associate-kms-key command, and specify the key Amazon Resource Name (ARN)."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136975-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 5:06 a.m.",
      "textHash": "2be2adfc9b0dda6f",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "CloudWatch Logs stocke des journaux (logs) dans un ¬´ log group ¬ª.\nPour chiffrer ces donn√©es, on utilise AWS KMS, un service qui g√®re des cl√©s de chiffrement.\nLe log group existe d√©j√† (cr√©√© il y a 2 mois) : il ne faut donc pas le recr√©er, il faut juste lui associer une cl√© KMS pour chiffrer les nouveaux logs.\nLa commande AWS CLI ¬´ aws logs associate-kms-key ¬ª sert pr√©cis√©ment √† attacher une cl√© KMS √† un log group existant, en fournissant l‚ÄôARN de la cl√©.\nAinsi, tous les futurs √©v√©nements √©crits dans ce log group seront chiffr√©s avec cette cl√©, ce qui respecte la politique de s√©curit√©.\nA demande de modifier l‚Äôapplication (plus d‚Äôeffort) et n‚Äôest pas n√©cessaire car CloudWatch peut chiffrer c√¥t√© service.\nC recr√©e un log group (inutile et risqu√©) et ne r√©pond pas au cas ‚Äúd√©j√† existant‚Äù.\nB peut fonctionner via console, mais l‚Äôoption la plus directe et attendue ici est la commande d√©di√©e d‚Äôassociation (D).",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:352:3b09bf9a4710b499",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 352,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is working on a web application that requires selective activation of specific features. The developer wants to keep the features hidden from end users until the features are ready for public access.Which solution will meet these requirements?",
      "choices": {
        "A": "Create a feature flag configuration profile in AWS AppSync. Store the feature flag values in the configuration profile. Activate and deactivate feature flags as needed.",
        "B": "Store prerelease data in an Amazon DynamoDB table. Enable Amazon DynamoDB Streams in the table. Toggle between hidden and visible states by using DynamoDB Streams.",
        "C": "Create a feature flag configuration profile in AWS AppConfig. Store the feature flag values in the configuration profile. Activate and deactivate feature flags as needed.",
        "D": "Store prerelease data in AWS Amplify DataStore. Toggle between hidden and visible states by using Amplify DataStore cloud synchronization."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136973-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 4:57 a.m.",
      "textHash": "3b09bf9a4710b499",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:293b1ea9",
      "frExplanation": "Ici, on parle de ‚Äúfeature flags‚Äù (drapeaux de fonctionnalit√©s) : des interrupteurs qui permettent d‚Äôactiver/d√©sactiver une fonction sans red√©ployer l‚Äôapplication.\nAWS AppConfig (dans AWS Systems Manager) sert justement √† g√©rer des configurations d‚Äôapplication √† distance, dont des feature flags, avec contr√¥le de version et d√©ploiement progressif.\nOn cr√©e un ‚Äúconfiguration profile‚Äù de type feature flags, on y met des valeurs (ex: nouvelleRecherche=true/false), puis l‚Äôapplication lit ces valeurs au d√©marrage ou r√©guli√®rement.\nAinsi, la fonctionnalit√© reste cach√©e aux utilisateurs tant que le flag est √† false, puis devient visible quand on passe √† true.\nAppSync est un service GraphQL pour APIs temps r√©el, pas l‚Äôoutil standard pour g√©rer des feature flags (A faux).\nDynamoDB Streams sert √† r√©agir √† des changements de donn√©es, pas √† piloter proprement des flags de fonctionnalit√©s (B hors sujet).\nAmplify DataStore synchronise des donn√©es client, ce n‚Äôest pas un syst√®me central de feature flags pour contr√¥ler des releases (D faux).\nDonc la bonne solution est AWS AppConfig avec un profil de feature flags (C).",
      "domainKey": "development",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:322:0fed8249c4a66e09",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 322,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a simple proof-of-concept demo by using AWS CloudFormation and AWS Lambda functions. The demo will use a CloudFormation template to deploy an existing Lambda function. The Lambda function uses deployment packages and dependencies stored in Amazon S3. The developer defined an AWS::Lambda::Function resource in a CloudFormation template. The developer needs to add the S3 bucket to the CloudFormation template.What should the developer do to meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Add the function code in the CloudFormation template inline as the code property.",
        "B": "Add the function code in the CloudFormation template as the ZipFile property.",
        "C": "Find the S3 key for the Lambda function. Add the S3 key as the ZipFile property in the CloudFormation template.",
        "D": "Add the relevant key and bucket to the S3Bucket and S3Key properties in the CloudFormation template."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133632-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 12, 2024, 6:11 p.m.",
      "textHash": "0fed8249c4a66e09",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:a1bb6957",
      "frExplanation": "CloudFormation est un service AWS qui d√©ploie des ressources √† partir d‚Äôun fichier (template) au lieu de tout cr√©er √† la main.\nUne fonction AWS Lambda peut √™tre fournie soit en code ‚Äúinline‚Äù (petit script dans le template), soit comme un fichier ZIP stock√© dans Amazon S3 (stockage d‚Äôobjets).\nIci, la fonction existe d√©j√† sous forme de package + d√©pendances dans S3 : il faut donc simplement dire √† CloudFormation o√π se trouve ce ZIP.\nDans la ressource AWS::Lambda::Function, la propri√©t√© Code accepte S3Bucket et S3Key pour pointer vers le bucket et le chemin (cl√©) de l‚Äôobjet ZIP.\nC‚Äôest l‚Äôeffort minimal : pas besoin de recopier le code dans le template ni de transformer le package.\nA et B (inline/ZipFile) servent surtout pour du code tr√®s court, pas pour un package complet avec d√©pendances.\nC est incorrect car ZipFile n‚Äôest pas fait pour mettre une ‚Äúcl√© S3‚Äù, mais du code inline.\nDonc la bonne r√©ponse est d‚Äôajouter S3Bucket et S3Key avec le bucket et la cl√© correspondants (D).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois monter un expos√© en classe avec un ‚Äúplan de montage‚Äù (CloudFormation). Ton expos√© (la fonction Lambda) est d√©j√† pr√™t, mais il est rang√© dans un casier du CDI (un bucket S3) et dans un dossier pr√©cis de ce casier (la cl√© S3Key).**\n\nConcept : CloudFormation, c‚Äôest une fiche d‚Äôinstructions qui dit quoi installer et o√π le trouver. Lambda, c‚Äôest l‚Äôexpos√© √† pr√©senter. S3, c‚Äôest le CDI o√π sont stock√©s les fichiers.\nPour utiliser un expos√© d√©j√† stock√© au CDI, tu dois donner 2 infos : quel casier (S3Bucket) et quel dossier exact dedans (S3Key).\nDonc la bonne r√©ponse est D : on met le nom du bucket et la cl√© du fichier dans les propri√©t√©s S3Bucket et S3Key.\nA et B, c‚Äôest comme recopier tout l‚Äôexpos√© directement sur la fiche : plus long et inutile.\nC se trompe de ‚Äúcase‚Äù : la cl√© S3Key ne va pas dans ZipFile, ZipFile sert √† coller du code directement, pas √† pointer vers le CDI.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:321:1b34f2cd61bbcbec",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 321,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs its website on AWS. The company posts daily polls on its website and publishes the poll results next day. The website stores user responses in an Amazon DynamoDB table. After the poll results are published, the company does not need to keep the user responses.A developer needs to implement a solution that will automatically remove old user responses from the DynamoDB table. The developer adds a new expiration_date attribute to the DynamoDB table. The developer plans to use the expiration_date attribute for the automation.Which solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Create an AWS Lambda function to delete old user responses based on the expiration_date attribute. Create an Amazon EventBridge schedule to run the Lambda function daily.",
        "B": "Create an AWS Fargate task in Amazon Elastic Container Service (Amazon ECS) to delete old user responses based on the expiration_date attribute. Create an Amazon EventBridge schedule to run the Fargate task daily.",
        "C": "Create an AWS Glue job to delete old user responses based on the expiration_date attribute. Create an AWS Glue trigger schedule to run the job daily.",
        "D": "Enable TTL on the DynamoDB table and specify the expiration_date attribute. Expire old user responses by using DynamoDB TTL."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133631-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 12, 2024, 6:08 p.m.",
      "textHash": "1b34f2cd61bbcbec",
      "rawFormat": "discussion-md",
      "conceptKey": "eventbridge_routing",
      "frExplanation": "Ici, on veut supprimer automatiquement des √©l√©ments (r√©ponses) d‚Äôune table Amazon DynamoDB apr√®s une date. DynamoDB est une base NoSQL g√©r√©e par AWS.\nLa solution la plus simple est d‚Äôutiliser DynamoDB TTL (Time To Live) : c‚Äôest une fonction int√©gr√©e qui supprime les √©l√©ments quand un attribut contient une date/heure d‚Äôexpiration.\nEn activant TTL et en indiquant l‚Äôattribut expiration_date, DynamoDB s‚Äôoccupe de l‚Äôeffacement sans √©crire de code ni planifier de t√¢ches.\nLes options A, B et C demandent de d√©velopper et maintenir du traitement (Lambda, conteneur Fargate/ECS, ou job Glue) + une planification, donc plus d‚Äôeffort.\nAvec TTL, il suffit de remplir expiration_date pour chaque r√©ponse (timestamp Unix en secondes) et DynamoDB les supprimera automatiquement apr√®s l‚Äô√©ch√©ance.\nC‚Äôest donc l‚Äôeffort de d√©veloppement minimal et r√©pond exactement au besoin ‚Äúne pas conserver apr√®s publication‚Äù.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la bo√Æte des objets trouv√©s au coll√®ge : chaque objet a une √©tiquette avec une date limite. Apr√®s cette date, le surveillant jette automatiquement l‚Äôobjet, sans que quelqu‚Äôun fasse une tourn√©e tous les jours.**\n\nIci, la table DynamoDB = la bo√Æte des r√©ponses au sondage. Les r√©ponses ne servent plus apr√®s publication des r√©sultats. L‚Äôattribut expiration_date = l‚Äô√©tiquette ‚Äú√† jeter apr√®s telle date‚Äù. La solution D (TTL) active le mode ‚Äúnettoyage automatique‚Äù : DynamoDB supprime tout seul les r√©ponses quand la date est d√©pass√©e. √áa demande le moins d‚Äôeffort car tu n‚Äô√©cris pas de programme qui passe chaque jour (comme A), ni de ‚Äúrobot‚Äù plus compliqu√© √† lancer (B), ni un gros outil de traitement (C). Tu dis juste : ‚Äúutilise expiration_date comme date d‚Äôexpiration‚Äù, et DynamoDB fait le m√©nage.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:222:87a4ccba6f5c18d0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 222,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is designing a serverless application that customers use to select seats for a concert venue. Customers send the ticket requests to an Amazon API Gateway API with an AWS Lambda function that acknowledges the order and generates an order ID. The application includes two additional Lambda functions: one for inventory management and one for payment processing. These two Lambda functions run in parallel and write the order to an Amazon Dynamo DB table.The application must provide seats to customers according to the following requirements. If a seat is accidently sold more than once, the first order that the application received must get the seat. In these cases, the application must process the payment for only the first order. However, if the first order is rejected during payment processing, the second order must get the seat. In these cases, the application must process the payment for the second order.Which solution will meet these requirements?",
      "choices": {
        "A": "Send the order ID to an Amazon Simple Notification Service (Amazon SNS) FIFO topic that fans out to one Amazon Simple Queue Service (Amazon SQS) FIFO queue for inventory management and another SQS FIFO queue for payment processing.",
        "B": "Change the Lambda function that generates the order ID to initiate the Lambda function for inventory management. Then initiate the Lambda function for payment processing.",
        "C": "Send the order ID to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the Lambda functions for inventory management and payment processing to the topic.",
        "D": "Deliver the order ID to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda functions for inventory management and payment processing to poll the queue."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124805-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 12:36 p.m.",
      "textHash": "87a4ccba6f5c18d0",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, il faut garantir l‚Äôordre d‚Äôarriv√©e des commandes et √©viter de d√©biter deux fois si deux clients demandent le m√™me si√®ge.\nUn topic SNS FIFO + des files SQS FIFO permet de diffuser le m√™me message (order ID) vers deux traitements en parall√®le (inventaire et paiement) tout en gardant un ordre strict par groupe de messages.\nFIFO signifie ¬´ premier arriv√©, premier servi ¬ª : la premi√®re commande re√ßue sera trait√©e avant la suivante pour un m√™me si√®ge (via un MessageGroupId bas√© sur l‚ÄôID du si√®ge).\nSQS FIFO apporte aussi la d√©duplication et √©vite les traitements en double.\nAvec deux files s√©par√©es, inventaire et paiement peuvent travailler en parall√®le, mais chacun respecte l‚Äôordre des commandes.\nSi la premi√®re commande √©choue au paiement, la suivante sera ensuite trait√©e et pourra obtenir le si√®ge.\nLes options SNS standard (C) et SQS standard (D) ne garantissent pas l‚Äôordre.\nL‚Äôencha√Ænement direct de Lambdas (B) ne g√®re pas bien la concurrence et ne fournit pas de file FIFO pour arbitrer correctement les conflits.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une billetterie au lyc√©e pour un concert, avec un seul carnet de tickets num√©rot√©s et deux guichets: un qui r√©serve la place (stock) et un qui encaisse (paiement).**\n\nLe but: si deux √©l√®ves demandent la m√™me place, celui qui a fait la demande en premier doit l‚Äôavoir, et on n‚Äôencaisse que lui. Si son paiement √©choue, alors le deuxi√®me r√©cup√®re la place et on encaisse le deuxi√®me.\nA marche car ‚ÄúFIFO‚Äù = file d‚Äôattente en ordre d‚Äôarriv√©e (Premier arriv√©, premier servi), comme une queue √† la cantine.\nLe ‚Äútopic FIFO‚Äù envoie le m√™me num√©ro de commande aux deux guichets, mais en gardant le m√™me ordre pour tout le monde.\nLes deux ‚Äúqueues FIFO‚Äù garantissent que stock et paiement traitent les commandes dans le m√™me ordre, sans m√©langer.\nDonc si la commande 1 arrive avant la 2, elle est trait√©e avant partout: elle garde la place et seule elle est pay√©e.\nSi le paiement de la 1 est refus√©, la 2 est la suivante dans la file: elle prend la place et on la paie.\nB/C/D ne garantissent pas aussi bien cet ordre strict + copie contr√¥l√©e vers les deux traitements en parall√®le.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:379:998aa119379d1917",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 379,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is monitoring an application that runs on an Amazon EC2 instance. The developer has configured a custom Amazon CloudWatch metric with data granularity of 1 second. If any issues occur, the developer wants to be notified within 30 seconds by Amazon Simple Notification Service (Amazon SNS).What should the developer do to meet this requirement?",
      "choices": {
        "A": "Configure a high-resolution CloudWatch alarm.",
        "B": "Set up a custom CloudWatch dashboard.",
        "C": "Use Amazon CloudWatch Logs Insights.",
        "D": "Change to a default CloudWatch metric."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143760-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:53 a.m.",
      "textHash": "998aa119379d1917",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Vous avez une application sur une instance EC2 (un serveur virtuel). Vous envoyez d√©j√† une m√©trique personnalis√©e CloudWatch toutes les 1 seconde (granularit√© 1s).\nPour √™tre alert√© en moins de 30 secondes via SNS (service qui envoie des notifications email/SMS/HTTP), il faut qu‚Äôune alarme CloudWatch √©value les donn√©es tr√®s fr√©quemment.\nUne ‚Äúhigh-resolution CloudWatch alarm‚Äù est con√ßue pour les m√©triques haute r√©solution (1s) et peut d√©clencher plus vite car elle utilise des p√©riodes courtes (par ex. 10s ou 30s) au lieu de 1 minute.\nAinsi, CloudWatch peut d√©tecter un probl√®me rapidement et publier l‚Äôalerte vers SNS dans la fen√™tre de 30 secondes.\nUn dashboard (B) sert seulement √† visualiser, pas √† notifier automatiquement.\nLogs Insights (C) analyse des logs, ce n‚Äôest pas le m√©canisme standard pour d√©clencher une alarme en temps r√©el sur une m√©trique.\nPasser √† une m√©trique par d√©faut (D) ne garantit pas 1s et est souvent en 1 minute, donc trop lent.\nDonc il faut configurer une alarme CloudWatch haute r√©solution et la relier √† un topic SNS.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un surveillant au lyc√©e qui doit rep√©rer une bagarre et pr√©venir le CPE par SMS en moins de 30 secondes.**\n\nLe ‚Äúm√©trique‚Äù c‚Äôest comme une cam√©ra/compteur qui regarde l‚Äô√©tat de l‚Äô√©l√®ve (le serveur) tr√®s souvent.\nIci, tu as des infos chaque seconde : c‚Äôest comme une cam√©ra en mode ‚Äúimages tr√®s rapides‚Äù.\nMais pour envoyer l‚Äôalerte vite, il faut aussi une alarme qui v√©rifie aussi vite.\nUne alarme ‚Äúhaute r√©solution‚Äù (A) regarde les donn√©es √† la seconde et peut d√©clencher presque tout de suite.\nEnsuite, SNS c‚Äôest le ‚ÄúSMS automatique‚Äù qui pr√©vient en moins de 30 secondes.\nB (dashboard) c‚Äôest juste un √©cran pour regarder, √ßa n‚Äôalerte pas vite.\nC (Logs Insights) sert √† fouiller des journaux apr√®s coup, pas √† alerter en 30 secondes.\nD (m√©trique par d√©faut) est souvent moins pr√©cise (minutes), donc trop lent.\nDonc A est la bonne r√©ponse : alarme haute r√©solution + SMS SNS = notification rapide.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:335:a62e713a8e503d76",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 335,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is using an AWS CodePipeline pipeline to provide continuous integration and continuous delivery (CI/CD) support for a Java application. The developer needs to update the pipeline to support the introduction of a new application dependency .jar file. The pipeline must start a build when a new version of the .jar file becomes available.Which solution will meet these requirements?",
      "choices": {
        "A": "Create an Amazon S3 bucket to store the dependency .jar file. Publish the dependency .jar file to the S3 bucket. Use an Amazon Simple Notification Service (Amazon SNS) notification to start a CodePipeline pipeline build.",
        "B": "Create an Amazon Elastic Container Registry (Amazon ECR) private repository. Publish the dependency .jar file to the repository. Use an ECR source action to start a CodePipeline pipeline build.",
        "C": "Create an Amazon Elastic Container Registry (Amazon ECR) private repository. Publish the dependency .jar file to the repository. Use an Amazon Simple Notification Service (Amazon SNS) notification to start a CodePipeline pipeline build.",
        "D": "Create an AWS CodeArtifact repository. Publish the dependency .jar file to the repository. Use an Amazon EventBridge rule to start a CodePipeline pipeline build."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136644-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 19, 2024, 3:21 p.m.",
      "textHash": "a62e713a8e503d76",
      "rawFormat": "discussion-md",
      "conceptKey": "ecr_image_scanning",
      "frExplanation": "Ici, on veut d√©clencher automatiquement le pipeline CI/CD quand une nouvelle version d‚Äôune d√©pendance Java (.jar) est publi√©e.\nAWS CodeArtifact est le service AWS fait pour stocker et g√©rer des paquets/d√©pendances (comme Maven/Gradle pour Java), donc c‚Äôest l‚Äôendroit ‚Äúnaturel‚Äù pour publier un .jar de d√©pendance.\nAmazon EventBridge est un bus d‚Äô√©v√©nements : il peut √©couter des √©v√©nements AWS (ex. ‚Äúun nouveau package a √©t√© publi√© dans CodeArtifact‚Äù) et lancer une action.\nOn peut donc cr√©er une r√®gle EventBridge qui d√©tecte la publication d‚Äôune nouvelle version dans CodeArtifact et d√©clenche l‚Äôex√©cution de CodePipeline.\nLes options avec ECR ne conviennent pas : ECR sert aux images Docker/OCI, pas aux .jar.\nL‚Äôoption S3 + SNS est moins adapt√©e : S3 stocke des fichiers, mais ce n‚Äôest pas un gestionnaire de d√©pendances, et le d√©clenchement standard et propre pour ce cas est via √©v√©nements (EventBridge) li√©s au d√©p√¥t de packages.\nDonc D r√©pond exactement : d√©p√¥t de d√©pendances + √©v√©nement automatique pour d√©marrer le pipeline.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:394:71127814877820a2",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 394,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a web application that is hosted on AWS. The application is behind an Amazon CloudFront distribution. A developer needs a dashboard to monitor error rates and anomalies of the CloudFront distribution as frequently as possible.Which combination of steps should the developer take to meet these requirements? (Choose two.)",
      "choices": {
        "A": "Stream the CloudFront distribution logs to an Amazon S3 bucket. Detect anomalies and error rates by using Amazon Athena.",
        "B": "Enable real-time logs on the CloudFront distribution. Create a data stream in Amazon Kinesis Data Streams.",
        "C": "Set up Amazon Kinesis Data Streams to send the logs to Amazon OpenSearch Service by using an AWS Lambda function. Make a dashboard in OpenSearch Dashboards.",
        "D": "Stream the CloudFront distribution logs to Amazon Kinesis Data Firehose.",
        "E": "Set up Amazon Kinesis Data Firehose to send the logs to AWS CloudTrail. Create CloudTrail metrics, alarms, and dashboards."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143083-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 29, 2024, 10:18 p.m.",
      "textHash": "71127814877820a2",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:d40cea54",
      "frExplanation": "Objectif : surveiller les erreurs et d√©tecter des anomalies CloudFront le plus souvent possible (quasi temps r√©el).\nCloudFront est un CDN : il sert le contenu et peut produire des logs (journaux) sur les requ√™tes et les erreurs.\nLes logs ‚Äúclassiques‚Äù vers S3 arrivent avec du d√©lai (minutes/heures) : ce n‚Äôest pas id√©al pour une surveillance tr√®s fr√©quente.\nLes ‚Äúreal-time logs‚Äù de CloudFront envoient les √©v√©nements presque imm√©diatement.\nPour recevoir ce flux en continu, on cr√©e un flux dans Amazon Kinesis Data Streams (service de streaming temps r√©el).\nDonc il faut activer les real-time logs sur la distribution CloudFront et les envoyer vers Kinesis Data Streams.\nAthena (requ√™tes sur S3) est plut√¥t pour l‚Äôanalyse a posteriori, pas pour un tableau de bord temps r√©el.\nCloudTrail sert √† auditer les appels API AWS, pas √† analyser les erreurs de trafic CloudFront.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine le portail du lyc√©e qui laisse entrer les √©l√®ves. CloudFront, c‚Äôest ce portail: il distribue l‚Äôacc√®s au site et peut noter ce qui se passe. Tu veux un tableau au bureau du CPE qui affiche les probl√®mes presque en direct (bagarres, retards).**\n\nConcept: pour voir les erreurs ‚Äúle plus souvent possible‚Äù, il faut des infos en temps r√©el, pas un rapport de fin de journ√©e.\nLes ‚Äúlogs‚Äù sont comme le cahier o√π le portail note chaque passage et chaque souci.\nOption B: activer les ‚Äúreal-time logs‚Äù, c‚Äôest demander au portail d‚Äôenvoyer chaque incident tout de suite.\nCr√©er un ‚Äúdata stream‚Äù (Kinesis Data Streams), c‚Äôest comme un tuyau qui transporte ces messages en continu vers ton √©cran.\nDonc tu peux surveiller les erreurs et rep√©rer des anomalies presque instantan√©ment.\nLes autres options ressemblent √†: stocker les cahiers puis analyser plus tard (S3/Athena) ou faire des d√©tours inutiles.\nCloudTrail, c‚Äôest plut√¥t le journal des actions ‚Äúadministratives‚Äù sur AWS, pas le meilleur pour les erreurs CloudFront.\nDonc B est la bonne r√©ponse pour une surveillance la plus fr√©quente possible.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:177:8e490ebeb8c30695",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 177,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is expanding the compatibility of its photo-sharing mobile app to hundreds of additional devices with unique screen dimensions and resolutions. Photos are stored in Amazon S3 in their original format and resolution. The company uses an Amazon CloudFront distribution to serve the photos. The app includes the dimension and resolution of the display as GET parameters with every request.A developer needs to implement a solution that optimizes the photos that are served to each device to reduce load time and increase photo quality.Which solution will meet these requirements MOST cost-effectively?",
      "choices": {
        "A": "Use S3 Batch Operations to invoke an AWS Lambda function to create new variants of the photos with the required dimensions and resolutions. Create a dynamic CloudFront origin that automatically maps the request of each device to the corresponding photo variant.",
        "B": "Use S3 Batch Operations to invoke an AWS Lambda function to create new variants of the photos with the required dimensions and resolutions. Create a Lambda@Edge function to route requests to the corresponding photo variant by using request headers.",
        "C": "Create a Lambda@Edge function that optimizes the photos upon request and returns the photos as a response. Change the CloudFront TTL cache policy to the maximum value possible.",
        "D": "Create a Lambda@Edge function that optimizes the photos upon request and returns the photos as a response. In the same function, store a copy of the processed photos on Amazon S3 for subsequent requests."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122597-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:32 a.m.",
      "textHash": "8e490ebeb8c30695",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Objectif : servir une photo adapt√©e (taille/r√©solution) √† chaque appareil, sans tout pr√©-calculer pour des centaines de formats.\nAmazon S3 stocke les originaux, et CloudFront est un CDN qui met en cache pr√®s des utilisateurs pour acc√©l√©rer.\nLambda@Edge ex√©cute du code au plus pr√®s de l‚Äôutilisateur quand une requ√™te arrive sur CloudFront.\nLa solution D traite l‚Äôimage ¬´ √† la demande ¬ª selon les param√®tres GET (dimensions/r√©solution), puis renvoie l‚Äôimage optimis√©e.\nEnsuite, la fonction enregistre la version trait√©e dans S3 : les prochaines requ√™tes identiques peuvent r√©utiliser cette variante (et √™tre cach√©es par CloudFront), donc moins de calcul.\nC est moins bon car augmenter le TTL ne suffit pas : sans stocker la variante, on risque de retraiter souvent (cache manqu√©, invalidations, nouvelles combinaisons).\nA et B pr√©-g√©n√®rent des variantes via S3 Batch : avec des centaines d‚Äôappareils et des param√®tres vari√©s, cela explose le nombre de fichiers et le co√ªt de stockage/traitement.\nDonc D est le plus rentable : g√©n√©ration √† la demande + r√©utilisation via stockage et cache.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une pizzeria qui re√ßoit des commandes avec la taille exacte (petite, moyenne, grande) √©crite sur le ticket.**\n\nConcept : S3 = le frigo avec les pizzas ‚Äúformat g√©ant‚Äù (photos originales). CloudFront = des livreurs rapides proches des clients. Les param√®tres GET = la taille demand√©e par le t√©l√©phone.\nProbl√®me : si tu cuisines TOUTES les tailles possibles √† l‚Äôavance (A/B), tu gaspilles du temps et des ingr√©dients, car il y a des centaines d‚Äô√©crans diff√©rents.\nSolution D : le livreur (Lambda@Edge) pr√©pare la pizza √† la bonne taille au moment de la commande (optimise l‚Äôimage √† la demande).\nPuis il met une copie de cette taille dans le frigo (S3) pour la prochaine fois.\nR√©sultat : la 1re commande co√ªte un peu, mais les suivantes sont rapides et pas ch√®res.\nC est moins bien : tu refais la ‚Äúcuisine‚Äù √† chaque fois et tu ne gardes pas la bonne taille en stock.\nDonc D est le plus √©conomique et am√©liore vitesse + qualit√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:411:38f5990c2c1b6b50",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 411,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to freeze changes to an AWS CodeCommit repository before a production release. The developer will work on new features while a quality assurance (QA) team tests the release.The QA testing and all bug fixes must take place in isolation from the main branch. After the release, the developer must integrate all bug fixes into the main branch.Which solution will meet these requirements?",
      "choices": {
        "A": "Create a release branch from the latest Git commit that will be in the release. Apply fixes to the release branch. Continue developing new features, and merge the features into the main branch. Merge the release branch into the main branch after the release.",
        "B": "Create a Git tag on the latest Git commit that will be in the release. Continue developing new features, and merge the features into the main branch. Apply fixes to the main branch. Update the Git tag for the release to be on the latest commit on the main branch.",
        "C": "Create a release branch from the latest Git commit that will be in the release. Apply fixes to the release branch. Continue developing new features, and merge the features into the main branch. Rebase the main branch onto the release branch after the release.",
        "D": "Create a Git tag on the latest Git commit that will be in the release. Continue developing new features, and merge the features into the main branch. Apply the Git commits for fixes to the Git tag for the release."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144464-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 24, 2024, 1:02 a.m.",
      "textHash": "38f5990c2c1b6b50",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:51aa2d14",
      "frExplanation": "AWS CodeCommit est un service AWS qui h√©berge des d√©p√¥ts Git (comme GitHub, mais dans AWS). Ici, on veut ‚Äúgeler‚Äù ce qui part en production, pendant que le d√©veloppeur continue d‚Äôajouter des nouveaut√©s.\nLa bonne pratique Git est de cr√©er une branche de release : c‚Äôest une copie isol√©e du code √† livrer √† un instant donn√©.\nLa QA teste uniquement cette branche de release, et les corrections de bugs sont faites sur cette m√™me branche : ainsi, elles n‚Äôimpactent pas la branche main.\nEn parall√®le, le d√©veloppeur peut continuer les nouvelles fonctionnalit√©s sur d‚Äôautres branches et les fusionner dans main sans polluer la release.\nApr√®s la mise en production, on fusionne (merge) la branche de release dans main pour r√©cup√©rer toutes les corrections de bugs valid√©es.\nC‚Äôest exactement ce que d√©crit la r√©ponse A : isolation pendant les tests + int√©gration propre des correctifs apr√®s la release.\nLes tags (B, D) ne sont pas faits pour recevoir des commits : un tag pointe juste vers un commit, ce n‚Äôest pas une zone de travail.\nLe rebase (C) r√©√©crit l‚Äôhistorique et complique l‚Äôint√©gration; un merge de la branche de release vers main est plus simple et s√ªr pour une release.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üéÆ Imagine ton jeu vid√©o favori** : les d√©veloppeurs veulent tester une nouvelle arme l√©gendaire avec seulement 10% des joueurs.\n\n**AppConfig** = C'est comme **les mises √† jour silencieuses** üîÑ - Active/d√©sactive des fonctionnalit√©s sans changer le code (interrupteurs).\n\n**AppSync** = C'est comme **les param√®tres du jeu en ligne** ‚öôÔ∏è - G√®re QUI voit QUOI en temps r√©el (\"Montre la nouvelle arme aux joueurs VIP\").\n\n**üß† Mn√©motechnique :** \"App**Config** = **CONFiguration** discr√®te\" | \"App**Sync** = **SYNChronise** les joueurs\"\n\n**Pourquoi AppConfig ici :** C'est pour activer/d√©sactiver des boutons/fonctionnalit√©s, pas pour synchroniser des donn√©es entre utilisateurs.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:403:8061281339198cd7",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 403,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer needs to store files in an Amazon S3 bucket for a company's application. Each S3 object can have multiple versions. The objects must be permanently removed 1 year after object creation.The developer creates an S3 bucket that has versioning enabled.What should the developer do next to meet the data retention requirements?",
      "choices": {
        "A": "Create an S3 Lifecycle rule on the S3 bucket. Configure the rule to expire current versions of objects and permanently delete noncurrent versions 1 year after object creation.",
        "B": "Create an event notification for all object creation events in the S3 bucket. Configure the event notification to invoke an AWS Lambda function. Program the Lambda function to check the object creation date and to delete the object if the object is older than 1 year.",
        "C": "Create an event notification for all object removal events in the S3 bucket. Configure the event notification to invoke an AWS Lambda function. Program the Lambda function to check the object creation date and to delete the object if the object is older than 1 year.",
        "D": "Create an S3 Lifecycle rule on the S3 bucket. Configure the rule to delete expired object delete markers and permanently delete noncurrent versions 1 year after object creation."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/144455-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 23, 2024, 8:07 p.m.",
      "textHash": "8061281339198cd7",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:s3:159771b3",
      "frExplanation": "Amazon S3 est un service de stockage de fichiers (objets) dans un ‚Äúbucket‚Äù. Avec le versioning, un m√™me fichier peut avoir plusieurs versions : une version ‚Äúcourante‚Äù (current) et des versions ‚Äúanciennes‚Äù (noncurrent).\nLe besoin dit : supprimer d√©finitivement les objets 1 an apr√®s leur cr√©ation. Il faut donc g√©rer √† la fois la version courante et les versions non courantes.\nLa fa√ßon standard et automatique dans S3 est une r√®gle ‚ÄúLifecycle‚Äù (cycle de vie) : S3 applique la suppression selon l‚Äô√¢ge, sans code.\nL‚Äôoption A configure exactement cela : expirer la version courante et supprimer d√©finitivement les versions non courantes apr√®s 1 an.\nLes options avec Lambda (B/C) ajoutent du code, des d√©clencheurs et des risques d‚Äôoubli/erreur, alors que S3 sait le faire nativement.\nL‚Äôoption D parle surtout des ‚Äúdelete markers‚Äù (marqueurs de suppression) : utile dans certains cas, mais ne garantit pas la suppression de la version courante 1 an apr√®s cr√©ation.\nDonc la bonne r√©ponse est A : une r√®gle Lifecycle qui traite current + noncurrent pour respecter la r√©tention.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**üçî Imagine une cha√Æne de resto fast-food** : plusieurs recettes de burger qui √©voluent.\n\n**Version** = C'est comme **le num√©ro de recette imprim√©** üìú - Immuable, grav√© dans le marbre (v1 = classique, v2 = nouvelle).\n\n**Alias** = C'est comme **les pancartes \"SP√âCIALIT√â DU MOMENT\"** ü™ß - Pointe vers une version, changeable (\"prod\" ‚Üí v2, \"beta\" ‚Üí v3).\n\n**üß† Mn√©motechnique :** \"**VER**sion = **VER**rouill√© (chiffre fixe)\" | \"**AL**ias = **AL**ternatif (√©tiquette mobile)\"\n\n**Pourquoi cr√©er une version :** Pour figer l'ancien code avant de modifier, pouvoir revenir en arri√®re si probl√®me.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:144:304fb8011316fa36",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 144,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an Amazon S3 bucket containing premier content that it intends to make available to only paid subscribers of its website. The S3 bucket currently has default permissions of all objects being private to prevent inadvertent exposure of the premier content to non-paying website visitors.How can the company limit the ability to download a premier content file in the S3 bucket to paid subscribers only?",
      "choices": {
        "A": "Apply a bucket policy that allows anonymous users to download the content from the S3 bucket.",
        "B": "Generate a pre-signed object URL for the premier content file when a paid subscriber requests a download.",
        "C": "Add a bucket policy that requires multi-factor authentication for requests to access the S3 bucket objects.",
        "D": "Enable server-side encryption on the S3 bucket for data protection against the non-paying website visitors."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122562-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:33 a.m.",
      "textHash": "304fb8011316fa36",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Amazon S3 est un service de stockage de fichiers. Ici, les objets sont priv√©s, donc personne ne peut les t√©l√©charger sans autorisation.\nLe besoin est : autoriser le t√©l√©chargement uniquement aux abonn√©s payants, au cas par cas.\nUne URL pr√©-sign√©e (pre-signed URL) est un lien temporaire g√©n√©r√© par votre application (ou un backend) avec des droits pr√©cis (ex: t√©l√©charger un fichier) et une dur√©e d‚Äôexpiration.\nQuand un utilisateur prouve qu‚Äôil est abonn√© (connexion + paiement valid√©), le site g√©n√®re ce lien et le lui donne.\nLe lien fonctionne seulement pendant un temps limit√© et uniquement pour l‚Äôobjet vis√©, sans rendre le bucket public.\nA est faux car ‚Äúanonymous‚Äù rendrait le contenu accessible √† tout le monde.\nC (MFA) est plut√¥t pour des humains avec comptes AWS, pas pour des visiteurs d‚Äôun site web.\nD (chiffrement) prot√®ge les donn√©es au repos, mais n‚Äôemp√™che pas un visiteur non pay√© de t√©l√©charger si l‚Äôacc√®s est autoris√©.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine une biblioth√®que du lyc√©e avec une salle ‚ÄúVIP‚Äù o√π il y a des mangas rares. La porte est ferm√©e √† cl√© pour tout le monde, et seuls les √©l√®ves qui ont pay√© un abonnement peuvent emprunter un manga.**\n\nConcept : Amazon S3, c‚Äôest comme cette biblioth√®que qui stocke des fichiers. ‚ÄúPriv√©‚Äù = porte ferm√©e, personne ne peut prendre un fichier directement.\nPour laisser seulement les abonn√©s t√©l√©charger, le site donne un ‚Äúticket d‚Äôemprunt‚Äù sp√©cial, valable juste pour ce manga et pendant peu de temps.\nCe ticket, c‚Äôest l‚ÄôURL pr√©-sign√©e : un lien temporaire qui autorise le t√©l√©chargement uniquement si tu l‚Äôas re√ßu.\nPourquoi B : quand un abonn√© clique ‚Äút√©l√©charger‚Äù, le site g√©n√®re ce lien-ticket et le donne √† lui seul.\nPourquoi pas A : √ßa revient √† laisser la salle VIP ouverte √† tout le monde.\nPourquoi pas C : demander un code en plus peut aider, mais √ßa ne g√®re pas simplement ‚Äúun lien temporaire par fichier‚Äù pour chaque abonn√©.\nPourquoi pas D : chiffrer, c‚Äôest mettre le manga dans une bo√Æte ferm√©e, mais si tu donnes la bo√Æte √† tout le monde, √ßa ne r√®gle pas ‚Äúqui a le droit de l‚Äôemprunter‚Äù.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:362:ddc9cdf98ca6d269",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 362,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer manages an application that writes customer orders to an Amazon DynamoDB table. The orders use customer_id as the partition key, order_id as the sort key, and order_date as an attribute. A new access pattern requires accessing data by order_date and order_id. The developer needs to implement a new AWS Lambda function to support the new access pattern.How should the developer support the new access pattern in the MOST operationally efficient way?",
      "choices": {
        "A": "Add a new local secondary index (LSI) to the DynamoDB table that specifies order_date as the partition key and order_id as the sort key. Write the new Lambda function to query the new LSI index.",
        "B": "Write the new Lambda function to scan the DynamoDB table. In the Lambda function, write a method to retrieve and combine results by order_date and order_id.",
        "C": "Add a new global secondary index (GSI) to the DynamoDB table that specifies order_date as the partition key and order_id as the sort key. Write the new Lambda function to query the new GSI index.",
        "D": "Enable DynamoDB Streams on the table. Choose the new and old images information to write to the DynamoDB stream. Write the new Lambda function to query the DynamoDB stream"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143367-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 5, 2024, 4:51 p.m.",
      "textHash": "ddc9cdf98ca6d269",
      "rawFormat": "discussion-md",
      "conceptKey": "dynamodb_keys_indexes",
      "frExplanation": "DynamoDB est une base NoSQL o√π l‚Äôon lit vite si on utilise une cl√© adapt√©e (partition key + sort key). Ici, la table est organis√©e par customer_id (partition) puis order_id (tri), donc lire par order_date n‚Äôest pas naturel.\nUn nouveau besoin veut chercher par order_date + order_id : il faut un ‚Äúchemin de lecture‚Äù optimis√© sans parcourir toute la table.\nUn GSI (Global Secondary Index) est un index secondaire qui permet de d√©finir une autre cl√© de partition et de tri, ind√©pendante de la cl√© principale, et il peut √™tre ajout√© apr√®s coup.\nAvec un GSI ayant order_date comme partition key et order_id comme sort key, la fonction Lambda peut faire une requ√™te (Query) efficace directement sur cet index.\nUn Scan (option B) lit toute la table puis filtre : c‚Äôest lent, co√ªteux et peu efficace op√©rationnellement.\nUn LSI (option A) doit garder la m√™me partition key que la table (customer_id) et doit √™tre cr√©√© √† la cr√©ation de la table : on ne peut pas mettre order_date en partition key ni l‚Äôajouter facilement apr√®s.\nDynamoDB Streams (option D) sert √† r√©agir aux changements (√©v√©nements), pas √† faire des requ√™tes de lecture par date.\nDonc la solution la plus simple et efficace √† op√©rer est d‚Äôajouter un GSI et de requ√™ter ce GSI (C).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une biblioth√®que au lyc√©e : chaque livre est rang√© d‚Äôabord par ‚Äú√©l√®ve‚Äù (customer_id), puis par ‚Äúnum√©ro de devoir‚Äù (order_id). La date (order_date) est juste une info √©crite sur la couverture, pas un crit√®re de rangement.**\n\nConcept : si tu veux retrouver vite, il faut un bon ‚Äúclassement‚Äù (un index), pas feuilleter tous les livres.\nIci, on veut chercher par date + num√©ro de commande. Or la table est rang√©e par client + num√©ro.\nB (scan) = parcourir toute la biblioth√®que pour trouver les livres d‚Äôune date : trop lent et p√©nible √† g√©rer.\nA (LSI) ne marche pas : un LSI garde le m√™me ‚Äúpremier rangement‚Äù que la table (le client). Tu ne peux pas changer pour ‚Äúdate‚Äù.\nD (Streams) = un journal des changements, pas un syst√®me de recherche : tu ne vas pas chercher des commandes dedans.\nC (GSI) = cr√©er une nouvelle √©tag√®re de classement en plus, rang√©e par date (order_date) puis num√©ro (order_id).\nLa Lambda (le petit robot biblioth√©caire) va directement √† cette √©tag√®re et trouve vite. C‚Äôest le plus simple √† exploiter.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:183:64048915d69bbd37",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 183,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is building a microservices application that consists of many AWS Lambda functions. The development team wants to use AWS Serverless Application Model (AWS SAM) templates to automatically test the Lambda functions. The development team plans to test a small percentage of traffic that is directed to new updates before the team commits to a full deployment of the application.Which combination of steps will meet these requirements in the MOST operationally efficient way? (Choose two.)",
      "choices": {
        "A": "Use AWS SAM CLI commands in AWS CodeDeploy to invoke the Lambda functions to test the deployment.",
        "B": "Declare the EventInvokeConfig on the Lambda functions in the AWS SAM templates with OnSuccess and OnFailure configurations.",
        "C": "Enable gradual deployments through AWS SAM templates.",
        "D": "Set the deployment preference type to Canary10Percent30Minutes. Use hooks to test the deployment.",
        "E": "Set the deployment preference type to Linear10PercentEvery10Minutes. Use hooks to test the deployment."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122606-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:43 a.m.",
      "textHash": "64048915d69bbd37",
      "rawFormat": "discussion-md",
      "conceptKey": "codepipeline_cicd",
      "frExplanation": "Ici, l‚Äôobjectif est de d√©ployer des mises √† jour de fonctions AWS Lambda (du code ex√©cut√© √† la demande) en limitant le risque : on envoie d‚Äôabord une petite partie du trafic vers la nouvelle version, puis on augmente si tout va bien.\nAWS SAM (Serverless Application Model) est un fichier ‚Äútemplate‚Äù qui d√©crit l‚Äôinfrastructure serverless et peut piloter le d√©ploiement.\nLa fa√ßon la plus simple et automatis√©e avec SAM est d‚Äôactiver les ‚Äúgradual deployments‚Äù dans le template : SAM s‚Äôappuie sur CodeDeploy pour faire du canary/linear sans scripts complexes.\nCela r√©pond exactement au besoin ‚Äútester un petit pourcentage de trafic avant le d√©ploiement complet‚Äù.\nLes options EventInvokeConfig (succ√®s/√©chec) concernent surtout le traitement asynchrone et les destinations, pas le routage progressif du trafic.\nUtiliser des commandes SAM CLI dans CodeDeploy est moins efficace op√©rationnellement (plus de bricolage) que la fonctionnalit√© native de d√©ploiement progressif via SAM.\nDonc la bonne r√©ponse est d‚Äôactiver les d√©ploiements progressifs via les templates SAM (C).",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu testes une nouvelle recette de pizza √† la cantine. Tu ne la sers pas √† toute l‚Äô√©cole d‚Äôun coup : tu la fais go√ªter √† quelques √©l√®ves d‚Äôabord, puis tu d√©cides si tu la mets au menu pour tout le monde.**\n\nConcept : ‚Äúd√©ployer progressivement‚Äù, c‚Äôest comme faire go√ªter la pizza √† un petit groupe avant de la servir √† tous.\nAWS SAM, c‚Äôest la ‚Äúfiche de recette‚Äù qui d√©crit comment ton appli (plein de petites fonctions Lambda) est install√©e et test√©e.\nLa bonne r√©ponse C (‚ÄúEnable gradual deployments through AWS SAM templates‚Äù) veut dire : dans la fiche de recette, tu actives le mode ‚Äúd√©gustation progressive‚Äù.\nComme √ßa, seulement un petit pourcentage des √©l√®ves (du trafic) re√ßoit la nouvelle pizza (la mise √† jour) au d√©but.\nSi √ßa se passe bien, tu augmentes jusqu‚Äô√† toute l‚Äô√©cole (d√©ploiement complet).\nC‚Äôest le plus efficace car tout est g√©r√© automatiquement par la fiche SAM, sans bricoler des √©tapes manuelles.\nLes autres choix parlent de d√©tails de tests ou de r√©glages pr√©cis, mais l‚Äôexigence cl√© est : activer le d√©ploiement progressif via SAM.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:323:b6d63fee3c6f7cfa",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 323,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a microservices-based application by using Python on AWS and several AWS services. The developer must use AWS X-Ray. The developer views the service map by using the console to view the service dependencies. During testing, the developer notices that some services are missing from the service map.What can the developer do to ensure that all services appear in the X-Ray service map?",
      "choices": {
        "A": "Modify the X-Ray Python agent configuration in each service to increase the sampling rate.",
        "B": "Instrument the application by using the X-Ray SDK for Python. Install the X-Ray SDK for all the services that the application uses.",
        "C": "Enable X-Ray data aggregation in Amazon CloudWatch Logs for all the services that the application uses.",
        "D": "Increase the X-Ray service map timeout value in the X-Ray console."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133633-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 12, 2024, 6:14 p.m.",
      "textHash": "b6d63fee3c6f7cfa",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "AWS X-Ray sert √† ¬´ tracer ¬ª une requ√™te quand elle traverse plusieurs services (microservices) pour voir les d√©pendances dans une carte (service map).\nSi un service n‚Äôappara√Æt pas, c‚Äôest souvent parce qu‚Äôil n‚Äôenvoie pas de traces X-Ray : X-Ray ne peut pas deviner ce qui se passe √† l‚Äôint√©rieur d‚Äôun service non instrument√©.\nLa solution est donc d‚Äôajouter l‚Äôinstrumentation dans le code de chaque microservice avec le X-Ray SDK pour Python (et/ou le daemon/agent selon le runtime) afin qu‚Äôil cr√©e des segments/subsegments et propage l‚ÄôID de trace.\nEn installant et configurant le SDK dans tous les services, chaque appel entrant/sortant est enregistr√© et la carte peut relier correctement les services entre eux.\nAugmenter le sampling (A) peut aider √† voir plus de requ√™tes, mais ne fera pas appara√Ætre un service qui n‚Äô√©met aucune trace.\nCloudWatch Logs (C) n‚Äôest pas la source de la service map X-Ray.\nLe timeout de la console (D) n‚Äôajoute pas de donn√©es : il ne fait qu‚Äôafficher ce qui existe d√©j√†.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine une journ√©e au coll√®ge : tu veux une carte qui montre quels √©l√®ves (services) parlent √† quels autres √©l√®ves, pour comprendre qui d√©pend de qui.**\n\nConcept : AWS X-Ray, c‚Äôest comme un surveillant qui dessine la carte des discussions entre √©l√®ves. Mais il ne peut voir que les √©l√®ves qui portent un badge.\nSi certains services manquent sur la carte, c‚Äôest souvent qu‚Äôils n‚Äôont pas le ‚Äúbadge‚Äù qui dit √† X-Ray : ¬´ je participe, je note mes √©changes ¬ª.\nLa bonne action est donc B : ajouter le badge partout, en ‚Äúinstrumentant‚Äù chaque service avec le X-Ray SDK pour Python (un petit kit qui fait √©crire et envoyer les traces).\nA (augmenter l‚Äô√©chantillonnage) revient √† demander au surveillant de regarder plus souvent, mais si un √©l√®ve n‚Äôa pas de badge, il reste invisible.\nC parle de logs (cahiers de notes) : √ßa ne remplace pas les badges X-Ray pour la carte des d√©pendances.\nD (timeout) c‚Äôest attendre plus longtemps, mais √ßa ne fait pas appara√Ætre un √©l√®ve qui ne s‚Äôest jamais signal√©.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:254:edaf46867c75bccb",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 254,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a machine learning (ML) pipeline in AWS Step Functions that contains AWS Lambda functions. The developer has configured an Amazon Simple Queue Service (Amazon SQS) queue to deliver ML model parameters to the ML pipeline to train ML models. The developer uploads the trained models are uploaded to an Amazon S3 bucket.The developer needs a solution that can locally test the ML pipeline without making service integration calls to Amazon SQS and Amazon S3.Which solution will meet these requirements?",
      "choices": {
        "A": "Use the Amazon CodeGuru Profiler to analyze the Lambda functions used in the AWS Step Functions pipeline.",
        "B": "Use the AWS Step Functions Local Docker Image to run and locally test the Lambda functions.",
        "C": "Use the AWS Serverless Application Model (AWS SAM) CLI to run and locally test the Lambda functions.",
        "D": "Use AWS Step Functions Local with mocked service integrations."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124865-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 29, 2023, 2:13 a.m.",
      "textHash": "edaf46867c75bccb",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, on veut tester localement un workflow AWS Step Functions (un orchestrateur d‚Äô√©tapes) qui appelle des fonctions AWS Lambda (code ex√©cut√© √† la demande), sans contacter de vrais services AWS.\nLe pipeline lit des param√®tres via Amazon SQS (file de messages) et √©crit des mod√®les dans Amazon S3 (stockage d‚Äôobjets).\nSi on teste ‚Äúpour de vrai‚Äù, Step Functions ferait des appels r√©seau vers SQS et S3, ce qui n‚Äôest pas souhait√© (co√ªt, d√©pendances, besoin d‚Äôun compte/config).\nLa bonne approche est d‚Äôutiliser Step Functions Local (version ex√©cutable en local) et de ‚Äúmock‚Äù les int√©grations : on simule les r√©ponses de SQS/S3.\nAinsi, le workflow s‚Äôex√©cute comme en production, mais les appels √† SQS/S3 sont remplac√©s par des r√©ponses fictives contr√¥l√©es.\nB est incomplet : l‚Äôimage Docker Step Functions Local ne suffit pas si on ne mocke pas les int√©grations.\nC (SAM CLI) aide surtout √† tester Lambda/API localement, pas √† simuler les int√©grations Step Functions vers SQS/S3.\nA (CodeGuru Profiler) sert √† profiler les performances, pas √† tester un pipeline sans appels aux services.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o o√π tu testes une mission compl√®te sur ton PC, sans te connecter aux serveurs en ligne. Les ‚Äúserveurs‚Äù donnent des objets (param√®tres) et stockent tes r√©compenses (mod√®les) dans un coffre distant.**\n\nLe pipeline, c‚Äôest la mission avec plusieurs √©tapes. Step Functions, c‚Äôest le ‚Äúsc√©nario‚Äù qui dit quelle √©tape se lance apr√®s l‚Äôautre. Les fonctions Lambda, ce sont les petits ‚Äúbots‚Äù qui font chaque action. SQS, c‚Äôest comme une bo√Æte aux lettres qui apporte les param√®tres d‚Äôentra√Ænement. S3, c‚Äôest comme un grand coffre de stockage o√π on d√©pose le mod√®le entra√Æn√©. Pour tester en local sans appeler SQS et S3, il faut des ‚Äúfaux serveurs‚Äù qui r√©pondent comme eux, mais sans Internet. Step Functions Local permet de lancer le sc√©nario sur ton PC. Et les mocked service integrations, c‚Äôest exactement le mode ‚Äúobjets factices‚Äù : tu simules SQS et S3. Donc D est la bonne r√©ponse, car tu testes tout le pipeline localement sans vraies connexions √† SQS/S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:400:3d5da8e6f3ae4632",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 400,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that uses an Amazon S3 bucket for object storage. A developer needs to configure in-transit encryption for the S3 bucket. All the S3 objects containing personal data needs to be encrypted at rest with AWS Key Management Service (AWS KMS) keys, which can be rotated on demand.Which combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Write an S3 bucket policy to allow only encrypted connections over HTTPS by using permissions boundary.",
        "B": "Configure an S3 bucket policy to enable client-side encryption for the objects containing personal data by using an AWS KMS customer managed key.",
        "C": "Configure the application to encrypt the objects by using an AWS KMS customer managed key before uploading the objects containing personal data to Amazon S3.",
        "D": "Write an S3 bucket policy to allow only encrypted connections over HTTPS by using the aws:SecureTransport condition.",
        "E": "Configure S3 Block Public Access settings for the S3 bucket to allow only encrypted connections over HTTPS."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143803-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 3:50 p.m.",
      "textHash": "3d5da8e6f3ae4632",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Objectif 1 : chiffrer ‚Äúen transit‚Äù = prot√©ger les donn√©es pendant le trajet entre l‚Äôapplication et Amazon S3. Le moyen simple est d‚Äôobliger HTTPS (TLS). Dans une policy S3, la condition aws:SecureTransport permet de refuser toute requ√™te en HTTP.\nObjectif 2 : chiffrer ‚Äúau repos‚Äù = prot√©ger les fichiers une fois stock√©s dans S3. On veut utiliser AWS KMS (service qui g√®re des cl√©s de chiffrement) avec une cl√© g√©r√©e par le client, car on peut d√©clencher une rotation √† la demande.\nLa bonne action c√¥t√© application est de chiffrer avant l‚Äôenvoi : l‚Äôapplication utilise une cl√© KMS (customer managed key) pour chiffrer les objets contenant des donn√©es personnelles, puis les t√©l√©verse dans S3 (c‚Äôest le ‚Äúclient-side encryption‚Äù).\nDonc C r√©pond au chiffrement au repos avec KMS et permet la rotation de la cl√©.\nPour le chiffrement en transit, on ajoute une bucket policy qui n‚Äôaccepte que HTTPS via aws:SecureTransport.\nLes autres choix : Block Public Access ne force pas HTTPS, et ‚Äúpermissions boundary‚Äù n‚Äôest pas le m√©canisme pour imposer HTTPS sur S3.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un casier au lyc√©e (le ‚Äúbucket S3‚Äù) o√π tu ranges des dossiers. Tu veux 1) que le trajet jusqu‚Äôau casier soit s√©curis√© (personne ne lit le dossier dans le couloir), et 2) que le dossier soit aussi verrouill√© une fois dans le casier avec une cl√© sp√©ciale que tu peux changer quand tu veux (cl√© KMS).**\n\nConcept : ‚Äúchiffrer en transit‚Äù = prot√©ger le dossier pendant le trajet (comme une enveloppe scell√©e). ‚ÄúChiffrer au repos‚Äù = le dossier est verrouill√© dans le casier (m√™me si quelqu‚Äôun ouvre le casier, il ne peut pas lire). La r√©ponse C est bonne car elle dit : l‚Äôappli verrouille (chiffre) les dossiers sensibles AVANT de les mettre dans le casier, avec une cl√© KMS g√©r√©e par toi, que tu peux faire tourner (changer) quand tu veux. Les autres choix parlent surtout de forcer le trajet en HTTPS (le couloir s√©curis√©) ou de r√©glages ‚Äúpublic‚Äù, mais ils ne garantissent pas que les fichiers perso soient bien verrouill√©s avec ta cl√© KMS. Donc pour √™tre s√ªr du verrouillage des donn√©es perso au repos avec rotation de cl√©, il faut chiffrer c√¥t√© appli avant l‚Äôenvoi : C.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:388:be6a530dd02ee1eb",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 388,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has an application that receives batches of orders from partners every day. The application uses an AWS Lambda function to process the batches.If a batch contains no orders, the Lambda function must publish to an Amazon Simple Notification Service (Amazon SNS) topic as soon as possible.Which combination of steps will meet this requirement with the LEAST implementation effort? (Choose two.)",
      "choices": {
        "A": "Update the existing Lambda function's code to send an Amazon CloudWatch custom metric for the number of orders in a batch for each partner.",
        "B": "Create a new Lambda function as an Amazon Kinesis data stream consumer. Configure the new Lambda function to track orders and to publish to the SNS topic when a batch contains no orders.",
        "C": "Set up an Amazon CloudWatch alarm that will send a notification to the SNS topic when the value of the custom metric is 0.",
        "D": "Schedule a new Lambda function to analyze Amazon CloudWatch metrics every 24 hours to identify batches that contain no orders. Configure the Lambda function to publish to the SNS topic.",
        "E": "Modify the existing Lambda function to log orders to an Amazon Kinesis data stream."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143799-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 3:25 p.m.",
      "textHash": "be6a530dd02ee1eb",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, on veut √™tre alert√© imm√©diatement quand un lot (batch) ne contient aucune commande. Le moyen le plus simple est d‚Äôutiliser ce que Lambda peut faire facilement : envoyer une m√©trique.\nAWS Lambda = code qui s‚Äôex√©cute √† la demande. Amazon SNS = service qui envoie des notifications (email, SMS, HTTP, etc.). Amazon CloudWatch = service de m√©triques/alertes.\n√âtape 1 (A) : modifier la fonction Lambda existante pour publier une m√©trique CloudWatch personnalis√©e ‚Äúnombre de commandes dans le lot‚Äù (par partenaire). C‚Äôest juste quelques lignes de code et pas de nouvelle architecture.\n√âtape 2 (C) : cr√©er une alarme CloudWatch qui se d√©clenche quand cette m√©trique vaut 0, et configurer l‚Äôalarme pour notifier le topic SNS.\nPourquoi c‚Äôest le moindre effort : pas besoin d‚Äôajouter Kinesis (streaming) ni une nouvelle fonction planifi√©e ; CloudWatch + alarme g√®rent la d√©tection et l‚Äôenvoi automatiquement.\nLes options avec Kinesis (B, E) ajoutent des services et du suivi inutile. L‚Äôoption D est lente (toutes les 24h) et demande plus de code/maintenance.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine le lyc√©e re√ßoit chaque jour des cartons de devoirs venant de plusieurs coll√®ges partenaires. Un surveillant (la fonction Lambda) ouvre chaque carton et corrige. Si un carton est vide, il faut pr√©venir tout de suite le CPE via un haut-parleur (SNS).**\n\nConcept : pour pr√©venir vite, il faut un ‚Äúcompteur‚Äù simple qui dit combien de copies il y a dans chaque carton, puis une r√®gle qui crie si le compteur vaut 0.\nPourquoi A : on modifie juste le surveillant pour qu‚Äôil note un chiffre ‚Äúnombre de copies‚Äù √† chaque carton (un compteur/mesure). C‚Äôest rapide √† ajouter et √ßa marche pour chaque partenaire.\nEnsuite, une alarme peut dire : ‚Äúsi le compteur = 0, annonce au haut-parleur‚Äù.\nLes autres choix demandent de cr√©er tout un nouveau syst√®me de suivi (comme installer une nouvelle cha√Æne de tri ou un nouveau registre), donc plus de boulot.\nDonc le minimum d‚Äôeffort commence par A : ajouter le compteur du nombre de commandes par lot.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:402:e1169705d5f4c9a0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 402,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company uses AWS CloudFormation templates to manage infrastructure for a public-facing application in its development, pre-production, and production environments. The company needs to scale for increasing customer demand. A developer must upgrade the Amazon RDS DB instance type to a larger instance.The developer deploys an update to the CloudFormation stack with the instance size change in the pre-production environment. The developer notices that the stack is in an UPDATE_ROLLBACK_FAILED slate in CloudFormation.Which option is the cause of this issue?",
      "choices": {
        "A": "The new instance type specified in the CloudFormation template is invalid",
        "B": "The database was deleted or modified manually outside of the CloudFormation stack",
        "C": "There is a syntax error in the CloudFormation template",
        "D": "The developer has insufficient IAM permissions to provision an instance of the specified type"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143030-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "June 28, 2024, 9:20 p.m.",
      "textHash": "e1169705d5f4c9a0",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "CloudFormation est un service AWS qui cr√©e et met √† jour votre infrastructure √† partir d‚Äôun ‚Äútemplate‚Äù (fichier de configuration). Un ‚Äústack update‚Äù modifie les ressources existantes (ici une base Amazon RDS, service de base de donn√©es g√©r√©e). Quand une mise √† jour √©choue, CloudFormation tente automatiquement un ‚Äúrollback‚Äù pour revenir √† l‚Äô√©tat pr√©c√©dent. L‚Äô√©tat UPDATE_ROLLBACK_FAILED signifie que m√™me le retour arri√®re n‚Äôa pas pu √™tre termin√©, souvent parce que l‚Äô√©tat r√©el des ressources ne correspond plus √† ce que CloudFormation croit g√©rer. La cause la plus fr√©quente est qu‚Äôune ressource a √©t√© chang√©e ou supprim√©e manuellement en dehors de CloudFormation (console, CLI, autre outil). Dans ce cas, CloudFormation ne peut ni appliquer la mise √† jour ni restaurer l‚Äôancien √©tat, car la ressource attendue n‚Äôest plus identique. Donc la bonne r√©ponse est B : la base a √©t√© modifi√©e/supprim√©e hors du stack.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine que tu organises la salle de classe avec un plan officiel affich√© au mur (CloudFormation). Le plan dit o√π sont les tables, la taille du tableau, etc. Tu as 3 salles: entra√Ænement (dev), r√©p√©tition g√©n√©rale (pr√©-prod) et spectacle (prod).**\n\nLe plan officiel (CloudFormation) doit √™tre le SEUL √† d√©cider des changements. Tu changes sur le plan: ‚Äúon met un plus grand tableau‚Äù (DB plus puissante). Pendant la mise √† jour, si √ßa rate, le plan essaie de revenir en arri√®re (rollback) pour remettre l‚Äôancien tableau. Mais si quelqu‚Äôun a d√©plac√© ou carr√©ment enlev√© le tableau √† la main, sans suivre le plan, le retour en arri√®re devient impossible. R√©sultat: le prof voit ‚ÄúUPDATE_ROLLBACK_FAILED‚Äù car le plan ne retrouve plus l‚Äôobjet comme pr√©vu. Donc la cause la plus logique est B: la base a √©t√© modifi√©e/supprim√©e manuellement en dehors du stack.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:387:b7ea4901730c2492",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 387,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating a new batch application that will run on an Amazon EC2 instance. The application requires read access to an Amazon S3 bucket. The developer needs to follow security best practices to grant S3 read access to the application.Which solution meets these requirements?",
      "choices": {
        "A": "Add the permissions to an IAM policy. Attach the policy to a role. Attach the role to the EC2 instance profile.",
        "B": "Add the permissions inline to an IAM group. Attach the group to the EC2 instance profile.",
        "C": "Add the permissions to an IAM policy. Attach the policy to a user. Attach the user to the EC2 instance profile.",
        "D": "Add the permissions to an IAM policy. Use IAM web identity federation to access the S3 bucket with the policy."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143764-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 6:09 a.m.",
      "textHash": "b7ea4901730c2492",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Sur AWS, une application sur une instance EC2 ne doit pas utiliser des identifiants ‚Äúutilisateur‚Äù (cl√©s d‚Äôacc√®s) stock√©s sur le serveur : c‚Äôest risqu√© et contraire aux bonnes pratiques.\nPour donner des droits √† une instance EC2, on utilise un r√¥le IAM (IAM Role) : c‚Äôest une identit√© ‚Äúpour un service‚Äù qui peut recevoir des permissions.\nOn √©crit une politique IAM (IAM Policy) qui autorise uniquement la lecture du bucket S3 (principe du moindre privil√®ge).\nOn attache cette politique au r√¥le IAM.\nEnsuite, on associe ce r√¥le √† l‚Äôinstance via un ‚Äúinstance profile‚Äù (le m√©canisme qui relie le r√¥le √† EC2).\nEC2 fournit alors automatiquement des identifiants temporaires √† l‚Äôapplication, sans cl√©s statiques √† g√©rer.\nB est faux car les groupes IAM sont pour des utilisateurs humains, pas pour des instances EC2.\nC est faux car attacher un utilisateur √† une instance profile n‚Äôest pas le mod√®le recommand√© et implique des cl√©s longues dur√©es.\nD concerne la f√©d√©ration web (souvent pour applis mobiles/web), inutile ici pour une appli qui tourne directement sur EC2.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:381:9b939fb7ef558c4e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 381,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer updates an AWS Lambda function that an Amazon API Gateway API uses. The API is the backend for a web application.The developer needs to test the updated Lambda function before deploying the Lambda function to production. The testing must not affect any production users of the web application.Which solution will meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "A": "Create a canary release deployment for the existing API stage. Deploy the API to the existing stage. Test the updated Lambda function by using the existing URL.",
        "B": "Update the API Gateway API endpoint type to private. Deploy the changes to the existing API stage. Test the API by using the existing URL.",
        "C": "Create a new test API stage in API Gateway. Add stage variables to deploy the updated Lambda function to only the test stage. Test the updated Lambda function by using the new stage URL.",
        "D": "Create a new AWS CloudFormation stack to deploy a copy of the entire production API and Lambda function. Use the stack's API URL to test the updated Lambda function."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143721-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 11, 2024, 2:48 p.m.",
      "textHash": "9b939fb7ef558c4e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:edd79777",
      "frExplanation": "Objectif : tester une nouvelle version d‚Äôune fonction AWS Lambda (code ex√©cut√© √† la demande) utilis√©e par Amazon API Gateway (service qui expose une URL d‚ÄôAPI) sans impacter les utilisateurs en production.\nLa solution la plus simple est d‚Äôisoler le trafic de test : on cr√©e un nouveau ¬´ stage ¬ª (environnement) dans API Gateway, par exemple /test, s√©par√© du stage /prod.\nAvec des ¬´ stage variables ¬ª, on peut faire pointer uniquement le stage de test vers la nouvelle version/alias de la Lambda, tandis que le stage de production continue d‚Äôappeler l‚Äôancienne version.\nOn teste alors via la nouvelle URL du stage (ex: .../test) : aucun utilisateur qui appelle .../prod n‚Äôest touch√©.\nC‚Äôest op√©rationnellement efficace : pas besoin de dupliquer toute l‚Äôinfrastructure, juste un stage et une variable.\nA est risqu√© : un canary sur le stage existant enverrait quand m√™me une partie du trafic r√©el vers la nouvelle Lambda.\nB ne r√©pond pas au besoin : rendre l‚ÄôAPI priv√©e change l‚Äôacc√®s r√©seau et peut casser l‚Äôapp, sans isoler une version de Lambda.\nD fonctionne mais est trop lourd : copier toute la stack co√ªte du temps et de la maintenance.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine le site web comme la cantine du lyc√©e. L‚Äôentr√©e (API) est la porte de la cantine, et la cuisine (Lambda) pr√©pare les plats. Tu veux tester une nouvelle recette sans servir les √©l√®ves qui viennent manger d‚Äôhabitude.**\n\nConcept : tu cr√©es une ‚Äúporte de test‚Äù s√©par√©e, qui m√®ne √† une ‚Äúcuisine de test‚Äù. Comme √ßa, les vrais √©l√®ves continuent d‚Äôutiliser la porte normale et ne go√ªtent jamais la recette en test.\nPourquoi C : un ‚Äústage‚Äù dans API Gateway, c‚Äôest comme une porte diff√©rente (URL diff√©rente). En cr√©ant un stage de test, tu as une nouvelle porte juste pour toi.\nLes ‚Äústage variables‚Äù, c‚Äôest comme un panneau derri√®re la porte qui dit : ‚Äúpour cette porte, envoie les commandes vers la cuisine version test‚Äù.\nDonc tu testes la nouvelle Lambda via l‚ÄôURL du stage de test, sans toucher les utilisateurs de production.\nPourquoi pas A : canary = tu fais go√ªter un petit pourcentage d‚Äô√©l√®ves, donc √ßa peut toucher des vrais utilisateurs.\nPourquoi pas B : rendre la porte ‚Äúpriv√©e‚Äù change l‚Äôacc√®s, mais ne cr√©e pas un vrai espace de test s√©par√©.\nPourquoi pas D : recopier toute la cantine est trop lourd √† g√©rer juste pour tester.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:374:38af037365de3f84",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 374,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer creates an AWS Lambda function that is written in Java. During testing, the Lambda function does not work how the developer expected. The developer wants to use tracing capabilities to troubleshoot the problem.Which AWS service should the developer use to accomplish this goal?",
      "choices": {
        "A": "AWS Trusted Advisor",
        "B": "Amazon CloudWatch",
        "C": "AWS X-Ray",
        "D": "AWS CloudTrail"
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143756-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:41 a.m.",
      "textHash": "38af037365de3f84",
      "rawFormat": "discussion-md",
      "conceptKey": "cloudwatch_logs_metrics",
      "frExplanation": "Pour diagnostiquer une fonction AWS Lambda (code Java) qui ne se comporte pas comme pr√©vu, il faut voir ce qui se passe ¬´ √† l‚Äôint√©rieur ¬ª pendant l‚Äôex√©cution.\nAWS X-Ray est un service de tra√ßage distribu√© : il suit une requ√™te de bout en bout et montre une chronologie (traces/segments) avec les appels, la latence, et o√π le temps est perdu ou o√π l‚Äôerreur appara√Æt.\nAvec Lambda, X-Ray permet de visualiser chaque invocation, les d√©pendances (ex: appels HTTP, bases de donn√©es), et d‚Äôidentifier pr√©cis√©ment l‚Äô√©tape qui √©choue.\nAmazon CloudWatch sert surtout aux logs et m√©triques (messages, CPU, erreurs), utile mais moins d√©taill√© pour suivre un parcours complet.\nAWS CloudTrail enregistre les actions API (qui a cr√©√©/modifi√© quoi), pas le comportement interne du code.\nAWS Trusted Advisor donne des recommandations de bonnes pratiques, pas du d√©bogage.\nDonc le bon choix pour le tra√ßage et le troubleshooting est AWS X-Ray.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine un jeu vid√©o o√π ton personnage fait une mission en passant par plusieurs salles (porte, couloir, boss). Tu veux savoir exactement o√π √ßa bug et combien de temps chaque salle prend.**\n\nDans le cloud, une fonction AWS Lambda, c‚Äôest comme un mini-bouton ‚Äúlancer la mission‚Äù qui ex√©cute du code. Le ‚Äútracing‚Äù, c‚Äôest une carte d√©taill√©e du trajet : quelles √©tapes ont √©t√© faites, dans quel ordre, et o√π √ßa s‚Äôest bloqu√©. AWS X-Ray (C) sert justement √† suivre ce trajet de bout en bout, comme un replay avec des marqueurs sur chaque salle. Amazon CloudWatch (B) ressemble plut√¥t √† un tableau de scores (messages et graphiques), utile mais moins pr√©cis pour suivre tout le parcours. AWS CloudTrail (D) est comme la liste des actions ‚Äúqui a cliqu√© sur quoi‚Äù, pas le d√©tail du bug dans la mission. Trusted Advisor (A) donne des conseils g√©n√©raux, pas une trace du chemin. Donc pour ‚Äútracer‚Äù et d√©panner, la bonne r√©ponse est C : AWS X-Ray.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:372:164bc5a8b23f6caf",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 372,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company runs an ecommerce application on AWS. The application stores data in an Amazon Aurora database.A developer is adding a caching layer to the application. The caching strategy must ensure that the application always uses the most recent value for each data item.Which caching strategy will meet these requirements?",
      "choices": {
        "A": "Implement a TTL strategy for every item that is saved in the cache.",
        "B": "Implement a write-through strategy for every item that is created and updated.",
        "C": "Implement a lazy loading strategy for every item that is loaded.",
        "D": "Implement a read-through strategy for every item that is loaded."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143755-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:38 a.m.",
      "textHash": "164bc5a8b23f6caf",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:133a974c",
      "frExplanation": "Ici, on ajoute un cache (ex: Amazon ElastiCache) devant une base Amazon Aurora (base de donn√©es relationnelle g√©r√©e). Le besoin cl√© est: toujours lire la valeur la plus r√©cente.\nLa strat√©gie ¬´ write-through ¬ª √©crit d‚Äôabord (ou en m√™me temps) dans le cache √† chaque cr√©ation/mise √† jour, puis dans la base (ou l‚Äôinverse selon l‚Äôimpl√©mentation), ce qui garde le cache synchronis√© avec la derni√®re valeur.\nAinsi, quand l‚Äôapplication lit ensuite, elle r√©cup√®re une donn√©e √† jour depuis le cache, sans risque d‚Äôavoir une ancienne version.\nTTL (A) garde des donn√©es jusqu‚Äô√† expiration: avant l‚Äôexpiration, on peut servir une valeur p√©rim√©e si la base a chang√©.\nLazy loading (C) et read-through (D) remplissent le cache au moment de la lecture: si une donn√©e est modifi√©e en base, le cache peut encore contenir l‚Äôancienne valeur tant qu‚Äôil n‚Äôest pas invalid√©.\nDonc la seule strat√©gie qui garantit syst√©matiquement la fra√Æcheur des donn√©es en cache lors des mises √† jour est le write-through (B).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e : la cuisine (base de donn√©es) est la source officielle, et le comptoir (cache) garde des plats pr√™ts pour aller plus vite.**\n\nConcept : le cache, c‚Äôest un ‚Äúraccourci‚Äù pour servir vite, mais il peut devenir p√©rim√© si la cuisine change une recette.\nIci on veut TOUJOURS la valeur la plus r√©cente : donc le comptoir doit √™tre mis √† jour √† chaque fois que la cuisine change.\nR√©ponse B (write-through) : d√®s qu‚Äôon cr√©e ou modifie un plat, on met √† jour en m√™me temps la cuisine ET le comptoir.\nComme √ßa, le comptoir a toujours la derni√®re version, jamais une ancienne.\nA (TTL) : on attend une dur√©e avant de jeter ‚Üí pendant ce temps, on peut servir un vieux plat.\nC (lazy loading) : on met en cache seulement quand quelqu‚Äôun demande ‚Üí si la recette change apr√®s, le comptoir peut rester vieux.\nD (read-through) : on remplit le cache quand on lit ‚Üí √ßa ne garantit pas la mise √† jour quand on modifie.\nDonc B est la seule strat√©gie qui ‚Äúsynchronise‚Äù le cache √† chaque √©criture.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:369:97f87cd5e0576c51",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 369,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company is working on a new serverless application. A developer needs to find an automated way to deploy AWS Lambda functions and the dependent infrastructure with minimum coding effort. The application also needs to be reliable.Which method will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Build the application by using shell scripts to create .zip files for each Lambda function. Manually upload the .zip files to the AWS Management Console.",
        "B": "Build the application by using the AWS Serverless Application Model (AWS SAM). Use a continuous integration and continuous delivery (CI/CD) pipeline and the SAM CLI to deploy the Lambda functions.",
        "C": "Build the application by using shell scripts to create .zip files for each Lambda function. Upload the .zip files. Deploy the .zip files as Lambda functions by using the AWS CLI in a continuous integration and continuous delivery (CI/CD) pipeline.",
        "D": "Build a container for each Lambda function. Store the container images in AWS CodeArtifact. Deploy the containers as Lambda functions by using the AWS CLI in a continuous integration and continuous delivery (CI/CD) pipeline."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/143752-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "July 12, 2024, 5:32 a.m.",
      "textHash": "97f87cd5e0576c51",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:bceee2e7",
      "frExplanation": "La question demande une fa√ßon automatis√©e de d√©ployer des fonctions AWS Lambda (ex√©cution de code sans serveur) et toute l‚Äôinfrastructure d√©pendante (API, r√¥les IAM, files, etc.) avec peu de code et peu d‚Äôop√©rations.\nAWS SAM (Serverless Application Model) est un outil ‚ÄúInfrastructure as Code‚Äù : on d√©crit l‚Äôapplication dans un fichier template simple, et SAM/CloudFormation cr√©e et met √† jour automatiquement les ressources.\nAvec une pipeline CI/CD, chaque changement de code d√©clenche build, tests et d√©ploiement sans actions manuelles, ce qui augmente la fiabilit√© et r√©duit les erreurs humaines.\nLe SAM CLI simplifie l‚Äôempaquetage et le d√©ploiement (upload, versions, permissions) sp√©cialement pour le serverless.\nA est manuel (console) donc non automatis√© et peu fiable.\nC automatise un peu, mais reste bas√© sur scripts .zip et commandes CLI √† maintenir, et g√®re moins bien l‚Äôinfrastructure compl√®te que SAM.\nD utilise des conteneurs et CodeArtifact (plut√¥t pour d√©pendances), ajoute de la complexit√© et plus d‚Äôeffort op√©rationnel que n√©cessaire.\nDonc B offre le minimum de code ‚Äúglue‚Äù, une automatisation native et une meilleure fiabilit√© avec le moins d‚Äôoverhead.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine que tu dois pr√©parer un spectacle au coll√®ge : il faut la sc√®ne, les micros, les lumi√®res, et que tout soit pr√™t √† chaque r√©p√©tition sans tout refaire √† la main.**\n\nConcept : une appli ‚Äúserverless‚Äù, c‚Äôest comme un spectacle o√π tu ne construis pas le b√¢timent : tu apportes juste les num√©ros, et l‚Äô√©cole g√®re la salle. Mais il faut quand m√™me installer tout le mat√©riel autour.\nA et C : c‚Äôest comme emballer chaque micro dans un sac (zip) et aller les poser toi-m√™me √† chaque fois. M√™me avec un couloir ‚Äúautomatique‚Äù (pipeline), tu dois g√©rer plein de d√©tails, donc plus d‚Äôefforts et plus de risques d‚Äôoubli.\nD : c‚Äôest comme mettre chaque micro dans une grosse caisse sp√©ciale (container) et la stocker dans un mauvais local (CodeArtifact n‚Äôest pas fait pour √ßa). Plus compliqu√© que n√©cessaire.\nB : AWS SAM, c‚Äôest une ‚Äúliste de mat√©riel + plan de montage‚Äù pour le spectacle. Le pipeline + SAM CLI, c‚Äôest une √©quipe qui installe tout pareil √† chaque fois, automatiquement.\nDonc B = moins de travail au quotidien (moins d‚Äôoverhead) et plus fiable, car tout est d√©ploy√© de fa√ßon r√©p√©table.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:329:3997c572d117c6e3",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 329,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is storing many objects in a single Amazon S3 bucket. The developer needs to optimize the S3 bucket for high request rates.How should the developer store the objects to meet this requirement?",
      "choices": {
        "A": "Store the objects by using S3 Intelligent-Tiering.",
        "B": "Store the objects at the root of the S3 bucket.",
        "C": "Store the objects by using object key names distributed across multiple prefixes.",
        "D": "Store each object with an object tag named \"prefix\" that contains a unique value."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136628-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 19, 2024, 2:27 p.m.",
      "textHash": "3997c572d117c6e3",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:s3:eac5f36a",
      "frExplanation": "Amazon S3 est un service de stockage d‚Äôobjets : chaque fichier est un ¬´ objet ¬ª identifi√© par une cl√© (object key), qui ressemble √† un chemin comme photos/2026/01/img1.jpg.\nQuand beaucoup de requ√™tes arrivent en m√™me temps, S3 r√©partit la charge en interne en fonction des ¬´ pr√©fixes ¬ª (la partie avant le premier /).\nSi tous vos objets commencent pareil (ex: logs/...), trop de requ√™tes peuvent viser le m√™me pr√©fixe et limiter le d√©bit.\nLa bonne pratique est donc de choisir des noms de cl√©s vari√©s, r√©partis sur plusieurs pr√©fixes (ex: a1/logs/..., b7/logs/..., 2026/01/..., etc.).\nS3 Intelligent-Tiering (A) concerne le co√ªt de stockage selon l‚Äôacc√®s, pas le nombre de requ√™tes par seconde.\nMettre tout √† la racine (B) ne cr√©e pas de r√©partition par pr√©fixes.\nLes tags (D) sont des m√©tadonn√©es pour la gestion/co√ªt, ils n‚Äôinfluencent pas la performance des requ√™tes.\nDonc il faut distribuer les object key names sur plusieurs pr√©fixes : r√©ponse C.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cantine au lyc√©e avec un seul grand comptoir o√π tout le monde vient chercher √† manger.**\n\nConcept : si tous les √©l√®ves font la queue au m√™me endroit, √ßa bloque. Pour aller plus vite, la cantine ouvre plusieurs files (p√¢tes, salades, desserts) et r√©partit les √©l√®ves.\nDans S3, un ‚Äúbucket‚Äù = le grand comptoir, et chaque fichier = un plateau. Le ‚Äúnom du fichier‚Äù (object key) commence souvent par un d√©but (pr√©fixe), comme l‚Äô√©tiquette de la file.\nPour avoir beaucoup de demandes en m√™me temps, il faut r√©partir les fichiers dans plusieurs ‚Äúd√©buts de noms‚Äù (plusieurs pr√©fixes), comme ouvrir plusieurs files.\nDonc C est bon : des noms de fichiers bien r√©partis sur plusieurs pr√©fixes = moins d‚Äôembouteillage, plus de vitesse.\nA (Intelligent-Tiering) c‚Äôest surtout pour payer moins cher selon l‚Äôusage, pas pour g√©rer la foule.\nB (tout √† la racine) c‚Äôest comme mettre tout le monde dans une seule file.\nD (un tag ‚Äúprefix‚Äù) c‚Äôest une √©tiquette coll√©e apr√®s coup : √ßa ne cr√©e pas de nouvelles files.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:347:15eea5f43fefddb0",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 347,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing an application to analyze the traffic to a fleet of Amazon EC2 instances. The EC2 instances run behind a public Application Load Balancer (ALB). An HTTP server runs on each of the EC2 instances, logging all requests to a log file.The developer wants to capture the client public IP addresses. The developer analyzes the log files and notices only the IP address of the ALB.What must the developer do to capture the client public IP addresses in the log file?",
      "choices": {
        "A": "Add a Host header to the HTTP server log configuration file.",
        "B": "Install the Amazon CloudWatch Logs agent on each EC2 instance. Configure the agent to write to the log file.",
        "C": "Install the AWS X-Ray daemon on each EC2 instance. Configure the daemon to write to the log file.",
        "D": "Add an X-Forwarded-For header to the HTTP server log configuration file."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136971-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 4:39 a.m.",
      "textHash": "15eea5f43fefddb0",
      "rawFormat": "discussion-md",
      "conceptKey": "alb_cognito_authenticate_cognito",
      "frExplanation": "Derri√®re un Application Load Balancer (ALB), le client ne parle pas directement √† votre serveur HTTP : c‚Äôest l‚ÄôALB qui re√ßoit la requ√™te puis la ‚Äúrelaye‚Äù vers l‚Äôinstance EC2.\nDu coup, pour votre serveur, l‚Äôadresse IP ‚Äúsource‚Äù visible est souvent celle de l‚ÄôALB, pas celle du client.\nPour conserver l‚ÄôIP publique du client, l‚ÄôALB ajoute un en-t√™te HTTP standard appel√© \"X-Forwarded-For\" qui contient l‚ÄôIP d‚Äôorigine (et parfois une liste d‚ÄôIPs si plusieurs proxys).\nLa solution est donc de configurer votre serveur web (Apache/Nginx/app) pour enregistrer dans ses logs la valeur de l‚Äôen-t√™te X-Forwarded-For.\nLes autres options ne r√©solvent pas le probl√®me : CloudWatch Logs ne change pas le contenu des logs, X-Ray sert au tra√ßage applicatif, et le header Host indique le nom de domaine, pas l‚ÄôIP client.\nAinsi, en loggant X-Forwarded-For, vous r√©cup√©rez l‚ÄôIP publique r√©elle du client.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine l‚Äôentr√©e de ton lyc√©e : tout le monde passe d‚Äôabord par le surveillant (le ALB) avant d‚Äôaller dans une salle (les serveurs sur EC2).**\n\nConcept : le surveillant transmet les √©l√®ves vers les salles, donc la salle voit surtout ‚Äúle surveillant‚Äù arriver, pas l‚Äô√©l√®ve.\nIci, le ALB re√ßoit les visiteurs d‚ÄôInternet puis envoie la requ√™te √† un serveur EC2.\nDu coup, dans les logs du serveur, l‚Äôadresse IP vue est celle du ALB (comme si le surveillant signait √† la place de l‚Äô√©l√®ve).\nPour conna√Ætre le vrai √©l√®ve, le surveillant doit ajouter un mot sur le billet : ‚Äúcet √©l√®ve vient de telle adresse‚Äù.\nSur le web, ce ‚Äúmot‚Äù s‚Äôappelle l‚Äôen-t√™te (header) X-Forwarded-For.\nDonc il faut configurer le serveur pour enregistrer X-Forwarded-For dans le fichier de logs.\nA (Host) dit juste quel site est demand√©, pas qui est le visiteur.\nB et C ajoutent des outils de collecte/tra√ßage, mais ne remplacent pas l‚Äôinfo manquante : l‚ÄôIP du client.\nDonc la bonne r√©ponse est D.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:345:51d17cb1cebd16bf",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 345,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer uses AWS IAM Identity Center (AWS Single Sign-On) to interact with the AWS CLI and AWS SDKs on a local workstation. API calls to AWS services were working when the SSO access was first configured. However, the developer is now receiving Access Denied errors. The developer has not changed any configuration files or scripts that were previously working on the workstation.What is the MOST likely cause of the developer's access issue?",
      "choices": {
        "A": "The access permissions to the developer's AWS CLI binary file have changed.",
        "B": "The permission set that is assumed by IAM Identity Center does not have the necessary permissions to complete the API call.",
        "C": "The credentials from the IAM Identity Center federated role have expired.",
        "D": "The developer is attempting to make API calls to the incorrect AWS account."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/136969-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "March 23, 2024, 4:29 a.m.",
      "textHash": "51d17cb1cebd16bf",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "Avec IAM Identity Center (ex-SSO), votre poste local n‚Äôutilise pas des cl√©s AWS permanentes : il r√©cup√®re des identifiants temporaires (jetons) pour appeler les services AWS via le CLI/SDK.\nCes identifiants temporaires ont une dur√©e de vie limit√©e (souvent quelques heures). Quand ils expirent, AWS refuse les appels et renvoie ¬´ Access Denied ¬ª.\nComme rien n‚Äôa chang√© dans les fichiers de config ou les scripts, la cause la plus logique est l‚Äôexpiration des identifiants, pas un changement de code.\nLa solution typique est de se reconnecter : ex√©cuter √† nouveau la commande de login SSO (ex. aws sso login) ou rafra√Æchir la session dans l‚Äôoutil.\nL‚Äôoption B impliquerait que √ßa n‚Äôaurait jamais march√© (ou qu‚Äôun admin a modifi√© le permission set), ce qui n‚Äôest pas indiqu√©.\nL‚Äôoption D donnerait plut√¥t des erreurs li√©es au mauvais compte/role, mais l√† encore √ßa aurait √©t√© visible d√®s le d√©but.\nL‚Äôoption A concerne les droits du fichier binaire local, ce qui provoquerait plut√¥t un probl√®me d‚Äôex√©cution, pas un ¬´ Access Denied ¬ª c√¥t√© AWS.\nDonc la cause la plus probable est l‚Äôexpiration des identifiants SSO temporaires.",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine que ton √©cole te donne un badge visiteur pour aller au CDI et √† la cantine. Au d√©but, le badge marche. Mais ce badge a une dur√©e limit√©e (genre 8 heures). Apr√®s, il ne passe plus aux portiques.**\n\nAWS IAM Identity Center, c‚Äôest comme le bureau qui te donne ce badge temporaire pour entrer dans les salles. Le AWS CLI/SDK sur ton PC, c‚Äôest toi qui essaies d‚Äôouvrir les portes (faire des demandes aux services AWS). Au d√©but tout marchait car ton badge √©tait encore valide. Puis, sans rien changer sur ton PC, tu re√ßois ‚ÄúAccess Denied‚Äù car le badge a expir√©. C‚Äôest exactement le choix C : les ‚Äúidentifiants‚Äù (le badge) du r√¥le obtenu via Identity Center ont une date de fin. A serait plut√¥t ‚Äúton badge est cass√©‚Äù, B serait ‚Äúton badge n‚Äôa jamais eu le droit d‚Äôaller au CDI‚Äù (√ßa aurait bloqu√© d√®s le d√©but), D serait ‚Äútu vas dans la mauvaise √©cole‚Äù (moins probable si √ßa marchait avant).",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:292:a6b7907528212a37",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 292,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is investigating an issue in part of a company's application. In the application, messages are sent to an Amazon Simple Queue Service (Amazon SQS) queue. The AWS Lambda function polls messages from the SQS queue and sends email messages by using Amazon Simple Email Service (Amazon SES). Users have been receiving duplicate email messages during periods of high traffic.Which reasons could explain the duplicate email messages? (Choose two.)",
      "choices": {
        "A": "Standard SQS queues support at-least-once message delivery.",
        "B": "Standard SQS queues support exactly-once processing, so the duplicate email messages are because of user error.",
        "C": "Amazon SES has the DomainKeys Identified Mail (DKIM) authentication incorrectly configured.",
        "D": "The SQS queue's visibility timeout is lower than or the same as the Lambda function's timeout.",
        "E": "The Amazon SES bounce rate metric is too high."
      },
      "answer": "A",
      "suggestedAnswer": "A",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/134291-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 21, 2024, 7:10 a.m.",
      "textHash": "a6b7907528212a37",
      "rawFormat": "discussion-md",
      "conceptKey": "sqs_visibility_timeout",
      "frExplanation": "Amazon SQS est une file de messages : une application y d√©pose des messages, puis un consommateur (ici AWS Lambda) les lit et les traite.\nAvec une file SQS ¬´ Standard ¬ª, la livraison est ¬´ au moins une fois ¬ª : SQS peut remettre le m√™me message plusieurs fois, surtout en cas de forte charge ou de retry r√©seau. Donc un m√™me email peut √™tre envoy√© deux fois (A).\nLambda lit un message et SQS le cache temporairement gr√¢ce au ¬´ visibility timeout ¬ª : pendant ce d√©lai, les autres consommateurs ne doivent pas le revoir.\nSi le visibility timeout est trop court (inf√©rieur ou √©gal au temps maximum d‚Äôex√©cution de Lambda), le message peut redevenir visible avant la fin du traitement.\nDans ce cas, Lambda peut le relire et le retraiter une seconde fois, ce qui cr√©e des emails en double (D).\nLes options SES (DKIM, bounce rate) concernent l‚Äôauthentification ou les rejets d‚Äôemails, pas la duplication d‚Äôenvoi.\nDonc les deux causes plausibles sont : comportement Standard SQS (au moins une fois) et mauvais r√©glage du visibility timeout vs timeout Lambda.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : les √©l√®ves (messages) d√©posent des tickets de commande dans une bo√Æte (la file SQS). Un surveillant (Lambda) prend un ticket, va chercher le plateau, puis le donne √† l‚Äô√©l√®ve (envoi d‚Äôemail via SES).**\n\nConcept : une bo√Æte ‚Äústandard‚Äù peut parfois donner le m√™me ticket deux fois, surtout quand √ßa se bouscule. Elle promet juste : ‚Äúau moins une fois‚Äù, pas ‚Äúune seule fois‚Äù.\nPourquoi A : SQS Standard = ‚Äúau moins une fois‚Äù. Donc en p√©riode de rush, le surveillant peut r√©cup√©rer deux fois le m√™me ticket, et l‚Äô√©l√®ve re√ßoit deux plateaux ‚Üí deux emails.\nLes autres : DKIM (C) c‚Äôest une carte d‚Äôidentit√© du mail, √ßa ne cr√©e pas des doublons. Le taux de rebond (E) c‚Äôest des mails refus√©s, pas des doublons. B est faux car ‚Äúexactement une fois‚Äù n‚Äôest pas garanti en standard.\nDonc la cause logique des doublons, c‚Äôest A.",
      "requiredAnswers": 2
    },
    {
      "id": "dva-c02:topic:1:question:168:1aa6484c6585e26e",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 168,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is building a serverless application that is based on AWS Lambda. The developer initializes the AWS software development kit (SDK) outside of the Lambda handler function.What is the PRIMARY benefit of this action?",
      "choices": {
        "A": "Improves legibility and stylistic convention",
        "B": "Takes advantage of runtime environment reuse",
        "C": "Provides better error handling",
        "D": "Creates a new SDK instance for each invocation"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122588-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:18 a.m.",
      "textHash": "1aa6484c6585e26e",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:lambda:c5ebb686",
      "frExplanation": "Dans AWS Lambda, votre code s‚Äôex√©cute dans un ‚Äúenvironnement‚Äù (un conteneur) qui peut √™tre r√©utilis√© pour plusieurs appels successifs (invocations).\nLe ‚Äúhandler‚Äù est la fonction appel√©e √† chaque invocation. Tout ce qui est dans le handler est refait √† chaque fois.\nInitialiser le SDK AWS (biblioth√®que pour appeler des services AWS comme S3, DynamoDB, etc.) en dehors du handler le fait une seule fois au d√©marrage du conteneur.\nSi Lambda r√©utilise le m√™me conteneur, cette initialisation est r√©utilis√©e aussi : on √©vite de recr√©er les clients SDK √† chaque appel.\nR√©sultat : moins de temps de d√©marrage par invocation, meilleures performances et parfois moins de co√ªts.\nDonc le b√©n√©fice principal est de profiter de la r√©utilisation de l‚Äôenvironnement d‚Äôex√©cution (runtime environment reuse).\nA est faux : ce n‚Äôest pas une question de style. C n‚Äôest pas l‚Äôobjectif principal. D est l‚Äôinverse de ce qu‚Äôon veut √©viter.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine une cuisine de cantine : le cuisinier pr√©pare les ustensiles et sort les ingr√©dients AVANT que les √©l√®ves arrivent.**\n\nDans AWS Lambda, le ‚Äúhandler‚Äù c‚Äôest le moment o√π une demande arrive (un √©l√®ve passe au self). Initialiser le SDK ‚Äúen dehors du handler‚Äù, c‚Äôest comme sortir les casseroles et ouvrir les cartons d‚Äôingr√©dients √† l‚Äôavance. Quand une nouvelle demande arrive, Lambda peut parfois r√©utiliser la m√™me ‚Äúcuisine d√©j√† pr√™te‚Äù au lieu de tout r√©installer. R√©sultat : √ßa d√©marre plus vite, car on √©vite de refaire la pr√©paration √† chaque √©l√®ve. C‚Äôest exactement ‚Äúruntime environment reuse‚Äù : r√©utiliser l‚Äôenvironnement d√©j√† charg√©. Donc B est la bonne r√©ponse. D est faux : on ne recr√©e pas √† chaque fois, justement on essaie d‚Äô√©viter √ßa.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:155:5abc13f6420e8f71",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 155,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is writing a serverless application that requires an AWS Lambda function to be invoked every 10 minutes.What is an automated and serverless way to invoke the function?",
      "choices": {
        "A": "Deploy an Amazon EC2 instance based on Linux, and edit its /etc/crontab file by adding a command to periodically invoke the Lambda function.",
        "B": "Configure an environment variable named PERIOD for the Lambda function. Set the value to 600.",
        "C": "Create an Amazon EventBridge rule that runs on a regular schedule to invoke the Lambda function.",
        "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic that has a subscription to the Lambda function with a 600-second timer."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122574-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 1:58 a.m.",
      "textHash": "5abc13f6420e8f71",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, on veut d√©clencher automatiquement une fonction AWS Lambda (un petit bout de code ex√©cut√© ‚Äú√† la demande‚Äù, sans serveur √† g√©rer) toutes les 10 minutes.\nLa solution la plus ‚Äúserverless‚Äù est d‚Äôutiliser Amazon EventBridge (anciennement CloudWatch Events), un service qui peut lancer des actions selon un calendrier.\nAvec une r√®gle EventBridge de type ‚Äúschedule‚Äù, on d√©finit une expression (par ex. toutes les 10 minutes) et la cible est la fonction Lambda : AWS s‚Äôoccupe de l‚Äôex√©cution r√©guli√®re.\nA est faux car une instance EC2 est un serveur √† g√©rer (maintenance, co√ªt, disponibilit√©) : ce n‚Äôest pas serverless.\nB est faux car une variable d‚Äôenvironnement ne d√©clenche rien : elle ne fait que fournir une valeur au code.\nD est faux car SNS sert surtout √† publier/recevoir des messages (notifications) et n‚Äôest pas un service de planification avec ‚Äútimer‚Äù int√©gr√©.\nDonc la bonne r√©ponse est C : une r√®gle EventBridge planifi√©e qui invoque Lambda.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine ton coll√®ge : tu veux qu‚Äôune sonnerie d√©clenche automatiquement une action toutes les 10 minutes, sans qu‚Äôun √©l√®ve reste √† appuyer sur un bouton.**\n\nUne fonction AWS Lambda, c‚Äôest comme un √©l√®ve ‚Äú√† la demande‚Äù : il se r√©veille, fait une t√¢che, puis repart, sans rester en classe. Pour la r√©veiller toutes les 10 minutes, il faut une ‚Äúsonnerie‚Äù automatique. Amazon EventBridge, c‚Äôest le syst√®me de sonnerie/emploi du temps : tu r√®gles ‚Äútoutes les 10 minutes‚Äù et il appelle Lambda tout seul. A, c‚Äôest comme embaucher un surveillant (un ordinateur EC2) juste pour regarder l‚Äôhorloge : pas vraiment serverless. B, c‚Äôest juste √©crire ‚Äú10 minutes‚Äù sur un post-it : √ßa ne d√©clenche rien. D, SNS c‚Äôest plut√¥t un m√©gaphone pour envoyer des messages, pas une minuterie fiable. Donc la bonne r√©ponse est C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:135:dda897bffbc2242c",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 135,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company created four AWS Lambda functions that connect to a relational database server that runs on an Amazon RDS instance. A security team requires the company to automatically change the database password every 30 days.Which solution will meet these requirements MOST securely?",
      "choices": {
        "A": "Store the database credentials in the environment variables of the Lambda function. Deploy the Lambda function with the new credentials every 30 days.",
        "B": "Store the database credentials in AWS Secrets Manager. Configure a 30-day rotation schedule for the credentials.",
        "C": "Store the database credentials in AWS Systems Manager Parameter Store secure strings. Configure a 30-day schedule for the secure strings.",
        "D": "Store the database credentials in an Amazon S3 bucket that uses server-side encryption with customer-provided encryption keys (SSE-C). Configure a 30-day key rotation schedule for the customer key."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/117333-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Aug. 4, 2023, 1:41 p.m.",
      "textHash": "dda897bffbc2242c",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "Ici, 4 fonctions AWS Lambda (du code qui s‚Äôex√©cute sans serveur) se connectent √† une base relationnelle sur Amazon RDS (service de base de donn√©es g√©r√©). L‚Äô√©quipe s√©curit√© veut changer automatiquement le mot de passe tous les 30 jours.\nLa solution la plus s√ªre est AWS Secrets Manager : c‚Äôest un coffre-fort pour identifiants (login/mot de passe) avec une fonction int√©gr√©e de rotation automatique.\nAvec une rotation tous les 30 jours, Secrets Manager peut g√©n√©rer un nouveau mot de passe, le mettre √† jour dans RDS, puis stocker la nouvelle valeur au m√™me endroit.\nLes fonctions Lambda lisent le secret au moment de l‚Äôex√©cution, donc elles utilisent toujours le mot de passe √† jour sans red√©ploiement.\nA est moins s√ªr (mots de passe dans variables d‚Äôenvironnement, gestion manuelle et red√©ploiements). C ne g√®re pas aussi simplement la rotation automatique des identifiants de base de donn√©es. D n‚Äôest pas adapt√© (S3 n‚Äôest pas un gestionnaire de secrets et la rotation de cl√© ne change pas automatiquement le mot de passe RDS).\nDonc B r√©pond au besoin ‚Äúautomatique + s√©curis√©‚Äù avec le service con√ßu pour √ßa.",
      "domainKey": "security",
      "frExplanationPedagogique": "**üè∞ Imagine ton immeuble r√©sidentiel** : plusieurs appartements avec syst√®mes de s√©curit√©.\n\n**VPC** = **Tout l'immeuble** üè¢ - Ton terrain priv√© dans le cloud.\n\n**Subnet** = **Chaque √©tage** üè† - Public (rdc) ou Priv√© (r√©sidents).\n\n**Security Group** = **La porte de ton appart** üö™ - Qui peut frapper ? (Stateful = se souvient des entr√©es).\n\n**NACL** = **Le digicode de l'√©tage** üî¢ - Filtre au niveau de l'√©tage (Stateless = sans m√©moire).\n\n**üß† Mn√©motechnique :** \"**S**G = **S**ouviens des connexions\" | \"**N**ACL = **N**e souviens de **R**ien\"\n\n**Quoi v√©rifier d'abord :** Le badge de l'EC2 (r√¥le IAM) autorise-t-il S3 ? Puis la policy du bucket S3.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:234:32db7dc5329472d7",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 234,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is troubleshooting an application in an integration environment. In the application, an Amazon Simple Queue Service (Amazon SQS) queue consumes messages and then an AWS Lambda function processes the messages. The Lambda function transforms the messages and makes an API call to a third-party service.There has been an increase in application usage. The third-party API frequently returns an HTTP 429 Too Many Requests error message. The error message prevents a significant number of messages from being processed successfully.How can the developer resolve this issue?",
      "choices": {
        "A": "Increase the SQS event source‚Äôs batch size setting.",
        "B": "Configure provisioned concurrency for the Lambda function based on the third-party API‚Äôs documented rate limits.",
        "C": "Increase the retry attempts and maximum event age in the Lambda function‚Äôs asynchronous configuration.",
        "D": "Configure maximum concurrency on the SQS event source based on the third-party service‚Äôs documented rate limits."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124821-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 28, 2023, 2:26 p.m.",
      "textHash": "32db7dc5329472d7",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, SQS (une file d‚Äôattente) envoie des messages √† Lambda (du code qui s‚Äôex√©cute automatiquement). Quand l‚Äôusage augmente, Lambda traite plus de messages en parall√®le et appelle plus souvent l‚ÄôAPI externe.\nL‚Äôerreur HTTP 429 signifie ‚Äútrop de requ√™tes‚Äù : le service tiers limite le nombre d‚Äôappels par seconde/minute (rate limit).\nLa solution logique est donc de r√©duire/contr√¥ler le nombre d‚Äôappels simultan√©s vers l‚ÄôAPI, pas d‚Äôacc√©l√©rer le traitement.\nAvec une source d‚Äô√©v√©nements SQS, on peut d√©finir une ‚Äúconcurrency‚Äù maximale (nombre d‚Äôex√©cutions Lambda en parall√®le) pour limiter le d√©bit de consommation.\nEn configurant cette concurrence max selon les limites document√©es du service tiers, on √©vite de d√©passer le quota et on r√©duit les 429.\nA (augmenter la taille de lot) risque d‚Äôenvoyer encore plus d‚Äôappels d‚Äôun coup.\nB (provisioned concurrency) garantit des d√©marrages rapides, mais n‚Äôemp√™che pas de d√©passer le rate limit.\nC (plus de retries) retente apr√®s √©chec, ce qui peut aggraver la surcharge et augmenter les 429.\nDonc D est correct : limiter la concurrence de la source SQS pour respecter le rate limit du service tiers.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : une file d‚Äô√©l√®ves (la queue SQS) attend, et un groupe de serveurs (Lambda) prend les plateaux et va demander un dessert √† une boutique ext√©rieure. Sauf que la boutique dit souvent : ¬´ stop, trop de clients d‚Äôun coup ¬ª (HTTP 429).**\n\nConcept : SQS stocke les demandes en attente, et Lambda les traite. Si Lambda envoie trop de demandes √† la boutique, elle refuse (429).\nPourquoi D : on limite le nombre de serveurs qui travaillent en m√™me temps (concurrence max) pour respecter la limite de la boutique. Comme √ßa, la file avance plus lentement mais sans refus.\nPourquoi pas A : prendre plus d‚Äô√©l√®ves par serveur, c‚Äôest encore plus de demandes d‚Äôun coup, donc plus de refus.\nPourquoi pas B : ajouter des serveurs pr√™ts √† l‚Äôavance augmente la vitesse, donc aggrave le ‚Äútrop de clients‚Äù.\nPourquoi pas C : r√©essayer plus souvent, c‚Äôest revenir frapper √† la porte encore plus, donc toujours des 429.\nDonc : r√©gler la concurrence max de la source SQS selon les limites de la boutique = D.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:278:1c3be9f74da15f77",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 278,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has a web application that is hosted on Amazon EC2 instances. The EC2 instances are configured to stream logs to Amazon CloudWatch Logs. The company needs to receive an Amazon Simple Notification Service (Amazon SNS) notification when the number of application error messages exceeds a defined threshold within a 5-minute period.Which solution will meet these requirements?",
      "choices": {
        "A": "Rewrite the application code to stream application logs to Amazon SNS. Configure an SNS topic to send a notification when the number of errors exceeds the defined threshold within a 5-minute period.",
        "B": "Configure a subscription filter on the CloudWatch Logs log group. Configure the filter to send an SNS notification when the number of errors exceeds the defined threshold within a 5-minute period.",
        "C": "Install and configure the Amazon Inspector agent on the EC2 instances to monitor for errors. Configure Amazon Inspector to send an SNS notification when the number of errors exceeds the defined threshold within a 5-minute period.",
        "D": "Create a CloudWatch metric filter to match the application error pattern in the log data. Set up a CloudWatch alarm based on the new custom metric. Configure the alarm to send an SNS notification when the number of errors exceeds the defined threshold within a 5-minute period."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/133406-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Feb. 9, 2024, 1:12 a.m.",
      "textHash": "1c3be9f74da15f77",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Les instances EC2 envoient d√©j√† leurs logs dans CloudWatch Logs (service qui stocke et recherche des journaux). Pour d√©clencher une alerte, il faut d‚Äôabord transformer certains messages de log (ex: ‚ÄúERROR‚Äù) en un compteur mesurable. Un ‚Äúmetric filter‚Äù CloudWatch Logs permet de rep√©rer un motif dans les logs et d‚Äôincr√©menter une m√©trique CloudWatch (un chiffre qui √©volue dans le temps). Ensuite, une ‚ÄúCloudWatch Alarm‚Äù surveille cette m√©trique sur une fen√™tre de 5 minutes et compare au seuil d√©fini. Quand le seuil est d√©pass√©, l‚Äôalarme peut publier un message vers Amazon SNS (service d‚Äôenvoi de notifications vers email, SMS, etc.). Les autres choix ne conviennent pas: SNS ne lit pas les logs directement, un subscription filter sert surtout √† envoyer les logs vers Lambda/Kinesis/Firehose, et Inspector est pour l‚Äôanalyse de s√©curit√©/vuln√©rabilit√©s, pas pour compter des erreurs applicatives.",
      "domainKey": "troubleshooting",
      "frExplanationPedagogique": "**Imagine ton coll√®ge : les EC2 sont des surveillants, les logs sont le cahier d‚Äôincidents, CloudWatch Logs est la salle o√π on range tous ces cahiers, SNS c‚Äôest le SMS envoy√© au CPE.**\n\nConcept : tu veux un SMS si, en 5 minutes, il y a trop d‚Äôincidents ‚Äúerreur‚Äù.\nDans la vraie vie, tu ne lis pas tout le cahier √† la main : tu cr√©es une r√®gle qui compte les mots ‚Äúbagarre‚Äù ou ‚Äúretard‚Äù.\nD = on cr√©e un ‚Äúcompteur‚Äù dans CloudWatch qui rep√®re le motif d‚Äôerreur dans les logs (metric filter).\nCe compteur augmente √† chaque message d‚Äôerreur, comme un compteur d‚Äôincidents.\nPuis on met une ‚Äúalarme‚Äù sur ce compteur : si √ßa d√©passe le seuil en 5 minutes, elle se d√©clenche.\nEt quand l‚Äôalarme se d√©clenche, elle envoie un SMS via SNS.\nA est faux : r√©√©crire l‚Äôappli juste pour √ßa, c‚Äôest comme changer tout le r√®glement pour compter des retards.\nB est faux : un filtre d‚Äôabonnement sert surtout √† copier/diriger des logs, pas √† faire une alarme de seuil.\nC est hors-sujet : Inspector, c‚Äôest plut√¥t un contr√¥leur de s√©curit√©, pas un compteur d‚Äôerreurs d‚Äôappli.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:253:2e3c092f1e1feb16",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 253,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company has migrated an application to Amazon EC2 instances. Automatic scaling is working well for the application user interface. However, the process to deliver shipping requests to the company‚Äôs warehouse staff is encountering issues. Duplicate shipping requests are arriving, and some requests are lost or arrive out of order.The company must avoid duplicate shipping requests and must process the requests in the order that the requests arrive. Requests are never more than 250 KB in size and take 5-10 minutes to process. A developer needs to rearchitect the application to improve the reliability of the delivery and processing of the requests.What should the developer do to meet these requirements?",
      "choices": {
        "A": "Create an Amazon Kinesis Data Firehose delivery stream to process the requests. Create an Amazon Kinesis data stream. Modify the application to write the requests to the Kinesis data stream.",
        "B": "Create an AWS Lambda function to process the requests. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the Lambda function to the SNS topic. Modify the application to write the requests to the SNS topic.",
        "C": "Create an AWS Lambda function to process the requests. Create an Amazon Simple Queue Service (Amazon SQS) standard queue. Set the SQS queue as an event source for the Lambda function. Modify the application to write the requests to the SQS queue.",
        "D": "Create an AWS Lambda function to process the requests. Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the SQS queue as an event source for the Lambda function. Modify the application to write the requests to the SQS queue."
      },
      "answer": "D",
      "suggestedAnswer": "D",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124864-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 29, 2023, 2:12 a.m.",
      "textHash": "2e3c092f1e1feb16",
      "rawFormat": "discussion-md",
      "conceptKey": "sns_vs_sqs",
      "frExplanation": "Ici, le probl√®me vient du fait que les demandes d‚Äôexp√©dition arrivent en double, se perdent, ou n‚Äôarrivent pas dans le bon ordre. Il faut donc une file d‚Äôattente fiable qui garantit l‚Äôordre et √©vite les doublons.\nAmazon SQS est un service de ‚Äúqueue‚Äù (file) : l‚Äôapplication d√©pose un message, puis un worker le traite plus tard. Cela d√©couple l‚Äôenvoi et le traitement et √©vite les pertes.\nUne queue SQS Standard peut livrer des messages ‚Äúau moins une fois‚Äù et ne garantit pas l‚Äôordre : on peut donc avoir des doublons et du d√©sordre.\nUne queue SQS FIFO (First-In-First-Out) garantit l‚Äôordre d‚Äôarriv√©e et permet la d√©duplication (SQS peut rejeter les doublons via un identifiant de d√©duplication).\nAWS Lambda peut √™tre d√©clench√©e automatiquement par SQS pour traiter les messages, m√™me si le traitement prend 5‚Äì10 minutes (dans la limite du timeout configur√©).\nLa taille max 250 KB correspond √† la limite d‚Äôun message SQS, donc c‚Äôest compatible.\nKinesis/SNS ne r√©pondent pas aussi directement au besoin ‚Äúpas de doublons + ordre strict‚Äù pour ce cas simple de traitement de commandes.\nDonc la bonne solution est d‚Äôutiliser SQS FIFO comme source d‚Äô√©v√©nements pour Lambda (r√©ponse D).",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine la cantine du lyc√©e : les √©l√®ves d√©posent des tickets de commande (plat) dans une bo√Æte, et la cuisine les traite.**\n\nConcept : il faut une ‚Äúfile d‚Äôattente‚Äù fiable, comme une bo√Æte o√π chaque ticket attend son tour.\nIci, on veut 2 r√®gles : pas de doublons et ordre d‚Äôarriv√©e respect√©.\nUne file SQS FIFO, c‚Äôest la bo√Æte ‚ÄúPremier arriv√©, premier servi‚Äù : les tickets sortent dans le bon ordre.\nEt elle peut bloquer les doublons (comme un surveillant qui refuse 2 fois le m√™me ticket).\nLambda, c‚Äôest l‚Äôemploy√© de cuisine automatique qui prend un ticket et le pr√©pare (5-10 min, √ßa va).\nLes tickets font moins de 250 KB, donc √ßa rentre dans la bo√Æte.\nPourquoi pas C (SQS standard) : c‚Äôest une bo√Æte o√π l‚Äôordre n‚Äôest pas garanti et des doublons peuvent arriver.\nPourquoi pas SNS/Kinesis : ce n‚Äôest pas fait pour garantir ‚Äúz√©ro doublon + ordre strict‚Äù comme une FIFO.\nDonc D : SQS FIFO + Lambda = commandes dans l‚Äôordre, sans doublons, et aucune perdue.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:211:0193335508bd8e05",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 211,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is creating an AWS Lambda function that will generate and export a file. The function requires 100 MB of temporary storage for temporary files while running. These files will not be needed after the function is complete.How can the developer MOST efficiently handle the temporary files?",
      "choices": {
        "A": "Store the files in Amazon Elastic Block Store (Amazon EBS) and delete the files at the end of the Lambda function.",
        "B": "Copy the files to Amazon Elastic File System (Amazon EFS) and delete the files at the end of the Lambda function.",
        "C": "Store the files in the /tmp directory and delete the files at the end of the Lambda function.",
        "D": "Copy the files to an Amazon S3 bucket with a lifecycle policy to delete the files."
      },
      "answer": "C",
      "suggestedAnswer": "C",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/124750-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 27, 2023, 9:55 p.m.",
      "textHash": "0193335508bd8e05",
      "rawFormat": "discussion-md",
      "conceptKey": "iam_least_privilege",
      "frExplanation": "AWS Lambda ex√©cute du code sans serveur et fournit un espace disque temporaire local pendant l‚Äôex√©cution : le dossier /tmp.\nCe stockage /tmp est fait pour les fichiers temporaires (ex: g√©n√©ration d‚Äôun fichier, d√©compression, traitement), puis il peut √™tre r√©utilis√© ou supprim√© apr√®s.\nLe besoin est seulement 100 MB et les fichiers ne servent plus apr√®s la fin : /tmp est la solution la plus simple et la plus rapide (pas de r√©seau, pas de service √† configurer).\nEBS ne peut pas √™tre attach√© directement √† une ex√©cution Lambda comme √† une instance EC2, donc ce n‚Äôest pas adapt√©.\nEFS est un syst√®me de fichiers r√©seau persistant : utile si plusieurs ex√©cutions doivent partager des fichiers ou si on veut conserver les donn√©es, mais c‚Äôest plus complexe et moins ‚Äúefficace‚Äù ici.\nS3 est du stockage objet persistant : bon pour archiver/exporter le r√©sultat final, mais inutile pour des fichiers temporaires et ajoute des transferts + gestion de cycle de vie.\nDonc on √©crit les fichiers temporaires dans /tmp (et on peut les supprimer √† la fin pour √™tre propre) : r√©ponse C.",
      "domainKey": "development",
      "frExplanationPedagogique": "**Imagine un contr√¥le en classe o√π tu as besoin d‚Äôun brouillon pour faire tes calculs, puis tu rends ta copie et le brouillon ne sert plus.**\n\nUne fonction AWS Lambda, c‚Äôest comme un √©l√®ve qui arrive, fait une t√¢che, puis repart.\nPendant qu‚Äôelle travaille, elle peut utiliser un ‚Äúbrouillon‚Äù int√©gr√© : le dossier /tmp.\nCe /tmp est un espace temporaire pr√©vu justement pour stocker des fichiers pendant l‚Äôex√©cution.\nIci, 100 MB, c‚Äôest comme quelques feuilles de brouillon : √ßa tient largement.\n√Ä la fin, on jette le brouillon (on supprime les fichiers) et c‚Äôest fini.\nLes autres choix (EBS, EFS, S3) ressemblent √† louer un casier, une salle d‚Äôarchives ou envoyer au CDI : trop lourd pour un simple brouillon.\nDonc la fa√ßon la plus efficace est de mettre les fichiers temporaires dans /tmp et les supprimer √† la fin.\nR√©ponse C.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:195:2be24682ec2865bf",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 195,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA company wants to automate part of its deployment process. A developer needs to automate the process of checking for and deleting unused resources that supported previously deployed stacks but that are no longer used.The company has a central application that uses the AWS Cloud Development Kit (AWS CDK) to manage all deployment stacks. The stacks are spread out across multiple accounts. The developer‚Äôs solution must integrate as seamlessly as possible within the current deployment process.Which solution will meet these requirements with the LEAST amount of configuration?",
      "choices": {
        "A": "In the central AWS CDK application, write a handler function in the code that uses AWS SDK calls to check for and delete unused resources. Create an AWS CloudFormation template from a JSON file. Use the template to attach the function code to an AWS Lambda function and to invoke the Lambda function when the deployment stack runs.",
        "B": "In the central AWS CDK application, write a handler function in the code that uses AWS SDK calls to check for and delete unused resources. Create an AWS CDK custom resource. Use the custom resource to attach the function code to an AWS Lambda function and to invoke the Lambda function when the deployment stack runs.",
        "C": "In the central AWS CDK, write a handler function in the code that uses AWS SDK calls to check for and delete unused resources. Create an API in AWS Amplify. Use the API to attach the function code to an AWS Lambda function and to invoke the Lambda function when the deployment stack runs.",
        "D": "In the AWS Lambda console, write a handler function in the code that uses AWS SDK calls to check for and delete unused resources. Create an AWS CDK custom resource. Use the custom resource to import the Lambda function into the stack and to invoke the Lambda function when the deployment stack runs."
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122617-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:53 a.m.",
      "textHash": "2be24682ec2865bf",
      "rawFormat": "discussion-md",
      "conceptKey": "fallback:general:094757ff",
      "frExplanation": "Ici, on veut automatiser un ‚Äúnettoyage‚Äù (d√©tecter et supprimer des ressources AWS inutilis√©es) pendant le d√©ploiement, et que cela s‚Äôint√®gre naturellement dans une appli centrale d√©j√† g√©r√©e avec AWS CDK (outil pour d√©crire l‚Äôinfrastructure en code).\nLa fa√ßon la plus simple dans CDK est d‚Äôutiliser une ‚ÄúCustom Resource‚Äù : c‚Äôest un m√©canisme CloudFormation/CDK qui ex√©cute du code (souvent une fonction AWS Lambda) automatiquement lors de la cr√©ation/mise √† jour/suppression d‚Äôune stack.\nAvec l‚Äôoption B, on √©crit la fonction (handler) directement dans le projet CDK, puis CDK cr√©e la Lambda et la relie √† la Custom Resource : peu de configuration, tout est versionn√© et d√©ploy√© avec les stacks.\nC‚Äôest id√©al pour plusieurs comptes, car le m√™me code CDK peut √™tre d√©ploy√© partout et la Custom Resource s‚Äôex√©cute dans le contexte de chaque stack.\nL‚Äôoption A ajoute une √©tape inutile (g√©n√©rer/manipuler un template CloudFormation JSON s√©par√©), donc plus de configuration.\nL‚Äôoption C (Amplify) est fait pour des applis web/mobile et des APIs, pas pour d√©clencher du nettoyage √† chaque d√©ploiement de stack.\nL‚Äôoption D demande de cr√©er/maintenir la Lambda √† la main dans la console, ce qui s‚Äôint√®gre moins bien et augmente la configuration.\nDonc B est la solution la plus ‚Äúnative CDK‚Äù et la plus simple √† int√©grer au processus de d√©ploiement existant.",
      "domainKey": "deployment",
      "frExplanationPedagogique": "**Imagine le lyc√©e : tu as un ‚Äúplan de cours‚Äù central pour organiser toutes les salles (plusieurs b√¢timents = plusieurs comptes). Apr√®s un √©v√©nement, il reste parfois des chaises et des cartons inutiles (ressources inutilis√©es) qu‚Äôil faut rep√©rer et jeter automatiquement.**\n\nLe concept : l‚ÄôAWS CDK, c‚Äôest comme ton plan de cours √©crit en ‚Äúlangage humain‚Äù (du code) qui cr√©e/organise tout. Une ‚Äústack‚Äù, c‚Äôest un pack d‚Äôinstallations (tables, chaises, projecteur) d√©ploy√© dans un b√¢timent. Une ‚ÄúLambda‚Äù, c‚Äôest un √©l√®ve robot qui ex√©cute une t√¢che courte. Un ‚Äúcustom resource CDK‚Äù, c‚Äôest une r√®gle int√©gr√©e au plan : ‚Äú√† chaque installation, appelle le robot pour nettoyer‚Äù.\nPourquoi B : tu √©cris la r√®gle de nettoyage directement dans l‚Äôappli CDK centrale, et le custom resource branche automatiquement le robot (Lambda) et le d√©clenche pendant le d√©ploiement, sans bricolage √† c√¥t√©. A demande de fabriquer un autre plan s√©par√© (template JSON) = plus de config. C ajoute un service d‚Äôapplis web (Amplify) inutile ici. D oblige √† cr√©er le robot √† la main dans une console, donc moins ‚Äúint√©gr√©‚Äù et plus de config.",
      "requiredAnswers": 1
    },
    {
      "id": "dva-c02:topic:1:question:188:5cbd8c7cdd575a14",
      "exam": "aws-certified-developer-associate-dva-c02",
      "topic": 1,
      "questionNumber": 188,
      "stem": "Exam question from\n\nAWS Certified Developer - Associate DVA-C02\n\nA developer is storing sensitive data generated by an application in Amazon S3. The developer wants to encrypt the data at rest. A company policy requires an audit trail of when the AWS Key Management Service (AWS KMS) key was used and by whom.Which encryption option will meet these requirements?",
      "choices": {
        "A": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "B": "Server-side encryption with AWS KMS managed keys (SSE-KMS)",
        "C": "Server-side encryption with customer-provided keys (SSE-C)",
        "D": "Server-side encryption with self-managed keys"
      },
      "answer": "B",
      "suggestedAnswer": "B",
      "sourceUrl": "https://www.examtopics.com/discussions/amazon/view/122610-exam-aws-certified-developer-associate-dva-c02-topic-1/",
      "timestampRaw": "Oct. 6, 2023, 2:46 a.m.",
      "textHash": "5cbd8c7cdd575a14",
      "rawFormat": "discussion-md",
      "conceptKey": "kms_sse_options",
      "frExplanation": "On veut chiffrer des fichiers stock√©s dans Amazon S3 (un service de stockage). ¬´ Chiffrer au repos ¬ª signifie que les donn√©es sont illisibles sur le disque sans la cl√©.\nLa politique exige un audit : savoir quand la cl√© a √©t√© utilis√©e et par qui. C‚Äôest exactement ce que fournit AWS KMS (service de gestion de cl√©s) via des logs (CloudTrail) pour chaque utilisation de cl√©.\nAvec SSE-KMS, S3 chiffre/d√©chiffre automatiquement les objets en appelant une cl√© KMS, et chaque appel est enregistr√© : date/heure, identit√© (utilisateur/role), action.\nSSE-S3 chiffre aussi, mais avec des cl√©s g√©r√©es uniquement par S3 et sans le m√™me niveau de tra√ßabilit√© ¬´ qui a utilis√© la cl√© ¬ª dans KMS.\nSSE-C utilise une cl√© fournie par le client √† chaque requ√™te : pas d‚Äôaudit KMS car KMS n‚Äôest pas utilis√©.\n¬´ Self-managed keys ¬ª implique g√©rer soi-m√™me les cl√©s et l‚Äôaudit, ce qui ne r√©pond pas simplement √† l‚Äôexigence d‚Äôaudit KMS.\nDonc l‚Äôoption qui chiffre au repos ET fournit un audit trail KMS est : SSE-KMS (B).",
      "domainKey": "security",
      "frExplanationPedagogique": "**Imagine un casier au lyc√©e o√π tu ranges des affaires tr√®s priv√©es. Tu veux fermer le casier √† cl√©, et le lyc√©e exige un carnet qui note √† chaque fois qui a utilis√© la cl√© et quand.**\n\nAmazon S3, c‚Äôest le casier o√π tu stockes tes fichiers. Chiffrer ‚Äúau repos‚Äù, c‚Äôest fermer le casier quand personne ne l‚Äôutilise. La ‚Äúcl√©‚Äù de chiffrement, c‚Äôest ce qui permet d‚Äôouvrir/fermer le casier. La r√®gle de l‚Äôentreprise veut un historique (audit) : qui a utilis√© la cl√© et √† quel moment. Avec SSE-KMS (B), la cl√© est g√©r√©e par AWS KMS, comme une cl√© gard√©e au bureau de la vie scolaire avec un registre de signatures : chaque usage est enregistr√©. SSE-S3 (A) ferme aussi le casier, mais sans le registre d√©taill√© ‚Äúqui a utilis√© la cl√©‚Äù. SSE-C (C) et les cl√©s auto-g√©r√©es (D), c‚Äôest comme apporter ta propre cl√© : le lyc√©e ne peut pas tenir le registre officiel. Donc la bonne r√©ponse est B.",
      "requiredAnswers": 1
    }
  ]
}